{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to Qualytics","text":"<p>Qualytics is your Active Data Quality Platform that empowers teams to manage data quality at scale through advanced automation. By analyzing the shapes and patterns in your historical data, Qualytics infers contextual data quality rules that actively monitor new data, including incremental loads, to identify anomalies. When issues arise, Qualytics provides your team with everything needed to take corrective actions using your existing data tools and preferred monitoring solutions.</p>"},{"location":"#managing-data-quality","title":"Managing Data Quality","text":"<p>Qualytics helps your data teams proactively address data issues by automating the discovery and maintenance of essential data quality measures.</p> <p>Here's how it works:</p> <ol> <li> <p>Analyzing Historical Data: Qualytics examines your existing data to understand its patterns and characteristics, creating a comprehensive set of rules that define good data quality.</p> </li> <li> <p>Finding Anomalies: These automatically inferred rules, along with any custom rules you create, work together to identify abnormalities in both historical and new data, even as it arrives incrementally.</p> </li> <li> <p>Taking Corrective Actions: When Qualytics detects an anomaly, it springs into action. Using tags, it can:</p> <ul> <li>Send notifications through your preferred platforms (Teams, Slack, PagerDuty)</li> <li>Trigger workflows in your tools (Airflow, Fivetran, Airbyte)</li> <li>Provide detailed anomaly information to your chosen datastore</li> <li>Suggest optimal solutions through its intuitive interface and API</li> </ul> </li> <li> <p>Continuous Monitoring and Improvement: Qualytics maintains constant vigilance over your data quality, automatically adapting quality checks to reflect changes in your data and business needs. This ongoing process strengthens data quality and builds confidence in your organization's data assets.</p> </li> </ol>"},{"location":"#key-features","title":"Key Features","text":"<p>Qualytics delivers powerful capabilities designed to transform your data quality management:</p> <ol> <li> <p>Automated Data Profiling: Qualytics creates comprehensive profiles of your data assets automatically, providing deep insights that form the foundation of robust data quality management.</p> </li> <li> <p>Rule Inference: Say goodbye to the challenge of manually crafting and maintaining data quality rules at scale. Qualytics automatically infers appropriate rules based on your data profiles, saving time while ensuring precise anomaly detection.</p> </li> <li> <p>Anomaly Detection: Detect data irregularities both at rest and in flight throughout your data ecosystem. Qualytics excels at highlighting outliers and anomalies, helping you maintain high data quality standards.</p> </li> <li> <p>Anomaly Remediation: When issues emerge, Qualytics seamlessly integrates with your preferred tools to enable swift corrective actions through automated workflows.</p> </li> <li> <p>Freshness Monitoring: Keep your data current with built-in monitoring of data freshness Service Level Agreements (SLAs). Define and track timeliness requirements to ensure your data meets critical business needs.</p> </li> <li> <p>Insights Dashboard: Access a clear, intuitive executive dashboard that provides a holistic view of your data health and quality. Visualize key metrics, track progress, and derive actionable insights to drive data-driven strategies.</p> </li> </ol>"},{"location":"#seamless-integration-and-deployment","title":"Seamless Integration and Deployment","text":"<p>Qualytics adapts to your infrastructure with flexible integration options:</p> <ul> <li> <p>Deployment Options: Choose the deployment model that works best for you - on-premise, single-tenant cloud, or SaaS. Qualytics meets you where your data lives.</p> </li> <li> <p>Support for Modern &amp; Legacy Data Stacks: Whether you use modern solutions like Snowflake and Amazon S3 or legacy systems like Oracle and MSSQL, Qualytics seamlessly integrates with your entire data stack to maintain quality across all sources.</p> </li> </ul>"},{"location":"#demo","title":"Demo","text":"<p>Here is a short video demonstrating the platform with a quick walkthrough:</p>"},{"location":"#embarking-on-your-journey","title":"Embarking on Your Journey","text":"<p>This user guide will walk you through Qualytics' key capabilities with clear, step-by-step instructions. Whether you're new to the platform or looking to deepen your expertise, we're here to help you optimize your data quality management journey.</p> <p>Let's begin empowering your organization with accurate, reliable, and trustworthy data using Qualytics!</p>"},{"location":"404/","title":"404 - Page Not Found","text":"<p>Oops! The page you're looking for doesn't exist.</p>"},{"location":"404/#what-happened","title":"What happened?","text":"<p>The page you requested could not be found. This might happen because:</p> <ul> <li>The URL was typed incorrectly</li> <li>The page has been moved or deleted</li> <li>The link you followed is broken</li> </ul>"},{"location":"404/#what-can-you-do","title":"What can you do?","text":"<ul> <li>Go back to the homepage</li> <li>Use the search function to find what you're looking for</li> <li>Check the navigation menu for available sections</li> <li>Contact support if you think this is an error</li> </ul> <p>Need help? Contact our support team</p>"},{"location":"changelog-2023/","title":"2023","text":""},{"location":"changelog-2023/#release-notes","title":"Release Notes","text":""},{"location":"changelog-2023/#2023.12.20","title":"2023.12.20","text":""},{"location":"changelog-2023/#general-fixes","title":"General Fixes","text":"<ul> <li> <p>Resolved Datastore Creation Issue with Databricks:</p> <ul> <li>Fixed an issue encountered when creating source datastores using Databricks with catalog names other than the default <code>hive_metastore</code>. This fix ensures a smoother and more flexible datastore creation process in Databricks environments.</li> </ul> </li> <li> <p>Conflict Resolution for 'anomaly_uuid' Field in Source Container:</p> <ul> <li>Corrected a problem where source containers with a field named <code>anomaly_uuid</code> were unable to run scan operations. This fix eliminates the conflict with internal system columns, allowing for uninterrupted operation of these containers.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2023/#2023.12.14","title":"2023.12.14","text":""},{"location":"changelog-2023/#feature-enhancements","title":"Feature Enhancements","text":"<ul> <li> <p>Auto-Detection of Partitioned Files:</p> <ul> <li>Improved file handling to automatically detect partitioned files like <code>*.delta</code> without the need for an explicit extension. This update resolves the issue of previously unrecognized delta tables.</li> </ul> </li> <li> <p>Anomaly Weight Threshold for Notifications:</p> <ul> <li>Enhanced the notification system to support a minimum anomaly weight threshold for the trigger type \"An anomaly is detected\". Notifications will now be triggered only for anomalies that meet or exceed the defined weight threshold.</li> </ul> </li> <li> <p>Team Assignment in Datastore Forms:</p> <ul> <li>Updated the Datastore Forms to enable users to manage teams. This enhancement provides Admins with the flexibility to assign or adjust teams right at the point of datastore setup, moving away from the default assignment to the Public team.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_1","title":"General Fixes","text":"<ul> <li> <p>Corrected Health Page Duplication:</p> <ul> <li>Addressed an issue on the Health Page where \"Max Executors\" information was being displayed twice. This duplication has been removed for clearer and more accurate reporting.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2023/#2023.12.12","title":"2023.12.12","text":""},{"location":"changelog-2023/#feature-enhancements_1","title":"Feature Enhancements","text":"<ul> <li>Incremental Catalog Results Posting:<ul> <li>Enhanced the catalog operation to post results incrementally for each container catalogued. Previously, results were only available after the entire operation was completed. With this enhancement, results from successfully catalogued containers are now preserved and posted incrementally, ensuring containers identified are not lost even if the operation does not complete successfully.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_2","title":"General Fixes","text":"<ul> <li> <p>Aggregation Comparison Rule Filter:</p> <ul> <li>Resolved an issue where filters were not being applied to the Aggregation Comparison Check, affecting both the reference and target filters.</li> </ul> </li> <li> <p>Case Sensitivity File Extension Support</p> <ul> <li>Addressed a limitation in handling file extensions, ensuring that uppercase formats like .TXT and .CSV are now correctly recognized and processed. This update enhances the system's ability to handle files consistently, irrespective of extension case.</li> </ul> </li> <li> <p>SLA Violation Notification Adjustment:</p> <ul> <li>Modified the SLA violation notifications to trigger only once per violation, preventing a flood of repetitive alerts and improving the overall user experience.</li> </ul> </li> <li> <p>Source record not Available for Max Length Rule</p> <ul> <li>Addressed a bug where the Max Length Rule was not producing source records in cases involving null values. The rule has been updated to correctly handle null values, ensuring accurate anomaly marking and data enrichment.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2023/#2023.12.8","title":"2023.12.8","text":""},{"location":"changelog-2023/#breaking-changes","title":"Breaking Changes","text":"<ul> <li> <p>Renaming of Enrichment Datastore Tables</p> <p>Due to lack of consistency and to avoid conflicts between different categories of Enrichments tables, changes were performed to the table name patterns:</p> <ul> <li>The Enrichment table previously named <code>&lt;enrichment_prefix&gt;_anomalies</code> has been renamed to <code>&lt;enrichment_prefix&gt;_failed_checks</code> due to its content and granularity.</li> <li>The terms <code>remediation</code> and <code>export</code> were added to distinguish Enrichment Remediation and Export tables from others, resulting in:<ul> <li><code>&lt;enrichment_prefix&gt;_remediation_&lt;container_name&gt;</code> for Remediation tables.</li> <li><code>&lt;enrichment_prefix&gt;_export_&lt;asset&gt;</code> for Export tables.</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog-2023/#feature-enhancements_2","title":"Feature Enhancements","text":"<ul> <li>Refactor Notifications Panel:<ul> <li>Introduced a new side panel for Notifications, categorizing alerts by type (Operations, Anomalies, SLA) for improved organization.</li> <li>Added notification tags, receivers, and an action menu enabling users to mute or edit notifications directly from the panel</li> <li>Enhanced UI for better readability and interaction, providing an overall improved user experience.</li> </ul> </li> <li>Add Enrichment Export Anomalies available asset: <ul> <li>Anomalies are now supported as a type of asset for export to an enrichment datastore, enhancing data export capabilities.</li> </ul> </li> <li>Add files count metric to profile operation summary <ul> <li>Displayed file count (number of partitions) in addition to existing file patterns count metric in profile operations for DFS datastores.</li> </ul> </li> <li>Improve Globing Logic:<ul> <li>Optimized support for multiple subgroups when globing files from DFS datastores during profile operations, enhancing efficiency.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_3","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.12.5","title":"2023.12.5","text":""},{"location":"changelog-2023/#feature-enhancements_3","title":"Feature Enhancements","text":"<ul> <li>Navigation Improvements in Explore Profiles Page:<ul> <li>Upgraded the Explore Profiles Page by adding direct link icons for more precise navigation. Users can now use these links on container and field cards/lists for a direct redirection to detailed views.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_4","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.12.1","title":"2023.12.1","text":""},{"location":"changelog-2023/#feature-enhancements_4","title":"Feature Enhancements","text":"<ul> <li> <p>List View Layout Support:</p> <ul> <li>Introduced list view layouts for Datastores, Profiles, Checks, and Anomalies, providing users with an alternative way to display and navigate through their data.</li> </ul> </li> <li> <p>Bulk Acknowledgement Performance:</p> <ul> <li>Improved the performance of bulk acknowledging in-app notifications, streamlining the user experience and enhancing the application's responsiveness.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_5","title":"General Fixes","text":"<ul> <li> <p>Checks and Anomalies Dialog Navigation:</p> <ul> <li>Resolved an issue with arrow key navigation in Checks and Anomalies dialogs where unintended slider movement occurred when using keyboard navigation. This fix ensures that arrow keys will only trigger slider navigation when the dialog is the main focus.</li> </ul> </li> <li> <p>Profiled Container Count Inconsistency</p> <ul> <li>Ensured that containers that fail to load data during profiling are not mistakenly counted as successfully profiled, improving the accuracy of the profiling process.</li> </ul> </li> <li> <p>Histogram Field Selection Update:</p> <ul> <li>Fixed a bug where histograms were not updating correctly when navigating to a new field. Histograms now properly reflect the data of the newly selected field.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2023/#2023.11.28","title":"2023.11.28","text":""},{"location":"changelog-2023/#feature-enhancements_5","title":"Feature Enhancements","text":"<ul> <li> <p>Operations with Tag Selectors:</p> <ul> <li>Users can now configure operations (including schedules) with multiple tags, enabling dynamic profile evaluation based on tags at the operation's trigger time.</li> </ul> </li> <li> <p>Asserted State Filter for Checks:</p> <ul> <li>Introduced a new check list filter, allowing users to filter checks by those that have passed or identified active anomalies.</li> </ul> </li> <li> <p>Bulk Delete for Profiles:</p> <ul> <li>Enhanced the system to allow bulk deletion of multiple profiles, streamlining the management process where previously only individual deletions were possible.</li> </ul> </li> <li> <p>Resizable Columns in Source Records Table:</p> <ul> <li>Columns in the anomaly dialog source records can now be manually resized, improving visibility and preventing content truncation.</li> </ul> </li> <li> <p>Automated Partition Field Setting for BigQuery:</p> <ul> <li>For BigQuery tables constrained by a required partition filter, the profile partition field setting is now automatically populated during the Catalog operation.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_6","title":"General Fixes","text":"<ul> <li> <p>Sharable Link Authentication Flow:</p> <ul> <li>Fixed an issue where direct links did not work if the user was not signed in. Now, users are redirected to the intended page post-authentication.</li> </ul> </li> <li> <p>Clarified Violation Messages for 'isUnique' Check:</p> <ul> <li>Updated the violation message for the 'isUnique' check to describe the anomaly, reducing misinterpretation clearly.</li> </ul> </li> <li> <p>Access Restriction and Loading Fix for Health Page:</p> <ul> <li>Corrected the health page visibility so only admin users can view it, and improved loading behavior for Qualytics services.</li> </ul> </li> <li> <p>Availability of Requested Tables During Operations:</p> <ul> <li>The dialog displaying requested tables/files is now accessible immediately after an operation starts, enhancing transparency for both Profile and Scan operations.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2023/#2023.11.14","title":"2023.11.14","text":""},{"location":"changelog-2023/#feature-enhancements_6","title":"Feature Enhancements","text":"<ul> <li>Qualytics App Color Palette and Design Update:<ul> <li>Implemented a comprehensive design update across the Qualytics App, introducing a new color palette for a refreshed and modern look. This update includes a significant change to the anomalies color, transitioning from red to orange for a more distinct visual cue. Additionally, the font-family has been updated to enhance readability and provide a more cohesive aesthetic experience across the application.</li> </ul> </li> <li>System Health Readout:<ul> <li>A new <code>Health</code> tab has been added to the Admin menu, offering a comprehensive view of each deployment's operational status. This feature encompasses critical details such as the status of app services, current app version, and analytics engine information, enabling better control over system health.</li> </ul> </li> <li>Enhanced Check with Metadata Input:<ul> <li>The Check form now includes a new input field for custom metadata. This enhancement allows users to add key-value pairs for tailored metadata, significantly increasing the flexibility and customization of the Check definition.</li> </ul> </li> <li>Responsiveness Improvement in Cards Layout:<ul> <li>The Cards layout has been refined to improve responsiveness and compactness. This adjustment addresses previous UI inconsistencies and ensures a consistent visual experience across different devices, enhancing overall usability and aesthetic appeal.</li> </ul> </li> <li>Source Record Enrichment for 'isUnique' Checks:<ul> <li>The <code>isUnique</code> check has been enhanced to support source record enrichment. This significant update allows users to view specific records that fail to meet the 'isUnique' condition. This feature adds a layer of transparency and detail to data validation processes, enabling users to easily identify and address data uniqueness issues.</li> </ul> </li> <li>New Enrichment Data:<ul> <li>Scan operations now record operation metadata in a new enrichment table with the suffix <code>scan_operations</code> including an entry for each table/file scanned with the number of records processed and anomalies identified as well as start/stop time and other relevant details. </li> </ul> </li> <li>Insights Enhancement with Check Pass/Fail Metrics:<ul> <li>Insights now features the checks section with new metrics indicating the total number of checks passed and failed. This enhancement also offers a visual representation through a chart, detailing the passed and failed checks over a specified reporting period.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_7","title":"General Fixes","text":"<ul> <li><code>isAddress</code> now supports defining multiple checks against the same field with different required label permutations</li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.11.8","title":"2023.11.8","text":""},{"location":"changelog-2023/#feature-enhancements_7","title":"Feature Enhancements","text":"<ul> <li> <p>Is Address Check:</p> <ul> <li>Introduced a new check for address conformity that ensures the presence of required components such as road, city, and state, enhancing data quality controls for address fields. This check leverages machine learning to support multilingual street address parsing/normalization trained on over 1.2 billion records of data from over 230 countries, in 100+ languages. It achieves 99.45% full-parse accuracy on held-out addresses (i.e. addresses from the training set that were purposefully removed so we could evaluate the parser on addresses it hasn\u2019t seen before).</li> </ul> </li> <li> <p>Revamped Heatmap Flow in Activity Tab:</p> <ul> <li>Improved the user interaction with the heatmap by filtering the operation list upon selecting a date. A new feature has been added to operation details allowing users to view comprehensive information about the profiles scanned, with the ability to drill down to partitions and anomalies.</li> </ul> </li> <li> <p>Link to Schedule in Operation List:</p> <ul> <li>Enhanced the operation list with a new \"Schedule\" column, providing direct links to the schedules triggering the operations, thus improving traceability and scheduling visibility.</li> </ul> </li> <li> <p>Insights Tag Filtering Improvement:</p> <ul> <li>Enhanced the tag filtering capability on the Insights page to now include table/file-level analysis. This ensures a more granular and accurate reflection of data when using tags to filter insights.</li> </ul> </li> <li> <p>Support for Incremental Scanning of Partitioned Files:</p> <ul> <li>Optimized the incremental scanning process by tracking changes at the record level rather than the last modified timestamp of the folder. This enhancement prevents the unnecessary scanning of all records and focuses on newly added data.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_8","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.11.2","title":"2023.11.2","text":""},{"location":"changelog-2023/#feature-enhancements_8","title":"Feature Enhancements","text":"<ul> <li> <p>Auto Selection of All Fields in Check Form:</p> <ul> <li>Improved the user experience in the Check Form by introducing a \"select all\" option for fields. Users can now auto-select all fields when applying rules that expects a multi select input, streamlining the process especially for profiles with a large number of fields.</li> </ul> </li> <li> <p>Enhanced Profile Operations with User-Defined Starting Points for Profiling:</p> <ul> <li>Users can now specify a value for the incremental identifier, to determine the comprehensive set that will be analyzed.</li> <li>Two new options have been added:<ul> <li>Greater Than Time: Targets profiles with incremental timestamp strategies, allowing the inclusion of rows where the incremental field's value surpasses a specified time threshold.</li> <li>Greater Than Batch: Tailored for profiles employing an incremental batch strategy, focusing the analysis on rows where the incremental field\u2019s value is beyond a certain numeric threshold.</li> </ul> </li> </ul> </li> <li> <p>Configurable Enrichment Source Record Limit in Scan Operations:</p> <ul> <li>Users can now configure the <code>enrichment_source_record_limit</code> to dictate the number of anomalous records retained for analysis, adapting to various use case necessities beyond the default sample limit of 10 per anomaly. This improvement allows for a more tailored and comprehensive analysis based on user requirements.</li> </ul> </li> <li> <p>Introduction of Passed Status in Check Card:</p> <ul> <li>A new indicative icon has been added to the Check Card to assure users of a \"passed\" status based on the last scan. This icon will be displayed only when there are no active anomalies.</li> </ul> </li> <li> <p>Inclusion of Last Asserted Time in Check Card:</p> <ul> <li>Enhanced the Check Card by including the last asserted time, offering users more detailed and up-to-date information regarding the checks.</li> </ul> </li> <li> <p>Enhanced Anomaly Search with UUID Support:</p> <ul> <li>Improved the anomaly search functionality by enabling users to search anomalies using the UUID of the anomaly, making the search process more flexible and comprehensive.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_9","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.10.27","title":"2023.10.27","text":""},{"location":"changelog-2023/#feature-enhancements_9","title":"Feature Enhancements","text":"<ul> <li> <p>Check Creation through Field Details Page:</p> <ul> <li>Users can now initiate check creation directly from the Field Details page, streamlining the check creation process and improving usability.</li> </ul> </li> <li> <p>Tree View Enhancements:</p> <ul> <li>Introduced a favorite group feature where favorite datastores are displayed in a specific section, making them quicker and easier to access.</li> <li>Added search functionalities at both Profile and Field levels to improve the navigation experience.</li> <li>Nodes now follow the default sorting of pages, creating consistency across various views.</li> <li>Enhanced the descriptions in tree view nodes for non-catalogued datastores and non-profiled profiles, providing a clearer explanation for the absence of sub-items.</li> </ul> </li> <li> <p>Bulk Actions for Freshness &amp; SLAs:</p> <ul> <li>Users can now perform bulk actions in Freshness &amp; SLAs, enabling or disabling freshness tracking and setting or unsetting SLAs for profiles efficiently.</li> </ul> </li> <li> <p>Archived Check Details Visualization:</p> <ul> <li>Enhanced the anomaly modal to allow users to view the details of archived checks in a read-only mode, improving the visibility and accessibility of archived checks\u2019 information.</li> </ul> </li> <li> <p>User Pictures as Avatars:</p> <ul> <li>User pictures have been incorporated across the application as avatars, enhancing the visual representation in user listings, teams, and anomaly comments.</li> </ul> </li> <li> <p>Slide Navigation in Card Dialogs:</p> <ul> <li>Introduced a slide navigation feature in the Anomalies and Checks dialogs, enhancing user navigation. Users can now effortlessly navigate between items using navigational arrows, eliminating the need to close the dialog to view next or previous items.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_10","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.10.23","title":"2023.10.23","text":""},{"location":"changelog-2023/#feature-enhancements_10","title":"Feature Enhancements","text":"<ul> <li> <p>Enhanced Data Asset Navigation:</p> <ul> <li>Tree View Implementation: Easily navigate through your data assets with our new organized tree view structure</li> <li>Context-Specific Actions: Access settings and actions that matter most depending on your current level of interaction.</li> <li>Simplified User Experience: This update is designed to streamline and simplify your data asset navigation and management.</li> </ul> </li> <li> <p>Aggregation Comparison Check:</p> <ul> <li>New Rule Added: Ensure valid comparisons by checking the legitimacy of operators between two aggregation expressions. </li> <li>Improved Monitoring: Conduct in-depth comparisons, such as verifying if total row counts match across different source assets.</li> </ul> </li> <li> <p>Efficient Synchronization for Schema Changes:</p> <ul> <li>Seamless Integration: Our system now adeptly synchronizes schema changes in source datastores with Qualytics profiles. </li> <li>Avoid Potential Errors: We reduced the risk of creating checks with fields that have been removed or altered in the source datastore.</li> </ul> </li> <li> <p>Clarity in Quality Check Editors:</p> <ul> <li>Distinct Update Sources: Easily identify if an update was made manually by a user or automatically through the API.</li> </ul> </li> <li> <p>Dynamic Quality Score Updates:</p> <ul> <li>Live Anomaly Status Integration: Quality Scores now reflect real-time changes based on anomaly status updates.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_11","title":"General Fixes","text":"<ul> <li>Various bug fixes and system improvements for a smoother experience.</li> </ul>"},{"location":"changelog-2023/#2023.10.13","title":"2023.10.13","text":""},{"location":"changelog-2023/#feature-enhancements_11","title":"Feature Enhancements","text":"<ul> <li> <p>Export Metadata Enhancements:</p> <ul> <li>Added a \"weight\" property to the quality check asset</li> </ul> </li> <li> <p>New AWS Athena Connector:</p> <ul> <li>Introduced support for a new connector, AWS Athena, expanding the options and flexibility for users managing data connections.</li> </ul> </li> <li> <p>Operations List:</p> <ul> <li>Introduced a multi-select filter to the operation list, enabling users to efficiently view operations based on their status such as running, success, failure, and warning, thereby streamlining navigation and issue tracking.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_12","title":"General Fixes","text":"<ul> <li>Logging Adjustments:<ul> <li>Enhanced logging for catalog operations, ensuring that logs are visible and accessible even for catalogs with a warning status, facilitating improved tracking and resolution of issues.</li> </ul> </li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.10.9","title":"2023.10.9","text":""},{"location":"changelog-2023/#feature-enhancements_12","title":"Feature Enhancements","text":"<ul> <li> <p>Check Categorization:</p> <ul> <li>Introduced new check categories on the checks page to streamline UX and prioritize viewing:<ol> <li>Important: Designed around a check's weight value, this category will by default comprise authored checks and inferred checks with active anomalies.</li> <li>Favorite: Featuring all user-favorited checks</li> <li>Metrics: Incorporating all metric checks</li> <li>All: Displaying all checks, whether inferred, authored, or anomalous</li> </ol> </li> <li>The default view is set to \"Important\" (if available) to highlight critical checks and avoid overwhelming users</li> </ul> </li> <li> <p>Anomalies Page Update:</p> <ul> <li>Revamped the Anomalies page with a simplified status filter, adopting a design in alignment with the checks page:<ul> <li>Quick Status Filter: Facilitates an effortless switch between anomaly statuses.</li> <li>The \"Active\" tab is presented as the default, providing immediate visibility into ongoing anomalies.</li> </ul> </li> </ul> </li> <li> <p>Notification Testing:</p> <ul> <li>Enhanced the Notification Form with a \"Test Notification\" button, enabling users to validate notification settings before saving</li> </ul> </li> <li> <p>Metadata Export to Enrichment Stores:</p> <ul> <li>Enabled users to export metadata from their datastore directly into enrichment datastores, with initial options for quality checks and field profiles.</li> <li>Users can specify which profiles to include in the export operation, ensuring relevant data transfer.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_13","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.10.4","title":"2023.10.4","text":""},{"location":"changelog-2023/#feature-enhancements_13","title":"Feature Enhancements","text":"<ul> <li> <p>Anomalies Details User Experience:</p> <ul> <li>Implemented a \"skeleton loading\" feature in the Anomaly Details dialog, enhancing user feedback during data loading.</li> </ul> </li> <li> <p>Enhanced Check Dialog:</p> <ul> <li>Added \"Last Updated\" date to the Check Dialog to provide users with additional insights regarding check modifications.</li> </ul> </li> <li> <p>API Engine Control:</p> <ul> <li>Exposed a new endpoint allowing users to gracefully restart the analytics engine through the API.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_14","title":"General Fixes","text":"<ul> <li>Timezone Handling on MacOS:<ul> <li>Resolved an issue affecting timezone retrieval due to MacOS privacy updates, ensuring accurate timezone handling.</li> </ul> </li> <li>Notifications and Alerts:<ul> <li>Pager Duty Integration: Resolved issues preventing message sending and improved UI for easier configuration.</li> <li>HTTP Action Notification: Fixed Anomaly meta-data serialization issues affecting successful delivery in some circumstances.</li> </ul> </li> <li>Scan Duration Accuracy:<ul> <li>Adjusted scan duration calculations to accurately represent the actual processing time, excluding time between a failed scan and a successful retry.</li> </ul> </li> <li>Spark Partitioning:<ul> <li>Certain datastores may fail to properly coerce types into Spark-compatible partition column values if that column itself contains anomalous values. When this occurs, an attempt will be made to load the data without a partition column and a warning will be generated for the user.</li> </ul> </li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.9.29","title":"2023.9.29","text":""},{"location":"changelog-2023/#feature-enhancements_14","title":"Feature Enhancements","text":"<ul> <li> <p>Operations &amp; Schedules UI Update:</p> <ul> <li>Redesigned the UI for the operations and schedules lists for a more intuitive UX and to provide additional information.<ul> <li>Introduced pagination, filtering, and sorting for the schedules list.</li> <li>Added a \"Next Trigger\" column to the schedules list to inform users of upcoming schedule triggers.</li> </ul> </li> <li>Improved Profile List Modal:<ul> <li>Enhanced the profile list modal accessible from operations and schedules.</li> <li>Users can now search by both ID and profile name.</li> </ul> </li> </ul> </li> <li> <p>Check Navigation Enhancements:</p> <ul> <li>Enhanced navigation between Standard and Metric Cards by introducing direct links that allow users to access metric charts seamlessly from check forms.</li> <li>The checks page navigation state is now reflected in the URL, enhancing UX and enabling precise redirect capabilities.</li> </ul> </li> <li> <p>Computed Table Enhancements:</p> <ul> <li>Upon the creation or update of a computed table, a minimalistic profile operation is now automatically triggered. This basic profile limits sampling to 1,000 and does not infer quality checks.</li> <li>This enhancement streamlines the process when working with computed tables. Users can now directly create checks after computed table creation without manually initiating a profile operation, as the system auto-fetches required field data types.</li> </ul> </li> <li> <p>Analytics Engine Enhancements:</p> <ul> <li>This release replaces our previous consistency model with a more robust one relying upon AMQP brokered durable messaging. The change dramatically improves Qualytics' internal fault tolerance with accompanying performance enhancements for common operations.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_15","title":"General Fixes","text":"<ul> <li>Insights Filter Consistency:<ul> <li>Fixed an inconsistency issue with the datastore filter that was affecting a couple of charts in Insights</li> </ul> </li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.9.21","title":"2023.9.21","text":""},{"location":"changelog-2023/#feature-enhancements_15","title":"Feature Enhancements","text":"<ul> <li> <p>Anomalies Modal Redesign:</p> <ul> <li>Streamlined the presentation of Failed Checks by removing the Anomalous Fields grouping. The new layout focuses on a list of Failed Checks, each tagged with the associated field(s) name, if applicable. This eliminates redundancy and simplifies the UI, making it easier to compare failed checks directly against the highlighted anomalous fields in the Source Record.</li> <li>Added the ability to filter Failed Checks by anomalous fields.</li> <li>Introduced direct links to datastores and profiles for enhanced navigation.</li> <li>Updated the tag input component for better UX.</li> <li>Removed the 'Hide Anomalous' option and replaced it with an 'Only Anomalous' option for more focused analysis.</li> <li>Included a feature to display the number of failed checks a field has across the modal.</li> <li>Implemented a menu allowing users to copy Violation messages easily.</li> </ul> </li> <li> <p>Bulk Operation for Profiles:</p> <ul> <li>Extended the profile selection functionality to allow initiating bulk operations like profiling and scanning directly from the selection interface.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_16","title":"General Fixes","text":"<ul> <li>DFS Incremental Scans:<ul> <li>Addressed an issue that caused incremental scans to fail when no new files were detected on globs. Scans will now proceed without failure or warning in such cases.</li> </ul> </li> <li>Improve performance of the Containers endpoint</li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.9.16","title":"2023.9.16","text":""},{"location":"changelog-2023/#feature-enhancements_16","title":"Feature Enhancements","text":"<ul> <li> <p>Insights Timeframe and Grouping:</p> <ul> <li>Trend tooltips have been refined to change responsively based on the selected timeframe and grouping, ensuring that users receive the most relevant information at a glance.</li> </ul> </li> <li> <p>Enhanced PDF export for Insights:</p> <ul> <li>Incorporated the selected timeframe and grouping settings into the exported PDF, ensuring that users experience consistent detail and clarity both within the application and in the exported document.</li> <li>Added a \"generated at\" timestamp to the PDF exports, providing traceability and context to when the data was captured, further enhancing the comprehensiveness of exported insights.</li> </ul> </li> <li> <p>Source Record Display Improvements:</p> <ul> <li>The internal columns' background color has been calibrated to offer a seamless appearance in both light and dark themes.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_17","title":"General Fixes","text":"<ul> <li> <p>Time Series Chart Rendering:</p> <ul> <li>Addressed an issue where the time series chart would not display data points despite having valid measurements. The core of the problem was pinpointed to how the system handled <code>0</code> values, especially when set as min and/or max thresholds.</li> <li>Resolved inconsistencies in how undefined min/max thresholds were displayed across different comparison types. While we previously had a UI indicator displaying for some comparison types, this was missing for \"Absolute Change\" and \"Absolute Value\".</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2023/#2023.9.14","title":"2023.9.14","text":""},{"location":"changelog-2023/#feature-enhancements_17","title":"Feature Enhancements","text":"<ul> <li> <p>Insights Improvements:</p> <ul> <li>Performance has been significantly optimized for smoother interactions.</li> <li>Introduced timeframe filters, allowing users to view insights data by week, month, quarter, or year.</li> <li>Introduced grouping capabilities, enabling users to segment visualizations within a timeframe, such as by days or weeks.</li> </ul> </li> <li> <p>Metric Checks Enhancements:</p> <ul> <li>Introduced a new Metric Checks tab in both the datastore and explore perspectives.</li> <li>Added a Time Series Chart within the Metric Checks tab:<ul> <li>Displays check measurements over time.</li> <li>Allows on-the-fly adjustments of min/max threshold values.</li> <li>Showcases enhanced check metadata including tags, active anomaly counts, and check weights.</li> </ul> </li> </ul> </li> <li> <p>Check Form Adjustments:</p> <ul> <li>Disabled the <code>Comparison Type</code> input for asserted checks</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_18","title":"General Fixes","text":"<ul> <li>Configuring Metric Checks through the Check Form:<ul> <li>Resolved a bug where users were unable to clear optional inputs such as \"min\" or \"max\".</li> </ul> </li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.9.8","title":"2023.9.8","text":""},{"location":"changelog-2023/#feature-enhancements_18","title":"Feature Enhancements","text":"<ul> <li>Presto &amp; Trino Connectors:<ul> <li>We've enhanced our suite of JDBC connectors by introducing dedicated support for both Presto and Trino. Whether you're utilizing the well-established Presto or the emerging Trino, our platform ensures seamless compatibility to suit your data infrastructure needs.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_19","title":"General Fixes","text":"<ul> <li>Incremental Scan: <ul> <li>Resolved an issue where the scan operation would fail during the \"Exists In Check\" if there were no records to be processed.</li> </ul> </li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.9.7","title":"2023.9.7","text":""},{"location":"changelog-2023/#feature-enhancements_19","title":"Feature Enhancements","text":"<ul> <li> <p>Concurrent Operations:</p> <ul> <li>Introduced the ability to run multiple operations of the same type concurrently within a single datastore, even if one is yet to finish. This brings more flexibility and efficiency in executing operations</li> </ul> </li> <li> <p>Autocomplete Widget:</p> <ul> <li>A hint for a shortcut has been added, allowing users to manually trigger the autocomplete widget and enhancing usability</li> </ul> </li> <li> <p>Source Record Display Enhancements:</p> <ul> <li>Added a new 'Hide Anomalous' option, providing users with the choice to hide anomalous records for clearer viewing</li> <li>Transitioned from hover-based tooltips to click-activated ones for better UX</li> <li>For a consistent data presentation, internal columns will now always be displayed first</li> </ul> </li> <li> <p>Check Form Improvements:</p> <ul> <li>Users now receive feedback directly within the form upon successful validation, replacing the previous toast notification method</li> <li>Additionally, for 504 validation timeouts, a more detailed and context-specific message is provided</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_20","title":"General Fixes","text":"<ul> <li>Addressed issues for 'Is Replica Of' failed checks in source record handling</li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.8.31","title":"2023.8.31","text":""},{"location":"changelog-2023/#general-fixes_21","title":"General Fixes","text":"<ul> <li>Fixed an issue where the Source Record remediation was incorrectly displayed for all fields</li> <li>Adjusted the display of field Quality Scores and Suggestion Scores within the Source Record</li> <li>Fixed a bug in the Check Form where the field input wouldn\u2019t display when cloning a check that hasn\u2019t been part of a scan yet</li> <li>Resolved an issue where failed checks for shape anomalies were not receiving violation messages</li> </ul>"},{"location":"changelog-2023/#2023.8.30","title":"2023.8.30","text":""},{"location":"changelog-2023/#feature-enhancements_20","title":"Feature Enhancements","text":"<ul> <li> <p>Anomaly Dialog Updates:</p> <ul> <li>Optimized Source Data Columns Presentation: To facilitate faster identification of issues, anomalous fields are now presented first. This enhancement will prove particularly useful for data sources with a large number of columns.</li> <li>Enhanced Sorting Capabilities: Users can now sort the source record data by name, weight, and quality score, providing more flexible navigation and ease of use.</li> <li>Field Information at a Glance: A new menu box has been introduced to deliver quick insights about individual fields. Users can now view weight, quality score, and suggested remediation for each field directly from this menu box.</li> </ul> </li> <li> <p>Syntax Highlighting Autocomplete Widget:</p> <ul> <li>Improved UX: The widget has been enhanced to better identify and display hint types, including distinctions between tables, keywords, views, and columns. This enhancement enriches the autocomplete experience.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_22","title":"General Fixes","text":"<ul> <li>Check Dialog Accessibility:<ul> <li>Addressed an issue where the check dialog was not opening as expected when accessed through a direct link from the profile page.</li> </ul> </li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.8.23","title":"2023.8.23","text":""},{"location":"changelog-2023/#feature-enhancements_21","title":"Feature Enhancements","text":"<ul> <li> <p>Profiles Page:</p> <ul> <li>Introduced two new sorting methods to provide users with more intuitive ways to explore their profiles: Sort by last profiled and Sort by last scanned.</li> <li>Updated the default sorting behavior. Profiles will now be ordered by name right from the start, rather than by their creation date.</li> </ul> </li> <li> <p>Add New isNotReplicaOf Check:</p> <ul> <li>With this rule, users can assert that certain datasets are distinct and don't contain matching data, enhancing the precision and reliability of data comparisons and assertions.</li> </ul> </li> <li> <p>Introduce new Metric Check</p> <ul> <li>We've added a new Metric check tailored specifically for handling timeseries data. This new check is set to replace the previous Absolute and Relative Change Checks.</li> <li>To offer a more comprehensive and customizable checking mechanism, the Metric check comes with a comparison input:<ul> <li>Percentage Change: Asserts that the field hasn't deviated by more than a certain percentage (inclusive) since the last scan.</li> <li>Absolute Change: Ensures the field hasn't shifted by more than a predetermined fixed amount (inclusive) from the previous scan.</li> <li>Absolute Value: During each scan, this option records the field value and asserts that it remains within a specified range (inclusive).</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_23","title":"General Fixes","text":"<ul> <li> <p>Schema Validation:</p> <ul> <li>We've resolved an issue where the system was permitting the persistence of empty values under certain conditions for datastores and checks. This fix aims to prevent unintentional data inconsistencies, ensuring data integrity.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2023/#2023.8.18","title":"2023.8.18","text":""},{"location":"changelog-2023/#feature-enhancements_22","title":"Feature Enhancements","text":"<ul> <li> <p>Auditing:</p> <ul> <li>Introduced significant enhancements to the auditing capabilities of the platform, designed to provide better insights and control over changes. The new auditing features empower users to keep track of change sets across all entities, offering transparency and accountability like never before. A new activity endpoint has been introduced, providing a log of user interactions across the application.</li> </ul> </li> <li> <p>Search Enhancements:</p> <ul> <li>Profiles and Anomalies lists can now be searched by both identifiers and descriptions using the same search input.</li> </ul> </li> <li> <p>Catalog Operation Flow Update:</p> <ul> <li>Made a minor update to the datastore creation and catalog flow to enhance user flexibility and experience. Instead of automatically running a catalog operation post datastore creation, users now have a clearer, intuitive manual process. This change offers users the flexibility to set custom catalog configurations, like syncing only tables or views.</li> </ul> </li> <li> <p>Operation Flow Error Handling:</p> <ul> <li>Enhanced user experience during failures in the Operation Flow. Along with the failure message, a \"Try Again\" link has been added. Clicking this link will revert to the configuration state, allowing users to make necessary edits without restarting the entire operation process.</li> </ul> </li> <li> <p>Sorting Enhancements:</p> <ul> <li>Introduced new sorting options: \"Completeness\" and \"Quality Score\". These options are now available on the profiles &amp; fields pages.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_24","title":"General Fixes","text":"<ul> <li> <p>Datastore Connection Edit:</p> <ul> <li>Improved the Datastore connection edit experience, especially for platforms like BigQuery. Resolved an issue where file inputs were previously obligatory for minor edits. For instance, renaming a BigQuery Datastore no longer requires a file input, addressing this past inconvenience.</li> </ul> </li> <li> <p>Pagination issues:</p> <ul> <li>Resolved an issue with paginated endpoints returning 500 instead of 422 on requests with invalid parameters.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#2023.8.11","title":"2023.8.11","text":""},{"location":"changelog-2023/#feature-enhancements_23","title":"Feature Enhancements","text":"<ul> <li>Insights Export: Added a new feature that allows users to export Insights directly to PDF, making it easier to share and review data insights.</li> <li>Check Form UX: <ul> <li>Fields in the Check Form can now be updated if the check hasn't been used in a Scan operation, offering more flexibility to users.</li> <li>Enhanced visual cues in the form with boxed information to clarify the limitations certain properties have, depending on the state of the form.</li> <li>A new icon has been introduced to represent the number of scan operations that have utilized the check, providing users with a clearer overview.</li> </ul> </li> <li>SLA Form UX: <ul> <li>Revamped Date Time handling for enhanced time zone coverage, allowing for user-specified date time configurations based on their preferred time zone.</li> </ul> </li> <li>Filter and Sorting:<ul> <li>Added Datastore Type filter and sorting for source datastores</li> <li>Added Profile Completeness sorting and type filtering and sorting</li> <li>Added Check search by identifier or description</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_25","title":"General Fixes","text":"<ul> <li>SparkSQL Expressions: Added support to field names with special characters to SparkSQL expressions using backticks</li> <li>Pagination Adjustment: The pagination limit has been fine-tuned to support a maximum of 100 items per page, improving readability and navigation.</li> </ul>"},{"location":"changelog-2023/#2023.8.3","title":"2023.8.3","text":""},{"location":"changelog-2023/#maintenance-release","title":"Maintenance Release","text":"<ul> <li>Updated enrichment sidebar details design.</li> <li>Tweaked SQL input dialog sizing.</li> <li>Fixed filter components width bug.</li> <li>Retain the start time of operation on restart.</li> <li>Fixed exclude fields to throw exceptions on errors.</li> <li>Improved performance when using DFS to load reference data.</li> </ul>"},{"location":"changelog-2023/#2023.7.31","title":"2023.7.31","text":""},{"location":"changelog-2023/#maintenance-release_1","title":"Maintenance Release","text":"<ul> <li>Changed UX verbiage and iconography for Anomaly status updates.</li> <li>Fixed intermittent notification template failure.</li> <li>Fixed UI handling of certain rule types where unused properties were required.</li> <li>Improved error messages when containers are no longer accessible.</li> <li>Fixed Hadoop authentication conflicts with ABFS.</li> <li>Fixed an issue where a Profile operation run on an empty container threw a runtime exception.</li> </ul>"},{"location":"changelog-2023/#2023.7.29","title":"2023.7.29","text":""},{"location":"changelog-2023/#feature-enhancements_24","title":"Feature Enhancements","text":"<ul> <li>Added a NotExistsIn Check Type: Introducing a new rule type that asserts that values assigned to this field do not exist as values in another field.</li> <li>Check Authoring UI enhancements: Improved user interface with larger edit surfaces and parenthesis highlighting for better usability.</li> <li>Container Details UI enhancement: Improved presentation of container information in sidebars for easier accessibility and understanding.</li> <li>Added Check Authoring Validation: Users can now perform a dry run of the proposed check against representative data to ensure accuracy and effectiveness.</li> <li>Change in default linkage between Checks and Anomalies: Filters now default to \"Active\" status, providing more refined results and support for specific use cases.</li> </ul>"},{"location":"changelog-2023/#2023.7.25","title":"2023.7.25","text":""},{"location":"changelog-2023/#feature-enhancements_25","title":"Feature Enhancements","text":"<ul> <li>Satisfies Expression Enhancement: The Satisfies Expression feature has been upgraded to automatically bind fields referenced in the user-defined expressions, streamlining integration and improving usability.</li> </ul>"},{"location":"changelog-2023/#added-support","title":"Added Support","text":"<ul> <li>Extended Support for ExistsIn Checks: The ExistsIn checks now offer support for computed tables, empowering users to perform comprehensive data validation on computed data.</li> </ul>"},{"location":"changelog-2023/#general-fixes_26","title":"General Fixes","text":"<ul> <li> <p>Enhanced Check Referencing: Checks can now efficiently reference the full dataframe by using the alias \"qualytics_self,\" simplifying referencing and providing better context within checks.</p> </li> <li> <p>Improved Shape Anomaly Descriptions: Shape anomaly descriptions now include totals alongside percentages, providing more comprehensive insights into data irregularities.</p> </li> <li> <p>Fix for Computed Table Record Calculation: A fix has been implemented to ensure accurate calculation of the total number of records in computed tables, improving data accuracy and reporting.</p> </li> <li> <p>Enhanced Sampling Source Records Anomaly Detection: For shape anomalies, sampling source records now explicitly exclude replacement, leading to more precise anomaly detection and preserving data integrity during analysis.</p> </li> </ul>"},{"location":"changelog-2023/#2023.7.23","title":"2023.7.23","text":""},{"location":"changelog-2023/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fix for total record counts when profiling large tables</li> </ul>"},{"location":"changelog-2023/#2023.7.21","title":"2023.7.21","text":""},{"location":"changelog-2023/#feature-enhancements_26","title":"Feature Enhancements","text":"<ul> <li>Notification Form: Enhanced the user interface and experience by transforming the Channel and Tag inputs into a more friendly format.</li> <li>Checks &amp; Anomalies: Updated the default Sort By criterion to be based on \"Weight\", enabling a more effective overview of checks and anomalies.</li> <li>Profile Details (Side Panel): Introduced a tooltip to display the actual value of the records metric, providing clearer and instant information.</li> <li>Freshness Page: Added a new navigation button that directly leads to the Profile Details page, making navigation more seamless.</li> <li>Profile Details: Introduced a settings option for the user to perform actions identical to those from the Profile Card, such as changing profile settings and configuring Checks and SLAs.</li> <li>SparkSQL Inputs: Implemented a new autocomplete feature to enhance user experience. Writing SQL queries is now more comfortable and less error-prone.</li> </ul>"},{"location":"changelog-2023/#2023.7.19","title":"2023.7.19","text":""},{"location":"changelog-2023/#general-fixes_27","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.7.14","title":"2023.7.14","text":""},{"location":"changelog-2023/#feature-enhancements_27","title":"Feature Enhancements","text":"<ul> <li>API enhancements <ul> <li>Improved performance of our json validation through the adoption of Pydantic 2.0</li> <li>Upgraded our API specification to OpenAPI 3.1.0 compatible, this uses JSON Schema 2020-12.</li> </ul> </li> <li>Upgraded to Spark 3.4<ul> <li>Significant performance enhancements for long-running tasks and shuffles</li> </ul> </li> <li>Added support for Kerberos authentication for Hive datastores</li> <li>Enhanced processing for large dataframes with JDBC sources<ul> <li>Handle arbitrarily large tables and views by chunking into sequentially processed dataframes</li> </ul> </li> <li>Improvements for Insights view when limited data is available</li> <li>Various user experience enhancements</li> </ul>"},{"location":"changelog-2023/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>Date Picker fix for Authored Checks</li> <li>Allow tags with special characters to be edited</li> </ul>"},{"location":"changelog-2023/#2023.7.3","title":"2023.7.3","text":""},{"location":"changelog-2023/#feature-enhancements_28","title":"Feature Enhancements","text":"<ul> <li>Insights Made Default View on Data Explorer<ul> <li>Gain valuable data insights more efficiently with the revamped Insights feature, now set as the default view on the Data Explorer.</li> </ul> </li> <li>Reworked Freshness with Sorting and Grouping<ul> <li>Easily analyze and track data freshness based on specific requirements thanks to the improved Freshness feature, now equipped with sorting and grouping functionalities.</li> </ul> </li> <li>Enhanced Tables/Files Cards Design:<ul> <li>Experience improved data analysis with the updated design of tables/files cards, including added average completeness information and reorganized identifiers.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#added-support_1","title":"Added Support","text":"<ul> <li> <p>Support for Recording Sample Shape Anomalies to Remediation Tables</p> <ul> <li>Address potential data shape issues more effectively as the platform now supports recording a sample of shape anomalies to remediation tables.</li> </ul> </li> <li> <p>New Metrics and Redirect to Anomalies for Profile/Scan Results</p> <ul> <li>Access additional metrics for profile/scan results and easily redirect to anomalies generated by a scan from Activity tab for efficient identification and resolution of data issues.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_28","title":"General Fixes","text":"<ul> <li>Reduced Margin Between Form Input Fields:<ul> <li>Enjoy a more compact and streamlined design with a reduced margin between form input fields for an improved user experience.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>Fixed Pagination Reset Issue During Check Updates<ul> <li>Pagination will no longer reset when checks are updated, providing a smoother user experience, with reset now occurring only during filtering.</li> </ul> </li> <li>Resolved Vertical Misalignment of Check and Anomaly Icons<ul> <li>The issue causing vertical misalignment between Check and Anomaly icons on the Field Profile page has been fixed, resulting in a visually pleasing and intuitive user interface.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#2023.6.24","title":"2023.6.24","text":""},{"location":"changelog-2023/#feature-enhancements_29","title":"Feature Enhancements","text":"<ul> <li>Refactored Partition Reads on JDBC <ul> <li>Refactored partitioned reads on JDBC to improve performance, resulting in faster and more efficient data retrieval.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li> <p>Fixed Inputs on Change Checks</p> <ul> <li>Refined inputs on change checks to differentiate between Absolute and Relative measurements, ensuring precise detection and handling of data modifications based on numeric values (Absolute) and percentage (Relative) variations.</li> </ul> </li> <li> <p>Resolved Enum Type Ordering Bug for Paginated Views</p> <ul> <li>Fixed bug causing inconsistent and incorrect sorting of enum values across all paginated views, ensuring consistent and accurate sorting of enum types.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_29","title":"General Fixes","text":"<ul> <li>Added Success Effect<ul> <li>Added effect when a datastore is configured successfully, enhancing the user experience by providing visual confirmation of a successful configuration process.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#2023.6.20","title":"2023.6.20","text":""},{"location":"changelog-2023/#feature-enhancements_30","title":"Feature Enhancements","text":"<ul> <li> <p>Reworked Tags View</p> <ul> <li>Improved the usability and visual appeal of the tags view. Added new properties like description and weight modifier to provide more detailed information and assign relative importance to tags. The weight value directly correlates with the level of importance, where a higher weight indicates higher significance.</li> </ul> </li> <li> <p>Inherited Tags Support</p> <ul> <li>Implemented support for inherited tags in taggable entities. Now tags can be inherited from parent entities, streamlining the tagging process and ensuring consistency across related items. Inherited Tags will be applied to anomalies AFTER a Scan operation.</li> </ul> </li> <li> <p>Added Total Data Under Management to Insights</p> <ul> <li>Introduced a new metric under Insights that displays the total data under management. This provides users with valuable insights into the overall data volume being managed within the system.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#added-support_2","title":"Added Support","text":"<ul> <li> <p>Bulk Update Support</p> <ul> <li>Introduced bulk update functionality for tables, files, and fields. Users can now efficiently Tag multiple items simultaneously, saving time and reducing repetitive tasks.</li> </ul> </li> <li> <p>Smart Partitioning of BigQuery</p> <ul> <li>Enabled smart partitioning in BigQuery using cluster keys. Optimized data organization within BigQuery for improved query performance and cost savings.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>Fixed Scheduling Operation Issues<ul> <li>Addressed a bug causing scheduling operations to fail with invalid days in crontabs. Users can now rely on accurate scheduling for time-based tasks without encountering errors.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_30","title":"General Fixes","text":"<ul> <li> <p>Improved Backend Performance</p> <ul> <li>Implemented various internal fixes to optimize backend performance. This results in faster response times, smoother operations, and an overall better user experience.</li> </ul> </li> <li> <p>Enhanced Tag Input:</p> <ul> <li>Improved tag input functionality in the Check form dialog. Users can now input tags more efficiently with enhanced suggestions and auto-complete features, streamlining the tagging process.</li> </ul> </li> <li> <p>Enhanced File Input Component</p> <ul> <li>Upgraded the file input component in the Datastore form dialog, providing a more intuitive and user-friendly interface for uploading files. Simplifies attaching files to data entries and improves overall usability.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#2023.6.12","title":"2023.6.12","text":""},{"location":"changelog-2023/#feature-enhancements_31","title":"Feature Enhancements","text":"<ul> <li>Explore is the new centralized view of Activities, Containers (Profiles, Tables, Computed Tables), Checks, Anomalies and Insights across ALL Datastores. This new view allows for filtering by Datastores &amp; Tags, which will persist the filters across all of the submenu tabs. The goal is to help with Critical Data Elements and filter out irrelevant information.</li> <li>Enhanced Navigation Features<ul> <li>The navigation tabs have been refined for increased user-friendliness.</li> <li>Enhanced the Profile View and added a toggle between card and list views.</li> <li><code>Datastores</code> and <code>Enrichment Datastores</code> have been unified, with a tabular view introduced to distinguish between your Source Datastores and Enrichment Datastores.</li> <li><code>Explore</code> has been added to the main navigation, and <code>Insights</code> has been conveniently relocated into the Explore submenu.</li> <li>Renamed <code>Tables/Files</code> to <code>Profiles</code> in the Datastore details page.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#added-support_3","title":"Added Support","text":"<ul> <li> <p>We're thrilled to introduce two new checks, the <code>Absolute Change Limit</code> and the <code>Relative Change Limit</code>, tailored to augment data change monitoring. These checks enable users to set thresholds on their numeric data fields and monitor fluctuations from one scan to the next. If the changes breach the predefined limits, an anomaly is generated. </p> <ul> <li> </li> <li>The <code>Absolute Change Limit</code> check is designed to monitor changes in a field's value by a fixed amount. If the field's value changes by more than the specified limit since the last applicable scan, an anomaly is generated.</li> <li>The <code>Relative Change Limit</code> check works similarly but tracks changes in terms of percentages. If the change in a field's value exceeds the defined percentage limit since the last applicable scan, an anomaly is generated.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_31","title":"General Fixes","text":"<ul> <li>General UI fixes with new navigational tabs</li> <li>Resolved an issue when creating a computed table</li> <li>Incorporated functionality to execute delete operations and their related results.</li> <li>Renamed \"Rerun\" button to \"Retry\" in the operation list</li> </ul>"},{"location":"changelog-2023/#2023.6.2","title":"2023.6.2","text":""},{"location":"changelog-2023/#general-fixes_32","title":"General Fixes","text":"<ul> <li> <p>Added GCS connector with Keyfile support:</p> <ul> <li>The GCS connector now supports Keyfile authentication, allowing users to securely connect to Google Cloud Storage.</li> </ul> </li> <li> <p>Improved BigQuery connector by removing unnecessary inputs:</p> <ul> <li>Enhancements have been made to the BigQuery connector by streamlining the inputs, eliminating any unnecessary fields or options.</li> <li>This results in a more user-friendly and efficient experience.</li> </ul> </li> <li> <p>Renamed satisfiesEquation to satisfiesExpression:</p> <ul> <li>The function \"satisfiesEquation\" has been renamed to \"satisfiesExpression\" to better reflect its functionality.</li> <li>This change makes it easier for users to understand and use the function.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#added-support_4","title":"Added Support","text":"<ul> <li> <p>Added Check Description to Notification rule messages:</p> <ul> <li>Notification rule messages now include the Check Description.</li> <li>This allows users to add additional context and information about the specific rule triggering the notification and passing that information to downstream workflows.</li> </ul> </li> <li> <p>Added API support for tuning operations with a high correlation threshold for profiles and high count rollup threshold for anomalies in scan:</p> <ul> <li>The API now supports tuning operations by allowing users to set a higher correlation threshold for profiles.</li> <li>It also enables users to set a higher count rollup threshold for anomalies in scan.</li> <li>This customization capability helps users fine-tune the behavior of the system according to their specific needs and preferences.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#2023.5.26","title":"2023.5.26","text":""},{"location":"changelog-2023/#usability","title":"Usability","text":"<ul> <li>Improved the navigation in the Activity tab\u2019s side panel for easier and more intuitive browsing including exposing the ability to comment directly into an anomaly</li> <li>Added a redirect to the Activity tab when an operation is initiated for a smoother workflow.</li> </ul>"},{"location":"changelog-2023/#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>Resolved an issue where the date and time were not displaying correctly for the highest value in profiles.</li> <li>Fixed a problem with scheduled operations when the configured timing was corrupted.</li> <li>Addressed an issue where filtered checks were causing unexpected errors outside of the intended dataset.</li> </ul>"},{"location":"changelog-2023/#2023.5.23","title":"2023.5.23","text":""},{"location":"changelog-2023/#feature-enhancements_32","title":"Feature Enhancements","text":"<ul> <li>Scheduled operation editing<ul> <li>Added the ability for users to edit a scheduled operation. This allows users to make changes to the schedule of an operation.</li> </ul> </li> <li>Catalog includes filters<ul> <li>Added catalog include filters to only process tables, views, or both in JDBC datastores. This allows users to control which object types are processed in the datastore.</li> </ul> </li> <li>isReplicaOf check filters<ul> <li>Added filter support to the isReplicaOf check. This allows users to control which tables are checked for replication.</li> </ul> </li> <li>Side panel updates<ul> <li>Updated side panel design and added an enrichment redirect option.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#added-support_5","title":"Added Support","text":"<ul> <li>IBM DB2 datastore<ul> <li>Added support for the IBM DB2 datastore. This allows users to connect to and process data from IBM DB2 databases.</li> </ul> </li> <li>API support for tagging fields<ul> <li>Added API support for tagging fields. This allows users to tag fields in the datastore with custom metadata.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>Freshness attempting to measure views<ul> <li>Fixed an issue with freshness attempting to measure views.</li> </ul> </li> <li>Enrichment to Redshift and string data types<ul> <li>Fixed an issue with enrichment to Redshift and string data types. This issue caused enrichment to fail for tables that contained string data types</li> </ul> </li> </ul>"},{"location":"changelog-2023/#2023.5.10","title":"2023.5.10","text":""},{"location":"changelog-2023/#feature-enhancements_33","title":"Feature Enhancements","text":"<ul> <li> <p>Container Settings</p> <ul> <li>Introducing the ability to Group fields for improved insights and profiling precision.</li> <li>Added functionality to Exclude fields from the container, allowing associated checks to be ignored during operations, leading to reduced processing time and power consumption.</li> <li>We now support identifiers on commuted tables during profiling operations.</li> </ul> </li> <li> <p>Checks</p> <ul> <li>Improved usability by enabling quick cloning of checks within the same datastore.<ul> <li>Users can now easily create a new check with minor edits to tables, fields, descriptions, and tags based on an existing check.</li> </ul> </li> <li>Introducing the ability to write Check Descriptions to the Enrichment store, enabling better organization and management of check-related data downstream. <ul> <li>Note: Updating the Enrichment store data requires a new Scan operation.</li> </ul> </li> <li>Enhanced anomaly management by providing a convenient way to filter and view all anomalies generated by a specific check.<ul> <li>Users can now access the Anomaly warning sign icon within the Check dialog, providing quick access to two options: View Anomalies and Archive Anomalies.</li> </ul> </li> </ul> </li> <li>Usability<ul> <li>Introducing the ability to generate an API token from within the user interface.<ul> <li>This can be done through the Settings &gt; Security section, providing a convenient way to manage API authentication.</li> </ul> </li> <li>Added the ability to search tables/files and apply filters to running operations.<ul> <li>This feature eliminates the need to rely solely on pagination, making it easier to select specific tables/files for operations.</li> </ul> </li> <li>Included API and SparkSQL links in the documentation for easy access to additional resources and reference materials.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#added-support_6","title":"Added Support","text":"<ul> <li>Hive datastore support has been added, allowing seamless integration with Hive data sources.</li> <li>Timescale datastore support has been added, enabling efficient handling of time-series data.</li> <li>Added support for HTTP(S) and SOCKS5 proxies, allowing users to configure proxy settings for data operations.</li> <li>Default encryption for rabbitMQ has been implemented, enhancing security for data transmission.</li> </ul>"},{"location":"changelog-2023/#bug-fixes_7","title":"Bug Fixes","text":"<ul> <li>Resolved a bug related to updating tag names, ensuring that tag name changes are properly applied.</li> <li>Fixed an overflow bug in freshness measurements for data size, resulting in accurate measurements and improved reliability.</li> </ul>"},{"location":"changelog-2023/#general-fixes_33","title":"General Fixes","text":"<ul> <li>Updated default weighting for shape anomalies, enhancing the accuracy of anomaly detection and analysis.</li> <li>Increased datastore connection timeouts, improving stability and resilience when connecting to data sources.</li> <li>Implemented general bug fixes and made various improvements to enhance overall performance and user experience.</li> </ul>"},{"location":"changelog-2023/#2023.4.19","title":"2023.4.19","text":"<p>We're pleased to announce the latest update that includes enhancements to UI for an overall better experience:</p>"},{"location":"changelog-2023/#feature-enhancements_34","title":"Feature Enhancements","text":"<ul> <li>Added Volumetric measurements to Freshness Dashboard:<ul> <li>Gain valuable insights into your data's scale and storage requirements with our new volumetric measurements. SortBy Row Count or Data Size to make informed decisions about your data resources.</li> </ul> </li> <li>Added <code>isReplicaOf</code> check:<ul> <li>The new <code>isReplicaOf</code> check allows you to easily compare data between two different tables or fields, helping you identify and resolve data inconsistencies across your datastores.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#added-support_7","title":"Added Support","text":"<ul> <li>Redesigned Checks and Anomalies listing:<ul> <li>Enjoy a cleaner, more organized layout with more information that makes navigating and managing checks and anomalies even easier.</li> <li></li> </ul> </li> <li>Redesigned Anomaly Details view:<ul> <li>The updated anomaly view provides a more thoughtful and organized layout.</li> <li></li> </ul> </li> <li>Improved Filter components:<ul> <li>With a streamlined layout and organized categories, filtering your data is now more intuitive. Dropdown options are now to the right to allow view of the Clear and Apply buttons</li> </ul> </li> <li>Updated Importance score to Weight &amp; added SortBy support:<ul> <li>Manage checks and anomalies more effectively with our updated \u2018Weight' feature (formerly \u2018Importance Score') and the new SortBy support function, allowing you to quickly identify high-priority issues.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_34","title":"General Fixes","text":"<ul> <li>General Fixes and Performance Improvements</li> </ul>"},{"location":"changelog-2023/#2023.4.7","title":"2023.4.7","text":""},{"location":"changelog-2023/#feature-enhancements_35","title":"Feature Enhancements","text":"<ul> <li>We've just deployed an MVP version of the Freshness Dashboard! This feature lets you create, manage, and monitor all of the SLAs for each of your datastores and their child files/tables/containers, all in one place. It's like having a birds-eye view of how your datastores are doing in relation to their freshness.<ul> <li>To access the Freshness Dashboard, just locate and click on the clock icon in the top navigation between Insights and Anomalies. By default, you'll see a rollup of all the datastores in a list view with their child files/tables/containers collapsed. Simply click on a datastore row to expand the list.</li> </ul> </li> <li>We've also made some improvements to the UI, including more sorting and filtering options in Datastores, Files/Tables, Checks, and Anomalies. Plus, we've added the ability to search the description field in checks, making it easier to find what you're looking for.</li> <li>Last but not least, we've added a cool new feature to checks - the ability to archive ALL anomalies generated by a check. Simply click on the anomaly warning icon at the top of the check details box to bring up the archive anomalies dialog box.</li> </ul>"},{"location":"changelog-2024/","title":"2024","text":""},{"location":"changelog-2024/#release-notes","title":"Release Notes","text":""},{"location":"changelog-2024/#2024.12.23","title":"2024.12.23","text":""},{"location":"changelog-2024/#feature-enhancements","title":"Feature Enhancements","text":"<ul> <li> <p>User and Teams Permissions</p> <ul> <li>We are excited to introduce an enhancement to User and Team Permissions.<ul> <li>Users can now have <code>Admin</code>, <code>Manager</code>, or <code>Member</code> roles.<ul> <li>The Manager role provides a subset of Admin permissions for global assets or settings but does not include the \"Admin exemption to team roles.\"</li> </ul> </li> <li>Teams can have specific permissions: <code>Editor</code>, <code>Author</code>, <code>Drafter</code>, <code>Viewer</code>, and <code>Reporter</code>.<ul> <li>Each permission type includes restricted capabilities tailored to its role.</li> </ul> </li> <li>Admins can now create special tokens that grant access exclusively to SCIM endpoints. These tokens allow customers to enable SCIM integrations with minimal access, ensuring the holder cannot access other endpoints or log in to the platform.</li> </ul> </li> </ul> </li> <li> <p>Improve Visibility of Datastore Teams</p> <ul> <li>Users can now view respective teams in the tree view footer. Depending on privileges, they can manage this field.</li> <li>Teams are also visible in the table and field context for improved collaboration and data transparency.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes","title":"General Fixes","text":"<ul> <li> <p>Completeness Rounding</p> <ul> <li>Added two more decimal places to the Completeness metric in the Overview tab.<ul> <li>Previously, percentages were being rounded up incorrectly.</li> </ul> </li> </ul> </li> <li> <p>\"Is Replica Of\" Check Validation</p> <ul> <li>Fixed a bug that occurred when users attempted to validate this check using the same container.</li> </ul> </li> <li> <p>Global Search</p> <ul> <li>Fixed the label to better distinguish between Enrichment Datastores and Source Datastores.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2024/#2024.12.11","title":"2024.12.11","text":""},{"location":"changelog-2024/#feature-enhancements_1","title":"Feature Enhancements","text":"<ul> <li>Add <code>Max Parallelization</code> Field on Datastore Connection<ul> <li>Users can now configure the maximum parallelization level for certain datastores, providing greater control over operation performance.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_1","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2024/#2024.11.29","title":"2024.11.29","text":""},{"location":"changelog-2024/#feature-enhancements_2","title":"Feature Enhancements","text":"<ul> <li>Activity List<ul> <li>Removed the <code>Warning</code> status for a cleaner and more concise status display.</li> <li>Added an alert icon to indicate if an operation completed with warnings, improving visibility into operation outcomes.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_2","title":"General Fixes","text":"<ul> <li>Better handling of Oracle Date and Numeric columns during Catalog operations for improved partition field selection.</li> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2024/#2024.11.21","title":"2024.11.21","text":""},{"location":"changelog-2024/#feature-enhancements_3","title":"Feature Enhancements","text":"<ul> <li>Improved Operations Container Dialogs<ul> <li>Added container status details based on profile and scan results, providing better visibility of container-level operations.</li> <li>Introduced a loading tracker component for containers, enhancing feedback during operation processing.</li> <li>Made the entire modal reactive to operation updates, enabling real-time tracking of operation progress within the modal.</li> <li>Removed \"containers requested\" and \"containers analyzed\" dialogs for a cleaner interface.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_3","title":"General Fixes","text":"<ul> <li> <p>Resolved an issue where the table name was not rendering correctly in notifications when using the <code>{{ customer_name }}</code> variable.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2024/#2024.11.12","title":"2024.11.12","text":""},{"location":"changelog-2024/#feature-enhancements_4","title":"Feature Enhancements","text":"<ul> <li> <p>Enhance Data Catalog Integration</p> <ul> <li>Introduced a new domain input field that allows users to select specific domains, enabling more granular control over assets synchronization.</li> </ul> </li> <li> <p>Scan Results Enhancements</p> <ul> <li>Added partition label to the scan results modal for improved partition identification.</li> <li>Removed unnecessary metadata partitions created solely for volumetric checks, reducing clutter in scan results.</li> </ul> </li> <li> <p>Activity Tab</p> <ul> <li>Display of Unprocessed Containers in the Operation List<ul> <li>Unprocessed containers are now visible in the operation list within the operation summary.</li> <li>A total count label was added to indicate if the number of analyzed containers exceeds the total requested.</li> <li>The search icon now highlights in a different color if not all containers were analyzed, making it easier to identify incomplete operations.</li> </ul> </li> <li>Reorder the Datastore Column in the Activity Tab<ul> <li>Users can now reorder columns in the Activity tab for easier navigation and data organization.</li> </ul> </li> <li>Profile Operations<ul> <li>Users can now view added, updated, and total inferred checks within Profile operations.</li> </ul> </li> <li>Triggered by Column<ul> <li>Updated the term \"Triggered by API\" to \"Triggered by System\" for clarity.</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_4","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2024/#2024.11.1","title":"2024.11.1","text":""},{"location":"changelog-2024/#feature-enhancements_5","title":"Feature Enhancements","text":"<ul> <li> <p>Observability Enhancements</p> <ul> <li>An observability heatmap was added to the volumetric card in the Observability tab.<ul> <li>The heatmap allows users to monitor volumetric status and check for new anomalies.</li> </ul> </li> <li>Improved observability chart for clearer insights.<ul> <li>Users can now view the count of volumetric anomalies produced over time, along with the last recorded measurements for each period.</li> <li>Introduced new color indicators to help distinguish volumetric measures outside thresholds that didn\u2019t produce anomalies from those that did.</li> </ul> </li> </ul> </li> <li> <p>Editable Tags in Field Details</p> <ul> <li>Users with write permissions can now manage tags directly in the Field Details within the Explore context.</li> </ul> </li> <li> <p>Distinct Count Rule Update</p> <ul> <li>The Distinct Count rule now excludes the Coverage field for more accurate assessments.</li> </ul> </li> <li> <p>Support for Pasting into Expected Values</p> <ul> <li>Users can now paste values from spreadsheets directly into Expected Values, saving time on data entry.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_5","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2024/#2024.10.23","title":"2024.10.23","text":""},{"location":"changelog-2024/#feature-enhancements_6","title":"Feature Enhancements","text":"<ul> <li> <p>Dremio Connector</p> <ul> <li>We\u2019ve expanded our connectivity options by supporting a new connection with Dremio.</li> </ul> </li> <li> <p>Full View of Abbreviated Metrics in Operation Summary</p> <ul> <li>Users can now hover over abbreviated metrics to see the full value for better clarity.</li> </ul> </li> <li> <p>Redirect to Conflicting Check</p> <ul> <li>Added a redirect link to the conflicting check from the error message, improving navigation when addressing errors.</li> </ul> </li> <li> <p>Enhanced Visibility and Engagement for Tags and Notifications Setup</p> <ul> <li>Introduced a Call to Action to encourage users to manage Tags and Notifications for better engagement.</li> </ul> </li> <li> <p>Favorite Containers</p> <ul> <li>Users can now favorite individual containers.</li> <li>The option to favorite datastores and containers is now available in both card and list views.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_6","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2024/#2024.10.16","title":"2024.10.16","text":""},{"location":"changelog-2024/#feature-enhancements_7","title":"Feature Enhancements","text":"<ul> <li> <p>Improved Anomaly Modal</p> <ul> <li>Introduced an information icon in each failed check to display the check's description.</li> <li>Anomaly links now persist filters for sort order and displayed fields.</li> <li>Added integration details to fields in a source record.</li> </ul> </li> <li> <p>Secrets Management</p> <ul> <li>Added support for Secrets Manager in connection properties, enabling integration with Vault and other secrets management systems.</li> </ul> </li> <li> <p>Alation Data Dictionary</p> <ul> <li>Enhanced the dictionary to display friendly names in anomaly screens for improved usability.</li> <li>Added integration information to the datastore, container, and fields in the tree view footer.</li> </ul> </li> <li> <p>Tag Category</p> <ul> <li>Introduced support for tag categories to improve tag management, with sorting and filtering options based on the category field.</li> </ul> </li> <li> <p>Call to Action for Volumetric Measurements</p> <ul> <li>A call to action was added in the overview tab within the container context, and the observability page per container was added to enable volumetric measurements.</li> </ul> </li> <li> <p>Error Display for Check Operations</p> <ul> <li>Bulk operations like Edit, Activate, Update, and Template Edit now display error messages clearly when validation fails.</li> </ul> </li> <li> <p>Check Validation</p> <ul> <li>Improved check validation logic to enhance bulk check validation speed and prevent timeouts.</li> </ul> </li> <li> <p>Tag Filtering for Fields</p> <ul> <li>Users can now filter fields by tags in the field list under the datastore context.</li> </ul> </li> <li> <p>Field Remarks in Native Field Properties</p> <ul> <li>Added support for displaying field remarks alongside other native field properties.</li> </ul> </li> <li> <p>Customer Support Link</p> <ul> <li>Users can now access the Qualytics Helpdesk via the Discover menu in the main header.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_7","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2024/#2024.10.4","title":"2024.10.4","text":""},{"location":"changelog-2024/#feature-enhancements_8","title":"Feature Enhancements","text":"<ul> <li> <p>Insights Page Redesign</p> <ul> <li>Introduced a new Overview card displaying key metrics such as <code>Data Under Management</code>, <code>Source Datastores</code>, and <code>Containers</code>.</li> <li>Added a doughnut chart visualization for checks and anomalies, providing a clearer view of data health.</li> <li>Expanded available metrics to include profile runs and scan runs.</li> <li>Users can now easily navigate to Checks and Anomalies based on their current states and statuses.</li> <li>Implemented data volume visualizations to give users better insight into data trends.</li> <li>Introduced a legend option that allows users to compare specific metrics against the primary one.</li> <li>Enhanced the check distribution visualization across the platform within the overview tabs.</li> </ul> </li> <li> <p>Check Filter</p> <ul> <li>Now users can filter <code>Not Asserted</code> checks.</li> </ul> </li> <li> <p>Team Management</p> <ul> <li>Now admin users can modify the <code>Read</code> and <code>Write</code> permissions of the <code>Public</code> Team.</li> </ul> </li> <li> <p>Reapplying Clone Field</p> <ul> <li>Check cloning functionality by attempting to reapply the field from the original (source) check when a new container is selected. If the selected container matches the field and type from the original check, the cloned field will be reapplied automatically.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_8","title":"General Fixes","text":"<ul> <li> <p>Allow saving checks with attached templates as drafts</p> <ul> <li>Adjusted the behavior to allow checks attached to a template to be saved as drafts. The <code>Save as draft</code> feature now remains functional when a template is attached.</li> </ul> </li> <li> <p>Incremental identifier strange behavior</p> <ul> <li>When a user tries to modify a query in a computed table, the Incremental Modifier is set to null.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.9.25","title":"2024.9.25","text":""},{"location":"changelog-2024/#feature-enhancements_9","title":"Feature Enhancements","text":"<ul> <li> <p>Observability</p> <ul> <li>Time-series charts are presented to monitor data volume and related anomalies for each data asset.<ul> <li>Custom thresholds were added to adjust minimum and maximum volume expectations.</li> </ul> </li> <li>The Metrics tab has been moved to the Observability tab.</li> <li>The Observability tab has replaced the Freshness page.</li> </ul> </li> <li> <p>Check Category Options for Scan Operations</p> <ul> <li>Users can select one or multiple check categories when running a scan operation.</li> </ul> </li> <li> <p>Anomaly Trigger Rule Type Filter</p> <ul> <li>Added a filter by check rule types to anomaly triggers. A help component was added to the tags selector to improve clarity.</li> </ul> </li> <li> <p>Auto-Archive Anomalies</p> <ul> <li>A new Duplicate status has been introduced for anomalies.</li> <li>Users can now use Incremental Identifier ranges to auto-archive anomalies with the new Duplicate status.</li> <li>An option has been added to scan operations to automatically archive anomalies identified as duplicates if the containers analyzed have incremental identifiers configured.</li> </ul> </li> <li> <p>A dedicated tab for filtering duplicate anomalies has been added for better visibility.</p> </li> <li> <p>Tree View and Breadcrumb Context Menu</p> <ul> <li>A context menu has been added, allowing users to copy essential information and open links in new tabs.</li> <li>Users can access the context menu by right-clicking on the assets.</li> </ul> </li> <li> <p>Incremental Identifier Support</p> <ul> <li>Users can manage incremental identifiers for computed tables and computed files.</li> </ul> </li> <li> <p>Native Field Properties</p> <ul> <li>Users can now see native field properties in the field profile, displayed through an info icon next to the Type Inferred section.</li> </ul> </li> <li> <p>Qualytics CLI Update</p> <ul> <li>Users can now import check templates.</li> <li>A status filter has been added to check exports. Users can filter by <code>Active</code>, <code>Draft</code>, or <code>Archived</code> (which will include <code>Invalid</code> and <code>Discarded</code> statuses).</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_9","title":"General Fixes","text":"<ul> <li>The Oracle connector now handles invalid schemas when creating connections.</li> <li>Anomalies identified in scan operations were not counting archived statuses.</li> <li>Improved error message when a user creates a schedule name longer than 50 characters.</li> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2024/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Freshness and SLA references have been removed from user notifications and notification rules, users should migrate to Observability using volumetric checks.</li> </ul>"},{"location":"changelog-2024/#2024.9.14","title":"2024.9.14","text":""},{"location":"changelog-2024/#feature-enhancements_10","title":"Feature Enhancements","text":"<ul> <li> <p>Volumetric Measurement</p> <ul> <li>We are excited to introduce support for volumetric measurements of views, computed tables and computed files.</li> </ul> </li> <li> <p>Enhanced Source Record CSV Download</p> <ul> <li>Users can now download all source records as CSV that have been written to the enrichment datastores.</li> </ul> </li> <li> <p>Tags and Notifications Moved to Left-Side Navigation</p> <ul> <li>Users can now quickly switch between Tags, Notifications, and Data Assets through the left-side navigation.</li> <li>Access to the Settings page is restricted to admin users.</li> </ul> </li> <li> <p>Last Asserted Information in Checks</p> <ul> <li>The <code>Created Date</code> information has been replaced with <code>Last Asserted</code> to improve visibility.</li> <li>Users can hover over an info icon to view the <code>Created Date</code>.</li> </ul> </li> <li> <p>Auto-Generated Description in Check Template Dialog</p> </li> <li> <p>Descriptions are now automatically generated in the Template Dialog based on the rule type, ensuring consistency with the check form.</p> </li> <li> <p>Exposed Properties in Profile and Scan Operations</p> <ul> <li>Profile and scan operations now expose properties when listed:<ul> <li>Record Limit</li> <li>Infer As Draft</li> <li>Starting Threshold</li> <li>Enrichment Record Limit</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_10","title":"General Fixes","text":"<ul> <li>Fixed a bug where the container list would not update when a user created a computed container.</li> <li>Fixed an issue where deactivated users were not filtered on the Settings page under the Security tab.</li> <li>Improved error messages when operations fail.</li> <li>Fixed a bug where the <code>Last Editor</code> field was empty after a user was deactivated by an admin.</li> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2024/#2024.9.10","title":"2024.9.10","text":""},{"location":"changelog-2024/#feature-enhancements_11","title":"Feature Enhancements","text":"<ul> <li> <p>Add Source Datastore Modal</p> <ul> <li>Enhanced text messages and labels for better clarity and user experience.</li> </ul> </li> <li> <p>Add Datastore</p> <ul> <li>Users can now add a datastore directly from the Settings page under the Connections tab, simplifying connection management.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_11","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2024/#2024.9.6","title":"2024.9.6","text":""},{"location":"changelog-2024/#feature-enhancements_12","title":"Feature Enhancements","text":"<ul> <li>Introducing Bulk Activation on Draft Checks<ul> <li>Users can now activate and validate multiple draft checks at once, streamlining the workflow and reducing manual effort.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_12","title":"General Fixes","text":"<ul> <li> <p>Improved error message for BigQuery temporary dataset configuration exceptions.</p> </li> <li> <p>Added a retry operation for Snowflake when no active warehouse is selected in the current session.</p> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#breaking-changes_1","title":"Breaking Changes","text":"<ul> <li>API fields (<code>type</code> and <code>container_type</code>) are now mandatory in request payloads where they were previously optional.<ul> <li>POST /global-tags: <code>type</code> is now required.</li> <li>PUT /global-tags/{name}: <code>type</code> is now required.</li> <li>POST /containers: <code>container_type</code> is now required.</li> <li>PUT /containers/{id}: <code>container_type</code> is now required.</li> <li>POST /operations/schedule: <code>type</code> is now required.</li> <li>PUT /operations/schedule/{id}: <code>type</code> is now required.</li> <li>POST /operations/run: <code>type</code> is now required.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#2024.9.3","title":"2024.9.3","text":""},{"location":"changelog-2024/#feature-enhancements_13","title":"Feature Enhancements","text":"<ul> <li>Introducing Catalog Scheduling<ul> <li>Users can now schedule a Catalog operation like Profile and Scan Operations, allowing automated metadata extraction.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_13","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2024/#2024.8.31","title":"2024.8.31","text":""},{"location":"changelog-2024/#feature-enhancements_14","title":"Feature Enhancements","text":"<ul> <li> <p>New Draft Status for Checks</p> <ul> <li>Introduced a new 'draft' status for checks to enhance lifecycle management, allowing checks to be prepared and reviewed without impacting scan operations.</li> <li>Validation is only applied to active checks, ensuring draft checks remain flexible for adjustments without triggering automatic validations.</li> </ul> </li> <li> <p>Introduce Draft Check Inference in Profile Operations</p> <ul> <li>Added a new option to infer checks as drafts, offering more flexibility during data profiling.</li> </ul> </li> <li> <p>Improve Archive Capabilities for Checks and Anomalies</p> <ul> <li>Enhanced the archive capabilities for both checks and anomalies, allowing recovery of archived items.</li> <li>Introduced a hard delete option that allows permanent removal of archived items, providing greater control over their management.</li> <li>The Anomaly statuses 'Resolved' and 'Invalid' are now treated as archived states, aligning with the consistent approach used for checks.</li> </ul> </li> <li> <p>Introduce a new Volumetric Check</p> <ul> <li>Introduced the Volumetric Check to monitor and maintain data volume stability within a specified range. This check ensures that the volume of data assets does not fluctuate beyond acceptable limits based on a moving daily average.</li> <li>Automatically inferred and maintained by the system for daily, weekly, and monthly averages, enabling proactive management of data volume trends.</li> </ul> </li> <li> <p>Incremental Identifier Warning in Scan Dialog</p> <ul> <li>Enhanced the dialog to notify users when they attempt an incremental scan on containers lacking an incremental identifier, ensuring transparency and preventing unexpected full scans.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_14","title":"General Fixes","text":"<ul> <li> <p>Improve enrichment writes with queuing all writes (up to a queue threshold) for the entire scan operation. This will dramatically reduce the number of write operations performed.</p> </li> <li> <p>Explicit casting to avoid weak CSV parser support for typing.</p> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.8.19","title":"2024.8.19","text":""},{"location":"changelog-2024/#feature-enhancements_15","title":"Feature Enhancements","text":"<ul> <li> <p>Enhance Auto-Refresh Mechanism on Tree View</p> <ul> <li>The datastore and container tree footers are now automatically refreshed after specific actions, eliminating the need for manual page refreshes.</li> </ul> </li> <li> <p>Support Oracle Client-Side Encryption</p> <ul> <li>Connections with Oracle now feature end-to-end encryption. Database connection encryption adds an extra layer of protection, especially for transmissions over long-distance, insecure channels.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_15","title":"General Fixes","text":"<ul> <li> <p>UI Label on Explore Page</p> <ul> <li>Fixed an issue where the labels on the Explore page did not change based on the selected time frame.</li> </ul> </li> <li> <p>Inferred Field Type Enhancements</p> <ul> <li>Behavior updated to infer field types at data load time rather than implicitly cast them to latest profiled type. This change supports more consistent expected schema verification for delimited file types and resolves issues when comparing inferred fields to non-inferred fields in some rule types.</li> </ul> </li> <li> <p>Boolean Type Inference</p> <ul> <li>Behavior updated to align boolean inference with Spark Catalyst so that profiled types are more robustly handled during Spark based comparisons</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.8.10","title":"2024.8.10","text":""},{"location":"changelog-2024/#feature-enhancements_16","title":"Feature Enhancements","text":"<ul> <li>Introducing Profile Inference Threshold<ul> <li>This feature allows users to adjust which check types will be automatically created and updated during data profiling, enabling them to manage data quality expectations based on the complexity of inferred data quality rules.</li> </ul> </li> <li>Anomaly Source Records Retrieval Retry Option<ul> <li>Enabled users to manually retry fetching anomaly source records when the initial request fails.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_16","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2024/#2024.7.31","title":"2024.7.31","text":""},{"location":"changelog-2024/#feature-enhancements_17","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing Field Count to the Datastore Overview</p> <ul> <li>This enhancement allows users to easily view the total number of fields present in a datastore across all containers.</li> </ul> </li> <li> <p>Search Template</p> <ul> <li>Added a check filter to the templates page.</li> <li>Added a template filter to the checks page in the datastore context and explore.</li> </ul> </li> <li> <p>Driver Free Memory</p> <ul> <li>Added driver free memory information on the Health Page.</li> </ul> </li> <li> <p>Anomalous Record Count to the Anomaly Sidebar Card</p> <ul> <li>Added the anomalous record count information to the anomaly sidebar card located under the Scan Results dialog.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_17","title":"General Fixes","text":"<ul> <li> <p>Enhanced write performance on scan operations with enrichment and relaxed hard timeouts.</p> </li> <li> <p>Updated Azure Blob Storage connector to use TLS encrypted access by default.</p> </li> <li> <p>Overview Tab is not refreshing asset details automatically.</p> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.7.26","title":"2024.7.26","text":""},{"location":"changelog-2024/#feature-enhancements_18","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing Event Bus for Extended Auto-Sync with Data Catalog Integrations</p> <ul> <li>We are excited to expand our auto-sync capabilities with data catalog integrations by implementing an event bus pattern.</li> <li>Added functionality to delete any DQ values that do not meet important checks.</li> <li>Included support for a WARNING status in the Alation Data Health tab for checks that have not been asserted yet.</li> </ul> </li> <li> <p>Add Autocomplete to the Notification Form</p> <ul> <li>Improved the notification message form by implementing autocomplete. Users can now easily include internal variables when crafting custom messages, streamlining the message creation process.</li> </ul> </li> <li> <p>Redesign the Analytics Engine Functions</p> <ul> <li>The functions are now accessible through a menu, which displays the icon and full functionality.</li> <li>Added a modal to alert users before proceeding with the restart. The modal informs users that the system will be unavailable for a period during the restart process.</li> </ul> </li> <li> <p>Improve Qualytics metadata presentation in Alation</p> <ul> <li>Previously, multiple custom fields were used to persist data quality metrics measured by Qualytics. This process has been simplified by consolidating the metrics into a single rich text custom field formatted in HTML, making it easier for users to analyze the data.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_18","title":"General Fixes","text":"<ul> <li> <p>Normalize Enrichment Internal Containers</p> <ul> <li>To improve user recognition and differentiate between our internal tables and those in source systems, we now preserve the original case of table names.</li> </ul> </li> <li> <p>Validation Error on Field Search Result</p> <ul> <li>Resolved the logic for cascade deletion of dependencies on containers that have been soft deleted, ensuring proper handling of related data.</li> </ul> </li> <li> <p>Members Cannot Add Datastore on the Onboarding Screen</p> <ul> <li>Updated permissions so that members can no longer add Datastores during the onboarding process. Only Admins now have this capability.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.7.19","title":"2024.7.19","text":""},{"location":"changelog-2024/#feature-enhancements_19","title":"Feature Enhancements","text":"<ul> <li>Global Search<ul> <li>We are thrilled to introduce the \u201cGlobal Search\u201d feature into Qualytics! This enhancement is designed to streamline the search across the most crucial assets: Datastores, Containers, and Fields. It provides quick and precise search results, significantly improving navigation and user interaction.</li> <li>Navigation Update: To integrate the new global search bar seamlessly, we have relocated the main menu icons to the left side of the interface. This adjustment ensures a smoother user experience.</li> </ul> </li> <li>Teradata Connector<ul> <li>We\u2019ve expanded our connectivity options by supporting a new connection with Teradata. This enhancement allows users to connect and interact with Teradata databases directly from Qualytics, facilitating more diverse data management capabilities.</li> </ul> </li> <li>Snowflake Key-pair Authentication<ul> <li>In our ongoing efforts to enhance security, we have implemented support for Snowflake Key-pair authentication. This new feature provides an additional layer of security for our users accessing Snowflake, ensuring that data transactions are safe and reliable.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_19","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2024/#2024.7.15","title":"2024.7.15","text":""},{"location":"changelog-2024/#feature-enhancements_20","title":"Feature Enhancements","text":"<ul> <li> <p>Alation Data Catalog Integration</p> <ul> <li>We're excited to introduce integration with Alation, enabling users to synchronize and manage assets across both Qualytics and Alation.</li> <li>Metadata Customization:<ul> <li>Trust Check Flags: We now support warning flags at both the container and field levels, ensuring users are aware of deprecated items.</li> <li>Data Health: Qualytics now pushes important checks to Alation's Data Health tab, providing a comprehensive view of data health at the container level.</li> <li>Custom Fields: Quality scores and related metadata are pushed under a new section in the Overview page of Alation. This includes quality scores, quality score factors, URLs, anomaly counts, and check counts.</li> </ul> </li> </ul> </li> <li> <p>Support for Never Expiration Option for Tokens</p> <ul> <li>Users now have the option to create tokens that never expire, providing more flexibility and control over token management.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_20","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2024/#2024.7.5","title":"2024.7.5","text":""},{"location":"changelog-2024/#feature-enhancements_21","title":"Feature Enhancements","text":"<ul> <li>Enhanced Operations Listing Performance<ul> <li>Optimized the performance of operations listings and streamlined the display of container-related information dialogs. These enhancements include improved handling of operations responses and the addition of pagination for enhanced usability</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_21","title":"General Fixes","text":"<ul> <li> <p>Fix Computed Field Icon Visibility</p> <ul> <li>Resolved an issue where the computed field icon was not being displayed in the table header.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.6.29","title":"2024.6.29","text":""},{"location":"changelog-2024/#feature-enhancements_22","title":"Feature Enhancements","text":"<ul> <li> <p>Computed Field Support</p> <ul> <li>Introduced computed fields allowing users to dynamically create new virtual fields within a container by applying transformations to existing data.</li> <li>Computed fields offer three transformation options to cater to various data manipulation needs. Each transformation type is designed to address specific data characteristics:<ul> <li>Cleaned Entity Name: Automates the removal of business signifiers such as 'Inc.' or 'Corp.' from entity names, simplifying entity recognition.</li> <li>Convert Formatted Numeric: Strip formatting like parentheses (for negatives) and commas (as thousand separators) from numeric data, converting them into a clean, numerically-typed format.</li> <li>Custom Expression: Allows users to apply any valid Spark SQL expression to combine or transform fields, enabling highly customized data manipulations.</li> </ul> </li> <li>Users can define specific checks on computed fields to automatically detect anomalies during scan operations.</li> <li>Computed fields are also visible in the data preview tab, providing immediate insight into the results of the defined transformations.</li> </ul> </li> <li> <p>Autogenerated Descriptions for Authored Checks</p> <ul> <li>Implemented an auto-generation feature for check descriptions to streamline the check authoring process. This feature automatically suggests descriptions based on the selected rule type, reducing manual input and simplifying the setup of checks.</li> </ul> </li> <li> <p>Event-Driven Catalog Integrations and Sync Enhancements</p> <ul> <li>Enhanced the Atlan integration and synchronization functionalities to include event-driven support, automatically syncing assets during Profile and Scan operations. This update also refines the Sync and Integration dialogs, offering clearer control options and flexibility.</li> </ul> </li> <li> <p>Sorting by Anomalous Record Count</p> <ul> <li>Added a new sorting filter in the Anomalies tabs that allow users to sort anomalies by record count, improving the manageability and analysis of detected anomalies.</li> </ul> </li> <li> <p>Refined Tag Sorting Hierarchy:</p> <ul> <li>Updated the tag sorting logic to consistently apply a secondary alphabetical sort by name. This ensures that tags will additionally be organized by name within any primary sorting category.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_22","title":"General Fixes","text":"<ul> <li> <p>Profile Operation Support for Empty Containers</p> <ul> <li>Resolved an issue where profiling operations failed to record fields in empty containers. Now, fields are generated even if no data rows are present.</li> </ul> </li> <li> <p>Persistent Filters on the Explore Page</p> <ul> <li>Fixed a bug that caused Explore to disable when switching tabs on the Explore page. Filters now remain active and consistent, enhancing user navigation and interaction.</li> </ul> </li> <li> <p>Visibility of Scan Results Button</p> <ul> <li>Corrected the visibility issue of the 'results' button in the scan operation list at the container level. The button now correctly appears whenever at least one anomaly is detected, ensuring users have immediate access to detailed anomaly results.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.6.18","title":"2024.6.18","text":""},{"location":"changelog-2024/#feature-enhancements_23","title":"Feature Enhancements","text":"<ul> <li> <p>Improvement to Anomaly Dialog</p> <ul> <li>Enhanced the anomaly dialog to include a direct link to the operation that generated the anomaly. Users can now easily navigate from an anomaly to view other anomalies generated by the same operation directly from the Activity tab.</li> </ul> </li> <li> <p>Sorting by Duration in Activity Tab</p> <ul> <li>Introduced the ability to sort by the duration of operations in the Activity tab by ascending or descending order.</li> </ul> </li> <li> <p>Last Editor Information for Scheduled Operations</p> <ul> <li>Added visibility of which users have created or last updated scheduled operations, enhancing traceability in scheduling management.</li> </ul> </li> <li> <p>Display Total Anomalous Records for Anomalies</p> <ul> <li>Added the total count of anomalous records in the anomalies listing view.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_23","title":"General Fixes","text":"<ul> <li> <p>Performance Fixes on Computed Table Creation and Check Validation</p> <ul> <li>Optimized the processes for creating computed tables and validating checks. Users previously experiencing slow performance or timeouts during these operations will now find the processes significantly faster and more reliable.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.6.14","title":"2024.6.14","text":""},{"location":"changelog-2024/#feature-enhancements_24","title":"Feature Enhancements","text":"<ul> <li> <p>Improvements to Atlan Integration</p> <ul> <li> <p>When syncing Qualytics with Atlan, badges now display the \"Quality Score Total,\" increasing visibility and emphasizing key data quality indicators on Atlan assets.</p> </li> <li> <p>Improved performance of the synchronization operation.</p> </li> <li> <p>Implemented the propagation of external tags to checks, now automatically aligned with the container synchronization process, enabling better accuracy and relevance of data tagging.</p> </li> </ul> </li> <li> <p>Refactor Metric Check Creation</p> <ul> <li>Enhanced the encapsulated Metric Check creation flow to improve user experience and efficiency. Users can now seamlessly create computed tables and schedule operations simultaneously with the metric check creation.</li> </ul> </li> <li> <p>Support Update of Weight Modifier for External Tags</p> </li> <li> <p>Add Validation on Updated Connections</p> <ul> <li>Added support for testing the connection if there's at least one datastore attached to the connection, ensuring more reliable and accurate connection updates.</li> </ul> </li> <li> <p>Standardize Inner Tabs under the Settings Page</p> <ul> <li> <p>Tags and Notifications Improvements: The layout has been revamped for better consistency and clarity. General headers have been removed, and now each item features specific headers to enhance readability.</p> </li> <li> <p>Security Tab Improvements: The redesign features chip tabs for improved navigation and consistency. Filters have been updated to ensure they meet application standards.</p> </li> <li> <p>Tokens Tab Accessibility: Moved the action button to the top of the page to make it more accessible.</p> </li> <li> <p>Refine Connector Icons Display: Improved the display of connector icons for Datastores and Enrichments in the Connections Tab.</p> </li> </ul> </li> <li> <p>Streamlined Container Profiling and Scanning</p> <ul> <li>In the container context, the profile and scan modals have been updated to automatically display the datastore and container, eliminating the need for a selection step and streamlining the process.</li> </ul> </li> <li> <p>Swap Order During Check Creation</p> <ul> <li> <p>Rule Type Positioning: The Rule Type now appears before the container selection, making the form more intuitive.</p> </li> <li> <p>Edit Mode Header: In edit mode, the Rule Type is prominently displayed in the modal header, immediately under the check ID.</p> </li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_24","title":"General Fixes","text":"<ul> <li> <p>Address Minor Issues in the Datastore Activity Page</p> <ul> <li> <p>Operation ID Auto-Search: Restored the auto-search feature by operation ID for URL access, enhancing navigation, especially for Catalog Operations.</p> </li> <li> <p>Tree View Auto-Refresh: Implemented an auto-refresh feature for the tree view, which activates after any operation in the CTA flow (Catalog, Profile, Scan).</p> </li> </ul> </li> <li> <p>Fix \"Greater Than Field\" Quality Check</p> <ul> <li>Corrected the inclusive property of the greater than field quality check.</li> </ul> </li> <li> <p>Fix Exporting Field Profiles for Non-Admin User with Write Permission</p> <ul> <li>Resolved issues for non-admin users with write permissions to allow proper exporting of field profile metadata to enrichment.</li> </ul> </li> <li> <p>Fix \"Is Replica Of\" Quality Check validation on Field Names with Special Characters</p> <ul> <li>Improved validation logic to handle field names with special characters</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.6.7","title":"2024.6.7","text":""},{"location":"changelog-2024/#feature-enhancements_25","title":"Feature Enhancements","text":"<ul> <li> <p>Atlan Integration Improvements</p> <ul> <li>Enhanced the Atlan assets fetch and external tags syncing. </li> <li>Added support for external tag propagation to checks and anomalies. </li> <li>Merged Global and External tags section for streamlined tag management.</li> </ul> </li> <li> <p>Restart Button for Analytics Engine</p> <ul> <li>Introduced a new \"Restart\" button under the Settings - Health section, allowing admins to manually restart the Analytics Engine if it is offline or unresponsive.</li> </ul> </li> <li> <p>Interactive Tooltip Component</p> <ul> <li>Added a new interactive tooltip component that remains visible upon hovering, enhancing user interaction across various modules of the application.</li> <li>Refactored existing tooltip usage to integrate this new component for a more consistent user experience.</li> </ul> </li> <li> <p>Defaulting to Last-Used Enrichment Datastore for Check Template Exports</p> <ul> <li>Improved user experience by persisting the last selected enrichment datastore as the default option when exporting a check template.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_25","title":"General Fixes","text":"<ul> <li> <p>Shared Links Fixes</p> <ul> <li>Fixed issues with shared operation result links, ensuring that dialogs for scan/profile results and anomalies now open correctly.</li> <li>Addressed display inaccuracies in the \"Field Profiles Updated\" metrics.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.6.4","title":"2024.6.4","text":""},{"location":"changelog-2024/#feature-enhancements_26","title":"Feature Enhancements","text":"<ul> <li> <p>Atlan Data Catalog Integration</p> <ul> <li>We're excited to introduce integration with Atlan, enabling users to synchronize and manage assets across both Qualytics and Atlan:<ul> <li>Tag Sync: Sync tags assigned to data assets in Atlan with the corresponding assets in Qualytics, enabling tag-based quality score reporting, notifications, and bulk data quality operations using Atlan-managed tags.</li> <li>Metadata Sync: Automatically synchronize Atlan with Qualytics metadata, including asset URL, total score, and factor scores such as completeness, coverage, conformity, consistency, precision, timeliness, volume, and accuracy.</li> </ul> </li> </ul> </li> <li> <p>Entity Resolution Check</p> <ul> <li>We've removed the previous limitation on the maximum number of distinct entity names that could be resolved with the Entity Resolution rule type. This release includes various performance enhancements that support an unlimited number of entity names.</li> </ul> </li> <li> <p>Enhancements to Catalog Operation Results</p> <ul> <li>We've improved the catalog operation results by now including detailed information on whether tables, views, or both were involved in each catalog operation.</li> </ul> </li> <li> <p>Enhancements to 'Equal to Field' Rule Type</p> <ul> <li>The 'Equal to Field' rule now supports string values, allowing for direct comparisons between text-based data fields.</li> </ul> </li> <li> <p>Enhancements to Enrichment</p> <ul> <li>Qualytics now includes a property for anomalousRecordCount on shape anomaly, which previously was neither populated nor persisted. This aims to accurately capture and record the total number of anomalous records identified in ShapeAnomaly, regardless of the max_source_records threshold.</li> </ul> </li> <li> <p>Dynamic Meta Titles</p> <ul> <li>Pages such as Datastore Details, Container Details, and Field Details now feature dynamic meta titles that accurately describe the page content and are visible in browser tabs providing better searchability.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_26","title":"General Fixes","text":"<ul> <li> <p>Fix Trends of Quality Scores on the Insights Page</p> <ul> <li>Addressed issues with displaying trends on the Insights page. Trends now accurately reflect changes and comparisons to the previous report period, providing more reliable and insightful analytics.</li> </ul> </li> <li> <p>Resolved a bug in Entity Resolution where the distinction constraint was only applied to entity names that differed.</p> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.5.22","title":"2024.5.22","text":""},{"location":"changelog-2024/#feature-enhancements_27","title":"Feature Enhancements","text":"<ul> <li> <p>Datastore Connection Updates:</p> <ul> <li>Users can now update the connection on a datastore if the new one has the same type as the current one.</li> </ul> </li> <li> <p>Enrichment Datastore Redirection:</p> <ul> <li>Enhanced the user interface to facilitate easier redirection to enrichment datastores, streamlining the process and improving user experience.</li> </ul> </li> <li> <p>Label Enhancements for Data Completeness:</p> <ul> <li>Updated labels to better distinguish between completeness percentages and Factor Scores. The label for completeness percentage has been changed to provide clear context when viewed alongside.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_27","title":"General Fixes","text":"<ul> <li> <p>Rule Type Anomaly Corrections:</p> <ul> <li>Fixed an issue where the violation messages for record anomalies incorrectly included \"None\" for some rule types. This update ensures accurate messaging across all scenarios.</li> </ul> </li> <li> <p>Shape Anomaly Logic Adjustment:</p> <ul> <li>Revised the logic for Shape Anomalies to prevent the combination of failed checks for high-count record checks on the same field. This change ensures that displayed sample rows have definitively failed the specific checks shown, enhancing the accuracy of anomaly reporting.</li> </ul> </li> <li> <p>Entity Resolution Anomalies:</p> <ul> <li>Addressed an inconsistency where some Entity Resolution Checks did not return source records. Ongoing investigations and fixes have improved the reliability of finding source records for entity resolution checks across DFS and JDBC datastores.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.5.16","title":"2024.5.16","text":""},{"location":"changelog-2024/#feature-enhancements_28","title":"Feature Enhancements","text":"<ul> <li> <p>Entity Resolution Check</p> <ul> <li>Introduced rule \"Entity Resolution\" to determine if multiple records reference the same real-world entity. This feature uses customizable fields and similarity settings to ensure accurate and tailored comparisons.</li> </ul> </li> <li> <p>Support for Rerunning Operations</p> <ul> <li>Added an option to rerun operations from the operations listing, allowing users to reuse the configuration from previously executed operations.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_28","title":"General Fixes","text":"<ul> <li> <p>Export Operations</p> <ul> <li>Fixed metadata export operations silently failing on writing to the enrichment datastores.</li> </ul> </li> <li> <p>Computed File/Table Creation</p> <ul> <li>Resolved an issue that prevented the creation of computed files/tables with the same name as previously deleted ones, even though it is a valid action.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.5.13","title":"2024.5.13","text":""},{"location":"changelog-2024/#general-fixes_29","title":"General Fixes","text":"<ul> <li> <p>Enhanced Quality Score Factors Computation</p> <ul> <li>Addressed issues in quality score calculation and its associated factors ensuring accuracy</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.5.11","title":"2024.5.11","text":""},{"location":"changelog-2024/#feature-enhancements_29","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing Quality Score Factors</p> <ul> <li>This new feature allows users to control the quality score factor weights at the datastore and container levels.<ul> <li>Quality Score Detail Expansion: Users can now click on the quality score number to expand its details, revealing the contribution of each factor to the overall score. This enhancement aids in understanding what drives the quality score.</li> <li>Insights Page Overhaul: The Insights page has been restructured to better showcase the quality score breakdown. This redesign aims to make the page more informative and focused on quality score metrics.</li> <li>Customization of Factor Weights: Users can now customize the weights of different factors at the Datastore and Container levels. This feature is essential for adapting the quality score to meet specific user needs, such as disregarding the Timeliness factor for dimensional tables where it might be irrelevant.</li> <li>Enhanced Inferred Checks: Introduced a new property in the Check Listing schema and a feature in the Check modal that displays validity metrics, which help quantify the accuracy of inferred checks. A timezone handling issue in the last_updated property of the Check model has also been addressed.</li> </ul> </li> </ul> </li> <li> <p>Quality Score UI Enhancements</p> <ul> <li>Enhancements have been made to the user interface to provide a clearer and more detailed view of the quality score metrics, including Completeness, Coverage, Conformity, Consistency, Precision, Timeliness, Volumetrics, and Accuracy. These changes aim to provide deeper insight into the components that contribute to the overall quality score.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_30","title":"General Fixes","text":"<ul> <li> <p>Fixes to JDBC Incremental Support</p> <ul> <li>Updated the conditional logic in the catalog operation for update tables to ensure the incremental identifier is preserved if already established.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.5.2","title":"2024.5.2","text":""},{"location":"changelog-2024/#feature-enhancements_30","title":"Feature Enhancements","text":"<ul> <li> <p>Datastore Connections:</p> <ul> <li>Users can now create connections that can be shared across different datastores. This introduces a more flexible approach to managing connections, allowing users to streamline their workflow and reduce duplication of effort. With shared connections, users can easily reuse common elements such as hostname and credentials across various datastores, enhancing efficiency and simplifying management.</li> </ul> </li> <li> <p>File Container Header Configuration:</p> <ul> <li>Adds support for setting the hasHeader boolean property on File Containers, enabling users to specify whether their flat file data sources include a header row. This enhances compatibility and flexibility when working with different file formats.</li> </ul> </li> <li> <p>Improved Error Handling in Delete Dialogs:</p> <ul> <li>Error handling within delete dialogs has been revamped across the application. Error messages will now be displayed directly within the dialog itself, providing clearer feedback and preventing misleading success messages in case of deletion issues.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_31","title":"General Fixes","text":"<ul> <li> <p>Locked Template Field Editing:</p> <ul> <li>Resolves an issue where selecting a new container in the check form would reset check properties, causing problems for locked templates. The fix ensures that checks derived from templates retain their properties, allowing users to modify the field_to_compare field as needed.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.4.25","title":"2024.4.25","text":""},{"location":"changelog-2024/#feature-enhancements_31","title":"Feature Enhancements","text":"<ul> <li> <p>Profile Results Modal:</p> <ul> <li>Introducing a detailed Results Modal for each profile operation. Users can now view comprehensive statistics about the produced container profiles and their partitions, enhancing their ability to analyze data effectively.</li> </ul> </li> <li> <p>Checks Synchronized Count:</p> <ul> <li>The operations list now includes the count of synchronized checks for datastore and explore operations. This addition streamlines the identification of operations, improving user experience.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_32","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2024/#2024.4.23","title":"2024.4.23","text":""},{"location":"changelog-2024/#feature-enhancements_32","title":"Feature Enhancements","text":"<ul> <li> <p>Introduction of Comparators for Quality Checks:</p> <ul> <li>Launched new Comparator properties across several rule types, enhancing the flexibility in defining quality checks. Comparators allow users to set margins of error, accommodating slight variations in data validation:<ul> <li>Numeric Comparators: Enables numeric comparisons with a specified margin, which can be set as either a fixed absolute value or a percentage, accommodating datasets where minor numerical differences are acceptable.</li> <li>Duration Comparators: Supports time-based comparisons with flexibility in duration differences, essential for handling time-based data with variable precision.</li> <li>String Comparators: Facilitates string comparisons by allowing for variations in spacing, ideal for textual data where minor inconsistencies may occur.</li> </ul> </li> <li>Applicable to rule types such as Equal To, Equal To Field, Greater Than, Greater Than Field, Less Than, Less Than Field, and Is Replica Of.</li> </ul> </li> <li> <p>Introduced Row Comparison in the isReplicaOf Rule:</p> <ul> <li>Improved the rule to support row comparison by id, enabling more precise anomaly detection by allowing users to specify row identifiers for unique row comparison. Key updates include:<ul> <li>Revamp of the source record presentation to highlight differences between the left and right containers at the cell level, enhancing visibility into anomalies.</li> <li>New input for specifying unique row identifiers, transitioning from symmetric difference to row comparison when set.</li> <li>The original behavior of symmetric comparison remains unchanged if no row identifiers are provided.</li> </ul> </li> </ul> </li> <li> <p>New equalTo Rule Type for Direct Value Comparisons</p> <ul> <li>Introduced the equalTo rule type, enabling precise assertions that selected fields match a specified value. This new rule not only simplifies the creation of checks for constant values across datasets but also supports the use of comparators, allowing for more flexible and nuanced data validation.</li> </ul> </li> <li> <p>Redirect Links for Requested Containers in Operation Details:</p> <ul> <li>Introduced redirect links in the \"Containers Requested\" section of operation results. This enhancement provides direct links to the requested containers (such as tables or files), facilitating quicker navigation and streamlined access to relevant operational data.</li> </ul> </li> <li> <p>Enhanced Description Input with Expandable Option:</p> <ul> <li>Implemented an expandable option for the Description input in the Check Form &amp; Template Form. This enhancement allows users to more comfortably manage lengthy text entries, improving the usability of the form by accommodating extensive descriptions without compromising the interface's usability.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_33","title":"General Fixes","text":"<ul> <li> <p>Addressed Data Preview Timeout Issues:</p> <ul> <li>Tackled the timeout problems in the data preview feature, ensuring that data retrieval processes complete successfully within the new extended timeout limits.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.4.12","title":"2024.4.12","text":""},{"location":"changelog-2024/#feature-enhancements_33","title":"Feature Enhancements","text":"<ul> <li>File Pattern Overrides:<ul> <li>We have added support in the UI to override a file pattern. Now, a file pattern overwritten by a user will replace the one that the system generated during the first catalog. To have a new file pattern in the UI, users need to perform a new catalog operation without prune.</li> </ul> </li> <li>Batch Edit in the Check Templates Library::<ul> <li>We are now supporting batch edits for check templates in the Library. This enhancement will allow filters and tags.</li> </ul> </li> <li>Improved Presentation of Incremental, Remediation, and Infer Constraints:<ul> <li>We have improved the presentation of Incremental, Remediation, and Infer Constraints in the operation listing for catalog, profile, and scan operations. The Incremental, Remediation, and Infer Constraints icons have been added to the list of items, and the visualization of these items has been enhanced.</li> </ul> </li> <li>Default Placeholders for Computed File in UI:<ul> <li>We are now automatically populating the form dialog with fields from the selected container. This improvement simplifies the process for users, especially in scenarios where they wish to select or cast specific fields directly from the source container.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_34","title":"General Fixes","text":"<ul> <li> <p>Tree View Default Ordering:</p> <ul> <li>We have updated the tree view default ordering. Datastore names are now grouped and presented in alphabetical order.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.4.6","title":"2024.4.6","text":""},{"location":"changelog-2024/#breaking-changes_2","title":"Breaking Changes","text":"<ul> <li> <p>Remediation Naming Convention Update:</p> <ul> <li>Updated the naming convention for remediation to <code>{enrich_container_prefix}_remediation_{container_id}</code>, standardizing remediation identifiers.</li> </ul> </li> <li> <p>Add file extension for DFS Enrichment:</p> <ul> <li>Introduced <code>.delta</code> extension to files in the enrichment process on DFS, aligning with data handling standards.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#feature-enhancements_34","title":"Feature Enhancements","text":"<ul> <li> <p>Revamp Enrichment Datastore Main Page:</p> <ul> <li>Tree View &amp; Data Navigation: Enhanced the enrichment page with an updated tree view that now lists source datastores linked to enrichment datastores, improving navigability. A newly introduced page for enrichment datastore enables:<ul> <li>Data preview across enrichment, remediation, and metadata tables with the ability to apply \"WHERE\" filters for targeted insights.</li> <li>Direct downloading of preview data as CSV.</li> </ul> </li> <li>UI Performance Optimization: Implemented UI caching to boost performance, reducing unnecessary network requests and smoothly preserving user-inputted filters and recent data views.</li> </ul> </li> <li> <p>User Sorting by Role:</p> <ul> <li>Introduced a sorting feature in the Settings &gt; Users tab, allowing users to be sorted by their roles in ascending or descending order, facilitating easier user management.</li> </ul> </li> <li> <p>Expanded Entity Interaction Options:</p> <ul> <li>Enhanced entity lists and breadcrumbs with new direct action capabilities. Users can now right-click on an item to access useful functions: copy the entity's ID or name, open the entity's link in a new tab, and copy the entity's link. This enhancement simplifies data management by making essential actions more accessible.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_35","title":"General Fixes","text":"<ul> <li> <p>Record Quality Scores Overlap Correction:</p> <ul> <li>Resolved a problem where multiple violations could be open for the same container simultaneously, contrary to logic. This fix ensures violations for containers are uniquely recorded, eliminating parallel open violations.</li> </ul> </li> <li> <p>Anomaly Details Text Overflow:</p> <ul> <li>Corrected text overflow issues in the anomaly details' violation box, ensuring all content is properly contained and readable.</li> </ul> </li> <li> <p>Enhanced \"Not Found\" Warnings with Quick Filters:</p> <ul> <li>Improved user guidance for Checks and Anomalies list filters by adding hints for \"not found\" items, suggesting users check the \"all\" group for unfiltered search results, clarifying navigation and search results.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.3.29","title":"2024.3.29","text":""},{"location":"changelog-2024/#feature-enhancements_35","title":"Feature Enhancements","text":"<ul> <li> <p>Data Preview</p> <ul> <li>Introducing the \"Data Preview\" tab, providing users with a streamlined preview of container data within the platform. This feature aims to enhance the user experience for tasks such as debugging checks, offering a grid view showcasing up to 100 rows from the container's source.<ul> <li>Data Preview Tab: Implemented a new tab for viewing container data, limited to displaying a maximum of 100 rows for improved performance.</li> <li>Filter Support: Added functionality to apply filter clauses to the data preview, enabling users to refine displayed rows based on specific criteria.</li> <li>UI Caching: Implemented a caching layer within the UI to enhance performance and reduce unnecessary network requests, storing the latest refreshed data along with applied filters.</li> </ul> </li> </ul> </li> <li> <p>Enhanced Syntax Highlight Inputs</p> <ul> <li>Improved the syntax highlight inputs for seamless inline editing, minimizing the friction of entering expressions. This feature includes a dual-mode capability, allowing users to type directly within the input field or utilize an expanded dialog for more complex entries, significantly improving user experience.</li> </ul> </li> <li> <p>Volumetric Measurements</p> <ul> <li>Periodically measure container volumetrics for a more robust approach. This update focuses on measuring only containers without a volume measure in the last 24 hours and scheduling multiple runs of the job daily.</li> </ul> </li> <li> <p>Sort Tags by Color</p> <ul> <li>Users can now sort tags by color, visually grouping similar colors for easier navigation and management.</li> </ul> </li> <li> <p>Download Source Records</p> <ul> <li>Added a \"Download Source Records\" feature to the Anomaly view in the UI, allowing users to export data held in the enrichment store for that anomaly in CSV format.</li> </ul> </li> <li> <p>Check Templates Navigation</p> <ul> <li>Implemented a breadcrumb trail for the Check Template page to improve user navigation.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_36","title":"General Fixes","text":"<ul> <li> <p>Fix Scheduling Issues</p> <ul> <li>Resolved scheduling issues affecting specific sets of containers, particularly impacting scheduled profile and scan operations. Users must manually add new profiles after catalog operations or computed file/table creation for inclusion in existing scheduled operations.</li> </ul> </li> <li> <p>Fix Notifications Loading Issue on Large Screens</p> <ul> <li>Fixed an issue where the infinity loading feature for the user notification list was not functioning properly on large screens. The fix ensures correct triggering of infinity loading regardless of screen size, allowing all notifications to be accessed properly.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.3.15","title":"2024.3.15","text":""},{"location":"changelog-2024/#feature-enhancements_36","title":"Feature Enhancements","text":"<ul> <li>Enhanced Observability<ul> <li>Automated daily volumetric measurements for all tables and file patterns</li> <li>Time-series capture and visualizations for volume, freshness, and identified anomalies</li> </ul> </li> <li> <p>Overview Tab:</p> <ul> <li>Introduced a new \"Overview\" tab with information related to monitoring at the datastore and container level.</li> <li>This dashboard interface is designed for monitoring and managing data related to qualytics for datastore and containers.</li> <li>Users can see:<ul> <li>Totals: Quality Score, Tables, Records, Checks and Anomalies</li> <li>Total of Quality Checks grouped by Rule type</li> <li>Data Volume Over Time: A line graph that shows the total amount of data associated with the project over time.</li> <li>Anomalies Over Time: A line graph that shows the number of anomalies detected in the project over time.</li> </ul> </li> </ul> </li> <li> <p>Datastore Field List Update:</p> <ul> <li>The datastore field profiles list has been updated to match the existing list views design.</li> <li>All card-listed pages now display information in a column format, conditionally using scrolling for smaller and larger screens.</li> <li>Now the field details will show on a modal with Profiling and Histogram</li> </ul> </li> <li> <p>Heatmap Simplification:</p> <ul> <li>Simplified the heatmap to consider only operations counted.</li> </ul> </li> <li> <p>Datastore Metrics:</p> <ul> <li>Improved distinction between 0 and null values in the datastore metrics (total records, total fields, etc).</li> </ul> </li> <li> <p>Explore Page Update:</p> <ul> <li>Added new metrics to the Explore page.</li> <li>We are now adding data volume over time (records and size).</li> <li>Improved distinction between 0 and null values in metrics (total records, total fields, etc).</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_37","title":"General Fixes","text":"<ul> <li> <p>UI Wording and Display for Cataloged vs Profiled Fields:</p> <ul> <li>Addressed user confusion surrounding the display and wording used to differentiate between fields that have been cataloged versus those that have been profiled.</li> <li>Updated the messaging within the tree view and other relevant UI components to accurately reflect the state of fields post-catalog operation.</li> <li>Implemented a clear distinction between non-profiled and profiled fields in the field count indicators.</li> <li>Conducted a thorough review of the CTAs and descriptive text surrounding the Catalog, Profile, and Scan operations to improve clarity and user understanding.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.3.7","title":"2024.3.7","text":""},{"location":"changelog-2024/#general-fixes_38","title":"General Fixes","text":"<ul> <li> <p>Corrected MatchesPattern Checks Inference:</p> <ul> <li>Fixed an issue where the inference engine generated MatchesPattern checks that erroneously asserted false on more than 10% of training data. This resolution ensures all inferred checks now meet the 99% coverage criterion, aligning accurately with their training datasets.</li> </ul> </li> <li> <p>Fixed Multi-Field Check Parsing Error in DFS:</p> <ul> <li>Addressed a bug in DFS environments that caused parsing errors for checks asserting against multiple fields, such as AnyNotNull and NotNull, when selected fields contained white spaces. This resolution ensures that checks involving multiple fields with spaces are now accurately parsed and executed.</li> </ul> </li> <li> <p>Volumetric Measurements Tracking Fix:</p> <ul> <li>Addressed a bug that prevented the recording of volumetric measurements for containers without a last modified time. This fix corrects the problem by treating last_modification_time as nullable, ensuring that containers are now accurately tracked for volumetric measurements regardless of their modification date status.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.3.5","title":"2024.3.5","text":""},{"location":"changelog-2024/#feature-enhancements_37","title":"Feature Enhancements","text":"<ul> <li>Check Validation Improvement:<ul> <li>Enhanced the validation process for the \"Is Replica Of\" check. Previously, the system did not validate the field name and type, potentially leading to undetected issues until a Scan Operation was executed. Now, the validation process includes checking the field name and type, providing users with immediate feedback on any issues.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_39","title":"General Fixes","text":"<ul> <li> <p>Matches Pattern Data Quality Check Handling White Space:</p> <ul> <li>Resolved a bug in the Matches Pattern data quality check that caused white space to be ignored during training. With this fix, the system now accounts for white space during training, ensuring accurate pattern inference even with data containing significant white space. If 1% or more of the training data contains blanks, the system will derive a pattern that includes blanks as a valid value, improving data quality assessment.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.2.28","title":"2024.2.28","text":""},{"location":"changelog-2024/#feature-enhancements_38","title":"Feature Enhancements","text":"<ul> <li>User Token Management:<ul> <li>Transitioned from Generic Tokens to a more robust User Token system accessible under Settings for all users. This enhancement includes features to list, create, revoke, and delete tokens, offering granular control of API access. User activities through the API are now attributable, aligning actions with user accounts for improved accountability and traceability.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_40","title":"General Fixes","text":"<ul> <li> <p>Datetime Validation in API Requests:</p> <ul> <li>Strict validation of datetime entries in API requests has been implemented to require the Zulu datetime format. This update addresses and resolves issues where incomplete datetime entries could disrupt Scan operations, enhancing API reliability.</li> </ul> </li> <li> <p>Context-Aware Redirection Post-Operation:</p> <ul> <li>Enhanced the operation modal redirect functionality to be context-sensitive, ensuring that users are directed to the appropriate activity tab after an operation, whether at the container or datastore level. This enhancement ensures a logical and intuitive post-operation navigation experience.</li> </ul> </li> <li> <p>Template Details Page Responsiveness:</p> <ul> <li>Addressed layout issues on the Template Details page caused by long descriptions. Adjustments ensure that the description section now accommodates larger text volumes without disrupting the page layout, maintaining a clean and accessible interface.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.2.23","title":"2024.2.23","text":""},{"location":"changelog-2024/#feature-enhancements_39","title":"Feature Enhancements","text":"<ul> <li> <p>Introduction of Operations Management at the Table/File Level:</p> <ul> <li>The Activity tab has been added at the table/file level, extending its previous implementation at the source datastore level. This update provides users with the ability to view detailed information on operations for individual tables/files, including scan metrics, and histories of operation runs and schedules. It enhances the user's ability to monitor and analyze operations at a granular level.</li> </ul> </li> <li> <p>Enhanced Breadcrumb Navigation UX:</p> <ul> <li>Breadcrumb navigation has been improved for better user interaction. Users can now click on the breadcrumb representing their current context, enabling more intuitive navigation. In addition, selecting the Source Datastore breadcrumb takes users directly to the Activity tab, streamlining the flow of user interactions.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_41","title":"General Fixes","text":"<ul> <li> <p>Improved Accuracy in Profile and Scan Metrics:</p> <ul> <li>Enhanced the accuracy of metrics for profiled and scanned operations by excluding failed containers from the count. Now, metrics accurately reflect only those containers that have been successfully processed.</li> </ul> </li> <li> <p>Streamlined input display for Aggregation Comparison rule in Check/Template forms:</p> <ul> <li>Removed the \"Coverage\" input for the \"Aggregation Comparison\" rule in Check/Template Forms, as the rule does not support coverage customization. This simplification helps avoid confusion during rule configuration.</li> </ul> </li> <li> <p>Increased Backend Process Timeouts:</p> <ul> <li>In response to frequent timeout issues, the backend process timeouts have been adjusted. This change aims to reduce interruptions and improve service reliability by ensuring that processes have sufficient time to complete.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.2.19","title":"2024.2.19","text":""},{"location":"changelog-2024/#feature-enhancements_40","title":"Feature Enhancements","text":"<ul> <li> <p>Support for exporting Check Templates to the Enrichment Datastore:</p> <ul> <li>Added the ability to export Check Library metadata to the enrichment datastore. This feature helps users export their Check Library, making it easier to share and analyze check templates.</li> </ul> </li> <li> <p>File Upload Size Limit Handling:</p> <ul> <li>Implemented a user-friendly error message for file uploads that exceed the 20MB limit. This enhancement aims to improve user experience by providing clear feedback when the file size limit is breached, replacing the generic error message previously displayed.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_42","title":"General Fixes","text":"<ul> <li> <p>Resolved Parsing Errors in Expected Values Rule:</p> <ul> <li>Fixed an issue where single quotes in the list of expected values caused parsing errors in the Analytics Engine, preventing the Expected Values rule from asserting correctly. This correction ensures values, including those with quotes or special characters, are now accurately parsed and asserted.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.2.17","title":"2024.2.17","text":""},{"location":"changelog-2024/#general-fixes_43","title":"General Fixes","text":"<ul> <li> <p>Corrected Typing for Expected Values Check:</p> <ul> <li>Resolved an issue with the expectedValues rule, where numeric comparisons were inaccurately processed due to a misalignment between the API and the analytics engine. This fix ensures numeric values are correctly typed and compared, enhancing the reliability of validations.</li> </ul> </li> <li> <p>Fixed Anomaly Filtering in Scan Results dialog:</p> <ul> <li>Addressed a flaw where scan results did not consistently filter anomalies based on the operation ID. The fix guarantees that anomalies are only displayed once the operation ID parameter is accurately defined in the URL, ensuring more precise and relevant scan outcome presentations.</li> </ul> </li> <li> <p>Check Validation Sampling Behavior Adjustment:</p> <ul> <li>Fixed intermittent validation issues encountered in specific source datastore types (DB2, Microsoft SQL Server). The problem, where validation could unpredictably fail or succeed based on container size, was corrected by fine-tuning the sampling method for these technologies, leading to consistent validation performance.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.2.15","title":"2024.2.15","text":""},{"location":"changelog-2024/#feature-enhancements_41","title":"Feature Enhancements","text":"<ul> <li> <p>UX Improvements for Profile and Scan Operation Dialogs:</p> <ul> <li>Implemented significant UX enhancements to Profile &amp; Scan Operation Dialogs for improved clarity and user flow. Key improvements include:<ul> <li>Visibility of incremental fields and their current starting positions in Scan Operation dialogs.</li> <li>Logical reordering of Profile and Scan Operation steps to align with user workflows, including prioritizing container selection and clarifying the distinction between \"Starting Threshold\" and \"Limit\" settings.</li> </ul> </li> <li>Simplified operation initiation, allowing users to start operations directly before the final scheduling step, streamlining the process for immediate execution.</li> </ul> </li> <li> <p>Naming for Scheduled Operations:</p> <ul> <li>Added a name field to scheduled operations, enabling users to assign descriptive names or aliases. This feature aids in distinguishing and managing multiple scheduled operations more effectively.</li> </ul> </li> <li> <p>Container Name Filters for Operations:</p> <ul> <li>Provided filtering options for operations and scheduled operations by container name, improving the ability to quickly locate and manage specific operations.</li> </ul> </li> <li> <p>Improved Design for Field Identifiers in Tooltips:</p> <ul> <li>The design of field identifiers within tooltips has been refined for greater clarity. Enhancements focus on displaying Grouping Fields, Excluded Fields, Incremental Fields, and Partition Fields, aiming to offer users a more intuitive experience.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_44","title":"General Fixes","text":"<ul> <li> <p>External Scan Rollup Threshold Correction:</p> <ul> <li>Fixed an issue in external scans where the rollup threshold was not applied as intended. This correction ensures that anomalies exceeding the threshold are now accurately consolidated into a single shape anomaly, rather than being reported as multiple individual record anomalies.</li> </ul> </li> <li> <p>Repetitive Release Notification and Live Update Fixes:</p> <ul> <li>Resolved a recurring issue with release notifications continually prompting users to refresh despite acknowledgment. Additionally, it restored the live update notifications' functionality, ensuring users are correctly alerted to new features while actively using the system, with suggestions for a hard refresh to access the latest version.</li> </ul> </li> <li> <p>Corrected Field Input Logic in Check &amp; Template Forms:</p> <ul> <li>Addressed a logic error that incorrectly disabled field inputs for certain rules in check and template forms. This correction re-enables the necessary field input, removing a significant barrier that previously prevented users from creating checks affected by this issue.</li> </ul> </li> <li> <p>Addressed Absence of Feedback for No-Match Field Filters on Explore Page:</p> <ul> <li>Rectified the absence of feedback when field filters on the Explore Page yield no results, ensuring users receive a clear message indicating no items match the specified filter criteria.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.2.10","title":"2024.2.10","text":""},{"location":"changelog-2024/#feature-enhancements_42","title":"Feature Enhancements","text":"<ul> <li> <p>Immediate Execution Option for Scheduled Operations:</p> <ul> <li>Introduced a \"Run Now\" feature for scheduled operations, enabling users to execute operations immediately without waiting for the scheduled time. This addition provides flexibility in operation management, ensuring immediate execution as needed without altering the original schedule.</li> </ul> </li> <li> <p>Simplified Customization of Notification Messages:</p> <ul> <li>Removed the \"use custom message\" toggle from the notification form, making the message input field always editable. This change simplifies the user interface and improves usability by allowing direct editing of notification messages.</li> <li>Enhanced default messages for each notification trigger type have also been implemented to improve clarity.</li> </ul> </li> <li> <p>Performance Improvement in User Notifications Management:</p> <ul> <li>Implemented infinite scrolling pagination for the user notifications side panel. This update addresses performance issues with loading large numbers of notifications, ensuring a smoother and more responsive experience for users with extensive notification histories.</li> </ul> </li> <li> <p>Enhanced Archive Template Confirmation:</p> <ul> <li>Updated the archive dialog for templates to include information on the number of checks associated with archiving the template. This enhancement ensures users are aware of the impact of checks linked to the template, promoting informed decision-making.</li> </ul> </li> <li> <p>Improved Interaction with Computed Tables:</p> <ul> <li>Refined the Containers list UX to allow navigation to container details immediately after the creation of a computed table, addressing delays caused by background profiling. This improvement ensures users can access computed table details without waiting for the profile operation to complete, drawing inspiration from Tree View functionality for a more seamless experience.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_45","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2024/#2024.2.2","title":"2024.2.2","text":""},{"location":"changelog-2024/#feature-enhancements_43","title":"Feature Enhancements","text":"<ul> <li>Excluded Fields Inclusion in Drop-downs:<ul> <li>Refined container settings to incorporate previously excluded fields in the dropdown list, enhancing user flexibility. In addition, a warning message has been added to notify users if a profile operation is required when deselecting excluded fields that were previously selected.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_46","title":"General Fixes","text":"<ul> <li> <p>Linkable Scan Results for Direct Access:</p> <ul> <li>Made Scan Results dialogs accessible via direct URL links, addressing previous issues with broken anomaly notification links. This enhancement provides users with a straightforward path to detailed scan outcomes.</li> </ul> </li> <li> <p>Property Display Refinement for Various Field Types:</p> <ul> <li>Corrected illogical property displays for specific field types like Date/Timestamp. The system now intelligently displays only properties relevant to the selected data type, eliminating inappropriate options. This update also includes renaming 'Declared Type' to 'Inferred Type' and adjusting the logic for accurate representation.</li> </ul> </li> <li> <p>Timezone Consistency in Insights and Activity Pages:</p> <ul> <li>Implemented improvements in timezone handling across Insights and Activity pages. These changes ensure that date aggregations are accurately aligned with the user's local time, eliminating previous inconsistencies compared to the Operations list results.</li> </ul> </li> <li> <p>Fixed breadcrumb display in the datastore for members with restricted permissions</p> <ul> <li>Enhanced the datastore interface to address issues faced by members with limited permissions. This update also fixes misleading breadcrumb displays and ensures that correct datastore enhancement information is visible.</li> </ul> </li> <li> <p>Resolved State Issue in Bulk Check Archive:</p> <ul> <li>Addressed a bug in the bulk selection process for archiving checks. The fix corrects an issue where the system recognized individual selections instead of the intended group selection due to an overlooked edge case.</li> </ul> </li> <li> <p>Improved Operation Modal State Management:</p> <ul> <li>Tackled state management inconsistencies in Operation Modals. Fixes include resetting the remediation strategy to its default and ensuring 'include' options do not carry over previous states erroneously.</li> </ul> </li> <li> <p>Eliminating Infinite Load for Non-Admin Enrichment Editing:</p> <ul> <li>Solved a persistent loading issue in the Enrichment form for non-admin users. Updates ensure a smoother, error-free interaction for these users, improving accessibility and functionality.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.1.30","title":"2024.1.30","text":""},{"location":"changelog-2024/#feature-enhancements_44","title":"Feature Enhancements","text":"<ul> <li> <p>Enhanced External Scan Operations:</p> <ul> <li>Improved data handling in External Scans by applying type casting to uploaded data using Spark. This update is particularly significant for date-time fields, which now expect and conform to ISO 8601 standards.</li> </ul> </li> <li> <p>Optimized DFS File Reading:</p> <ul> <li>Streamlined file reading in DFS by storing and utilizing the 'file_format' identified during the Catalog operation. This change eliminates the need for repeated format inspection on each read, significantly reducing overhead, especially for partitioned file types.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_47","title":"General Fixes","text":"<ul> <li> <p>Resolved DFS Reading Issues with Special Character Headers:</p> <ul> <li>Fixed a DFS reading issue where columns with headers containing special characters (like pipes |) adversely affected field profiling, including inaccuracies in histogram generation.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.1.26","title":"2024.1.26","text":""},{"location":"changelog-2024/#feature-enhancements_45","title":"Feature Enhancements","text":"<ul> <li> <p>Incremental Scan Starting Threshold:</p> <ul> <li>Introduced a \"Starting Threshold\" option for incremental Scans. This feature allows users to manually set a starting value for the incremental field in large tables, bypassing the need to scan the entire dataset initially. It's handy for first-time scans of massive databases, facilitating more efficient and targeted data scanning.</li> </ul> </li> <li> <p>Add Support for Archiving Anomalies:</p> <ul> <li>Implemented the capability of archiving anomalies. Users can now remove anomalies from view without permanently deleting them, providing greater control and flexibility in anomaly management.</li> </ul> </li> <li> <p>External Scan Operation for Ad hoc Processes:</p> <ul> <li>Introduced 'External Scan Operation' as a new feature enabling ad hoc data validation for all containers. This operation allows users to validate ad hoc data, such as Excel or CSV files, against a container's existing checks and enrichment configuration. The provided file's structure must align with the container's schema, ensuring a seamless validation process.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_48","title":"General Fixes","text":"<ul> <li> <p>Preventing Unrelated Entity Selection in Check Form:</p> <ul> <li>Fixed an issue in the Check Form where users could inadvertently select unrelated entities. Selecting datastores, containers, and fields is restricted during any ongoing data loading, preventing mismatched entity selections.</li> </ul> </li> <li> <p>Performance enhancements for BigQuery and Snowflake removing the need for count operations during full table analysis</p> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.1.23","title":"2024.1.23","text":""},{"location":"changelog-2024/#feature-enhancements_46","title":"Feature Enhancements","text":"<ul> <li> <p>Introduction of 'Expected Schema' Rule for Advanced Schema Validation:</p> <ul> <li>Introduced the 'Expected Schema' rule, replacing the 'Required Fields' rule. This new rule asserts that all selected fields are present and their data types match predefined expectations, offering more comprehensive schema validation. It also includes an option to validate additional fields added to the schema, allowing users to specify whether the presence of new fields should cause the check to fail.</li> </ul> </li> <li> <p>Refined Tree Navigation Experience:</p> <ul> <li>Updated the tree navigation to prevent automatic expansion of nodes upon selection and eliminated the auto-reset behavior when re-selecting an active node. These changes provide a smoother and more user-friendly navigation experience, especially in tables/files with numerous fields.</li> </ul> </li> <li> <p>Locked/Unlocked Status Filter in Library Page:</p> <ul> <li>Added a new filter feature to the Library page, enabling users to categorize and view check templates based on their Locked or Unlocked status. This enhancement simplifies the management and selection of templates.</li> </ul> </li> <li> <p>Improved Messaging for Locked Template Properties in Check Form:</p> <ul> <li>Enhanced the Check Form UX by adding informative messages explaining why certain inputs are disabled when a check is associated with a locked template. This update enhances user understanding and interaction with the form.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_49","title":"General Fixes","text":"<ul> <li> <p>Corrected Insights Metrics for Check Templates:</p> <ul> <li>Fixed an issue where check templates were incorrectly counted as checks in related metrics and counts on the Insights page. Templates are now appropriately filtered out, ensuring accurate representation of check-related data.</li> </ul> </li> <li> <p>Enabled Template Creation with Calculated Rules:</p> <ul> <li>Resolved a limitation that prevented the creation of templates using calculated rules like 'Satisfies Expression' and 'Aggregation Comparison'. This fix expands the capabilities and flexibility of template creation.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.1.11","title":"2024.1.11","text":""},{"location":"changelog-2024/#feature-enhancements_47","title":"Feature Enhancements","text":"<ul> <li> <p>Introduction of Check Templates:</p> <ul> <li>Implemented Check Templates to offer a balance between flexibility and consistency in quality check management. Checks can now be associated with templates in either a 'locked' or 'unlocked' state, allowing for synchronized properties or independent customization, respectively. This feature streamlines check management and enables efficient tracking and review of anomalies across all checks associated with a template.</li> </ul> </li> <li> <p>isType Rule Implementation:</p> <ul> <li>Replaced the previous dataType rule with the new isType rule for improved accuracy and understanding. The isType rule is now specifically tailored to assert only against string fields, enhancing its applicability and effectiveness.</li> </ul> </li> <li> <p>Enhanced Container Details Page with Identifier Icons:</p> <ul> <li>Updated the Container Details page to display icons for key container identifiers, including Partition Field, Grouping Fields, and Exclude Fields. This enhancement provides a more intuitive and informative user interface, facilitating easier identification and understanding of container characteristics.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_50","title":"General Fixes","text":"<ul> <li> <p>Notification System Reliability Improvement:</p> <ul> <li>Fixed intermittent failures in the notifications system. Users will now receive reliable notifications for identified anomalies, ensuring timely awareness and response to data irregularities.</li> </ul> </li> <li> <p>Safeguard Against Overlapping Scheduled Operations:</p> <ul> <li>Implemented a mechanism to prevent the overloading of deployments due to overlapping scheduled operations. If a scheduled operation doesn\u2019t complete before its next scheduled run, the subsequent run will be skipped, thereby avoiding potential strain on system resources.</li> </ul> </li> <li> <p>Correction of Group-by Field Display in Containers:</p> <ul> <li>Resolved an issue where selected grouping fields were not appearing in the list fields of a container. This fix ensures that user-specified fields for group-by operations are correctly displayed, maintaining the integrity of data organization and analysis.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.1.4","title":"2024.1.4","text":""},{"location":"changelog-2024/#feature-enhancements_48","title":"Feature Enhancements","text":"<ul> <li>Enhanced Warnings for Schema Inconsistencies in Files Profiled<ul> <li>Improved the warning message for cases where the user profiles files with different schemas under a single glob pattern. This update ensures users receive clear, helpful information when files within a glob have inconsistent structures.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_51","title":"General Fixes","text":"<ul> <li> <p>Containers with 'Group By' settings Leading to Erroneous Profile Operation</p> <ul> <li>Fixed an issue affecting profile operations which included containers with 'Group By' settings. Previously, running a profile without inferring checks resulted in all fields being erroneously removed from the field list.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog/","title":"2025","text":""},{"location":"changelog/#release-notes","title":"Release Notes","text":""},{"location":"changelog/#2025.10.23","title":"2025.10.23","text":""},{"location":"changelog/#feature-enhancements","title":"Feature Enhancements","text":"<ul> <li> <p>Expanded Command Palette with quick access to creation actions across datastores, flows, templates, and settings.</p> </li> <li> <p>Enhanced operation tracking by adding end time display in datastore activity and navigation links for container profiles and scans.</p> <ul> <li>Operation duration tooltips now show completion time for finished operations.</li> <li>Profile and scan tooltips now include clickable links in tooltips that navigate to the corresponding operation in the activity view.</li> </ul> </li> <li> <p>Added abort option for flow executions.</p> <ul> <li>New abort action available in flow execution list and details pages.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fixed volumetric chart threshold bar calculations to correctly use the configured window size in Absolute and Percentage Change comparisons.</p> </li> <li> <p>Corrected missing runtime display for fast export operations.</p> </li> <li> <p>Improved Quality Score calculation reliability and error handling.</p> </li> <li> <p>Fixed table count display showing double the actual number of tables in Snowflake and Oracle datastores when members belong to multiple teams.</p> </li> <li> <p>Added validation to prevent computed fields from being used as partition or incremental fields in container configurations.</p> </li> <li> <p>Corrected user list filtering and sorting errors when combining team filters with team-based sorting.</p> </li> <li> <p>Resolved container profile creation errors caused by concurrent operations processing inferred quality checks.</p> </li> <li> <p>Corrected materialize operation failures for glob-pattern containers by sanitizing invalid characters in output table names.</p> </li> <li> <p>Resolved Quality Score calculation failures when container rowcount</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.10.15","title":"2025.10.15","text":""},{"location":"changelog/#feature-enhancements_1","title":"Feature Enhancements","text":"<ul> <li> <p>Introduced fuzzy search to form and filter inputs.</p> <ul> <li>Search results now tolerate typos and partial matches for more natural filtering.</li> </ul> </li> <li> <p>Improved Expected Values and Required Values check configuration with visual warnings for spacing issues.</p> <ul> <li>Values with trailing or leading spaces now display in warning-colored chips.</li> <li>A tooltip will show when a value containing extra spaces.</li> </ul> </li> <li> <p>Added informational message to Flow action datastores selection explaining filter criteria.</p> </li> <li> <p>Improved DFS datastore overview to display file format type.</p> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_1","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fixed data write failures in BigQuery enrichment datastores caused by message size limits exceeding API thresholds.</p> </li> <li> <p>Optimized operation triggering endpoint performance, including scheduled runs.</p> </li> <li> <p>Improved Quality Score calculation accuracy according to Quality Score Dimensions.</p> </li> <li> <p>Fixed breadcrumb navigation not updating correctly when cloning checks and changing field or container context.</p> </li> <li> <p>Improved background task processing for enhanced system reliability and performance with optimized task execution.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.10.3","title":"2025.10.3","text":""},{"location":"changelog/#feature-enhancements_2","title":"Feature Enhancements","text":"<ul> <li> <p>Optimized computed tables, files, and joins creation process.</p> <ul> <li>Now the creation is much faster with optimized validation and asynchronous profiling.</li> <li>Added \"Validate\" button to check only syntax and semantics, eliminating wait times for full data profiling.</li> </ul> </li> <li> <p>Enhanced Quality Score calculation with improved clarity and transparency.</p> <ul> <li>Renamed \"Quality Score Factors\" to \"Quality Score Dimensions\" throughout the application for better conceptual understanding.</li> <li>Quality scores now better reflect data fitness for intended use cases rather than simple error counts.</li> </ul> </li> <li> <p>Introduced \"Has Logs\" filter to display operations that completed with logs.</p> </li> <li> <p>Added sort by \"Last Triggered\" option to the Flows list page for better workflow management.</p> </li> <li> <p>Improved page metadata for better link sharing across Flow, Check, Anomaly, Library, and Enrichment pages.</p> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_2","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fixed validation error when clearing filter clause fields in computed table and join forms.</p> </li> <li> <p>Corrected Ctrl+C copy functionality that was blocked by command palette shortcuts.</p> </li> <li> <p>Resolved delete button visibility for archived anomalies in both light and dark themes.</p> </li> <li> <p>Fixed inconsistent text styling for \"No inference\" label.</p> </li> <li> <p>Corrected catalog operation incorrectly identifying Iceberg metadata files as data files.</p> </li> <li> <p>Resolved bulk selection checkboxes disappearing after multiple select/deselect cycles.</p> </li> <li> <p>Fixed error when updating computed tables with excluded fields.</p> </li> <li> <p>Corrected icon alignment in collapsed datastore tree view sidebar.</p> </li> <li> <p>Resolved missing background color for \"Not Asserted\" status in Insights checks section.</p> </li> <li> <p>Fixed misleading error messages when attempting to edit comments on archived anomalies.</p> </li> <li> <p>Corrected a regression in computed joins that prevented using columns with non-normalized names in join conditions.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.9.17","title":"2025.9.17","text":""},{"location":"changelog/#feature-enhancements_3","title":"Feature Enhancements","text":"<ul> <li> <p>We are happy to announce the new Keyboard Commands feature</p> <ul> <li>Execute common actions using context-sensitive shortcuts that adapt based on your current location and selection within the application.</li> <li>Quickly navigate between main pages using intuitive multi-key sequences with the new \"Go to\" shortcuts.  <ul> <li>Press <code>G</code> followed by a specific key to quickly jump to pages like: <code>G</code> + <code>E</code> for Explore, <code>G</code> + <code>L</code> for Library, <code>G</code> + <code>T</code> for Tags, and etc.</li> </ul> </li> <li>Enhance Tab navigation allows smooth movement through interface elements using <code>\u2325</code> / <code>Alt</code> + <code>\u2190</code> and <code>\u2325</code> / <code>Alt</code> + <code>\u2192</code> with clear visual focus indicators.</li> <li>Update the global search shortcut to be accessible via <code>\u2318</code>/<code>Ctrl</code> + <code>Shift</code> + <code>F</code>.</li> <li>Access the Command Palette by pressing <code>\u2318</code>/<code>Ctrl</code> + <code>K</code> to search, navigate, and execute actions without leaving your current context.</li> <li>View a complete list of all available keyboard shortcuts by pressing <code>\u2318</code>/<code>Ctrl</code> + <code>/</code> for easy reference and learning.</li> </ul> </li> <li> <p>Improved hover highlighting to Data Preview, Source Records, and Enrichment tables for consistent user experience across all data views.</p> </li> <li> <p>Enhanced queries with metadata comments for better cost tracking and operation identification in query logs.</p> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_3","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fixed profile operations failing on BigQuery tables with nested record types.</p> </li> <li> <p>Limited anomaly rollup threshold to 1,000 with visual markers to prevent system overload from excessive anomaly generation.</p> </li> <li> <p>Fixed dataDiff validation errors when using computed fields as row identifiers.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.9.12","title":"2025.9.12","text":""},{"location":"changelog/#feature-enhancements_4","title":"Feature Enhancements","text":"<ul> <li> <p>We are thrilled to introduce the Profiles Over Time for tracking field-level data changes</p> <ul> <li>Users can now compare the current field profile with previous profiles to track changes.</li> <li>Visual indicators highlight metrics that changed between selected profiles.</li> <li>Interactive charts display numeric metric trends across profile history.</li> <li>Easily identify data drift and type changes at the field level.</li> <li>Special badges indicate when field types have changed between profiles.</li> </ul> </li> <li> <p>Introducing the <code>Data Diff</code> check as an enhancement of the Is Replica Of functionality.</p> </li> <li> <p>Enhanced File Profile visibility with file format display and improved handling of long names with tooltips.</p> </li> <li> <p>Improved hover contrast for list items in light mode for better visibility.</p> </li> <li> <p>Optimized slim profile logic to protect existing field typing from being overwritten by limited data samples.</p> </li> <li> <p>Added OAuth support to the Databricks connector.</p> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_4","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fixed broken enrichment datastore redirect link in the datastore tree footer.</p> </li> <li> <p>Corrected filter application in <code>Exists In</code> checks that was causing inaccurate anomaly detection.</p> </li> <li> <p>Fixed grouped inference checks to properly validate against filtered test data for each group combination.</p> </li> <li> <p>Resolved <code>Is Address</code> check failing to assert during scan.</p> </li> <li> <p>Removed <code>User Defined Function</code> check support.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.9.4","title":"2025.9.4","text":""},{"location":"changelog/#feature-enhancements_5","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing Product Updates</p> <ul> <li>Users can now view feature announcements directly within the application.</li> <li>Read full posts by clicking the external link for each update.</li> </ul> </li> <li> <p>Enhanced filter clause display in readonly checks with copy functionality and improved text formatting.</p> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_5","title":"General Fixes and Improvements","text":"<ul> <li> <p>Resolved an issue where the API allowed creation of quality checks with string fields for rules that don't support them.</p> </li> <li> <p>Corrected cache logic to avoid unnecessary data refresh in insights.</p> </li> <li> <p>Fixed input field overflow when entering long filter values that caused UI layout issues.</p> </li> <li> <p>Fixed Data Preview errors when filters return no records, now properly displays empty results instead of failing with a server error.</p> </li> <li> <p>Fixed activation failures for Exists In checks that reference computed fields.</p> </li> <li> <p>Fixed issue where rule type filter options did not appear on the check list page for Member users.</p> </li> <li> <p>Updated enrichment processing for more frequent and reliable data writes.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.8.20","title":"2025.8.20","text":""},{"location":"changelog/#feature-enhancements_6","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing Bulk Quality Check Creation through Templates</p> <ul> <li>Users can now create multiple quality checks from a single template.</li> <li>Select multiple target containers across different datastores and apply the same template configuration.</li> </ul> </li> <li> <p>Announcing Pause Schedule operation</p> <ul> <li>Users can now deactivate and reactivate schedules without losing configuration.</li> <li>Added a new filter option to show only deactivated schedules.</li> </ul> </li> <li> <p>Introducing Test Connection capability for existing connections</p> <ul> <li>Users can now verify connection changes before saving when editing a connection configurations.</li> </ul> </li> <li> <p>Improved isReplicaOf check to better handle incremental data comparisons.</p> </li> <li> <p>Added support for connecting to Kerberos-secured Hive datastores.</p> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_6","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fixed Data Preview filtering to properly indicate that computed fields are not supported in WHERE clauses and not exposing as auto complete options.</p> </li> <li> <p>Fixed issues related to ANSI SQL compliance in Spark 4.</p> </li> <li> <p>Fixed issues with nested data types in the Databricks connector.</p> </li> <li> <p>Corrected inconsistent formatting in operation details where containers read appeared larger than total containers.</p> </li> <li> <p>Resolved filter UI issues where selected items caused layout misalignment.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.8.8","title":"2025.8.8","text":""},{"location":"changelog/#feature-enhancements_7","title":"Feature Enhancements","text":"<ul> <li> <p>Enhanced Source Record visualization</p> <ul> <li>Users can now view more source records with selectable limits (10, 100, 1000, or 10000 records).</li> <li>Added sticky headers for easier navigation when scrolling through large datasets.</li> </ul> </li> <li> <p>Introducing Quality Check Migration</p> <ul> <li>Users can now migrate quality checks from one container to another, even across different datastores.<ul> <li>Archived and inferred checks are excluded from migration.</li> </ul> </li> <li>Migrated checks are set to Draft status for users review before activation.</li> </ul> </li> <li> <p>Enhanced search functionality for datastores and flows</p> <ul> <li>Users can now search by ID or name using a single search field.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_7","title":"General Fixes and Improvements","text":"<ul> <li> <p>Updated Dataplane to Spark 4</p> </li> <li> <p>Fixed Is Replica Of dry run validation to correctly handle filtered datasets.</p> </li> <li> <p>Resolved an issue where anomaly action buttons redirect to the details page instead of performing the intended action.</p> </li> <li> <p>Corrected the navigation issue when switching between the Checks and Observability tabs that prevented lists from rendering properly.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#api-changes","title":"API Changes","text":"<ul> <li>The following endpoint <code>GET /operations</code> is affected:<ul> <li>Added <code>start_date</code> and <code>end_date</code> query parameters to filter operations by date range.</li> </ul> </li> </ul>"},{"location":"changelog/#2025.7.28","title":"2025.7.28","text":""},{"location":"changelog/#feature-enhancements_8","title":"Feature Enhancements","text":"<ul> <li>Improved Spark SQL autocomplete handling for complex field names<ul> <li>SQL autocomplete now automatically adds backticks to field names, preventing errors with special characters.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_8","title":"General Fixes and Improvements","text":"<ul> <li> <p>Resolved an issue where scheduled operations were not executing reliably under high load.</p> </li> <li> <p>Fixed last asserted timestamp accuracy for all check types.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.7.23","title":"2025.7.23","text":""},{"location":"changelog/#feature-enhancements_9","title":"Feature Enhancements","text":"<ul> <li> <p>Simplified Computed Join prefix management</p> <ul> <li>Select expressions automatically adjust when prefixes change, eliminating manual field name updates.</li> </ul> </li> <li> <p>Added User Guide links to Check Creation form</p> <ul> <li>Selecting a Rule Type provides a direct link to that specific rule's documentation in the User Guide.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_9","title":"General Fixes and Improvements","text":"<ul> <li> <p>Corrected Quality Check Update errors affecting <code>Greater Than Field</code>, <code>Less Than Field</code>, and <code>Equal To Field</code> rule types.</p> </li> <li> <p>Fixed source record display for high-precision decimal values</p> <ul> <li>Source records now display full decimal precision on hover for truncated numeric values.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.7.18","title":"2025.7.18","text":""},{"location":"changelog/#feature-enhancements_10","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing Computed Join for creating new containers by joining data across different datastores</p> <ul> <li>Users can now create computed joins between two containers, even from different datastores, enabling cross-datastore data analysis.</li> <li>Supports multiple join types: Inner Join, Left Join, Right Join, and Full Outer Join to accommodate various data combination needs.</li> <li>Configure joins using intuitive left and right reference selections with field mapping and optional prefixes.</li> <li>Add custom SELECT expressions and WHERE clauses to refine the joined data output.</li> </ul> </li> <li> <p>Introducing Dry Run operation for draft checks</p> </li> <li> <p>Enhanced Bulk Check Operations and Management Capabilities</p> <ul> <li>Added metadata field to bulk update dialog, enabling users to update metadata across multiple checks simultaneously without opening each individually.</li> <li>Extended bulk operations to support archived checks, previously limited to active only.</li> <li>Bulk activate and draft actions now available for archived checks, expanding beyond the previous delete-only option.</li> </ul> </li> <li> <p>Added Subject Field to Email Notifications</p> <ul> <li>Email notifications now support customizable subject lines, allowing users to add meaningful context to their messages.</li> </ul> </li> <li> <p>Enhanced Record Limit Configuration</p> <ul> <li>Users can now manually input custom record limit values in Profile and Scan operations, as well as Flow operations through a text field, providing flexibility beyond the predefined options.</li> <li>A dropdown menu provides quick access to common values (1M, 10M, 100M, All).</li> </ul> </li> <li> <p>Adding Unlink Enrichment Datastore</p> <ul> <li>Users can now unlink enrichment datastores directly from the \"Enrichment Datastore Settings\" dialog.</li> </ul> </li> <li> <p>Improved Datastore Deletion Experience</p> <ul> <li>Error messages during deletion now appear directly within the confirmation dialog instead of temporary toast notifications.</li> <li>When deleting an Enrichment Datastore, the dialog now displays the number of linked source datastores and uses clear labeling to distinguish between datastore types.</li> </ul> </li> <li> <p>Enhanced catalog operation to properly recognize subdirectories within partitioned file structures, ensuring more accurate container identification for complex directory hierarchies.</p> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_10","title":"General Fixes and Improvements","text":"<ul> <li> <p>Addressed modal dismissal issues across multiple dialogs where clicking outside or pressing ESC would cause accidental closure and data loss.</p> </li> <li> <p>Fixed \"Open in new tab\" and \"Copy link\" actions for checks and anomalies that were not functioning correctly.</p> </li> <li> <p>Fixed source record formatting for 'Is Replica Of' anomalies when check configuration changes after anomaly detection.</p> </li> <li> <p>Fixed an issue where anomaly URLs generated from check side panels were not functioning correctly when shared.</p> </li> <li> <p>Fixed incorrect redirection after creating checks from templates.</p> </li> <li> <p>Fixed an issue where source records weren't displaying correctly during dry run operations.</p> </li> <li> <p>Corrected cloning behavior to preserve tags from the check being cloned.</p> </li> <li> <p>Fixed scan operations failing after deleting or unlinking enrichment datastores.</p> </li> <li> <p>Corrected failed checks information in anomaly responses to accurately reflect the historical check version at the time the anomaly was detected, rather than showing the current check version.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.7.2","title":"2025.7.2","text":""},{"location":"changelog/#feature-enhancements_11","title":"Feature Enhancements","text":"<ul> <li> <p>Introduced Failed Check Version display, providing visibility into the exact check configuration that triggered each anomaly.</p> <ul> <li>Failed Check now displays the specific check properties and configuration that were active when the anomaly was generated.</li> </ul> </li> <li> <p>Enhanced Check Activities visualization.</p> <ul> <li>Users can now view historical check configurations directly from the timeline, including all properties and tags as they were at that point in time.</li> </ul> </li> <li> <p>Enhanced flow nodes with improved visual design and contextual information display for better user experience.</p> <ul> <li>Action nodes now show inline summaries with relevant details based on their type (e.g., datastore names for operation actions, channel names for Slack or Teams, URLs for webhooks, etc).</li> <li>Export nodes now display asset types in their titles (e.g., \"Export Anomalies\").</li> <li>Added filter tooltips to trigger nodes displaying applied conditions (tags, datastores, operation types) for quick configuration visibility.</li> </ul> </li> <li> <p>Supports Data Preview functionality for containers that haven't been profiled yet, removing the requirement to profile first before viewing data.</p> </li> <li> <p>Enhanced editing flexibility for Asserted Checks.</p> <ul> <li>Users can now edit SparkSQL expressions that define calculated fields.</li> <li>Row Identifiers and Passthrough Fields are now editable for Is Replica Of Check.</li> </ul> </li> <li> <p>Improving Computed Assets:</p> <ul> <li>Users can now add Additional Metadata to computed Tables/files and computed fields.</li> <li>Display the Last Editor information in the tree footer to provide context on who last modified the asset.</li> </ul> </li> <li> <p>Added Last Profile visibility in Field Overview and Field Details.</p> <ul> <li>Users can now see the last time a field was profiled, helping clarify the timeframe of the metrics shown in the Profile section.</li> </ul> </li> <li> <p>Improved Anomaly Bulk Archive with comment support.</p> <ul> <li>Similar to Acknowledge Anomaly, users can now add optional comments when bulk archiving anomalies.</li> </ul> </li> <li> <p>Improve dry run result message readability.</p> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_11","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fix a bug when user selected a date in Date Picker and this return user's timezone instead of UTC timezone.</p> </li> <li> <p>Fixed an issue with Metadata Checks Dry Run execution where status messages were not displaying properly.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.6.20","title":"2025.6.20","text":""},{"location":"changelog/#feature-enhancements_12","title":"Feature Enhancements","text":"<ul> <li> <p>Added Source Records Download for Check Dry Run: Users can now download the source records as a CSV file after executing a Dry Run.</p> </li> <li> <p>Improving Source Records Performance: Implemented caching for anomaly source records, significantly reducing load times and improving in user session level.</p> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_12","title":"General Fixes and Improvements","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog/#2025.6.13","title":"2025.6.13","text":""},{"location":"changelog/#feature-enhancements_13","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing Quality Check Dry Run: Users can now quickly assess the impact of a quality check without persisting results.</p> <ul> <li>The Dry Run functionality is accessible directly from the Data Quality Check settings configuration, enabling users to test checks before scan assertion.</li> <li>A comprehensive modal displays the execution results, presenting critical metrics including Dry Run status, operation time, and sampling limits.</li> <li>Dedicated section display potential Anomalies and Source Records that would be generated by the check. When no issues are detected, users receive a clear confirmation message indicating no anomalies were identified.</li> </ul> </li> <li> <p>Enhancing Fingerprint Visualization: Users can now easily view and manage related anomalies that share the same fingerprint.</p> <ul> <li>Clicking \"View Related Anomalies\" opens a right-side panel displaying all anomalies with matching fingerprints, including the total count of related anomalies.</li> <li>The panel enables direct anomaly management, allowing users to acknowledge, archive, or click individual cards to view detailed anomaly information without navigating away.</li> </ul> </li> <li> <p>Introducing Sticky Navigation: Users can now maintain access to navigation elements and actions while scrolling through content-rich pages.</p> <ul> <li>The sticky navigation feature ensures breadcrumb information and interaction buttons remain visible and accessible as users scroll down the page.</li> </ul> </li> <li> <p>Expanding Scan Operation Support for Iceberg Tables</p> <ul> <li>Incremental scans now fully support Iceberg table formats, significantly expanding the range of asset types eligible for incremental scanning operations.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_13","title":"General Fixes and Improvements","text":"<ul> <li>Resolved an issue where metric charts failed to display data when users accessed metric details.</li> <li>Fixed a bug that incorrectly allowed users to edit settings on computed fields inherited from parent computed files.</li> <li>Corrected the rendering logic for Authored Check Details that prevented information from displaying after tag update operations.</li> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog/#2025.6.6","title":"2025.6.6","text":""},{"location":"changelog/#feature-enhancements_14","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing the new Quality Check dedicated page, enabling users to analyze check properties and metrics.</p> <ul> <li>A Check Assert Visualization is provided to analyze assertions over time, helping users monitor assertion results, with the ability to hover over a timeline point to view the latest assertion and totals.</li> <li>Displays key metrics such as Status, Rule Type, Last Asserted, Weight, Coverage, and Active Anomalies and including the check description.</li> <li>Exposes all relevant check properties to provide a comprehensive view of each check's configuration without opening the edit modal.</li> <li>Shows the full activity history for the check, including property updates, and exposes previous and new values when a check setting is modified.</li> <li>Supports inline check tag editing by clicking the tag badge, allowing users to add or remove tags without opening a modal.</li> </ul> </li> <li> <p>Announcing the Anomaly exclusive page: This new page will allow users to get detailed information about Anomaly metrics, Failed Checks and Source Records.</p> <ul> <li>Exposes detailed anomaly information, including Status, Anomalous Records, Total Failed Checks, Weight, Detected DateTime, and Scan Operation, as well as Source Datastore, Computed Table, and Location.</li> <li>Lists the Failed Checks that were violated and led to the creation of the anomaly. Clicking on a failed check opens a right-side panel with the corresponding quality check information, eliminating the need to navigate to a different page.</li> <li>Show Source Records from your data that failed the checks when available. Users can apply filters and sorting options to personalize the data display according to their preferences.</li> <li>Displays the complete activity history, including all updates made to the anomaly over time. User comments are also shown, making it easier to follow discussions and decisions.</li> <li>Similar to the dedicated Quality Check page, users can edit Anomaly Tags inline.</li> </ul> </li> <li> <p>Datastore Connection Status Visibility</p> <ul> <li>A badge attached to the datastore icon now appears in both the breadcrumb and the tree view footer, clearly indicating the connection status of the datastore.</li> </ul> </li> <li> <p>Adding support for gzipped and .txt files in Catalog Operation</p> <ul> <li>Users can now use gzipped (.gz) and .txt files in DFS Datastores for Catalog Operations.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_14","title":"General Fixes and Improvements","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog/#2025.5.23","title":"2025.5.23","text":""},{"location":"changelog/#feature-enhancements_15","title":"Feature Enhancements","text":"<ul> <li> <p>Atlan Integration</p> <ul> <li>Users can now choose whether or not to receive notifications in the Atlan platform, giving more control over their notification preferences.</li> </ul> </li> <li> <p>Freshness Heatmap</p> <ul> <li>The freshness chart has been redesigned for an improved user experience.</li> <li>Milliseconds are now displayed in a more readable date/time format for better comprehension, while the underlying data still uses milliseconds.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_15","title":"General Fixes and Improvements","text":"<ul> <li> <p>Create User</p> <ul> <li>Fixed a bug that occurred when creating a service user with automatic admin permission enabled.</li> </ul> </li> <li> <p>Rerun Operations</p> <ul> <li>Catalog, export, and materialize operations will now only display rerun operations.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.5.16","title":"2025.5.16","text":""},{"location":"changelog/#feature-enhancements_16","title":"Feature Enhancements","text":"<ul> <li> <p>Anomaly</p> <ul> <li>Users can now view the Anomaly Fingerprint directly in the Anomaly Details page.</li> <li>A new button allows users to quickly copy the fingerprint value.</li> <li>A link to the User Guide has been added to explain how this feature works.</li> </ul> </li> <li> <p>Datastore Connection</p> <ul> <li>A new validation step was added to several connectors to verify if the specified schema exists.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_16","title":"General Fixes and Improvements","text":"<ul> <li> <p>Schedule Operation</p> <ul> <li>Fixed a bug in scheduled operations that allowed <code>None</code> as a value for <code>max_records_analyzed_per_partition</code> when updating.</li> </ul> </li> <li> <p>Check</p> <ul> <li>Fixed an issue where creating a metric check with a non-existent comparison value would fail..</li> <li>Fixed a bug where checks would fail if the filtered set was empty \u2014 now the check will pass in this case.</li> </ul> </li> <li> <p>Catalog Operation</p> <ul> <li>Fixed an issue in DB2 where evaluating the distribution of values caused an error.</li> </ul> </li> <li> <p>Scheduled Scan</p> <ul> <li>Fixed an issue that occurred when adding connection retries related to the Secrets Manager.</li> </ul> </li> <li> <p>Anomaly</p> <ul> <li>Fixed an issue where some triggered anomalies had no data available.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.5.9","title":"2025.5.9","text":""},{"location":"changelog/#feature-enhancements_17","title":"Feature Enhancements","text":"<ul> <li> <p>Microsoft Teams Integration</p> <ul> <li>We're excited to announce a native integration with Microsoft Teams, bringing powerful collaboration features directly into your Teams workspace.</li> <li>Users can now:<ul> <li>Share Qualytics links to datastores, containers, or fields and see rich previews directly in Teams.</li> <li>Receive proactive notifications when:<ul> <li>An operation completes</li> <li>An anomalous table or file is detected</li> <li>A specific anomaly is triggered</li> </ul> </li> <li>Note that the message content and actions will adapt based on the trigger type defined in the Flow.</li> <li>Manage anomalies without leaving Teams:<ul> <li>View, acknowledge, comment on, or archive anomalies from within the Teams UI</li> <li>Click to open linked anomalies directly in Qualytics</li> </ul> </li> </ul> </li> <li>The integration must be configured by a Qualytics admin. A dedicated setup guide is available in our User Guide.</li> <li>As part of this rollout:<ul> <li>Any Flows previously configured to send Teams notifications via incoming webhooks or workflows (Teams) have been automatically migrated to the Webhook action.</li> <li>The Teams notification action is now only available through the new integration.</li> </ul> </li> </ul> </li> <li> <p>Tokens</p> <ul> <li>Users can now view the last time a token was used.</li> </ul> </li> <li> <p>Session Expiration</p> <ul> <li>Improved handling of session expiration for users who are logged in but inactive for an extended period.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_17","title":"General Fixes and Improvements","text":"<ul> <li> <p>Check Template</p> <ul> <li>Fixed an issue with the message displayed when a user archives a check that has associated checks.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.5.5","title":"2025.5.5","text":""},{"location":"changelog/#feature-enhancements_18","title":"Feature Enhancements","text":"<ul> <li>Tokens<ul> <li>Users can now view the last time a token was used, providing better visibility into token activity.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_18","title":"General Fixes and Improvements","text":"<ul> <li> <p>Is Replica Of Check</p> <ul> <li>Fixed a bug where anomalies using pass-through fields on both the left and right side were not handled correctly.</li> </ul> </li> <li> <p>Action Operations</p> <ul> <li>Fixed an issue where cloning an Action Operation could exceed the allowed action limit.</li> </ul> </li> <li> <p>Group By</p> <ul> <li>Fixed a bug that occurred when users added single quotes to string columns in the Group By clause.</li> </ul> </li> <li> <p>Atlan Sync</p> <ul> <li>Updated and created custom metadata objects to align with the latest schema.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.4.24","title":"2025.4.24","text":""},{"location":"changelog/#feature-enhancements_19","title":"Feature Enhancements","text":"<ul> <li> <p>Improved Filters</p> <ul> <li>Tag Filter<ul> <li>Users will only see tag options corresponding to items currently visible in the list pages (Datastore List, Container List, and Filter List).</li> <li>The same filtering behavior has been applied to Anomalies and Checks within the datastore context.</li> <li>If no visible items contain a specific tag, a <code>No option found</code> message will be displayed in the filter dropdown.</li> </ul> </li> </ul> </li> <li> <p>Scan Operation</p> <ul> <li>Scan form<ul> <li>The scan form has been reorganized to improve the user experience.</li> <li>Now, the following steps to configure the Scan Flow are Check Categories, Reading Settings and Scan Settings.</li> </ul> </li> <li>Enrichment Settings<ul> <li>The Remediation Strategy option is now in the Enrichment Datastore Settings.</li> <li>The option will be a Datastore global value.</li> <li>Also, the Anomaly Rollup Threshold and Source Record Limit can be configured as defaults.</li> <li>During scan operations, these options will be pre-filled but can still be edited within the scan form.</li> </ul> </li> </ul> </li> <li> <p>Tree View</p> <ul> <li>The tree view layout is now adjustable, allowing users to customize it to their preferences.</li> <li>The footer in the tree view can be expanded or collapsed based on user needs.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_19","title":"General Fixes and Improvements","text":"<ul> <li> <p>Filters</p> <ul> <li>Fixed an issue where filters behaved inconsistently when navigating between different datastores.</li> </ul> </li> <li> <p>The container page is not loading</p> <ul> <li>Fixed a bug that caused the container page to fail to load under certain conditions.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#api-changes_1","title":"API Changes","text":"<ul> <li>Incoming Breaking Changes<ul> <li>REQUEST PAYLOAD: The fields <code>enrich_only</code> and <code>enrichment_only</code> will be replaced by <code>enrich_container_prefix</code> and <code>enrichment_prefix</code> and will affect the following endpoints:<ul> <li><code>POST /datastores</code></li> <li><code>PUT /datastores/{id}</code></li> </ul> </li> <li>RESPONSE PAYLOAD: The fields <code>enrich_only</code> and <code>enrichment_only</code> will be replaced by <code>enrich_container_prefix</code> and <code>enrichment_prefix</code> and will affect the following endpoints:<ul> <li><code>GET /anomalies/{id}</code></li> <li><code>PUT /anomalies/{id}</code></li> <li><code>POST /containers</code></li> <li><code>GET /containers/{id}</code></li> <li><code>PUT /containers/{id}</code></li> <li><code>GET /containers/{id}/profile</code></li> <li><code>GET /containers/{id}/scan</code></li> <li><code>POST /containers/{id}/scan</code></li> <li><code>PATCH /containers/{id}/favorite</code></li> <li><code>GET /container-profiles/{id}</code></li> <li><code>GET /container-scans/{id}</code></li> <li><code>POST /datastores</code></li> <li><code>PUT /datastores/{id}</code></li> <li><code>GET /datastores/{id}</code></li> <li><code>PATCH /datastores/{id}/favorite</code></li> <li><code>PATCH /datastores/{datastore_id}/enrichment/{enrich_store_id}</code></li> <li><code>GET /field-profiles/{id}</code></li> <li><code>POST /operations/schedule</code></li> <li><code>PUT /operations/schedule/{id}</code></li> <li><code>GET /operations/schedule/{id}</code></li> <li><code>GET /operations/{id}</code></li> <li><code>POST /operations/run</code></li> <li><code>PUT /operations/run/{id}</code></li> <li><code>PUT /operations/rerun/{id}</code></li> <li><code>PUT /operations/abort/{id}</code></li> </ul> </li> <li><code>POST /operations/run</code>, <code>POST /operations/schedule</code>, <code>PUT /operations/schedule/{id}</code>, <code>POST /flows, and PUT /flows/{id}</code><ul> <li>DEPRECATE PARAMETER: <code>remediation</code> (To specify a remediation strategy going forward, use the new <code>enrichment_remediation_strategy</code> field available in the POST /datastores and PUT /datastores/{id} endpoints.)</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog/#2025.4.11","title":"2025.4.11","text":""},{"location":"changelog/#feature-enhancements_20","title":"Feature Enhancements","text":"<ul> <li> <p>Anomalies and Checks Filter</p> <ul> <li>The Rule filter is now dynamically populated based on the types present in the current view.</li> <li>Each rule type displayed in the filter now includes a counter showing the total number of occurrences.</li> </ul> </li> <li> <p>Status Page</p> <ul> <li>The Health page has been renamed to Status page.</li> <li>Users can view additional information like Cloud Platform and Deployment Size.</li> <li>Deployment size details are now displayed to provide better visibility into environment configurations.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_20","title":"General Fixes and Improvements","text":"<ul> <li> <p>Anomaly</p> <ul> <li>Resolved a bug when handling <code>NaN</code> values in float-type data.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.4.5","title":"2025.4.5","text":""},{"location":"changelog/#general-fixes-and-improvements_21","title":"General Fixes and Improvements","text":"<ul> <li> <p>Quality Score</p> <ul> <li>Enhanced the descriptions for each Factor Weight to provide clearer guidance on how they impact the overall score.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.3.28","title":"2025.3.28","text":""},{"location":"changelog/#feature-enhancements_21","title":"Feature Enhancements","text":"<ul> <li> <p>Anomalies</p> <ul> <li>The \u201cAnomalies Identified\u201d count now shows the sum of Open and Archived anomalies.</li> <li>Anomalies are now categorized into two groups:<ul> <li>Open: anomalies currently active (Active, Acknowledged).</li> <li>Archived: anomalies that have been archived (Resolved, Duplicated, Invalid).</li> </ul> </li> <li>Users can now view the total counts of Duplicate and Invalid anomalies in the Overview tab under the Explore page.</li> <li>Users can now see the total counts of Open and Archived anomalies directly in Scan Operations.</li> <li>The Scan Results modal now displays the totals for Open and Archived anomalies.<ul> <li>Users can filter anomalies by status using a new dropdown selector.</li> </ul> </li> </ul> </li> <li> <p>Quality Checks</p> <ul> <li>Users can now sort the Quality Checks list by the \u201cLast Asserted\u201d date.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_22","title":"General Fixes and Improvements","text":"<ul> <li> <p>Teams Permissions</p> <ul> <li>Fixed an issue where users with the Drafter role were unable to restore a check as a draft.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.3.23","title":"2025.3.23","text":""},{"location":"changelog/#feature-enhancements_22","title":"Feature Enhancements","text":"<ul> <li> <p>Slack Integration</p> <ul> <li>We are excited to introduce a major enhancement to our integration with Slack.</li> <li>Users can now add the new Qualytics Slack App for enhanced capabilities.<ul> <li>Qualytics admins can configure the Slack Integration with two easy steps.<ul> <li>After configuring the integration, a Slack administrator must approve the Qualytics Slack App.</li> </ul> </li> </ul> </li> <li>The Qualytics Slack App supports selecting specific Slack channels for receiving Qualytics notifications in the context of a Qualytics Flow.<ul> <li>Different types of messages will be sent for each trigger in Flow operations.<ul> <li>The text and actions will vary depending on the selected trigger.</li> <li>The message state (Slack message color) will change based upon the message status.</li> </ul> </li> </ul> </li> <li>The Qualytics Slack App allows users to interact with Qualytics from the Slack interface:<ul> <li>Interact with anomalies by acknowledging, commenting, or archiving them without leaving the Slack UI where the flow notification is received.</li> <li>Click a link in Slack to be redirected to the Qualytics UI for more details regarding a specific notification.</li> <li>View anomalous tables and files detected without leaving Slack.</li> </ul> </li> </ul> </li> <li> <p>Anomaly Fingerprint</p> <ul> <li>We are thrilled to introduce support for a new feature that will begin identifying duplicate anomalies.</li> <li>Anomalies created after this release will be \"fingerprinted\" so that re-detection of that same anomaly can be readily identified as a duplicate detection.<ul> <li>New Scan Operation options allow users to define how detected duplicates should be handled.</li> <li>This feature helps maintain the history and timeline of specific data errors, allowing users to track how long a specific issue has persisted and a log of detections over time.</li> <li>Anomaly fingerprints will also be exposed in API responses and written to enrichment</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_23","title":"General Fixes and Improvements","text":"<ul> <li>External Scan<ul> <li>Users can now use and rerun external scans only from the activity listing for the targeted asset.</li> </ul> </li> <li>Check Last Asserted<ul> <li>Fixed an issue where checks were still being marked as never asserted even after producing anomalies.</li> </ul> </li> </ul>"},{"location":"changelog/#api-changes_2","title":"API Changes","text":"<ul> <li> <p>Integration Impacting Changes</p> <ul> <li>POST &amp; PUT api/integrations<ul> <li>REMOVED PARAMETERS: name and api_token</li> <li>NEW PARAMETERS: api_access_token, api_refresh_token and api_service_token</li> </ul> </li> <li>POSTs to api/flows/actions/notifications/ endpoints<ul> <li>MODIFIED PARAMETERS: tokenized_message is now a json object instead of a string</li> </ul> </li> <li>All requests to api/datastores endpoints<ul> <li>REMOVED RESPONSE PROPERTY METRIC: The metric for archived_anomalies previously returned as metrics.archived_anomalies has been removed from responses</li> </ul> </li> </ul> </li> <li> <p>Deprecation Notices</p> <ul> <li>POST api/operations/run<ul> <li>DEPRECATED PARAMETER: archive_overlapping_anomalies (migrate to the new parameter archive_duplicate_anomalies for enhanced functionality)</li> </ul> </li> <li>POST api/operations/schedule<ul> <li>DEPRECATED PARAMETER: archive_overlapping_anomalies (migrate to the new parameter archive_duplicate_anomalies for enhanced functionality)</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog/#2025.3.17","title":"2025.3.17","text":""},{"location":"changelog/#feature-enhancements_23","title":"Feature Enhancements","text":"<ul> <li> <p>Activity Operation</p> <ul> <li>Added a column showing the anomaly rollup threshold.</li> </ul> </li> <li> <p>Export Operations</p> <ul> <li>Added an option allowing users to schedule Export Operations.</li> </ul> </li> <li> <p>DFS Enrichment Datastore</p> <ul> <li>Normalized delta file names to lowercase.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_24","title":"General Fixes and Improvements","text":"<ul> <li> <p>Anomaly Rollup Threshold</p> <ul> <li>Fixed an issue where the field was not accepting the maximum value.</li> </ul> </li> <li> <p>Volumetric Tracking Observability</p> <ul> <li>Fixed an issue where inferred check validation errors disrupted observability measurements.</li> </ul> </li> <li> <p>Export Operation</p> <ul> <li>Updated wording on the Export dialog.</li> </ul> </li> <li> <p>Scan and External Scan</p> <ul> <li>Fixed an issue where schema checks were failing along with other checks but were not persisting in the enrichment of source records for anomalies.</li> </ul> </li> <li> <p>External Scan</p> <ul> <li>Fixed an issue where CSV data was not being properly cast for non-text fields.</li> </ul> </li> <li> <p>Enrichment Datastore</p> <ul> <li>Fixed a bug where exported tables were not appearing in the UI.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.3.5","title":"2025.3.5","text":""},{"location":"changelog/#general-fixes-and-improvements_25","title":"General Fixes and Improvements","text":"<ul> <li> <p>Tag Updating</p> <ul> <li>Fixed an issue where some containers failed to receive tag changes when updating tags across multiple containers or performing bulk updates.</li> </ul> </li> <li> <p>Edit Scheduled Materialize Operation</p> <ul> <li>Fixed an issue where the modal did not appear when users attempted to edit a scheduled materialize operation.</li> </ul> </li> <li> <p>Restart Analytics Engine</p> <ul> <li>Fixed an issue where restarting the Analytics Engine did not take effect.</li> </ul> </li> <li> <p>Anomaly Count</p> <ul> <li>Fixed an inconsistency in the anomaly count.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.2.26","title":"2025.2.26","text":""},{"location":"changelog/#feature-enhancements_24","title":"Feature Enhancements","text":"<ul> <li> <p>Enrichment Operations</p> <ul> <li>Introducing Materialize Operation<ul> <li>This will \"capture a snapshot\" of selected containers from your source datastore and export them to the enrichment datastore for seamless data loading.</li> <li>Users can define the maximum number of records to be materialized per table.</li> <li>A schedule option is available for users to set up and schedule the operation according to their needs.</li> </ul> </li> <li>Introducing Export Operation<ul> <li>Users can extract metadata from selected assets in their source datastore and export it to the enrichment datastore for seamless integration.</li> <li>Assets metadata options are available to export to the enrichment datastore. Users can export:</li> <li>Anomalies</li> <li>Quality checks</li> <li>Field profiles</li> </ul> </li> <li>These operations are available in Flow Action.</li> </ul> </li> <li> <p>Flows</p> <ul> <li>Introduced a cloning feature for actions in flow.<ul> <li>Users can now clone a simple action by clicking the vertical ellipses.</li> </ul> </li> </ul> </li> <li> <p>Scan Operation</p> <ul> <li>Introducing Anomaly Rollup Threshold<ul> <li>Users can now roll up anomalies that will be created per check before they are merged into a single rolled-up anomaly.</li> </ul> </li> </ul> </li> <li> <p>Error Messages</p> <ul> <li>Improved custom messages when users receive 502 and 503 status codes.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_26","title":"General Fixes and Improvements","text":"<ul> <li> <p>System Timestamp</p> <ul> <li>Standardized timestamps across the platform.</li> </ul> </li> <li> <p>External Integrations</p> <ul> <li>Fixed an issue where external tags should be updated instead of being deleted and dropped during sync.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.2.13","title":"2025.2.13","text":""},{"location":"changelog/#feature-enhancements_25","title":"Feature Enhancements","text":"<ul> <li> <p>Explore View</p> <ul> <li>Users can now refresh the Insights data.</li> <li>A label will indicate when the page was last refreshed.</li> </ul> </li> <li> <p>System Appearance</p> <ul> <li>Users can now select the <code>System</code> theme.<ul> <li>This setting automatically adjusts based on the user's system theme, switching between dark and light mode.</li> </ul> </li> <li>Users can still manually select dark or light mode.</li> </ul> </li> <li> <p>Dismiss Popup Window</p> <ul> <li>Users can now dismiss popup windows by pressing the <code>Esc</code> key, improving the user experience.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_27","title":"General Fixes and Improvements","text":"<ul> <li> <p>External Scan</p> <ul> <li>Fixed an issue where attempting to run an external scan resulted in a \"Request Failed\" error message.</li> </ul> </li> <li> <p>Explore View</p> <ul> <li>Fixed an issue causing excessively long loading times.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.2.6","title":"2025.2.6","text":""},{"location":"changelog/#feature-enhancements_26","title":"Feature Enhancements","text":"<ul> <li> <p>Overview Improvement</p> <ul> <li>Added inferred and authored check totals<ul> <li>Users can now view the total number of inferred and authored checks, along with a comparison timeframe.</li> <li>A redirect link allows users to access the checks directly, displaying only their current statuses.</li> </ul> </li> <li>Improved check assertion-related metrics to reflect assertions as of the report date.</li> </ul> </li> <li> <p>Team Permissions</p> <ul> <li>Manager users can now update datastore teams.<ul> <li>Requires <code>Editor</code> permission within the team.</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_28","title":"General Fixes and Improvements","text":"<ul> <li> <p>Flow Graph</p> <ul> <li>Fixed an issue where the flow graph position was randomly changing when a user updated a flow-node.</li> </ul> </li> <li> <p>Observability Chart</p> <ul> <li>Fixed an issue where threshold calculations incorrectly referenced measurement values that did not account for grouping.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Container Overview Tab<ul> <li>Refactored the Totals section to clarify that metrics are based on sampled data rather than the full dataset.<ul> <li>Added the sampling percentage next to the spark chart to indicate that derived metrics are based on this sampling percentage.</li> <li>Updated titles and labels for better clarity.</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog/#2025.1.31","title":"2025.1.31","text":""},{"location":"changelog/#feature-enhancements_27","title":"Feature Enhancements","text":"<ul> <li> <p>Freshness View</p> <ul> <li>We are excited to introduce the \"Freshness View\" feature in Qualytics!</li> <li>Users can now visualize both volumetric and freshness checks within the same tab.</li> <li>The displayed data includes:<ul> <li>Unit: Day, Month, Hour, etc.</li> <li>Maximum Age: Defines the maximum allowed time since the last data update.</li> <li>Last Asserted: Indicates the last time the data was validated.</li> </ul> </li> </ul> </li> <li> <p>Datastore Filter Condition in Flows</p> <ul> <li>Users can now configure datastore filter conditions in triggers for flows, enhancing control over triggered actions.</li> </ul> </li> <li> <p>Treat Empty Value as Null for DFS</p> <ul> <li>A new option allows users to enable \"Empty value as null\" as the default behavior for File Patterns, improving data consistency.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_29","title":"General Fixes and Improvements","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog/#2025.1.24","title":"2025.1.24","text":""},{"location":"changelog/#feature-enhancements_28","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing Freshness Tracking in Containers</p> <ul> <li>Users can now enable a freshness tracking option for containers to measure and record the last time data was added or updated in a data asset. This helps ensure data timeliness and identifies pipeline delays.</li> </ul> </li> <li> <p>Private Routes on Analytics Engine</p> <ul> <li>Customers using private routes can now view their IP addresses along with relevant messages displayed in the Analytics Engine for improved transparency.</li> </ul> </li> <li> <p>Clone a Flow</p> <ul> <li>Users can duplicate existing flows, streamlining the process of reusing and modifying flow configurations for similar scenarios.</li> </ul> </li> <li> <p>Additional Option to Execute Manual Flows</p> <ul> <li>A new \"Start a Manual Flow\" option has been added to the vertical ellipsis menu, providing users with enhanced flexibility for executing manual flows.</li> </ul> </li> <li> <p>Cancel Action for Unpublished Flows</p> <ul> <li>A \"Cancel\" action has been introduced in the flow builder, allowing users to reset the graph to its initial state for unpublished flows. This update also addresses issues related to the execute button and read-only state logic.</li> </ul> </li> <li> <p>\"Is Replica Of\" Passthrough</p> <ul> <li>The <code>IsReplicaOf</code> rule now supports a passthrough property, allowing users to exclude specific fields from assertions. Fields listed under this property are no longer flagged as anomalous.</li> </ul> </li> <li> <p>Enhancement for Volumetric Rule Type</p> <ul> <li>Volumetric checks now include a <code>comparison</code> property, ensuring consistency with metric checks and offering greater flexibility in rule configurations.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog/#2025.1.20","title":"2025.1.20","text":""},{"location":"changelog/#feature-enhancements_29","title":"Feature Enhancements","text":"<ul> <li> <p>Enhanced behavior for \"All\" in Schedule Operations</p> <ul> <li>The \"All\" option in Schedule Operations has been updated to include future containers automatically. Previously, if you created a schedule with the \"All\" option and added new tables or containers later, the schedule would not include these new additions.</li> </ul> </li> <li> <p>Validate Button for Enrichment Datastore Connections</p> <ul> <li>Users can now validate their data when creating or editing an enrichment datastore connection, improving reliability and confidence in datastore setups.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes_1","title":"General Fixes","text":"<ul> <li> <p>Inaccurate Check Assertion Details</p> <ul> <li>Resolved an issue where some checks were being marked as never asserted despite producing anomalies, ensuring more accurate reporting.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2025.1.16","title":"2025.1.16","text":""},{"location":"changelog/#feature-enhancements_30","title":"Feature Enhancements","text":"<ul> <li> <p>Flows</p> <ul> <li> <p>Introducing Flows: Users can now create automated pipelines by chaining actions and configuring triggers based on predefined events and filters.</p> <ul> <li>Triggers: Configurable based on events, filters, and operation conditions.</li> <li>Actions: Include notifications (Email, Slack, PagerDuty, etc.) and operations (catalog, profile, and scan).</li> <li>Real-Time Execution: Monitor execution history and real-time progress in the Flow Executions tab.</li> </ul> </li> <li> <p>Setup Made Simple</p> <ul> <li>Add and configure flows using the \u201cAdd Flow\u201d button in the top-right corner.</li> <li>Deactivate, delete or edit flows via the vertical ellipses or node configurations.</li> </ul> </li> <li> <p>Enhanced Triggering Options</p> <ul> <li>Operations Complete, Anomalous Table/File Detected, and Anomaly Detected triggers provide flexible, event-driven automation.</li> <li>Fine-tune triggers using filters like tags, rule types, or anomaly weights.</li> </ul> </li> <li> <p>Diverse Action Support</p> <ul> <li>Notify through in-app messages, Emails, Slack, Microsoft Teams, PagerDuty, and custom HTTP actions</li> <li>Trigger operational tasks across cataloging, profiling, and scanning.</li> </ul> </li> <li> <p>Flow Identification on Activity Tab</p> <ul> <li>Operations executed by flows are marked in the new <code>Flow</code> column, displaying the associated flow name.</li> <li>Users can navigate directly to the flow execution view from this tab.</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes_2","title":"General Fixes","text":"<ul> <li> <p>Duplicate Anomalies on Scan Schedule Operations</p> <ul> <li>Fixed an issue where duplicate anomalies were not being archived during scan operations despite user selection.</li> </ul> </li> <li> <p>BigQuery Message Size</p> <ul> <li>Enhanced default batch insertion size to improve performance and reliability.</li> </ul> </li> <li> <p>Anomalous Record Integer out of Range</p> <ul> <li>Updated check metrics to use BigInteger, addressing large value handling.</li> </ul> </li> <li> <p>Fix the Last Asserted Date</p> <ul> <li>Resolved inconsistencies in the Last Asserted Date logic for partition and container scans.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#breaking-changes_1","title":"Breaking Changes","text":"<ul> <li>Notification Rules Replaced by Flows.<ul> <li>Existing notification rules have been migrated to Flows and will continue to function as before.</li> <li>For new notifications, the users must create a flow leveraging the Flows functionality.</li> </ul> </li> </ul>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"glossary/#anomaly","title":"Anomaly","text":"<p>Something that deviates from the standard, normal, or expected. This can be in the form of a single data point, record, or a batch of data.</p>"},{"location":"glossary/#accuracy","title":"Accuracy","text":"<p>The data represents the real-world values they are expected to model.</p>"},{"location":"glossary/#catalog-operation","title":"Catalog Operation","text":"<p>Used to read fundamental metadata from a Datastore required for the proper functioning of subsequent Operations such as Profile, Hash, and Scan.</p>"},{"location":"glossary/#comparison","title":"Comparison","text":"<p>An evaluation to determine if the structure and content of the source and target Datastores match.</p>"},{"location":"glossary/#comparison-runs","title":"Comparison Runs","text":"<p>An action to perform a comparison.</p>"},{"location":"glossary/#completeness","title":"Completeness","text":"<p>Required fields are fully populated.</p>"},{"location":"glossary/#conformity","title":"Conformity","text":"<p>Alignment of the content to the required standards, schemas, and formats.</p>"},{"location":"glossary/#connectors","title":"Connectors","text":"<p>Components that can be easily connected to and used to integrate with other applications and databases. Common uses include sending and receiving data.</p> <p>Info</p> <p>We can connect to any Apache Spark accessible datastore. If you have a datastore we don\u2019t yet support, talk to us! We currently support: Files (CSV, JSON, XLSX, Parquet) on Object Storage (S3, Azure Blob, GCS); ETL/ELT Providers (Fivetran, Stitch, Airbyte, Matillion \u2013 and any of their connectors!); Data Warehouses (BigQuery, Snowflake, Redshift); Data Pipelining (Airflow, DBT, Prefect); Databases (MySQL, PostgreSQL, MSSQL, SQLite, etc.) and any other JDBC source.</p>"},{"location":"glossary/#consistency","title":"Consistency","text":"<p>The value is the same across all datastores within the organization.</p>"},{"location":"glossary/#container-of-a-datastore","title":"Container (of a Datastore)","text":"<p>The uniquely named abstractions within a Datastore that hold data adhering to a known schema. The Containers within a RDBMS are tables, the containers in a filesystem are well formatted files, etc.</p>"},{"location":"glossary/#data-at-rest","title":"Data-at-rest","text":"<p>Data that is stored in a database, warehouse, file system, data lake, or other datastore.</p>"},{"location":"glossary/#data-drift","title":"Data Drift","text":"<p>Changes in a data set\u2019s properties or characteristics over time.</p>"},{"location":"glossary/#data-in-flight","title":"Data-in-flight","text":"<p>Data that is on the move, transporting from one location to another, such as through a message queue, API, or other pipeline.</p>"},{"location":"glossary/#data-lake","title":"Data Lake","text":"<p>\u200b\u200bA centralized repository that allows you to store all your structured and unstructured data at any scale.</p>"},{"location":"glossary/#data-quality","title":"Data Quality","text":"<p>Ensuring data is free from errors, including duplicates, inaccuracies, inappropriate fields, irrelevant data, missing elements, non-conforming data, and poor data entry.</p>"},{"location":"glossary/#data-quality-check","title":"Data Quality Check","text":"<p>aka \"Check\" is an expression regarding the values of a Container that can be evaluated to determine whether the actual values are expected or not.</p>"},{"location":"glossary/#datastore","title":"Datastore","text":"<p>Where data is persisted in a database, file system, or other connected retrieval systems. You can check more in Datastore Overview.</p>"},{"location":"glossary/#data-warehouse","title":"Data Warehouse","text":"<p>A system that aggregates data from different sources into a single, central, consistent datastore to support data analysis, data mining, artificial intelligence (AI), and machine learning.</p>"},{"location":"glossary/#distinctness-of-a-field","title":"Distinctness (of a Field)","text":"<p>The fraction of distinct values (appear at least once) to total values that appear in a Field.</p>"},{"location":"glossary/#enrichment-datastore","title":"Enrichment Datastore","text":"<p>Additional properties that are added to a data set to enhance its meaning. Qualytics enrichment includes whether a record is anomalous, what caused it to be an anomaly, what characteristics it was expected to have, and flags that allow other systems to act upon the data.</p>"},{"location":"glossary/#favorite","title":"Favorite","text":"<p>Users can mark instances of an abstraction (Field, Container, Datastore, Check, Anomaly, etc.) as a personalized favorite to ensure it ranks higher in default ordering and is prioritized in other personalized views &amp; workflows.</p>"},{"location":"glossary/#compute-daemon","title":"Compute Daemon","text":"<p>An application that protects a system from contamination due to inputs, reducing the likelihood of contamination from an outside source. The Compute Daemon will quarantine data that is problematic, allowing the user to act upon quarantined items.</p>"},{"location":"glossary/#incremental-identifier","title":"Incremental Identifier","text":"<p>A Field that can be used to group the records in the Table Container into distinct ordered Qualytics Partitions in support of incremental operations upon those partitions:</p> <ul> <li>a whole number - then all records with the same partition_id value are considered part of the same partition.</li> <li>a float or timestamp - then all records between two defined values are considered part of the same partition (the defining values will be set by incremental scan/profile business logic). Since Qualytics Partitions are required to support Incremental Operations, an Incremental Identifier is required for a Table Container to support incremental Operations.</li> </ul>"},{"location":"glossary/#incremental-scan-operation","title":"Incremental Scan Operation","text":"<p>A Scan Operation where only new records (inserted since the last Scan Operation) are analyzed. The underlying Container must support determining which records are new for incremental scanning to be a valid option for it.</p>"},{"location":"glossary/#inference-engine","title":"Inference Engine","text":"<p>After Compute Daemon gathers all the metadata generated by a profiling operation, it feeds that metadata into our Inference Engine. The inference engine then initiates a \"true machine learning\" (specifically, this is referred to as Inductive Learning) process whereby the available customer data is partitioned into a training set and a testing set. The engine applies numerous machine learning models &amp; techniques to the training data in an effort to discover well-fitting data quality constraints. Those inferred constraints are then filtered by testing them against the held out testing set &amp; only those that assert true are converted to inferred data quality Checks.</p>"},{"location":"glossary/#metadata","title":"Metadata","text":"<p>Data about other data, including descriptions and additional information.</p>"},{"location":"glossary/#object-storage","title":"Object Storage","text":"<p>A type of data storage used for handling large amounts of unstructured data managed as objects.</p>"},{"location":"glossary/#operation","title":"Operation","text":"<p>The asynchronous (often long-running) tasks that operate on Datastores are collectively referred to as \"Operations\". Examples include Catalog, Profile, Hash, and Scan.</p>"},{"location":"glossary/#partition-identifier","title":"Partition Identifier","text":"<p>A Field that can be used by Spark to group the records in a Dataframe into smaller sets that fit within our Spark worker\u2019s memory. The ideal Partition Identifier is an Incremental Identifier of type datetime since that can serve as both but we identify alternatives should that not be available.</p>"},{"location":"glossary/#pipeline","title":"Pipeline","text":"<p>A workflow that processes and moves data between systems.</p>"},{"location":"glossary/#precision","title":"Precision","text":"<p>Your data is the resolution that is expected- How tightly can you define your data?</p>"},{"location":"glossary/#profile-operation","title":"Profile Operation","text":"<p>An operation that generates metadata describing the characteristics of your actual data values.</p>"},{"location":"glossary/#profiling","title":"Profiling","text":"<p>The process of collecting statistics on the characteristics of a dataset involving examining, analyzing, and reviewing the data.</p>"},{"location":"glossary/#proprietary-algorithms","title":"Proprietary Algorithms","text":"<p>A procedure utilizing a combination of processes, tools, or systems of interrelated connections that are the property of a business or individual in order to solve a problem.</p>"},{"location":"glossary/#quality-score","title":"Quality Score","text":"<p>A measure of data quality calculated at the Field, Container, and Datastore level. Quality Scores are recorded as time-series enabling you to track movement over time. You can read more in Quality Scoring.</p>"},{"location":"glossary/#qualytics-app","title":"Qualytics App","text":"<p>aka \"App\" this is the user interface for our Product delivered as a web application.</p>"},{"location":"glossary/#qualytics-deployment","title":"Qualytics Deployment","text":"<p>A single instance of our product (the k8s cluster, postgres database, controlplane/app/compute daemon pods, etc).</p>"},{"location":"glossary/#qualytics-compute-daemon","title":"Qualytics Compute Daemon","text":"<p>aka \"Compute Daemon\" this is the layer of our Product that connects to Datastores and directly operates on users\u2019 data.</p>"},{"location":"glossary/#qualytics-implementation","title":"Qualytics Implementation","text":"<p>A customer\u2019s Deployment plus any associated integrations.</p>"},{"location":"glossary/#controlplane","title":"Controlplane","text":"<p>aka \"controlplane\" this is the layer of our Product that exposes an Application Programming Interface (API).</p>"},{"location":"glossary/#qualytics-partition","title":"Qualytics Partition","text":"<p>The smallest grouping of records that can be incrementally processed. For DFS datastores, each file is a Qualytics Partition. For JDBC datastores, partitions are defined by each table\u2019s incremental identifier values.</p>"},{"location":"glossary/#record-of-a-container","title":"Record (of a Container)","text":"<p>A distinct set of values for all Fields defined for a Container (e.g. a row of a table).</p>"},{"location":"glossary/#schema","title":"Schema","text":"<p>The organization of data in a datastore. This could be the columns of a table, the header of a CSV file, the fields in a JSON file, or other structural constraints.</p>"},{"location":"glossary/#schema-differences","title":"Schema Differences","text":"<p>Differences in the organization of information between two datastores that are supposed to hold the same content.</p>"},{"location":"glossary/#source","title":"Source","text":"<p>The origin of data in a pipeline, migration, or other ELT/ETL process. It\u2019s where data gets extracted.</p>"},{"location":"glossary/#tag","title":"Tag","text":"<p>Users can assign Tags to Datastores, Profiles (Files, Tables, Containers), Checks and Anomalies. Add a Description and Assign a Weight. The weight value directly correlates with the level of importance, where a higher weight indicates higher significance.</p>"},{"location":"glossary/#target","title":"Target","text":"<p>The destination of data in a pipeline, migration, or other ELT/ETL process. It\u2019s where data gets loaded.</p>"},{"location":"glossary/#third-party-data","title":"Third-party data","text":"<p>Data acquired from a source outside of your company which may not be controlled by the same data quality processes. You may not have the same level of confidence in the data and it may not be as trustworthy as internally vetted datasets.</p>"},{"location":"glossary/#timeliness","title":"Timeliness","text":"<p>It can be calculated as the time between when information should be available and when it is actually available, focused on if data is available when it\u2019s expected.</p>"},{"location":"glossary/#volumetrics","title":"Volumetrics","text":"<p>Data has the same size and shape across similar cycles. It includes statistics about the size of a data set including calculations or predictions on the rate of change over time.</p>"},{"location":"glossary/#weight","title":"Weight","text":"<p>The weight value directly correlates with the level of importance, where a higher weight indicates higher significance.</p>"},{"location":"keyboard-shortcuts/","title":"Keyboard Shortcuts","text":"<p>Qualytics offers a comprehensive set of keyboard shortcuts to reduce mouse usage and accelerate everyday tasks. These shortcuts enable you to navigate the platform, run operations, manage checks, and update entities directly from the keyboard.</p> <p>Let\u2019s get started \ud83d\ude80</p> <p>Step 1: Log in to your Qualytics account and click on the Search bar at the top of the dashboard.</p> <p>Alternatively, you can press Ctrl + K on Windows/Linux or \u2318 + K on macOS to open the Search bar directly.</p> <p></p> <p>A popup window will appear, presenting various search options.</p> <p></p> <p>Step 2: Click on Search Keyboard Shortcuts to see all the shortcuts in one place.</p> <p></p> <p>A window will appear, displaying a comprehensive list of all shortcuts organized by category.</p> <p></p> <p>Step 3: You can now either search for a specific shortcut or scroll through the list to see all available options.</p> <p></p> <p>Qualytics supports a wide range of keyboard shortcut categories, including but not limited to:</p> No. Shortcut Category 1. Navigation 2. Anomaly 3. Check 4. Checks 5. Container 6. Datastore 7. Enrichment 8. Field 9. Flow 10. Flow Execution 11. Interface 12. Operations 13. Search 14. Tags 15. Template <p>The table below shows the available shortcuts and the keys you can use to perform actions with your keyboard:</p>"},{"location":"keyboard-shortcuts/#search","title":"Search","text":"<p>The Search Shortcuts allow you to quickly find datastores, containers, fields, and even view the complete list of shortcuts without navigating through menus.</p> Action Windows/Linux macOS Search datastores, containers, and fields Ctrl + \u2191 + F \u2318 + \u2191 + F Search keyboard shortcuts Ctrl + / \u2318 + /"},{"location":"keyboard-shortcuts/#navigation","title":"Navigation","text":"<p>The Navigation Shortcuts make it easy to move between pages, switch tabs, and access core areas of the Qualytics platform efficiently.</p> Action Windows/Linux macOS Go back to previous page G then B G then B Go forward to next page G then N G then N Go to explore G then E G then E Go to flows G then F G then F Go to library G then L G then L Go to settings G then S G then S Go to tags G then T G then T Select next tab Alt + \u2192 \u2325 + \u2192 Select previous tab Alt + \u2190 \u2325 + \u2190"},{"location":"keyboard-shortcuts/#anomaly","title":"Anomaly","text":"<p>The Anomaly Shortcuts provide quick actions for archiving, deleting, or tagging anomalies, helping you manage data issues faster.</p> Action Windows/Linux macOS Archive anomaly Ctrl + E \u2318 + E Assign tags A then T A then T Delete anomaly Ctrl + Del \u2318 + Del"},{"location":"keyboard-shortcuts/#check","title":"Check","text":"<p>The Check Shortcuts give you instant access to actions like editing, archiving, tagging, or favoriting checks directly from the keyboard.</p> Action Windows/Linux macOS Archive check Ctrl + E \u2318 + E Assign tags A then T A then T Delete check Ctrl + Del \u2318 + Del Edit check E E Mark as favorite Alt + F \u2325 + F"},{"location":"keyboard-shortcuts/#checks","title":"Checks","text":"<p>The Checks Shortcuts are focused on creating new checks, whether from scratch or from templates, to speed up your validation process.</p> Action Windows/Linux macOS Add multiple checks from template Ctrl + Shift + C \u2318 + Shift + C Add new check from template Shift + C Shift + C Add new check C C"},{"location":"keyboard-shortcuts/#container","title":"Container","text":"<p>The Container Shortcuts let you manage containers efficiently by assigning tags, editing, deleting, or marking them as favorites.</p> Action Windows/Linux macOS Assign tags A then T A then T Delete container Ctrl + Del \u2318 + Del Mark as favorite Alt + F \u2325 + F Open container settings E E"},{"location":"keyboard-shortcuts/#datastore","title":"Datastore","text":"<p>The Datastore Shortcuts simplify datastore management, allowing you to quickly assign tags, edit, delete, or favorite a datastore.</p> Action Windows/Linux macOS Assign tags A then T A then T Delete datastore Ctrl + Del \u2318 + Del Edit datastore E E Mark as favorite Alt + F \u2325 + F"},{"location":"keyboard-shortcuts/#enrichment","title":"Enrichment","text":"<p>The Enrichment Shortcuts provide fast access to edit or delete enrichment configurations without leaving the keyboard.</p> Action Windows/Linux macOS Delete enrichment Ctrl + Del \u2318 + Del Edit enrichment E E"},{"location":"keyboard-shortcuts/#field","title":"Field","text":"<p>The Field Shortcuts help you manage fields by assigning tags, editing details, or deleting computed fields directly from the keyboard.</p> Action Windows/Linux macOS Assign tags A then T A then T Delete computed field Ctrl + Del \u2318 + Del Edit field E E"},{"location":"keyboard-shortcuts/#flow","title":"Flow","text":"<p>The Flow Shortcuts allow you to clone, delete, execute, or publish flows efficiently, reducing time spent in menus.</p> Action Windows/Linux macOS Clone flow Ctrl + D \u2318 + D Delete flow Ctrl + Del \u2318 + Del Execute flow Ctrl + Enter \u2318 + Enter Publish flow P P"},{"location":"keyboard-shortcuts/#flow-execution","title":"Flow Execution","text":"<p>The Flow Execution Shortcuts are designed for quick deletion of flow execution records to keep your workspace clean.</p> Action Windows/Linux macOS Delete flow execution Ctrl + Del \u2318 + Del"},{"location":"keyboard-shortcuts/#interface","title":"Interface","text":"<p>The Interface Shortcuts let you control the look and feel of the platform, such as collapsing the sidebar or switching themes.</p> Action Windows/Linux macOS Collapse navigation sidebar Ctrl + B \u2318 + B Switch theme Ctrl + Shift + L \u2318 + Shift + L"},{"location":"keyboard-shortcuts/#operations","title":"Operations","text":"<p>The Operations Shortcuts provide fast commands for running catalog, export, materialize, profile, and scan operations on datastores.</p> Action Windows/Linux macOS Run catalog operation R then C R then C Run export operation R then E R then E Run materialize operation R then M R then M Run profile operation R then P R then P Run scan operation R then S R then S"},{"location":"keyboard-shortcuts/#tags","title":"Tags","text":"<p>The Tags Shortcuts let you quickly add new tags to classify and organize your data assets.</p> Action Windows/Linux macOS Add new tag T T"},{"location":"keyboard-shortcuts/#template","title":"Template","text":"<p>The Template Shortcuts cover editing and archiving templates, helping you maintain reusable patterns with ease.</p> Action Windows/Linux macOS Archive template Ctrl + E \u2318 + E Edit template E E"},{"location":"onboarding/","title":"Onboarding","text":"<p>Qualytics is a comprehensive data quality management solution that helps enterprises proactively manage their full data quality lifecycle at scale. Through automated profiling, contextual quality checks, rule inference, anomaly detection, remediation, and tailored notifications, Qualytics transforms how organizations approach data quality.</p> <p>This guide will walk you through getting started with Qualytics, ensuring a smooth and efficient onboarding experience.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"onboarding/#onboarding-process","title":"Onboarding Process","text":"<p>Your Qualytics journey begins with understanding your enterprise's specific requirements. We'll work with you to create a tailored approach based on your data needs and goals.</p>"},{"location":"onboarding/#1-screening-criteria-gathering","title":"1. Screening &amp; Criteria Gathering","text":"<p>Schedule a demo with our team to help us understand your enterprise data needs. During this session, we'll: - Create a detailed plan - Identify key success criteria - Tailor the deployment to your specific requirements - Explore relevant use cases for your business</p>"},{"location":"onboarding/#2-user-invitations","title":"2. User Invitations","text":"<p>Once your deployment setup is complete, we'll send invitations to your team members' email addresses. These invitations include: - Instructions for accessing the platform - Role assignments (admin or member) based on your preferences - Access configuration details</p> <p>Admins receive full platform configuration and management capabilities, while members receive access based on admin-defined permissions.</p>"},{"location":"onboarding/#deployment-options","title":"Deployment Options","text":"<p>Qualytics offers flexible deployment options to seamlessly integrate with your existing data infrastructure:</p>"},{"location":"onboarding/#1-saas-deployment-default","title":"1. SaaS Deployment (Default)","text":"<p>Our Software as a Service (SaaS) deployment provides a fully managed experience hosted by Qualytics. This option offers: - Minimal maintenance requirements - Rapid scalability - Automatic updates and improvements - Focus on data quality rather than infrastructure</p>"},{"location":"onboarding/#2-on-premise-deployment","title":"2. On-Premise Deployment","text":"<p>For organizations that prefer complete control over their data environment, our on-premise deployment option allows you to: - Maintain data within your own data centers - Ensure compliance with internal policies and regulations - Exercise complete control over your data and security</p> <p>Tip</p> <p>This deployment option is recommended for customers with sensitive data</p>"},{"location":"onboarding/#frequent-asked-questions-faqs","title":"Frequent Asked Questions (FAQs)","text":"<p>Q 1: What type of support is provided during a POC?</p> <p>A 1: A dedicated Customer Success Manager, with mandatory weekly check-ins.</p> <p>Q 2: What are the deployment options for POC?</p> <p>A 2: Qualytics offers deployment options for Proof of Concept (POC) primarily as a Software as a Service (SaaS) solution.</p> <p>Q 3: What type of data should we use for a POC?</p> <p>A 3: In most cases, potential customers use their actual data during a POC for the most realistic evaluation. Some customers opt to use cleaned data (removing PII) or sample test data.</p> <p>Q 4: Are there limitations to data size for POC?</p> <p>A 4: There are no limitations to data size for a Proof of Concept (POC).</p> <p>Q 5: What type of support is provided during the Onboarding process?</p> <p>A 5: A dedicated Customer Success Manager, with mandatory weekly check-ins.</p> <p>Q 6: What types of data stacks does Qualytics support?</p> <p>A 6: Qualytics supports both modern solutions and legacy systems:</p> <ul> <li> <p>Modern Solutions</p> <p>Qualytics seamlessly integrates with modern data platforms like Snowflake, Amazon S3, BigQuery, and more to ensure robust data quality management.</p> </li> <li> <p>Legacy Systems</p> <p>We maintain high data quality standards across legacy systems including MySQL, Microsoft SQL Server, and other reliable relational database management systems.</p> </li> </ul> <p>For detailed integration instructions, please refer to the quick start guide.</p> <p>Q 7: What types of database technology can you connect in Qualytics?</p> <p>A 7: Qualytics supports any Apache Spark-compatible datastore, including: - Relational databases (RDBMS) - Raw file formats (CSV, XLSX, JSON, Avro, Parquet)</p> <p>Q 8: What is an enrichment datastore?</p> <p>A 8: An Enrichment Datastore is a user-managed storage location where Qualytics records and accesses metadata through system-defined tables. It's specifically designed to capture metadata generated during profiling and scanning operations.</p> <p>Q 9: Can I download my metadata and data quality checks?</p> <p>A 9: Yes, Qualytics's metadata export feature captures the mutable states of various data entities. You can export Quality Checks, Field Profiles, and Anomalies metadata from selected profiles into your designated enrichment datastore.</p> <p>Q 10: How is the Quality Score calculated?</p> <p>A 10: Quality Scores measure data quality at the field, container, and datastore levels, recorded as a time series to track improvements. Scores range from 0-100, with higher scores indicating better quality.</p> <p>Q 11: What is a catalog operation?</p> <p>A 11: A Catalog Operation scans your datastore to import named collections (tables, views, files). It automatically identifies optimal approaches for: - Incremental scanning - Data partitioning - Record identification</p> <p>Q 12: What is a profiling operation?</p> <p>A 12: A Profile Operation analyzes every available record across all containers in a datastore. Full Profiles deliver 100% fidelity metadata at the cost of maximum compute time.</p> <p>Q 13: What is a scan operation?</p> <p>A 13: The Scan Operation evaluates data quality checks across your datastore's collections, producing: - Record anomalies for individual anomalous values - Shape anomalies for multi-record anomalies - Detailed analysis in your Enrichment Datastore</p>"},{"location":"printing/","title":"Printing This Guide","text":"<p>This guide changes often, so we can't recommend printing it or saving it offline. However, we recognize that there are  circumstances beyond some users' control that make doing so more convenient. Thus, we are providing this link to our  userguide as a single page appropriate for saving as a PDF. If you find this useful, please let us know. </p>"},{"location":"quick-start-guide/","title":"Quick Start Guide","text":"<p>Welcome to Qualytics! This guide will help you quickly get up and running with the platform, from initial setup through your first data quality operations. Whether you're a business user or technical administrator, you'll find everything needed to start managing data quality at scale.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"quick-start-guide/#deployment-access","title":"Deployment Access","text":"<p>Each Qualytics deployment is a single-tenant, dedicated cloud instance, configured to your organization's requirements. Your deployment will be accessible via a custom URL (e.g., <code>https://acme.qualytics.io</code>), with corresponding API documentation at <code>/api/docs</code>.</p>"},{"location":"quick-start-guide/#onboarding-process","title":"Onboarding Process","text":"<p>The Qualytics onboarding process ensures your environment is perfectly tailored to your needs:</p>"},{"location":"quick-start-guide/#1-screening-and-criteria-gathering","title":"1. Screening and Criteria Gathering","text":"<p>Our team works with you to understand your specific needs, including:</p> <ul> <li>Evaluating sample data requirements.</li> <li>Identifying primary success criteria.</li> <li>Exploring relevant use cases for your environment.</li> <li>Determining deployment specifications.</li> </ul>"},{"location":"quick-start-guide/#2-environment-setup","title":"2. Environment Setup","text":"<p>Based on your requirements, we:</p> <ul> <li>Create your custom deployment URL.</li> <li>Configure your preferred cloud provider and region.</li> <li>Set up initial security parameters.</li> <li>Establish integration endpoints.</li> </ul>"},{"location":"quick-start-guide/#3-user-access","title":"3. User Access","text":"<p>Once deployment is complete:</p> <ul> <li>Team members receive email invitations.</li> <li>Roles are assigned based on your specifications.</li> <li>Access credentials are securely distributed.</li> </ul> <p>Tip</p> <p>Please check your spam folder if you don't see the invite.</p> <p>See our onboarding page for a more detailed view of what to expect during onboarding!</p>"},{"location":"quick-start-guide/#signing-in","title":"Signing In","text":"<p>Qualytics supports two authentication methods:</p>"},{"location":"quick-start-guide/#method-1-direct-credentials","title":"Method 1: Direct Credentials","text":"<p>Ideal for:</p> <ul> <li>Initial platform evaluation.</li> <li>Proof of Concept (POC) phases.</li> <li>Environments without SSO integration.</li> </ul> <p></p>"},{"location":"quick-start-guide/#method-2-enterprise-sso","title":"Method 2: Enterprise SSO","text":"<p>For production deployments:</p> <ul> <li>Integrates with your organization's Identity Provider.</li> <li>Supports standard SSO protocols.</li> <li>Provides seamless access management.</li> </ul> <p></p>"},{"location":"quick-start-guide/#getting-started-checklist","title":"Getting Started Checklist","text":"<p>To begin using Qualytics, you'll complete these key steps:</p> <ol> <li>Connect Your First Datastore.</li> <li>Run Initial Profile Operation.</li> <li>Review Generated Quality Checks.</li> <li>Configure Monitoring &amp; Alerts.</li> </ol> <p>Let's walk through each step in detail.</p>"},{"location":"quick-start-guide/#understanding-datastores","title":"Understanding Datastores","text":"<p>In Qualytics, a Datastore represents your data source connection. Qualytics supports any Apache Spark-compatible data source, including:</p>"},{"location":"quick-start-guide/#jdbc-datastores","title":"JDBC Datastores","text":"<ul> <li>Traditional relational databases (RDBMS).</li> <li>Data warehouses.</li> <li>Analytical databases.</li> </ul>"},{"location":"quick-start-guide/#distributed-file-system-dfs-datastores","title":"Distributed File System (DFS) Datastores","text":"<ul> <li>Cloud storage (AWS S3, Azure Blob, GCP).</li> <li>Raw files (CSV, XLSX, JSON, Avro, Parquet).</li> <li>Local file systems.</li> </ul>"},{"location":"quick-start-guide/#connecting-your-first-datastore","title":"Connecting Your First Datastore","text":""},{"location":"quick-start-guide/#adding-a-source-datastore","title":"Adding a Source Datastore","text":"<ol> <li>From the main menu, select \"Add Source Datastore\":</li> </ol> <ol> <li>Select your datastore type.</li> <li>Provide connection details.</li> <li>Test connectivity.</li> <li>Configure an Enrichment Datastore (strongly recommended).</li> </ol> <p>Warning</p> <p>While optional, not configuring an Enrichment Datastore limits platform capabilities.</p>"},{"location":"quick-start-guide/#enrichment-datastores","title":"Enrichment Datastores","text":"<p>An Enrichment Datastore serves as the storage location for:</p> <ul> <li>Anomaly detection results.</li> <li>Metadata and profiling information.</li> <li>Quality check outcomes.</li> <li>Historical analysis data.</li> </ul> <p>You can either:</p> <ol> <li>Configure a new Enrichment Datastore.</li> <li>Select an existing Enrichment Datastore from the dropdown.</li> </ol>"},{"location":"quick-start-guide/#core-operations","title":"Core Operations","text":"<p>After connecting your datastore, three fundamental operations manage data quality:</p>"},{"location":"quick-start-guide/#1-catalog-operation","title":"1. Catalog Operation","text":"<p>The first step in understanding your data:</p> <ul> <li>Systematically collects data structures.</li> <li>Analyzes existing metadata.</li> <li>Prepares for profiling and scanning.</li> <li>Runs automatically on datastore creation.</li> </ul> <p></p>"},{"location":"quick-start-guide/#2-profile-operation","title":"2. Profile Operation","text":"<p>The Profile operation performs deep analysis of your data:</p> <ul> <li>Generates comprehensive metadata.</li> <li>Calculates statistical measures:<ul> <li>Basic metrics (type, min/max, and lengths).</li> <li>Advanced analytics (skewness, kurtosis, and correlations).</li> <li>Value distributions and patterns.</li> </ul> </li> <li>Automatically infers data quality rules.</li> <li>Uses machine learning for pattern detection.</li> </ul> <p></p> <p>Our profiling engine analyzes:</p> <ul> <li>Field types and patterns.</li> <li>Value distributions.</li> <li>Statistical relationships.</li> <li>Data quality patterns.</li> <li>Structural consistency.</li> </ul> <p>The engine uses machine learning to:</p> <ul> <li>Identify column data types.</li> <li>Discover relationships.</li> <li>Generate quality rules.</li> <li>Detect anomaly patterns.</li> </ul>"},{"location":"quick-start-guide/#3-scan-operation","title":"3. Scan Operation","text":"<p>The Scan operation actively monitors data quality:</p> <ul> <li>Asserts all defined quality checks.</li> <li>Identifies anomalies and violations.</li> <li>Records results in the Enrichment Datastore.</li> <li>Generates quality scores.</li> </ul> <p></p> <p>The first scan runs as a \"Full\" scan to establish baselines. After completion, you can review:</p> <ul> <li>Start and finish times.</li> <li>Records processed.</li> <li>Anomalies detected.</li> <li>Quality scores.</li> </ul>"},{"location":"quick-start-guide/#managing-data-quality","title":"Managing Data Quality","text":""},{"location":"quick-start-guide/#quality-checks","title":"Quality Checks","text":"<p>Qualytics uses two types of quality checks:</p>"},{"location":"quick-start-guide/#1-inferred-checks","title":"1. Inferred Checks","text":"<ul> <li>Automatically generated during profiling.</li> <li>Cover 80-90% of common quality rules.</li> <li>Based on statistical analysis and ML.</li> <li>Continuously refined through operation.</li> </ul>"},{"location":"quick-start-guide/#2-authored-checks","title":"2. Authored Checks","text":"<ul> <li>Manually created by users.</li> <li>Support complex business rules.</li> <li>Use Spark SQL or Scala UDFs.</li> <li>Can be templated and shared.</li> </ul>"},{"location":"quick-start-guide/#platform-navigation","title":"Platform Navigation","text":""},{"location":"quick-start-guide/#explore-dashboard","title":"Explore Dashboard","text":"<p>The Explore interface provides comprehensive visibility:</p>"},{"location":"quick-start-guide/#1-insights","title":"1. Insights","text":"<ul> <li>Overview of anomaly detection.</li> <li>Quality monitoring metrics.</li> <li>Filterable by source, tags, and dates.</li> </ul>"},{"location":"quick-start-guide/#2-activity","title":"2. Activity","text":"<ul> <li>Operation history and status.</li> <li>Data volume heatmaps.</li> <li>Anomaly tracking.</li> </ul>"},{"location":"quick-start-guide/#3-profiles","title":"3. Profiles","text":"<p>Unified view of all data assets:</p> <ul> <li>Tables and Views.</li> <li>Computed Assets.</li> <li>Field-level Details.</li> </ul> <p></p>"},{"location":"quick-start-guide/#4-observability","title":"4. Observability","text":"<p>Monitor platform health and performance:</p> <ul> <li>Volume metrics.</li> <li>Quality trends.</li> <li>System health.</li> </ul> <p></p>"},{"location":"quick-start-guide/#5-checks","title":"5. Checks","text":"<p>Unified view of all data quality validations across datastores:</p> <ul> <li>Active, Draft, Favorite, and Archived checks.</li> <li>Filter by Source Datastore, Tags, or Importance.</li> <li>View validation results by table and field, including pass/fail status and anomaly counts.</li> </ul> <p></p>"},{"location":"quick-start-guide/#6-anomalies","title":"6. Anomalies","text":"<p>Centralized view of all detected data issues across datastores:</p> <ul> <li>Filter anomalies by status \u2014 Open, Active, Acknowledged, or Archived.</li> <li>View details including datastore, table, affected fields, rules triggered, and detection date.</li> <li>Track anomaly trends and weights to prioritize investigation and resolution.</li> </ul> <p></p>"},{"location":"quick-start-guide/#configuration-management","title":"Configuration &amp; Management","text":""},{"location":"quick-start-guide/#tags","title":"Tags","text":"<p>Organize and prioritize:</p> <ul> <li>Categorize data assets.</li> <li>Drive notifications.</li> <li>Weight importance.</li> </ul> <p></p>"},{"location":"quick-start-guide/#flows","title":"Flows","text":"<p>Automate and streamline:</p> <ul> <li>Trigger actions based on specific events.</li> <li>Manage workflows efficiently.</li> <li>Monitor and track execution status.</li> </ul> <p></p>"},{"location":"quick-start-guide/#platform-settings","title":"Platform Settings","text":"<p>Access key configuration areas:</p> <ol> <li> <p>Connections</p> <ul> <li>Manage datastores.</li> <li>Configure integrations.</li> </ul> <p></p> </li> <li> <p>Security</p> <ul> <li>User management.</li> <li>Role assignments.</li> </ul> <p></p> </li> <li> <p>Integrations</p> <ul> <li>External tool setup.</li> <li>API configuration.</li> </ul> <p></p> </li> <li> <p>Status</p> <ul> <li>Deployment status.</li> <li>Analytics engine management.</li> </ul> <p></p> </li> </ol>"},{"location":"quick-start-guide/#next-steps","title":"Next Steps","text":"<p>Now that you're familiar with the Qualytics basics, consider:</p> <ol> <li>Setting up additional datastores.</li> <li>Creating custom quality checks.</li> <li>Configuring notifications.</li> <li>Exploring advanced features.</li> </ol> <p>For detailed information on any topic, explore the relevant sections in our documentation.</p>"},{"location":"web-app/","title":"Web Application","text":"<p>Upon signing in to Qualytics, users are greeted with a thoughtfully designed web application that offers intuitive navigation and quick access to essential features and datasets, ensuring an efficient and comprehensive data quality management experience.</p> <p>In this documentation, we will explore every component of the Qualytics web application.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"web-app/#global-search","title":"Global Search","text":"<p>The Global Search feature in Qualytics is designed to streamline the process of finding crucial assets such as Datastores, Containers, and Fields. This enhancement provides quick and precise search results, significantly improving navigation and user interaction. By entering keywords in the search bar located at the top of the dashboard, users can efficiently locate specific data elements, facilitating better data management and access. This functionality is especially useful for large datasets, ensuring users can swiftly find the information they need without navigating through multiple layers of the interface.</p> <p>Tip</p> <p>Press the shortcut key: Ctrl+K for quick access to Global Search.</p> <p></p>"},{"location":"web-app/#in-app-notifications","title":"In-App Notifications","text":"<p>In Qualytics, notifications keep users updated on flow executions in real time. When a flow is triggered, users receive alerts with details like the flow name, status (success or failure), completion time, and actions performed. Clicking on a notification provides more details, including any detected anomalies. These notifications help users monitor workflows efficiently and respond quickly to important updates.</p> <p></p>"},{"location":"web-app/#discover","title":"Discover","text":"<p>The Discover option in Qualytics features a dropdown menu that provides access to various resources and tools to help users navigate and utilize the platform effectively. The menu includes the following options:</p> <p>Resources:</p> <ul> <li> <p>User Guide: Opens the comprehensive user guide for Qualytics, which provides detailed instructions and information on how to use the platform effectively.</p> </li> <li> <p>SparkSQL: Directs users to resources or documentation related to using SparkSQL within the Qualytics platform, aiding in advanced data querying and analysis.</p> </li> </ul> <p>API:</p> <ul> <li> <p>Docs: Opens the API documentation, offering detailed information on how to interact programmatically with the Qualytics platform. This is essential for developers looking to integrate Qualytics with other systems or automate tasks.</p> </li> <li> <p>Playground: Provides access to an interactive environment where users can test and experiment with API calls. This feature is particularly useful for developers who want to understand how the API works and try out different queries before implementing them in their applications.</p> </li> </ul> <p>Support:</p> <ul> <li>Qualytics Helpdesk: Provides users with access to a support environment where they can get assistance with any issues or questions related to the platform.</li> </ul> <p></p>"},{"location":"web-app/#theme","title":"Theme","text":"<p>Qualytics offers both dark mode and light mode to enhance user experience and cater to different preferences and environments.</p> <p>Light Mode:</p> <ul> <li> <p>This is the default visual theme of Qualytics, featuring a light background with dark text.</p> </li> <li> <p>It provides a clean and bright interface, which is ideal for use in well-lit environments.</p> </li> <li> <p>To switch from dark mode to light mode, click the Light Mode button.</p> </li> </ul> <p>Dark Mode:</p> <ul> <li> <p>Dark mode features a dark background with light text, reducing eye strain and glare, especially in low-light environments.</p> </li> <li> <p>It is designed to be easier on the eyes during prolonged usage and can help save battery life on devices.</p> </li> <li> <p>To activate dark mode, click the Dark Mode button.</p> </li> </ul> <p>System Appearance:</p> <ul> <li> <p>The system theme automatically adjusts based on the user\u2019s device settings.</p> </li> <li> <p>When enabled, Qualytics will switch between light and dark mode based on the system preference.</p> </li> <li> <p>It provides a seamless experience by adapting to the user\u2019s environment without manual adjustments.</p> </li> </ul> <p>Tip</p> <p>Users can still manually select dark or light mode if they prefer a fixed theme.</p> <p></p>"},{"location":"web-app/#view-mode","title":"View Mode","text":"<p>In Qualytics, users have the option to switch between two display modes: List View and Card View. These modes are available on the Source Datastore page, Enrichment Datastore page, and Library page, allowing users to choose their preferred method of displaying information.</p> <ul> <li> <p>List View: List View arranges items in a linear, vertical list format. This mode focuses on providing detailed information in a compact and organized manner. To activate List View, click the \"List View\" button (represented by an icon with three horizontal lines) located at the top of the page.</p> </li> <li> <p>Card View: Card View displays items as individual cards arranged in a grid. Each card typically includes a summary of the most important information about the item. To switch to Card View, click the \"Card View\" button (represented by an icon with a grid of squares) located at the top of the page.</p> </li> </ul> <p></p>"},{"location":"web-app/#product-updates","title":"Product Updates","text":"<p>In Qualytics, the Product Updates feature helps users stay up to date with the latest changes. They can see new features, bug fixes, and improvements directly in the app, with links to full release notes for more details.</p> <p></p>"},{"location":"web-app/#user-profile","title":"User Profile","text":"<p>The user profile section in Qualytics provides essential information and settings related to the user's account. Here's an explanation of each element:</p> <ul> <li> <p>Name: Displays the user's email address used as the account identifier.</p> </li> <li> <p>Role: Indicates the user's role within the Qualytics platform (e.g., Admin), which defines their level of access and permissions.</p> </li> <li> <p>Teams: Shows the teams to which the user belongs (e.g., Public), helping organize users and manage permissions based on group membership.</p> </li> <li> <p>Preview Features: A toggle switch that enables or disables preview features. When turned on, it adds an AI Readiness Benchmark for the Quality Score specifically on the Explore page.</p> </li> <li> <p>Logout: A button that logs the user out of their Qualytics account, ending the current session and returning them to the login page.</p> </li> <li> <p>Version: Displays the current version of the Qualytics platform being used, which is helpful for troubleshooting and ensuring compatibility with other tools and features.</p> </li> </ul> <p></p>"},{"location":"web-app/#navigation-menu-left-sidebar","title":"Navigation Menu (Left Sidebar)","text":"<p>The left sidebar of the app displays the primary navigation menu, which allows users to quickly access various functionalities of the Qualytics platform. The menu items include:</p>"},{"location":"web-app/#source-datastores-default-view","title":"Source Datastores (Default View)","text":"<p>Lists all the source datastores connected to Qualytics in the left sidebar. It also provides the option to:</p> <ul> <li> <p>Add a new source datastore.</p> </li> <li> <p>Search from existing source datastores.</p> </li> <li> <p>Sort existing datastores based on the name, records, checks, etc.</p> </li> <li> <p>Filter source datastores.</p> </li> </ul> <p></p>"},{"location":"web-app/#enrichment-datastores","title":"Enrichment Datastores","text":"<p>Lists all the enrichment datastores connected to Qualytics in the left sidebar. It also provides options to:</p> <ul> <li>Add an enrichment datastore.</li> <li>Search from existing enrichment datastores.</li> <li>Sort existing datastores based on the name, records, checks, etc.</li> </ul> <p></p>"},{"location":"web-app/#explore","title":"Explore","text":"<p>The Explore section in Qualytics enables effective data management and analysis through several key sections:</p> <ul> <li> <p>Insights: Offers an overview of anomaly detection and data monitoring, allowing users to filter by source datastores, tags, and dates. It displays profile data, applied checks, quality scores, records scanned, and more. Moreover, you can also export the insight reports in PDF format.  </p> </li> <li> <p>Activity: Provides a detailed view of operations (catalog, profile, and scan) across source datastores with a heatmap to visualize daily activities and detected anomalies.</p> </li> <li> <p>Profiles: Unifies all containers, including tables, views, computed tables, computed files, and fields, with search, sort, and filter functionalities.</p> </li> <li> <p>Observability: Observability gives users an easy way to track changes in data volume over time. It introduces two types of checks: Volumetric and Metric.</p> </li> <li> <p>Checks: Shows all applied checks, both inferred and authored, across source datastores to monitor and manage data quality rules.</p> </li> <li> <p>Anomalies: Lists all detected anomalies across source datastores for quick identification and resolution of issues.</p> </li> </ul> <p></p>"},{"location":"web-app/#library","title":"Library","text":"<p>The library section allows for managing check templates and editing applied checks in source datastores with two main functionalities:</p> <ul> <li> <p>Add Check Templates: Easily add new templates to apply standardized checks across datastores.</p> </li> <li> <p>Export Check Templates: Export template metadata to a specified Enrichment datastore.</p> </li> </ul> <p>Tip</p> <p>You can also search, sort, and filter checks across the source datastores.</p> <p></p>"},{"location":"web-app/#tags","title":"Tags","text":"<p>Tags help users organize and prioritize data assets by categorizing them. They can be applied to Datastores, Profiles, Fields, Checks, and Anomalies, improving data management and workflows.</p> <p></p>"},{"location":"web-app/#flows","title":"Flows","text":"<p>Qualytics allows users to set up flows, enabling them to create pipelines by chaining actions and configuring how they are triggered. Triggers can be set based on predefined events and filters, providing a flexible and efficient way to automate processes. These actions can include notifications or operations, allowing users to notify various channels or execute tasks based on specific operations.</p> <p></p>"},{"location":"web-app/#global-settings","title":"Global Settings","text":"<p>Manage global configurations with the following options:</p> <ul> <li> <p>Connection: Manage datastore sources (add, edit, delete).</p> </li> <li> <p>Integration: Configure parameters for integrating external tools.</p> </li> <li> <p>Security: Manage teams, roles, and user access.</p> </li> <li> <p>Tokens: Create tokens for secure API interactions.</p> </li> <li> <p>Status: Monitor and restart the Qualytics deployment.</p> </li> </ul> <p></p>"},{"location":"add-datastores/amazon-s3/","title":"Amazon S3","text":"<p>Adding and configuring an Amazon S3 connection within Qualytics empowers the platform to build a symbolic link with your file system to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Amazon S3 as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Amazon S3 environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/amazon-s3/#amazon-s3-setup-guide","title":"Amazon S3 Setup Guide","text":"<p>This section provides a simple walkthrough for setting up Amazon S3, including retrieving URIs. It also explains how to retrieve the Access Key and Secret Key to configure datastore permissions.</p> <p>By following the Amazon S3 setup process, you will ensure secure and efficient access to your stored data, allowing seamless datastore integration and proper access management in Qualytics.</p>"},{"location":"add-datastores/amazon-s3/#retrieve-the-uri","title":"Retrieve the URI","text":"<p>The S3 URI is the unique resource identifier within the context of the S3 protocol. They follow this naming convention: <code>S3://bucket-name/key-name</code></p> <p>To retrieve the URL of an S3 object via the AWS Console, follow these steps:</p> <ol> <li>Navigate to the AWS S3 console and click on your bucket's name (use the search input to find the object if necessary).</li> <li>Click on the checkbox next to the object's name</li> <li>Click on the Copy S3 URI button</li> </ol> <p></p>"},{"location":"add-datastores/amazon-s3/#retrieve-the-access-key-and-secret-key","title":"Retrieve the Access Key and Secret Key","text":"<p>The access keys are long-term credentials for an IAM user or the AWS account root user. You can use these keys to sign programmatic requests to the AWS CLI or AWS API (directly or using the AWS SDK).</p> <p>To retrieve the Access Key and Secret Access Key, follow these steps:</p> <ol> <li>Open the IAM console.</li> <li>From the navigation menu, click on the Users.</li> <li>Select your IAM user name.</li> <li>Click on the User Actions, and then click on the Manage Access Keys.</li> <li>Click on the Create Access Key.</li> <li>Your keys will look something like this:<ol> <li>Access key ID example: <code>AKIAIOSFODNN7EXAMPLE</code>.</li> <li>Secret access key example: <code>wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</code>.</li> </ol> </li> <li>Click on the Download Credentials, and store the keys in a secure location.</li> </ol> <p>Warning</p> <p>Your Secret Access Key will be visible only once at the time of creation. Please ensure you copy and securely store it for future use.</p>"},{"location":"add-datastores/amazon-s3/#datastore-privileges","title":"Datastore Privileges","text":"<p>If you are using a private bucket, authentication is required for the connection.</p>"},{"location":"add-datastores/amazon-s3/#source-datastore-permissions-read-only","title":"Source Datastore Permissions (Read-Only)","text":"<p>To create a policy, follow these steps:</p> <ol> <li>Open the IAM console.</li> <li>Navigate to Policies in the IAM dashboard and select Create Policy.</li> <li>Go to the JSON tab and paste the provided JSON into the Policy editor.</li> </ol> <p>Tip</p> <p>Ensure you replace <code>&lt;bucket/path&gt;</code> with your specific resource.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:ListBucketMultipartUploads\",\n                \"s3:Get*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket&gt;/*\",\n                \"arn:aws:s3:::&lt;bucket&gt;\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Warning</p> <p>Currently, object-level permissions alone are insufficient to authenticate the connection. Please ensure you also include bucket-level permissions as demonstrated in the example above.</p>"},{"location":"add-datastores/amazon-s3/#enrichment-datastore-permissions-read-write","title":"Enrichment Datastore Permissions (Read-Write)","text":"<p>To create a policy, follow these steps:</p> <ol> <li>Open the IAM console.</li> <li>Navigate to Policies in the IAM dashboard and select Create Policy.</li> <li>Go to the JSON tab and paste the provided JSON into the Policy editor.</li> </ol> <p>Tip</p> <p>Ensure you replace <code>&lt;bucket/path&gt;</code> with your specific resource.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:Get*\",\n                \"s3:ListBucket\",\n                \"s3:ListBucketMultipartUploads\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\",\n                \"s3:AbortMultipartUpload\",\n                \"s3:ListMultipartUploadParts\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket&gt;/*\",\n                \"arn:aws:s3:::&lt;bucket&gt;\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Warning</p> <p>Currently, object-level permissions alone are insufficient to authenticate the connection. Please ensure you also include bucket-level permissions as demonstrated in the example above.</p>"},{"location":"add-datastores/amazon-s3/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect and access data from external sources. Amazon S3 is an example of a source datastore, specifically a type of Distributed File System (DFS) datastore that is designed to handle data stored in distributed file systems. Configuring a DFS datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore (e.g., The specified name will appear on the datastore cards.). 2. Toggle Button Toggle ON to create a new source datastore from scratch or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select Amazon S3 from the dropdown list."},{"location":"add-datastores/amazon-s3/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Amazon S3 connector from the dropdown list and add connection details such as Secrets Management, URI, access key, secret key, root path, and teams.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. URI (Required) Enter the Uniform Resource Identifier (URI) of Amazon S3. 2. Access Key (Required) Input the access key provided for secure access. 3. Secret Key (Required) Input the secret key associated with the access key for secure authentication. 4. Root Path (Required) Specify the root path where the data is stored. 5. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 6. Initiate Cataloging (Optional) Check the checkbox to automatically perform a catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/amazon-s3/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Root Path, Teams, and initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to check and verify the source data connection. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/amazon-s3/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). This datastore is used to store the analyzed results, including any anomalies and additional metadata in files. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/amazon-s3/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> REF. FIELDS ACTIONS 1. URI (Required) Enter the Uniform Resource Identifier (URI) for the Amazon S3. 2. Access Key (Required) Input the access key provided for secure access. 3. Secret Key (Required) Input the secret key associated with the access key for secure authentication. 4. Root Path (Required) Specify the root path where the data is stored. 5. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 5: Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/amazon-s3/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use Enrichment Datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example - Marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>URI: The Uniform Resource Identifier (URI) points to the specific location of the source data and should be formatted accordingly (e.g., <code>s3://bucket-name</code> for Amazon S3).</p> </li> <li> <p>Root Path: Specify the root path where the data is stored. This path defines the base directory or folder from which all data operations will be performed.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/amazon-s3/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/amazon-s3/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating the Amazon S3 datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"trigger_catalog\": true,\n    \"root_path\": \"/s3_root_path\",\n    \"enrich_only\": false,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"s3\",\n        \"uri\": \"s3://&lt;bucket_name&gt;\",\n        \"access_key\": \"s3_access_key\",\n        \"secret_key\": \"s3_secret_key\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"trigger_catalog\": true,\n    \"root_path\": \"/s3_root_path\",\n    \"enrich_only\": false,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/amazon-s3/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"trigger_catalog\": true,\n    \"root_path\": \"/s3_root_path\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"s3\",\n        \"uri\": \"s3://&lt;bucket_name&gt;\",\n        \"access_key\": \"s3_access_key\",\n        \"secret_key\": \"s3_secret_key\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"trigger_catalog\": true,\n    \"root_path\": \"/s3_root_path\",\n    \"enrich_only\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/amazon-s3/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore:</p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/athena/","title":"Athena","text":"<p>Adding and configuring an Amazon Athena connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.  </p> <p>This documentation provides a step-by-step guide on adding Athena as a source datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Athena environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.  </p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/athena/#add-the-source-datastore","title":"Add the Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Athena is an example of such a datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the Athena datastore allows the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name Specify the name of the datastore (e.g., The specified name will appear on the datastore cards). 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector Select Athena from the dropdown list."},{"location":"add-datastores/athena/#option-i-create-a-datastore-with-a-new-connection","title":"Option I: Create a Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Athena connector from the dropdown list and add connection properties such as Secrets Management, host, port, username, and password, along with datastore properties like catalog, database, etc.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get Hostname from your Athena account and add it to this field. 2. Port Specify the Port number. 3. User Enter the User ID to connect. 4. Password Enter the password to connect to the database. 5. S3 Output Location Define the S3 bucket location where the output will be stored. This is specific to AWS Athena and specifies where query results are saved. 6. Catalog Enter the catalog name. In AWS Athena, this refers to the data catalog that contains database and table metadata. 7. Database Specify the database name. 8. Teams Select one or more teams from the dropdown to associate with this source datastore. 9. Initiate Cataloging Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified. </p>"},{"location":"add-datastores/athena/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials. </p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Catalog, Database, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to check and verify the source data connection. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>Click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/athena/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>After successfully testing and verifying your source datastore connection, you have the option to add an enrichment datastore (recommended). This datastore is used to store analyzed results, including any anomalies and additional metadata in tables. This setup provides comprehensive visibility into your data quality, enabling you to manage and improve it effectively.  </p> <p>Warning</p> <p>Qualytics does not support the Athena connector as an enrichment datastore, but you can point to a different enrichment datastore.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/athena/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for Add New Connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p>Note</p> <p>Qualytics does not support Athena as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using BigQuery as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p></p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection.</p> <p></p> <p>If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a success notification appears on the screen indicating that the datastore was added successfully.</p> <p>Step 5: Close the success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/athena/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Use an existing enrichment datastore is turned on, you will be prompted to configure the enrichment datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p>Note</p> <p>Qualytics does not support Athena as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using BigQuery as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example- Marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment.</p> </li> <li> <p>Database:  Refers to the specific database within the enrichment datastore environment where the data is stored.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a success notification appears on the screen indicating that the datastore was added successfully. </p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/athena/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"add-datastores/athena/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides a sample payload for creating an Athena datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint (Post): <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"athena_catalog\",\n    \"schema\": \"athena_database\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"host\": \"athena_host\",\n        \"port\": 443,\n        \"username\": \"athena_user\",\n        \"password\": \"athena_password\",\n        \"parameters\": { \"output\": \"s3://&lt;bucket_name&gt;\" },\n        \"type\": \"athena\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"athena_catalog\",\n    \"schema\": \"athena_database\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": connection_id\n}\n</code></pre>"},{"location":"add-datastores/athena/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/azure-datalake-storage/","title":"Azure Datalake Storage","text":"<p>Adding and configuring an Azure Datalake Storage connection within Qualytics empowers the platform to build a symbolic link with your file system to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Azure Datalake Storage as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Azure Datalake Storage environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/azure-datalake-storage/#azure-datalake-storage-setup-guide","title":"Azure Datalake Storage Setup Guide","text":"<p>This setup guide details the process for retrieving the Account Name and Access Key of your Azure Datalake Storage account, essential for seamless configuration in Qualytics.</p>"},{"location":"add-datastores/azure-datalake-storage/#azure-datalake-storage-uri","title":"Azure Datalake Storage URI","text":"<p>The Uniform Resource Identifier (URI) for Azure Datalake Storage is structured to uniquely identify resources within your storage account. The format of the URI is as follows:</p> <pre><code>abfs[s]://&lt;file_system&gt;@&lt;account_name&gt;.dfs.core.windows.net/&lt;path&gt;\n</code></pre> <ul> <li><code>abfs[s]</code>: The <code>abfs</code> or <code>abfss</code> protocol is used as the scheme identifier.</li> <li><code>\\&lt;file_system&gt;</code>: The parent location that holds the files and folders. This is similar to containers in the Azure Storage Blobs service.</li> <li><code>&lt;account-name&gt;</code>: The name assigned to your storage account during creation.</li> <li><code>&lt;path&gt;</code>: A forward slash delimited (/) representation of the directory structure.</li> </ul>"},{"location":"add-datastores/azure-datalake-storage/#retrieve-the-account-name-and-access-key","title":"Retrieve the Account Name and Access Key","text":"<p>To configure Azure Datalake Storage Datastore in Qualytics, you need the account name and access key. Follow these steps to retrieve them:</p> <ol> <li> <p>To get the <code>account_name</code> and <code>access_key</code> you need to access your local storage in Azure.</p> </li> <li> <p>Click on Access Keys tab and copy the values.</p> </li> </ol> <p></p> <p>Tip</p> <p>Refer to the Azure Datalake Storage documentation for more information on how to retrieve the account name and access key of your storage account.</p>"},{"location":"add-datastores/azure-datalake-storage/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect and access data from external sources. Azure Datalake Storage is an example of a source datastore, specifically a type of Distributed File System (DFS) datastore that is designed to handle data stored in distributed file systems. Configuring a DFS datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore. (e.g., The specified name will appear on the datastore cards.) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select Azure Datalake Storage from the dropdown list."},{"location":"add-datastores/azure-datalake-storage/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Azure Datalake Storage connector from the dropdown list and add connection details such as Secrets Management, URI, account name, access key, root path, and teams.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF FIELDS ACTIONS 1. URI (Required) Enter the Uniform Resource Identifier (URI) of the Azure Datalake Storage. 2. Account Name (Required) Input the account name to access the Azure Datalake Storage. 3. Access Key (Required) Input the access key provided for secure access. 4. Root Path (Required) Specify the root path where the data is stored. 5. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 6. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/azure-datalake-storage/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Root Path, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/azure-datalake-storage/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). This datastore is used to store the analyzed results, including any anomalies and additional metadata in files. This setup provides full visibility into your data quality, helping you manage and improve it effectively. </p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2:  A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/azure-datalake-storage/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window - Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for Add New Connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> REF FIELDS ACTIONS 1. URI (Required) Enter the Uniform Resource Identifier (URI) of the Azure Datalake Storage. 2. Account Name (Required) Input the account name to access the Azure Datalake Storage. 3. Access Key (Required) Input the access key provided for secure access. 4. Root Path (Required) Specify the root path where the data is stored. 5. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p></p> <p>Step 5: Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/azure-datalake-storage/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Use an existing enrichment datastore is turned on, you will be prompted to configure the enrichment datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example - Marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>URI: Uniform Resource Identifier (URI) points to the specific location of the source data and should be formatted accordingly (e.g., <code>abfss://storage-url</code> for Azure Datalake Storage).</p> </li> <li> <p>Root Path: Specify the root path where the data is stored. This path defines the base directory or folder from which all data operations will be performed.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p></p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/azure-datalake-storage/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/azure-datalake-storage/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating the Azure Datalake Storage datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{  \n    \"name\": \"your\\_datastore\\_name\",  \n    \"teams\": \\[\"Public\"\\],  \n    \"trigger_catalog\": true,  \n    \"root_path\": \"/azure\\_root\\_path\",  \n    \"enrich_only\": false,  \n    \"connection\": {  \n        \"name\": \"your\\_connection\\_name\",  \n        \"type\": \"abfs\",  \n        \"uri\": \"abfs://&lt;container&gt;@&lt;account_name&gt;.dfs.core.windows.net\",  \n        \"access_key\": \"azure\\_account\\_name\",  \n        \"secret_key\": \"azure\\_access\\_key\" \n    }  \n}\n</code></pre> <pre><code>{  \n    \"name\": \"your\\_datastore\\_name\",  \n    \"teams\": \\[\"Public\"\\],  \n    \"trigger_catalog\": true,  \n    \"root_path\": \"/azure\\_root\\_path\",  \n    \"enrich_only\": false,  \n    \"connection_id\": connection-id  \n}\n</code></pre>"},{"location":"add-datastores/azure-datalake-storage/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an Existing Connection <pre><code>{\n    \"name\": \"your\\_datastore\\_name\",  \n    \"teams\": \\[\"Public\"\\],  \n    \"trigger_catalog\": true,  \n    \"root_path\": \"/azure\\_root\\_path\",  \n    \"enrich_only\": true,  \n    \"connection\": {  \n        \"name\": \"your\\_connection\\_name\",  \n        \"type\": \"abfs\",  \n        \"uri\": \"abfs://&lt;container&gt;@&lt;account_name&gt;.dfs.core.windows.net\",  \n        \"access_key\": \"azure\\_account\\_name\",  \n        \"secret_key\": \"azure\\_access\\_key\"  \n    }  \n}\n</code></pre> <pre><code>{\n    \"name\": \"your\\_datastore\\_name\",  \n    \"teams\": \\[\"Public\"\\],  \n    \"trigger_catalog\": true,  \n    \"root_path\": \"/azure\\_root\\_path\",  \n    \"enrich_only\": true,  \n    \"connection_id\": connection-id  \n}\n</code></pre>"},{"location":"add-datastores/azure-datalake-storage/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore:</p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/bigquery/","title":"BigQuery","text":"<p>Adding and configuring a BigQuery connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.  </p> <p>This documentation provides a step-by-step guide on adding BigQuery as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their BigQuery environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"add-datastores/bigquery/#bigquery-setup-guide","title":"BigQuery Setup Guide","text":"<p>This guide explains how to create and use a temporary dataset with an expiration time in BigQuery. This dataset helps manage intermediate query results and temporary tables when using the Google BigQuery JDBC driver.</p> <p>It is recommended for efficient data management, performance optimization, and automatic reduction of storage costs by deleting data when it is no longer needed.</p>"},{"location":"add-datastores/bigquery/#access-the-bigquery-console","title":"Access the BigQuery Console","text":"<p>Step 1: Navigate to the BigQuery console within your Google Cloud Platform (GCP) account.</p> <p></p> <p>Step 2: Click on the vertical ellipsis, it will open a popup menu for creating a dataset. Click on the Create dataset to set up a new dataset.</p> <p></p> <p>Step 3: Fill details for the following fields to create a new dataset.</p> <p>Info</p> <ul> <li>Dataset Location: Select the location that aligns with where your other datasets reside to minimize data transfer delays.</li> <li>Default Table Expiration: Set the expiration to <code>1 day</code> to ensure any table created in this dataset is automatically deleted one day after its creation.</li> </ul> <p></p> <p>Step 4: Click the Create Dataset button to apply the configuration and create the dataset.</p> <p></p> <p>Step 5: Navigate to the created dataset and find the Dataset ID in the Dataset Info.</p> <p></p> <p>The Dataset info section contains the Dataset ID and other information related to the created dataset. This generated Dataset ID is used to configure the BigQuery datastore.</p>"},{"location":"add-datastores/bigquery/#bigquery-roles-and-permissions","title":"BigQuery Roles and Permissions","text":"<p>This section explains the roles required for viewing, editing, and running jobs in BigQuery. To integrate BigQuery with Qualytics, you need specific roles and permissions.</p> <p>Assigning these roles ensures Qualytics can perform data discovery, management, and analytics tasks efficiently while maintaining security and access control.</p>"},{"location":"add-datastores/bigquery/#bigquery-roles","title":"BigQuery Roles","text":"<ul> <li> <p>BigQuery Data Editor (<code>roles/bigquery.dataEditor</code>)     Allows modification of data within BigQuery, including adding new tables and changing table schemas. It is suitable if you want to regularly update or insert data.</p> </li> <li> <p>BigQuery Data Viewer (<code>roles/bigquery.dataViewer</code>)     Enables viewing datasets, tables, and the contents. It is essential if you need to read data structures and information.</p> </li> <li> <p>BigQuery Job User (<code>roles/bigquery.jobUser</code>)     Allows creating and managing jobs in BigQuery, such as queries, data imports, and data exports. It is important if you want to run automated queries.</p> </li> <li> <p>BigQuery Read Session User (<code>roles/bigquery.readSessionUser</code>)     Allows usage of the BigQuery Storage API for efficient retrieval of large data volumes. It provides capabilities to create and manage read sessions, facilitating large-scale data transfers.</p> </li> </ul> <p>Warning</p> <p>If a temporary dataset already exists in BigQuery and users want to use it when creating a new datastore connection, the service account must have the <code>bigquery.tables.create</code> permission to perform the test connection and proceed to the datastore creation.</p>"},{"location":"add-datastores/bigquery/#datastore-bigquery-privileges","title":"Datastore BigQuery Privileges","text":"<p>The following table outlines the privileges associated with BigQuery roles when configuring datastore connections in Qualytics:</p>"},{"location":"add-datastores/bigquery/#source-datastore-permissions-read-only","title":"Source Datastore Permissions (Read-Only)","text":"<p>Provides read access to view table data and metadata:</p> REF. READ-ONLY PERMISSIONS DESCRIPTION 1. <code>roles/bigquery.dataViewer</code> Allows viewing of datasets, tables, and their data. 2. <code>roles/bigquery.jobUser</code> Enables running of jobs such as queries and data loading. 3. <code>roles/bigquery.readSessionUser</code> Facilitates the creation of read sessions for efficient data retrieval."},{"location":"add-datastores/bigquery/#enrichment-datastore-permissions-read-write","title":"Enrichment Datastore Permissions (Read-Write)","text":"<p>Grants read and write access for data editing and management:</p> REF. WRITE-ONLY PERMISSIONS DESCRIPTION 1. <code>roles/bigquery.dataEditor</code> Provides editing permissions for table data and schemas. 2. <code>roles/bigquery.dataViewer</code> Allows viewing of datasets, tables, and their data. 3. <code>roles/bigquery.jobUser</code> Enables running of jobs such as queries and data loading. 4. <code>roles/bigquery.readSessionUser</code> Facilitates the creation of read sessions for efficient data retrieval."},{"location":"add-datastores/bigquery/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. BigQuery is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore (e.g. The specified name will appear on the datastore cards). 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select BigQuery from the dropdown list."},{"location":"add-datastores/bigquery/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the BigQuery connector from the dropdown list and add connection details such as temp dataset ID, service account key, project ID, and dataset ID.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use <code>${key}</code> in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Temp Dataset ID (Optional) Enter a temporary Dataset ID for intermediate data storage during BigQuery operations. 2. Service Account Key (Required) Upload a JSON file that contains the credentials required for accessing BigQuery. 3. Project ID (Required) Enter the Project ID associated with BigQuery. 4. Dataset ID (Required) Enter the Dataset ID (schema name) associated with BigQuery. 5. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 6. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/bigquery/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New Connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Project ID, Dataset ID, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/bigquery/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata for your selected datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/bigquery/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for Add New Connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>Once the HashiCorp Vault is set up, use the <code>${key}</code> format in Connection form to reference a Vault secret.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 3: The configuration form, requesting credential details after selecting enrichment datastore connector.</p> <p></p> REF. FIELD ACTIONS 1. Temp Dataset ID (Optional) Enter a temporary Dataset ID for intermediate data storage during BigQuery operations. 2. Service Account Key (Required) Upload a JSON file that contains the credentials required for accessing BigQuery. 3. Project ID (Required) Enter the Project ID associated with BigQuery. 4. Dataset ID (Required) Enter the Dataset ID (schema name) associated with BigQuery. 5. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. <p>Step 4: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the enrichment datastore has been successfully verified.</p> <p></p> <p>Step 5: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a success notification appears on the screen indicating that the datastore was added successfully.</p> <p>Step 6: Close the success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/bigquery/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use Enrichment Datastore option is selected from the dropdown menu, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example- Marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>Host: This is the server address where the BigQuery instance is hosted. It is the endpoint used to connect to the BigQuery environment.</p> </li> <li> <p>Database: Refers to the specific database within the BigQuery environment where the data is stored.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a success notification appears on the screen indicating that the datastore was added successfully.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/bigquery/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/bigquery/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a BigQuery datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"your_project_id\",\n    \"schema\": \"your_dataset_id\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"bigquery\",\n        \"password\": \"your_service_account_key\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"your_project_id\",\n    \"schema\": \"your_dataset_id\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/bigquery/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an Existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"your_project_id\",\n    \"schema\": \"your_enrichment_dataset_id\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"bigquery\",\n        \"password\": \"your_service_account_key\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"your_project_id\",\n    \"schema\": \"your_enrichment_dataset_id\",\n    \"enrich_only\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/bigquery/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore: </p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/databricks/","title":"Databricks","text":"<p>Adding and configuring a Databricks connection within Qualytics empowers the platform to build a symbolic link with your database to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.  </p> <p>This documentation provides a step-by-step guide on how to add Databricks as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Databricks environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"add-datastores/databricks/#databricks-setup-guide","title":"Databricks Setup Guide","text":"<p>This guide provides a comprehensive walkthrough for setting up Databricks. It highlights the distinction between SQL Warehouses and All-Purpose Compute, the functionality of node pools, and the enhancements they offer.</p> <p>Additionally, it details the process for attaching compute resources to node pools and explains the minimum requirements for effective operation.</p>"},{"location":"add-datastores/databricks/#understanding-sql-warehouses-and-all-purpose-compute","title":"Understanding SQL Warehouses and All-Purpose Compute","text":""},{"location":"add-datastores/databricks/#sql-warehouses-serverless","title":"SQL Warehouses (Serverless)","text":"<p>SQL Warehouses (Serverless) in Databricks utilize serverless SQL endpoints for running SQL queries.</p> REF. ATTRIBUTE DESCRIPTION 1. Cost-effectiveness Serverless SQL endpoints allow you to pay only for the queries you execute, without the need to provision or manage dedicated infrastructure, making it more cost-effective for ad-hoc or sporadic queries. 2. Scalability Serverless architectures automatically scale resources based on demand, ensuring optimal performance for varying workloads. 3. Simplified Management With serverless SQL endpoints, you don't need to manage clusters or infrastructure, reducing operational overhead. 4. Minimum Requirements The minimum requirements for using SQL Warehouse with serverless typically include access to a Databricks workspace and appropriate permissions to create and run SQL queries."},{"location":"add-datastores/databricks/#all-purpose-compute","title":"All-Purpose Compute","text":"<p>All-purpose compute in Databricks refers to clusters that are not optimized for specific tasks. While they offer flexibility, they may not provide the best performance or cost-effectiveness for certain workloads.</p> REF. ATTRIBUTE DESCRIPTION 1. Slow Spin-up Time All-purpose compute clusters may take longer to spin up compared to specialized clusters, resulting in delays before processing can begin. 2. Timeout Connections Due to longer spin-up times, there's a risk of timeout connections, especially for applications or services that expect quick responses."},{"location":"add-datastores/databricks/#node-pool-and-its-usage","title":"Node Pool and Its Usage","text":"<p>A node pool in Databricks is a set of homogeneous virtual machines (VMs) within a cluster. It allows you to have a fixed set of instances dedicated to specific tasks, ensuring consistent performance and resource isolation.</p> REF. ATTRIBUTE DESCRIPTION 1. Resource Isolation Node pools provide resource isolation, allowing different workloads or applications to run without impacting each other's performance. 2. Optimized Performance By dedicating specific nodes to particular tasks, you can optimize performance for those workloads. 3. Cost-effectiveness Node pools can be more cost-effective than using all-purpose compute for certain workloads, as you can scale resources according to the specific requirements of each task."},{"location":"add-datastores/databricks/#improving-all-purpose-compute-with-node-pools","title":"Improving All-Purpose Compute with Node Pools","text":"<p>To improve the performance of all-purpose compute using node pools, you can follow these steps:</p> REF. ATTRIBUTE DESCRIPTION 1. Define Workload-Specific Node Pools Identify the specific tasks or workloads that require optimized performance and create dedicated node pools for them. 2. Specify Minimum Requirements Determine the minimum resources (such as CPU, memory, and disk) required for each workload and configure the node pools accordingly. 3. Monitor and Adjust Continuously monitor the performance of your node pools and adjust resource allocations as needed to ensure optimal performance. <p>Step 1: Configure details for Qualytics Node Pool.</p> <p></p> <p>Step 2: Attach Compute details with the Node Pool.</p> <p></p>"},{"location":"add-datastores/databricks/#retrieve-the-connection-details","title":"Retrieve the Connection Details","text":"<p>This section explains how to retrieve the connection details that you need to connect to Databricks.</p>"},{"location":"add-datastores/databricks/#credentials-to-connect-with-qualytics","title":"Credentials to Connect with Qualytics","text":"<p>To configure Databricks, you need the following credentials:</p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your Databricks account and add it to this field. 2. HTTP Path (Required) Add HTTP Path (web address) to fetch data from your Databricks account. 3. Catalog (Required) Add a Catalog to fetch data structures and metadata from Databricks. 4. Database (Required) Specify the database name to be accessed. 5. Personal Access Token (Required) Generate a Personal Access Token from your Databricks account and add it for authentication."},{"location":"add-datastores/databricks/#get-connection-details-for-the-sql-warehouse","title":"Get Connection Details for the SQL Warehouse","text":"<p>Follow the given steps to get the connection details for the SQL warehouse:</p> <ol> <li>Click on the SQL Warehouses in the sidebar.</li> <li>Choose a warehouse to connect to.</li> <li>Navigate to the Connection Details tab.</li> <li>Copy the connection details.</li> </ol> <p></p>"},{"location":"add-datastores/databricks/#get-connection-details-for-the-cluster","title":"Get Connection Details for the Cluster","text":"<p>Follow the given steps to get the connection details for the cluster:</p> <ol> <li>Click on the Compute in the sidebar.</li> <li>Choose a cluster to connect to.</li> <li>Navigate to the Advanced Options.</li> <li>Click on the JDBC/ODBC tab.</li> <li>Copy the connection details.</li> </ol> <p></p>"},{"location":"add-datastores/databricks/#get-the-access-token","title":"Get the Access Token","text":"<p>Step 1: In your Databricks workspace, click your Databricks username in the top bar, and then select User Settings from the dropdown menu.</p> <p></p> <p>Note</p> <p>Refer to the Databricks Official Docs to generate the Access Token.</p> <p>Step 2: In the Settings page, select the Developer option in the User section.</p> <p></p> <p>Step 3: In the Developer page, click on Manage in Access Tokens.</p> <p></p> <p>Step 4: In the Access Tokens page, click on the Generate new token button.</p> <p></p> <p>Step 5: You will see a modal to add a description and validation time (in days) for the token.</p> <p></p> <p>Step 6: After adding the contents, click on Generate, and it will show the token.</p> <p></p> <p>Warning</p> <p>Before closing the modal window by clicking on the Done button, ensure the Personal Access Token is saved to a secure location.</p> <p>Step 7: You can see the new token on the Access Tokens page.</p> <p></p> <p>You can also revoke a token on the Access Tokens page by clicking on the Revoke token button.</p> <p></p>"},{"location":"add-datastores/databricks/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Databricks is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the datastore name (e.g., This name will appear on the datastore cards). 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select Databricks from the dropdown list."},{"location":"add-datastores/databricks/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Databricks connector from the dropdown list and add connection details such as Secrets Management, host, HTTP path, database, and personal access token.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use <code>${key}</code> in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELD ACTIONS 1. Host (Required) Get the hostname from your Databricks account and add it to this field. 2. HTTP Path (Required) Add the HTTP Path (web address) to fetch data from your Databricks account. 3. Personal Access Token (Required) Generate a Personal Access Token from your Databricks account and add it for authentication. 4. Catalog (Required) Add a Catalog to fetch data structures and metadata from the Databricks. 5. Database (Optional) Specify the database name to be accessed. 6. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 7. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/databricks/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New Connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Catalog, Database, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/databricks/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down button to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/databricks/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for Add New Connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>Once the HashiCorp Vault is set up, use the <code>${key}</code> format in Connection form to reference a Vault secret.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 3: The configuration form, requesting credential details after selecting enrichment datastore connector.</p> <p></p> REF. FIELD ACTIONS 1. Host (Required) Get the hostname from your Databricks account and add it to this field. 2. HTTP Path (Required) Add the HTTP Path (web address) to fetch data from your Databricks account. 3. Personal Access Token (Required) Generate a Personal Access Token from your Databricks account and add it for authentication. 4. Catalog (Required) Add a Catalog to fetch data structures and metadata from Databricks. 5. Database (Optional) Specify the database name. 6. Teams (Required) Select one or more teams from the dropdown to associate with this enrichment datastore. <p>Step 4: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the enrichment datastore has been successfully verified.</p> <p></p> <p>Step 5: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a success notification appears on the screen indicating that the datastore was added successfully.</p> <p>Step 6: Close the success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/databricks/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use Enrichment Datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example- Marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>Host: This is the server address where the Databricks instance is hosted. It is the endpoint used to connect to the Databricks environment.</p> </li> <li> <p>Database: Refers to the specific database within the Databricks environment where the data is stored.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a success notification appears on the screen indicating that the datastore was added successfully.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/databricks/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/databricks/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a Databricks datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"databricks_database\",\n    \"schema\": \"databricks_catalog\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"databricks\",\n        \"host\": \"databricks_host\",\n        \"password\": \"databricks_token\",\n        \"parameters\": {\n            \"path\": \"databricks_http_path\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"databricks_database\",\n    \"schema\": \"databricks_catalog\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/databricks/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an Existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"databricks_database\",\n    \"schema\": \"databricks_enrichment_catalog\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"databricks\",\n        \"host\": \"databricks_host\",\n        \"password\": \"databricks_token\",\n        \"parameters\": {\n            \"path\": \"databricks_http_path\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"databricks_database\",\n    \"schema\": \"databricks_enrichment_catalog\",\n    \"enrich_only\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/databricks/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore: </p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/db2/","title":"DB2","text":"<p>Adding and configuring a DB2 connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.  </p> <p>This documentation provides a step-by-step guide on how to add DB2 as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration. </p> <p>By following these instructions, enterprises can ensure their DB2 environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle. </p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/db2/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. DB2 is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface. </p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the datastore name (e.g., This name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select DB2 from the dropdown list."},{"location":"add-datastores/db2/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the DB2 connector from the dropdown list and add connection details such as Secrets Management, host, port, user, password, SSL connection, database, and schema.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your DB2 account and add it to this field. 2. Port (Required) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the password to connect to the database. 5. SSL Connection Enable the SSL connection to ensure secure communication between Qualytics and the selected datastore. 6. Database (Required) Specify the database name. 7. Schema (Required) Define the schema within the database that should be used. 8. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 9. Initial Catalog Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified. </p>"},{"location":"add-datastores/db2/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams, and Initiate Cataloging. </p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/db2/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). This datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window- Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1 Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2 Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3 Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/db2/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>Once the HashiCorp Vault is set up, use the ${key} format in Connection form to reference a Vault secret.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 3: The configuration form, requesting credential details after selecting the enrichment datastore connector.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get Hostname from your DB2 account and add it to this field. 2. Port Specify the Port number. 3. User Enter the User to connect. 4. Password Enter the password to connect to the database. 5. SSL Connection Enable the SSL connection to ensure secure communication between Qualytics and the selected datastore. 6. Database Specify the database name. 7. Schema Define the schema within the database that should be used. 8. Teams Select one or more teams from the dropdown to associate with this datastore. <p>Step 4:  Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the enrichment datastore has been successfully verified. </p> <p></p> <p>Step 5: Click on the Finish button to complete the configuration process. </p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 6: Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/db2/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details. </p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1 Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2 Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment: </p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example- Marked as Public means that this datastore is accessible to all the users. </p> </li> <li> <p>Host: This is the server address where the DB2 instance is hosted. It is the endpoint used to connect to the DB2 environment. </p> </li> <li> <p>Database: Refers to the specific database within the DB2 environment where the data is stored.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/db2/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/db2/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a DB2 datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"db2_database\",\n    \"schema\": \"db2_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"db2\",\n        \"host\": \"db2_host\",\n        \"port\": \"db2_port\",\n        \"username\": \"db2_username\",\n        \"password\": \"db2_password\",\n        \"parameters\": {\n            \"ssl\": true\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"db2_database\",\n    \"schema\": \"db2_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection_id\n}\n</code></pre>"},{"location":"add-datastores/db2/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"db2_database\",\n    \"schema\": \"db2_enrichment_schema\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"db2\",\n        \"host\": \"db2_host\",\n        \"port\": \"db2_port\",\n        \"username\": \"db2_username\",\n        \"password\": \"db2_password\",\n        \"parameters\": {\n            \"ssl\": true\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"db2_database\",\n    \"schema\": \"db2_enrichment_schema\",\n    \"enrich_only\": true,\n    \"connection_id\": connection_id\n}\n</code></pre>"},{"location":"add-datastores/db2/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore: </p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/dremio/","title":"Dremio","text":"<p>Adding and configuring a Dremio connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on adding Dremio as a source datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Dremio environment is properly connected with Qualytics, unlocking the platform\u2019s potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/dremio/#add-the-source-datastore","title":"Add the Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Dremio is an example of such a datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the Dremio datastore allows the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name Specify the name of the datastore (e.g., the specified name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection 3. Connector Select Dremio from the dropdown list."},{"location":"add-datastores/dremio/#option-i-create-a-datastore-with-a-new-connection","title":"Option I: Create a Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Dremio connector from the dropdown list and add connection properties such as Secrets Management, host, port, username, and password, along with datastore properties like catalog, database, etc.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get the Hostname from your Dremio account and add it to this field. 2. Port Specify the Port number. 3. Project ID Enter the Project ID associated with Dremio. 4. SSL Connection Enable the SSL connection to ensure secure communication between Qualytics and the selected datastore. 5. Authentication You can choose between Basic authentication or Access Token for validating and securing the connection to your Dremio instance.  Basic Authentication: This method uses a username and password combination for authentication. It is a straightforward method where the user's credentials are directly used to access Dremio. <ul><li>Type: Select the authentication type from the dropdown menu.</li><li>User: Enter the username that Qualytics will use to connect to Dremio.</li><li>Password: Enter the password associated with the specified user account.</li></ul>Access Token Authentication: This method uses an access token for authentication. This is a more secure method compared to basic authentication.<ul><li> Personal Access Token: Enter the personal access token here to authenticate and access the resources securely.</li> </ul> 6. Schema Define the schema within the database that should be used. 7. Teams Select one or more teams from the dropdown to associate with this source datastore. 8. Initial Cataloging Tick the checkbox to automatically perform a catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/dremio/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Schema, Teams and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to check and verify the source data connection. If the connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>Click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/dremio/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>After successfully testing and verifying your source datastore connection, you have the option to add an enrichment datastore (recommended). This datastore is used to store analyzed results, including any anomalies and additional metadata in tables. This setup provides comprehensive visibility into your data quality, enabling you to manage and improve it effectively.</p> <p>Warning</p> <p>Qualytics does not support the Dremio connector as an enrichment datastore, but you can point to a different enrichment datastore.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/dremio/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTION 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment datastore from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p>Note</p> <p>Qualytics does not support Dremio as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using DB2 as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.  </p> <p></p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.  </p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 5: Close the Success dialogue and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/dremio/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p>Note</p> <p>Qualytics does not support Dremio as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using DB2 as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example - Marked as Public means that this datastore is accessible to all the users.  </p> </li> <li> <p>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment.</p> </li> <li> <p>Database: Refers to the specific database within the enrichment datastore environment where the data is stored.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/dremio/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/dremio/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a Dremio datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint (Post): <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an Existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"dremio_database\",\n    \"schema\": \"dremio_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"dremio\",\n        \"host\": \"dremio_host\",\n        \"port\": 443,\n        \"project_id\": \"dremio_id\",\n        \"ssl\": true,\n        \"authentication\": {\n            \"type\": \"access_token\",\n            \"personal_access_token\": \"your_personal_access_token\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"dremio_database\",\n    \"schema\": \"dremio_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection_id\n}\n</code></pre>"},{"location":"add-datastores/dremio/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/google-cloud-storage/","title":"Google Cloud Storage","text":"<p>Adding and configuring a Google Cloud Storage connection within Qualytics empowers the platform to build a symbolic link with your file system to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Google Cloud Storage as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Google Cloud Storage environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/google-cloud-storage/#google-cloud-storage-setup-guide","title":"Google Cloud Storage Setup Guide","text":"<p>This guide will walk you through the steps to set up Google Cloud Storage, including how to retrieve the necessary URIs, access keys, and secret keys, which are essential for integrating this datastore into Qualytics.</p>"},{"location":"add-datastores/google-cloud-storage/#retrieve-the-google-cloud-storage-uri","title":"Retrieve the Google Cloud Storage URI","text":"<p>To retrieve the Cloud Storage URI, follow the given steps:</p> <ol> <li>Go to the Cloud Storage Console.</li> <li>Navigate to the location of the object (file) that holds the source data.</li> <li>At the top of the Cloud Storage console, locate and note down the path to the object.</li> <li>Create the URI using the following format: </li> </ol> <pre><code>gs://bucket/file\n</code></pre> <ul> <li> <p><code>bucket</code> is the name of the Cloud Storage bucket.</p> </li> <li> <p><code>file</code> is the name of the object (file) containing the data.</p> </li> </ul>"},{"location":"add-datastores/google-cloud-storage/#retrieve-the-access-key-and-secret-key","title":"Retrieve the Access Key and Secret Key","text":"<p>You need these keys when integrating Google Cloud Storage with other applications or services, such as when adding it as a datastore in Qualytics. The keys allow you to reuse existing code to access Google Cloud Storage without needing to implement a different authentication mechanism.</p> <p>To retrieve the access key and secret key in the Google Cloud Storage Console account, follow the given steps: </p> <p>Step 1: Log in to the Google Cloud Console, navigate to the Google Cloud Storage settings, and this will redirect you to the Settings page.</p> <p></p> <p>Step 2: Click on the Interoperability tab. </p> <p></p> <p>Step 3: Scroll down the Interoperability  page and under Access keys for your user account, click the CREATE A KEY button to generate a new Access Key and Secret Key.</p> <p></p> <p>Step 4: Use these generated Access Key and Secret Key values when adding your Google Cloud Storage account to SimpleBackups.</p> <p></p> <p>For example, once you generate the keys, they might look like this:</p> <ul> <li> <p>Access Key: <code>GOOG1234ABCDEFGH5678</code></p> </li> <li> <p>Secret Key: <code>abcd1234efgh5678ijklmnopqrstuvwx</code></p> </li> </ul> <p>Warning</p> <p>Make sure to store these keys securely, as they provide access to your Google Cloud Storage resources. </p>"},{"location":"add-datastores/google-cloud-storage/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect and access data from external sources. Google Cloud Storage is an example of a source datastore, specifically a type of Distributed File System (DFS) datastore that is designed to handle data stored in distributed file systems. Configuring a DFS datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore (e.g., The specified name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection 3. Connector (Required) Select Google Cloud Storage from the dropdown list."},{"location":"add-datastores/google-cloud-storage/#option-i-create-a-datastore-with-a-new-connection","title":"Option I: Create a Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Google Cloud Storage connector from the dropdown list and add connection details such as Secrets Management, URI, service account key, root path, and teams.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. URI (Required) Enter the Uniform Resource Identifier (URI) of the Google Cloud Storage. 2. Service Account Key (Required) Upload a JSON file that contains the credentials required for accessing the Google Cloud Storage. 3. Root Path (Required) Specify the root path where the data is stored. 4. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 5. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified. </p>"},{"location":"add-datastores/google-cloud-storage/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Root Path, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to check and verify the source data connection. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/google-cloud-storage/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). This datastore is used to store the analyzed results, including any anomalies and additional metadata files. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/google-cloud-storage/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window - Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for Add New Connection Toggle ON to create a new enrichment datastore from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> REF. FIELDS ACTIONS 1. URI (Required) Enter the Uniform Resource Identifier (URI) for the Google Cloud Storage. 2. Service Account Key (Required) Upload a JSON file that contains the credentials required for accessing the Google Cloud Storage. 3. Root Path (Required) Specify the root path where the data is stored. 4. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified. </p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process. </p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p></p> <p>Step 5: Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/google-cloud-storage/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Use an existing enrichment datastore is turned on, you will be prompted to configure the enrichment datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment: </p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example: Marked as Public means that this datastore is accessible to all the users. </p> </li> <li> <p>URI: Uniform Resource Identifier (URI) points to the specific location of the source data and should be formatted accordingly (e.g., <code>gs://bucket/file</code> for Google Cloud Storage).</p> </li> <li> <p>Root Path: Specify the root path where the data is stored. This path defines the base directory or folder from which all data operations will be performed.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p></p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/google-cloud-storage/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/google-cloud-storage/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating the Google Cloud Storage datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>        {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"trigger_catalog\": true,\n        \"root_path\": \"/gcs_root_path\",\n        \"enrich_only\": false,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"gcs\",\n            \"uri\": \"gs://&lt;bucket_name&gt;\",\n            \"secret_key\": \"gcs_service_account_key\"\n        }\n    }\n</code></pre> <pre><code>   {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"trigger_catalog\": true,\n        \"root_path\": \"/gcs_root_path\",\n        \"enrich_only\": false,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/google-cloud-storage/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an existing Connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"trigger_catalog\": true,\n        \"root_path\": \"/gcs_root_path\",\n        \"enrich_only\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"gcs\",\n            \"uri\": \"gs://&lt;bucket_name&gt;\",\n            \"secret_key\": \"gcs_service_account_key\"\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"trigger_catalog\": true,\n        \"root_path\": \"/gcs_root_path\",\n        \"enrich_only\": true,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/google-cloud-storage/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore:</p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/hive/","title":"Hive","text":"<p>Adding and configuring a Hive connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Hive as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Hive environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/hive/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Hive is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the datastore name (e.g., This name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select Hive from the dropdown list."},{"location":"add-datastores/hive/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Hive connector from the dropdown list and add the connection details.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically. </p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> REF. FIELDS ACTIONS 1. Host(Required) Get Hostname from your Hive account and add it to this field. 2. Port(Required) Specify the Port number. 3. Authentication You can choose between Basic Authentication and Kerberos Authentication for validating and securing the connection to your Hive instance.  Basic Authentication: This method uses a username and password combination for authentication. It is a straightforward method where the user's credentials are directly used to access Hive. <ul><li>Type: Select the authentication type from the dropdown menu.</li><li>User: Enter the username that Qualytics will use to connect to Hive.</li><li>Password: Enter the password associated with the specified user account.</li></ul> Kerberos Authentication: This method uses Kerberos tickets for authentication. It relies on a secure, ticket-based mechanism managed by your environment\u2019s Kerberos configuration. <ul> <li>Type: Select Kerberos from the authentication type dropdown.</li><li>Principal: Enter the Kerberos principal (for example: <code>hive/_HOST@DOMAIN.COM</code>) that Qualytics will use to connect to Hive. </li> </ul> 4. Schema(Required) Define the schema within the database that should be used. 5. Teams(Required) Select one or more teams from the dropdown to associate with this source datastore. 6. Initial Cataloging(Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p></p> <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/hive/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add new connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams, and Initiate Cataloging. </p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step. </p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page. </p>"},{"location":"add-datastores/hive/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Warning</p> <p>Qualytics does not support the Hive connector as an enrichment datastore, but you can point to a different enrichment datastore. </p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window- Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/hive/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p>Note</p> <p>Qualytics does not support Hive as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using BigQuery as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration. </p> <p></p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/hive/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example- Marked as Public means that this datastore is accessible to all the users.  </li> <li>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment. </li> <li>Database: Refers to the specific database within the enrichment datastore environment where the data is stored.  </li> <li>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/hive/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API.</p> <p>Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/hive/#creating-a-datastore","title":"Creating a Datastore","text":"<p>This section provides a sample payload for creating a datastore. Replace the placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/hive/#endpoint-post","title":"Endpoint (Post)","text":"<p><code>/api/datastores</code> (post)</p> Creating a datastore with a new connectionCreating a datastore with an existing connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"hive_database\",\n        \"schema\": \"hive_schema\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"hive\",\n            \"host\": \"hive_host\",\n            \"port\": \"hive_port\",\n            \"username\": \"hive_username\",\n            \"password\": \"hive_password\",\n            \"parameters\": {\n                \"zookeeper\": false\n            }\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"hive_database\",\n        \"schema\": \"hive_schema\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection_id\": connection_id\n    }\n</code></pre>"},{"location":"add-datastores/hive/#linking-datastore-to-an-enrichment-datastore-through-api","title":"Linking Datastore to an Enrichment Datastore through API","text":""},{"location":"add-datastores/hive/#endpoint-patch","title":"Endpoint (Patch)","text":"<p><code>/api/datastores/{datastore-id}/enrichment/{enrichment-id}</code> (patch)</p>"},{"location":"add-datastores/maria-db/","title":"MariaDB","text":"<p>Adding and configuring a MariaDB connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add MariaDB as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their MariaDB environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/maria-db/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. MariaDB is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the datastore name (e.g., this name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select MariaDB from the dropdown list."},{"location":"add-datastores/maria-db/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the MariaDB connector from the dropdown list and add connection details such as Secrets Management, host, port, user, password, SSL connection, database, and schema.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> <p></p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get Hostname from your MariaDB account and add it to this field. 2. Port Specify the Port number. 3. User Enter the User ID to connect. 4. Password Enter the password to connect to the database. 5. Database Specify the database name. 6. Teams Select one or more teams from the dropdown to associate with this source datastore. 7. Initial Cataloging Check the checkbox to automatically perform a catalog operation on the configured source to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/maria-db/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/maria-db/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). This datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/maria-db/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Enter a name for the enrichment datastore. 3. Toggle Button For Add New Connection Toggle ON to create a new enrichment datastore from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 5: Close the success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/maria-db/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. For example, marked as Public means that this datastore is accessible to all the users.  </li> <li>Host: This is the server address where the MariaDB instance is hosted. It is the endpoint used to connect to the MariaDB environment.  </li> <li>Database: Refers to the specific database within the MariaDB environment where the data is stored.  </li> <li>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal window will display and a success flash message stating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/maria-db/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"add-datastores/maria-db/#creating-a-datastore","title":"Creating a Datastore","text":"<p>This section provides a sample payload for creating a datastore. Replace the placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/maria-db/#endpoint-post","title":"Endpoint (Post)","text":"<p><code>/api/datastores</code> (post)</p> Creating a datastore with a new connectionCreating a datastore with an existing connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"mariadb_database\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"mariadb\",\n            \"host\": \"mariadb_host\",\n            \"port\": \"mariadb_port\",\n            \"username\": \"mariadb_username\",\n            \"password\": \"mariadb_password\"\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"mariadb_database\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/maria-db/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":""},{"location":"add-datastores/maria-db/#endpoint-post_1","title":"Endpoint (Post)","text":"<p><code>/api/datastores</code> (post)</p> <p>This section provides a sample payload for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> Creating an enrichment datastore with a new connectionCreating an enrichment datastore with an existing connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"mariadb_database\",\n        \"enrich_only\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"mariadb\",\n            \"host\": \"mariadb_host\",\n            \"port\": \"mariadb_port\",\n            \"username\": \"mariadb_username\",\n            \"password\": \"mariadb_password\"\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"mariadb_database\",\n        \"enrich_only\": true,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/maria-db/#linking-datastore-to-an-enrichment-datastore-through-api","title":"Linking Datastore to an Enrichment Datastore through API","text":""},{"location":"add-datastores/maria-db/#endpoint-patch","title":"Endpoint (Patch)","text":"<p><code>/api/datastores/{datastore-id}/enrichment/{enrichment-id}</code> (patch)</p>"},{"location":"add-datastores/microsoft-sql-server/","title":"Microsoft SQL Server","text":"<p>Adding and configuring Microsoft SQL Server connection within Qualytics empowers the platform to build a symbolic link with your database to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on adding Microsoft SQL Server as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Microsoft SQL Server environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/microsoft-sql-server/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Microsoft SQL Server is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface. </p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the datastore name (e.g., This name will appear on the datastore cards) 2. Toggle Button Toggle ON to  create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select Microsoft SQL Server from the dropdown list."},{"location":"add-datastores/microsoft-sql-server/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Microsoft SQL Server connector from the dropdown list and add connection details such as Secret Management, host, port, username, password, and database.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your Microsoft SQL Server account and add it to this field. 2. Port (Optional) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the password to connect to the database. 5. Database (Required) Specify the database name. 6. Schema (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 8. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified. </p>"},{"location":"add-datastores/microsoft-sql-server/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add new connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/microsoft-sql-server/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window- Link Enrichment Datastore will appear, providing you with the options to configure to add an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/microsoft-sql-server/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>Once the HashiCorp Vault is set up, use the ${key} format in Connection form to reference a Vault secret.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 3: The configuration form, requesting credential details after selecting the enrichment datastore connector.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your Microsoft SQL Server account and add it to this field. 2. Port (Optional) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the Password to connect to the database. 5. Database (Required) Specify the database name. 6. Schema (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this datastore. <p>Step 4: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified. </p> <p></p> <p>Step 5: Click on the Finish button to complete the configuration process. </p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/microsoft-sql-server/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment: </p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example- Marked as Public means that this datastore is accessible to all the users. </p> </li> <li> <p>Host: This is the server address where the Microsoft SQL Server instance is hosted. It is the endpoint used to connect to the Microsoft SQL Server environment. </p> </li> <li> <p>Database: Refers to the specific database within the Microsoft SQL Server environment where the data is stored. </p> </li> <li> <p>Schema:  The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/microsoft-sql-server/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. </p> <p>Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/microsoft-sql-server/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a Microsoft SQL Server datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"sqlserver_database\",\n    \"schema\": \"sqlserver_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"sqlserver\",\n        \"host\": \"sqlserver_host\",\n        \"port\": \"sqlserver_port\",\n        \"username\": \"sqlserver_username\",\n        \"password\": \"sqlserver_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"sqlserver_database\",\n    \"schema\": \"sqlserver_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection_id\n}\n</code></pre>"},{"location":"add-datastores/microsoft-sql-server/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an Existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"sqlserver_database\",\n    \"schema\": \"sqlserver_enrichment_schema\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"sqlserver\",\n        \"host\": \"sqlserver_host\",\n        \"port\": \"sqlserver_port\",\n        \"username\": \"sqlserver_username\",\n        \"password\": \"sqlserver_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"sqlserver_database\",\n    \"schema\": \"sqlserver_enrichment_schema\",\n    \"enrich_only\": true,\n    \"connection_id\": connection_id\n}\n</code></pre>"},{"location":"add-datastores/microsoft-sql-server/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore:</p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/mysql/","title":"MySQL","text":"<p>Adding and configuring a MySQL connection within Qualytics empowers the platform to build a symbolic link with your database to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add MySQL as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their MySQL environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/mysql/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. MySQL is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the datastore name (e.g., This name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select MySQL from the dropdown list."},{"location":"add-datastores/mysql/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the MySQL connector from the dropdown list and add connection details such as Secrets Management, host, port, username, password, and database.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your MySQL account and add it to this field. 2. Port (Required) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the password to connect to the database. 5. Database (Required) Specify the database name. 6. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 7. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/mysql/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add new connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Teams and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/mysql/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window- Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1 Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2 Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3 Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/mysql/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>Once the HashiCorp Vault is set up, use the ${key} format in Connection form to reference a Vault secret.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 3: The configuration form, requesting credential details after selecting the enrichment datastore connector.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your MySQL account and add it to this field. 2. Port (Required) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the password to connect to the database. 5. Database (Required) Specify the database name. 6. Teams (Required) Select one or more teams from the dropdown to associate with this datastore. <p>Step 4: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 5: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/mysql/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Team: The team associated with managing the enrichment datastore is based on the role of public or private. Example- Marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>Host: This is the server address where the MySQL  instance is hosted. It is the endpoint used to connect to the MySQL environment.</p> </li> <li> <p>Database: Refers to the specific database within the MySQL environment where the data is stored.</p> </li> </ul> <p></p> <p>Step 3: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/mysql/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API.</p> <p>Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/mysql/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a MySQL datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"mysql_database\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"mysql\",\n        \"host\": \"mysql_host\",\n        \"port\": \"mysql_port\",\n        \"username\": \"mysql_username\",\n        \"password\": \"mysql_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"mysql_database\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/mysql/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"mysql_database\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"mysql\",\n        \"host\": \"mysql_host\",\n        \"port\": \"mysql_port\",\n        \"username\": \"mysql_username\",\n        \"password\": \"mysql_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"mysql_database\",\n    \"enrich_only\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/mysql/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore:</p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/oracle/","title":"Oracle","text":"<p>Adding and configuring an Oracle connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Oracle as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Oracle environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/oracle/#add-the-source-datastore","title":"Add the Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Oracle, for example, is a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the Oracle datastore allows the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> Step FIELDS Description 1. Name Specify the name of the datastore (e.g., The specified name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector Select \u201cOracle\u201d from the dropdown list."},{"location":"add-datastores/oracle/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add new connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Oracle connector from the dropdown list and add connection details such as Secret Management, host, port, username, sid, and schema.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get \u201cHostname\u201d from your Oracle account and add it to this field. 2. Port Specify the \u201cPort\u201d number. 3. Protocol Specifies the connection protocol used for communicating with the database. Choose between TCP or TCPS. 4. Connect By You can choose between SID or Service Name to establish a connection with the Oracle database, depending on how your database instance is configured. 5. User Enter the \u201cUser ID\u201d to connect. 6. Password Enter the \u201cpassword\u201d to connect to the database. 7. Schema Define the schema within the database that should be used. 8. Teams Select one or more teams from the dropdown to associate with this source data store. 9. Initial Cataloging Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/oracle/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for add new connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If the connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/oracle/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Warning</p> <p>Qualytics does not support the Oracle connector as an enrichment datastore, but you can point to a different enrichment datastore.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/oracle/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Note</p> <p>Qualytics does not support Oracle as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using Microsoft SQL Server as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success  message indicating that your datastore has been successfully added.</p> <p>Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/oracle/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p>Note</p> <p>Qualytics does not support Oracle as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using  Microsoft SQL Server as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. For example, Marked as Public means that this datastore is accessible to all the users.</li> <li>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment.</li> <li>Database: Refers to the specific database within the enrichment datastore environment where the data is stored.</li> <li>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects(tables, views, etc.).Each schema belongs to a single database.</li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added**.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/oracle/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"add-datastores/oracle/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides a sample payload for creating an Oracle datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint (Post): <code>/api/datastores (post)</code></p> Creating a source datastore with a new connectionCreating a datastore with an existing connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"oracle_database\",\n    \"schema\": \"oracle_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"oracle\",\n        \"host\": \"oracle_host\",\n        \"port\": \"oracle_port\",\n        \"username\": \"oracle_username\",\n        \"password\": \"oracle_password\",\n        \"parameters\": {\n            \"sid\": \"orcl\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"oracle_database\",\n    \"schema\": \"oracle_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": \"connection-id\"\n}\n</code></pre>"},{"location":"add-datastores/oracle/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/overview-of-a-datastore/","title":"Source Datastore","text":"<p>Qualytics connects to source datastores using \"Datastores,\" a framework that enables organizations to:</p> <ul> <li> <p>Connect with Apache Spark-compatible source datastores.</p> </li> <li> <p>Support both traditional databases and modern object storage.</p> </li> <li> <p>Profile and monitor structured data across systems.</p> </li> <li> <p>Ensure secure and fast access to data.</p> </li> <li> <p>Scale data quality operations across platforms.</p> </li> <li> <p>Manage data quality centrally across all sources.</p> </li> </ul> <p>These source datastore integrations ensure comprehensive quality management across your entire data landscape, regardless of where your data resides.</p>"},{"location":"add-datastores/overview-of-a-datastore/#understanding-datastores","title":"Understanding Datastores","text":"<p>A Datastore in Qualytics represents any structured source datastore, such as:</p> <ul> <li> <p>Relational databases (RDBMS)</p> </li> <li> <p>Raw file formats like CSV, XLSX, JSON, Avro, or Parquet</p> </li> <li> <p>Cloud storage platforms like AWS S3, Azure Blob Storage, or GCP Cloud Storage</p> </li> </ul> <p>Qualytics integrates with these source datastores through a layered architecture:</p> <p></p>"},{"location":"add-datastores/overview-of-a-datastore/#configuring-source-datastores","title":"Configuring Source Datastores","text":"<p>Configure your source datastores in Qualytics by connecting them through a new datastore.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window, Add Datastore, will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name Specify the name of the datastore (e.g., the specified name will appear on the datastore cards). 2. Toggle Button Toggle on to create a new source datastore from scratch, or toggle off to reuse credentials from an existing connection. 3. Connector Select Any source datastore from the dropdown list."},{"location":"add-datastores/overview-of-a-datastore/#available-datastore-connectors","title":"Available Datastore Connectors","text":"<p>Qualytics supports a range of source datastores, including but not limited to:</p> <p>Tip</p> <p>Want to check which datastores have Enrichment support?  See the Supported Enrichment Datastores </p> No. Source Datastores 1. Amazon Redshift 2. Amazon S3 3. Athena 4. Azure Datalake Storage (ABFS) 5. Big Query 6. Databricks 7. DB2 8. Dremio 9. Google Cloud Storage 10. Hive 11. MariaDB 12. Microsoft SQL Server 13. MySQL 14. Oracle 15. PostgreSQL 16. Presto 17. Snowflake 18. Synapse 19. Teradata 20. Timescale DB 21. Trino"},{"location":"add-datastores/overview-of-a-datastore/#connection-management","title":"Connection Management","text":"<p>To connect to a datastore, users must provide the required connection details, such as Host/Port or URI. These fields may vary depending on the datastore and are essential for establishing a secure and reliable connection to the target database.</p> <p>For demonstration purposes, we have selected the Snowflake connector.</p>"},{"location":"add-datastores/overview-of-a-datastore/#option-i-create-a-datastore-with-a-new-connection","title":"Option I: Create a Datastore with a New Connection","text":"<p>If the toggle for Add New Connection is turned on, this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select any connector (as we are selecting the Snowflake connector) from the dropdown list and add connection properties such as Secrets Management, host, port, username, and password, along with datastore properties like catalog, database, etc.</p> <p></p> <p>For the next steps, refer to the \"Add Source Datastore\" section in the Snowflake Datastore documentation.</p> <p>Once a datastore is verified and created, it appears in your source datastores.</p> <p></p>"},{"location":"add-datastores/overview-of-a-datastore/#datastore-operations","title":"Datastore Operations","text":"<p>Once a datastore is added in Qualytics, you can perform three key operations to manage and ensure data quality effectively:</p> <p>1. Catalog Operation</p> <p>This operation imports named data collections such as tables, views, and files into the source datastore. It identifies incremental fields for scans and allows you to recreate or delete containers, streamlining data organization and enhancing discovery.  </p> <p>For more details about the catalog operation, refer to the \"Catalog Operation\" document.</p> <p>2. Profile Operation</p> <p>After cataloging, the Profile Operation analyzes each record within the collections to assess and improve data quality. By generating detailed metadata and interacting with the Qualytics Inference Engine, it identifies quality issues and refines checks for maintaining data integrity.</p> <p>For more details about the profile operation, refer to the \"Profile Operation\" document.</p> <p>3. Scan Operation</p> <p>Finally, the Scan Operation enforces data quality checks on the collections. It identifies anomalies at the record and schema levels, highlights structural issues, and records all findings for further analysis. Flexible options allow for incremental scans, specific table/file scans, and scheduling future scans.</p> <p>For more details about the scan operation, refer to the \"Scan Operation\" document.</p> <p>By performing these operations sequentially, you can efficiently manage and ensure the quality of your data in Qualytics.</p>"},{"location":"add-datastores/overview-of-a-datastore/#view-operation","title":"View Operation","text":"<p>Once the datastores are connected, you can run operations on the selected datastore. To track the progress, simply navigate to the Activity tab, where you can view the running operation.</p> <p>Step 1: Simply click to open the datastore on which you ran the operation.</p> <p></p> <p>Step 2: After clicking on the datastore, select the \"Activity\" tab to view the ongoing operation.</p> <p></p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/","title":"DFS Datastore Overview","text":"<p>The DFS (Distributed File System) Datastore feature in Qualytics is designed to handle data stored in distributed file systems. </p> <p>This includes file systems like Hadoop Distributed File System (HDFS) or similar distributed storage solutions.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#supported-distributed-file-systems","title":"Supported Distributed File Systems","text":"<p>Qualytics supports DFS Datastores, catering to distributed file systems like:</p> <ul> <li>Amazon S3</li> <li>Google Cloud Storage</li> <li>Windows Azure Storage Blob</li> </ul>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#connection-details","title":"Connection Details","text":"<p>Users provide connection details for DFS Datastores, allowing Qualytics to establish a connection to the distributed file system.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#catalog-operation","title":"Catalog Operation","text":"<p>The Catalog operation involves walking the directory tree, reading files with supported filename extensions, and creating containers based on file metadata.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#data-quality-and-profiling","title":"Data Quality and Profiling","text":"<p>DFS Datastores support the initiation of Profile Operations, allowing users to understand the structure and characteristics of the data stored in the distributed file system.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#containers-overview","title":"Containers Overview","text":"<p>For a more detailed understanding of how Qualytics manages and interacts with containers in DFS Datastores, please refer to the Containers section in our comprehensive user guide. </p> <p>This section covers topics such as container deletion, field deletion, and the initial profile of a Datastore's containers.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#multi-token-filename-globbing-and-container-formation","title":"Multi-Token Filename Globbing and Container Formation","text":"<p>Filenames with similar structures in the same folder are automatically included in a single globbed container during the Catalog operation.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#use-folders-for-precise-file-grouping","title":"Use Folders for Precise File Grouping","text":"<p>Organizing files within distinct folders is a straightforward and effective strategy in Distributed File Systems (DFS). </p> <p>When all files in a folder share a common schema, it simplifies the process of grouping and managing them. </p> <p>This approach ensures precise file grouping without relying on complex glob patterns.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#how-to-use-folders-for-shared-schema","title":"How to Use Folders for Shared Schema","text":""},{"location":"add-datastores/overview-of-a-dfs-datastore/#1-create-a-folder","title":"1. Create a Folder:","text":"<p>Begin by creating a new folder in your distributed filesystem.</p> <ul> <li> <p>Suppose you have order data files with filenames like <code>orders_20240229.csv</code>, <code>orders-20240228.csv</code>, <code>orders-20240227.csv</code>.</p> </li> <li> <p>Create a folder named <code>Orders</code> to group these files.</p> </li> </ul> <p>Qualytics Pattern: Qualytics will automatically create the container <code>orders_*.csv</code> based on the filenames.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#2-place-related-files-in-the-folder","title":"2. Place Related Files in the Folder:","text":"<p>Move or upload files that share a common schema into the created folder.</p> <ul> <li>Move the order data files into the <code>Orders</code> folder.</li> </ul>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#3-repeat-for-each-schema","title":"3. Repeat for Each Schema:","text":"<p>Create separate folders for different schemas, and organize files accordingly.</p> <ul> <li>Suppose you have customer data files with filenames like <code>customers_us.csv</code>, <code>customers_eu.csv</code>.</li> <li>Create a folder named <code>Customers</code> to group these files.</li> </ul> <p>Qualytics Pattern: Qualytics will automatically create the pattern <code>customers_*.csv</code> based on the filenames.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#4-naming-conventions","title":"4. Naming Conventions:","text":"<p>Consider adopting clear and consistent naming conventions for folders to enhance organization.</p> <ul> <li>Use descriptive names for folders, such as <code>Orders</code>, <code>Customers</code>, to make it easier to identify the contents.</li> </ul>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#flowchart-using-folders-for-shared-schema","title":"Flowchart: Using Folders for Shared Schema","text":"<pre><code>graph TD\n  A[Start] --&gt;|Create a Folder| B(Create Folder)\n  B --&gt;|Place Related Files| C(Move or Upload Files)\n  C --&gt;|Repeat for Each Schema| D(Create Separate Folders)\n  D --&gt;|Naming Conventions| E(Consider Clear Naming)\n  E --&gt; F[End]</code></pre>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#use-filename-conventions-for-posix-globs","title":"Use Filename Conventions for Posix Globs:","text":"<p>This option leverages filename conventions that align with POSIX globs, allowing our system to automatically organize files for you. </p> <p>The system intelligently analyzes filename patterns, making the process seamless and efficient.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#how-to-use-filename-conventions-for-posix-globs","title":"How to Use Filename Conventions for Posix Globs","text":""},{"location":"add-datastores/overview-of-a-dfs-datastore/#1-follow-clear-filename-conventions","title":"1. Follow Clear Filename Conventions:","text":"<p>Adopt clear and consistent filename conventions that lend themselves to POSIX globs.</p> <ul> <li>Suppose you have log files with filenames like <code>app_log_20240229.txt</code>, <code>app_log_20240228.txt</code>, <code>app_log_20240227.txt</code>.</li> <li>Use a consistent naming convention like <code>app_log_*.txt</code>, where <code>*</code> serves as a placeholder for varying elements.</li> <li>The <code>*</code> in the convention acts as a wildcard, representing any variation in the filename. In this example, it matches the date part (<code>20240229</code>, <code>20240228</code>, etc.).</li> </ul>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#2-upload-or-move-files","title":"2. Upload or Move Files:","text":"<p>Upload or move files with filenames following the adopted conventions to your distributed filesystem.</p> <ul> <li>Move log files with the specified naming convention to your DFS.</li> </ul>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#3-system-analysis","title":"3. System Analysis:","text":"<p> Our system will automatically detect and analyze the filename conventions, creating appropriate glob patterns.</p> <ul> <li>With filenames like <code>app_log_20240229.txt</code>, <code>app_log_20240228.txt</code>, the system will create the pattern <code>app_log_*.txt</code>.</li> </ul>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#flowchart-using-folders-for-filename-conventions","title":"Flowchart: Using Folders for Filename Conventions","text":"<pre><code>graph TD\n  A[Start] --&gt;|Follow Clear Conventions| B(Adopt Consistent Conventions)\n  B --&gt;|Upload or Move Files| C(Move Files to DFS)\n  C --&gt;|System Analysis| D(Automatic Pattern Creation)\n  D --&gt; E[End]</code></pre>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#why-not-manually-creating-your-own-globs","title":"Why not manually creating your own Globs?","text":"<p>While our system offers powerful features to automate file organization, we strongly discourage manually creating globs. </p> <p>This option may lead to errors, inconsistencies, and hinder the efficiency of our system. </p> <p>We recommend leveraging our automated tools for a seamless and error-free experience.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#complexity-and-error-prone","title":"Complexity and Error-Prone:","text":"<p>Manually creating globs can be complex, prone to typos, and susceptible to errors in pattern formation.</p> <ul> <li>Suppose you want to group log files with the pattern <code>app_log_*.txt</code>. A manual attempt might result in mistakes like <code>app_log_202*.txt</code> or <code>app_log_*.tx</code>.</li> </ul>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#inconsistencies-across-files","title":"Inconsistencies Across Files:","text":"<p>Manual glob creation may lead to inconsistencies across different files, making it challenging to establish a uniform file organization.</p> <ul> <li>Trying to manually create globs for order data files with varying date formats (<code>orders_20240229.csv</code>, <code>orders-20240228.csv</code>) can result in inconsistent patterns.</li> </ul>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#explore-deeper-knowledge","title":"Explore Deeper Knowledge","text":"<p>If you want to go deeper into the knowledge or if you are curious and want to learn more about DFS filename globbing, you can explore our comprehensive guide here: How DFS Filename Globbing Works.</p>"},{"location":"add-datastores/overview-of-a-jdbc-datastore/","title":"JDBC Datastore Overview","text":"<p>JDBC Datastore in Qualytics allows you to easily integrate and manage data from relational databases. Using the Java Database Connectivity (JDBC) API, you can securely connect to databases, analyze data, and perform data profiling. This feature supports a wide range of relational databases, providing you with a flexible solution for data discovery and quality checks.</p>"},{"location":"add-datastores/overview-of-a-jdbc-datastore/#adding-jdbc-datastore","title":"Adding JDBC Datastore","text":"<p>Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>For detailed steps on adding a JDBC Datastore, refer to the Add the Source Datastore section of the documentation.</p>"},{"location":"add-datastores/overview-of-a-jdbc-datastore/#supported-jdbc-databases","title":"Supported JDBC Databases","text":"<p>Qualytics supports a range of relational databases, including but not limited to:</p> <ul> <li>Athena </li> <li>Databricks </li> <li>DB2 </li> <li>Hive </li> <li>MariaDB </li> <li>Microsoft SQL Server </li> <li>MySQL </li> <li>Oracle </li> <li>PostgreSQL </li> <li>Presto </li> <li>Amazon Redshift </li> <li>Snowflake </li> <li>Synapse </li> <li>Timescale DB </li> <li>Trino</li> </ul>"},{"location":"add-datastores/overview-of-a-jdbc-datastore/#connection-details","title":"Connection Details","text":"<p>To connect to a JDBC datastore, users must provide the required connection details, such as Host/Port or URI. These fields may vary depending on the datastore and are essential for establishing a secure and reliable connection to the target database.</p> <p>For more information about connections, refer to the Connection Overview documentation.</p>"},{"location":"add-datastores/overview-of-a-jdbc-datastore/#catalog-operation","title":"Catalog Operation","text":"<p>After adding a JDBC Datastore, you can initiate a Catalog operation to extract key metadata from the database. This operation provides:</p> <ul> <li>A list of containers (schemas, tables, or views).  </li> <li>Field names within each container.  </li> <li>Record counts for data analysis and profiling.</li> </ul> <p></p> <p>For more information about how to run catalog operation, refer to the Catalog Operation documentation.</p>"},{"location":"add-datastores/overview-of-a-jdbc-datastore/#field-types-inference","title":"Field Types Inference","text":"<p>Qualytics employs weighted histogram analysis during the Catalog operation to infer field types automatically. This advanced method ensures accurate detection of data types within the JDBC Datastore, enhancing the precision of data profiling.</p>"},{"location":"add-datastores/overview-of-a-jdbc-datastore/#containers-overview","title":"Containers Overview","text":"<p>Containers are fundamental entities representing structured data sets. These containers could manifest as tables in JDBC datastores or as files within DFS datastores. They play a pivotal role in data organization, profiling, and quality checks within the Qualytics application. For a more detailed understanding of how Qualytics manages and interacts with containers in JDBC Datastores, please refer to the Containers overview documentation.</p>"},{"location":"add-datastores/postgresql/","title":"PostgreSQL","text":"<p>Adding and configuring a PostgreSQL connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add PostgreSQL as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their PostgreSQL environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/postgresql/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. PostgreSQL is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore. (e.g., The specified name will appear on the datastore cards.) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select PostgreSQL from the dropdown list."},{"location":"add-datastores/postgresql/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add new existing connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the PostgreSQL connector from the dropdown list and add connection details such as Secrets Management, host, port, username, database, and schema.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your PostgreSQL account and add it to this field. 2. Port (Required) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the password to connect to the database. 5. Database (Required) Specify the database name. 6. Schema (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 8. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/postgresql/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add new connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Info</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/postgresql/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1 Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2 Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3 Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/postgresql/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>Once the HashiCorp Vault is set up, use the ${key} format in Connection form to reference a Vault secret.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 3: The configuration form will expand, requesting credential details after the selected enrichment datastore connector is chosen.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your PostgreSQL account and add it to this field. 2. Port (Required) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the password associated with the PostgreSQL user account. 5. Database (Required) Specify the database name to be accessed. 6. Schema (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this datastore. <p>Step 4: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 5: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/postgresql/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment datastore:</p> <ul> <li> <p>Team: The team associated with managing the enrichment datastore is based on the role of public or private. Example- Marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>Host: This is the server address where the PostgreSQL instance is hosted. It is the endpoint used to connect to the PostgreSQL environment.</p> </li> <li> <p>Database: Refers to the specific database within the PostgreSQL environment where the data is stored.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/postgresql/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/postgresql/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a PostgreSQL datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"postgresql_database\",\n    \"schema\": \"postgresql_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"postgresql\",\n        \"host\": \"postgresql_host\",\n        \"port\": \"postgresql_port\",\n        \"username\": \"postgresql_username\",\n        \"password\": \"postgresql_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"postgresql_database\",\n    \"schema\": \"postgresql_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/postgresql/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"postgresql_database\",\n    \"schema\": \"postgresql_schema\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"postgresql\",\n        \"host\": \"postgresql_host\",\n        \"port\": \"postgresql_port\",\n        \"username\": \"postgresql_username\",\n        \"password\": \"postgresql_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"postgresql_database\",\n    \"schema\": \"postgresql_schema\",\n    \"enrich_only\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/postgresql/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore:</p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/presto/","title":"Presto","text":"<p>Adding and configuring a Presto connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Presto as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Presto environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/presto/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Presto is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore. (e.g., The specified name will appear on the datastore cards.) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select Presto from the dropdown list."},{"location":"add-datastores/presto/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New existing connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Presto connector from the dropdown list and add connection details such as Secrets Management, host, port, username, database, and schema.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically. </p> REF. FIELDS ACTION 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your Presto account and add it to this field. 2. Port (Required) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the Password to connect to the database. 5. Catalog (Required) Add a Catalog to fetch data structures and metadata from Presto. 6. Schema (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 8. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.  </p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/presto/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add new connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams, and Initiate Cataloging. </p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.  </p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step. </p> <p>Info</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page. </p>"},{"location":"add-datastores/presto/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Warning</p> <p>Qualytics does not support the Presto connector as an enrichment datastore, but you can point to a different enrichment datastore. </p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/presto/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment datastore from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Note</p> <p>Qualytics does not support Presto as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using Microsoft SQL Server as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration. </p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 5: Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/presto/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> <p>Note</p> <p>Qualytics does not support Presto as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using Microsoft SQL Server as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration. </p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. For example, marked as Public means that this datastore is accessible to all users.  </li> <li>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment.  </li> <li>Database: Refers to the specific database within the enrichment datastore environment where the data is stored.  </li> <li>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.  </p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.  </p> <p></p>"},{"location":"add-datastores/presto/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API.</p> <p>Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/presto/#creating-a-datastore","title":"Creating a Datastore","text":"<p>This section provides a sample payload for creating a datastore. Replace the placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/presto/#endpoint-post","title":"Endpoint (Post)","text":"<p><code>/api/datastores</code> (post)</p> Creating a datastore with a new connectionCreating a datastore with an existing connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"presto_database\",\n        \"schema\": \"presto_schema\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"presto\",\n            \"host\": \"presto_host\",\n            \"port\": \"presto_port\",\n            \"username\": \"presto_username\",\n            \"password\": \"presto_password\",\n            \"parameters\":{\n                \"ssl_truststore\":\"truststore.jks\"\n            }\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"presto_database\",\n        \"schema\": \"presto_schema\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/presto/#linking-datastore-to-an-enrichment-datastore-through-api","title":"Linking Datastore to an Enrichment Datastore through API","text":""},{"location":"add-datastores/presto/#endpoint-patch","title":"Endpoint (Patch)","text":"<p><code>/api/datastores/{datastore-id}/enrichment/{enrichment-id}</code> (patch)</p>"},{"location":"add-datastores/redshift/","title":"Redshift","text":"<p>Adding and configuring a Redshift connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Redshift as both a source and an enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Redshift environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/redshift/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Redshift is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore. (e.g., The specified name will appear on the datastore cards.) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select Redshift from the dropdown list."},{"location":"add-datastores/redshift/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Redshift connector from the dropdown list and add connection details such as Secrets Management, port, host, password, database, and schema.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your Redshift account and add it to this field. 2. Port (Required) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the password associated with the Redshift user account. 5. Database (Required) Specify the database name. 6. Schema (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 8. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/redshift/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add new connection is turned off, then this will prompt you to configure the source datastore using existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Info</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/redshift/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you can add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1 Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2 Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3 Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/redshift/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Enter a name for the enrichment datastore. 3. Toggle Button for Add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>Once the HashiCorp Vault is set up, use the ${key} format in Connection form to reference a Vault secret.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 3: The configuration form, requesting credential details after selected enrichment datastore connector.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your Redshift account and add it to this field. 2. Port (Required) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the password associated with the Redshift user account. 5. Database (Required) Specify the database name to be accessed. 6. Schema  (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this datastore. <p>Step 4: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 5: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 6: Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/redshift/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2:  A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Team: The team associated with managing the enrichment datastore is based on the role of public or private. Example - Marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>Host: This is the server address where the Redshift instance is hosted. It is the endpoint used to connect to the Redshift environment.</p> </li> <li> <p>Database: Refers to the specific database within the Redshift environment where the data is stored.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/redshift/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/redshift/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a Redshift datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"redshift_database\",\n    \"schema\": \"redshift_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"redshift\",\n        \"host\": \"redshift_host\",\n        \"port\": \"redshift_port\",\n        \"username\": \"redshift_username\",\n        \"password\": \"redshift_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"redshift_database\",\n    \"schema\": \"redshift_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/redshift/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"redshift_database\",\n    \"schema\": \"redshift_schema\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"redshift\",\n        \"host\": \"redshift_host\",\n        \"port\": \"redshift_port\",\n        \"username\": \"redshift_username\",\n        \"password\": \"redshift_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"redshift_database\",\n    \"schema\": \"redshift_schema\",\n    \"enrich_only\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/redshift/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore:</p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/snowflake/","title":"Snowflake","text":"<p>Adding and configuring a Snowflake connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Snowflake as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Snowflake environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/snowflake/#snowflake-setup-guide","title":"Snowflake Setup Guide","text":"<p>The Snowflake Setup Guide provides step-by-step instructions for configuring warehouses and roles, ensuring efficient data management and access control. It explains how to create a warehouse with minimal requirements and the setup of a default warehouse for a user. It also explains how to create custom read-only and read-write roles and grant the necessary privileges for data access and modification. </p> <p>This guide is designed to help you optimize your Snowflake environment for performance and security, whether setting it up for the first time or refining your configuration.</p>"},{"location":"add-datastores/snowflake/#warehouse-role-configuration","title":"Warehouse &amp; Role Configuration","text":"<p>This section provides instructions for configuring Snowflake warehouses and roles. It includes creating a warehouse with minimal requirements, assigning a default warehouse for a user, creating custom read-only and read-write roles, and granting privileges to these roles for data access and modification.</p>"},{"location":"add-datastores/snowflake/#create-a-warehouse","title":"Create a Warehouse","text":"<p>Use the following command to create a warehouse with minimal requirements:</p> <pre><code>CREATE WAREHOUSE qualytics_wh\nWITH\n    WAREHOUSE_SIZE = 'XSMALL'\n    AUTO_SUSPEND = 60\n    AUTO_RESUME = TRUE;\n</code></pre> <p>Set a specific warehouse as the default for a user:</p> <pre><code>ALTER USER &lt;username&gt; SET DEFAULT_WAREHOUSE = qualytics_wh;\n</code></pre>"},{"location":"add-datastores/snowflake/#source-datastore-privileges-and-permissions","title":"Source Datastore Privileges and Permissions","text":"<p>Create a new role called <code>qualytics_read_role</code> and grant it privileges:</p> <pre><code>CREATE ROLE qualytics_read_role;\nGRANT USAGE ON WAREHOUSE qualytics_wh TO ROLE qualytics_read_role;\nGRANT USAGE ON DATABASE &lt;database_name&gt; TO ROLE qualytics_read_role;\nGRANT USAGE ON SCHEMA &lt;database_name&gt;.&lt;schema_name&gt; TO ROLE qualytics_read_role;\nGRANT SELECT ON TABLE &lt;database_name&gt;.&lt;schema_name&gt;.&lt;table_name&gt; TO ROLE qualytics_read_role;\nGRANT SELECT ON ALL TABLES IN SCHEMA &lt;database_name&gt;.&lt;schema_name&gt; TO ROLE qualytics_read_role;\nGRANT SELECT ON ALL VIEWS IN SCHEMA &lt;database_name&gt;.&lt;schema_name&gt; TO ROLE qualytics_read_role;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA &lt;database_name&gt;.&lt;schema_name&gt; TO ROLE qualytics_read_role;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA &lt;database_name&gt;.&lt;schema_name&gt; TO ROLE qualytics_read_role;\nGRANT ROLE qualytics_read_role TO USER &lt;user_name&gt;;\n</code></pre>"},{"location":"add-datastores/snowflake/#enrichment-datastore-privileges-and-permissions","title":"Enrichment Datastore Privileges and Permissions","text":"<p>Create a new role called <code>qualytics_readwrite_role</code> and grant it privileges:</p> <pre><code>CREATE ROLE qualytics_readwrite_role;\nGRANT USAGE ON WAREHOUSE qualytics_wh TO ROLE qualytics_readwrite_role;\nGRANT USAGE, MODIFY ON DATABASE &lt;database_name&gt; TO ROLE qualytics_readwrite_role;\nGRANT USAGE, MODIFY ON SCHEMA &lt;database_name&gt;.&lt;qualytics_schema&gt; TO ROLE qualytics_readwrite_role;\nGRANT CREATE TABLE ON SCHEMA &lt;database_name&gt;.&lt;qualytics_schema&gt; TO ROLE qualytics_readwrite_role;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA &lt;database_name&gt;.&lt;qualytics_schema&gt; TO ROLE qualytics_readwrite_role;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA &lt;database_name&gt;.&lt;qualytics_schema&gt; TO ROLE qualytics_readwrite_role;\nGRANT SELECT ON ALL TABLES IN SCHEMA &lt;database_name&gt;.&lt;qualytics_schema&gt; TO ROLE qualytics_readwrite_role;\nGRANT SELECT ON ALL VIEWS IN SCHEMA &lt;database_name&gt;.&lt;qualytics_schema&gt; TO ROLE qualytics_readwrite_role;\nGRANT ROLE qualytics_readwrite_role TO USER &lt;user_name&gt;;\n</code></pre>"},{"location":"add-datastores/snowflake/#authentication-changes-in-snowflake","title":"Authentication Changes in Snowflake","text":"<p>Snowflake has announced a migration plan to phase out Basic authentication (username and password) for service accounts in favor of Key-Pair authentication. While basic authentication is still supported, organizations should begin planning their migration to ensure uninterrupted service.</p>"},{"location":"add-datastores/snowflake/#user-type-classification","title":"User Type Classification","text":"<p>Snowflake differentiates between user types based on their intended purpose:</p> User Type Purpose Current Authentication Support Human users (<code>TYPE=PERSON</code>) Interactive users accessing Snowflake Basic authentication supported Service users (<code>TYPE=SERVICE</code>) Applications and services (like Qualytics) Key-Pair authentication recommended Legacy service (<code>TYPE=LEGACY_SERVICE</code>) Temporary transition type Basic authentication (being phased out)"},{"location":"add-datastores/snowflake/#migration-timeline","title":"Migration Timeline","text":"<p>Snowflake's migration plan includes:</p> <ol> <li>Current Phase: Basic authentication still supported for service accounts</li> <li>Transition Phase: <code>LEGACY_SERVICE</code> user type available for organizations needing additional migration time</li> <li>Future Phase: Basic authentication will be fully deprecated for service users</li> </ol>"},{"location":"add-datastores/snowflake/#recommended-actions","title":"Recommended Actions","text":"<p>To prepare for this transition:</p> <ul> <li>New connections: Use Key-Pair authentication when creating new Snowflake datastores</li> <li>Existing connections: Plan migration from Basic to Key-Pair authentication</li> <li>Service accounts: Ensure proper user type classification (<code>TYPE=SERVICE</code>)</li> </ul>"},{"location":"add-datastores/snowflake/#additional-resources","title":"Additional Resources","text":"<p>For detailed information on the migration plan and implementation:</p> <ul> <li>Snowflake Security MFA Rollout (User Types &amp; Deprecation)</li> <li>Snowflake Key-Pair Authentication Guide</li> </ul> <p>Migration Recommendation</p> <p>While Basic authentication is currently supported, migrating to Key-Pair authentication ensures your Snowflake connections remain secure and future-proof as Snowflake implements their deprecation timeline.</p>"},{"location":"add-datastores/snowflake/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Snowflake is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore. (e.g., The specified name will appear on the datastore cards.) 2. Toggle Button Toggle ON to  create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select Snowflake from the dropdown list."},{"location":"add-datastores/snowflake/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a New Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Snowflake connector from the dropdown list and add the connection details.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Account (Required) Define the account identifier to be used for accessing the Snowflake. 2. Role (Required) Specify the user role that grants appropriate access and permissions. 3. Warehouse (Required) Provide the warehouse name that will be used for computing resources. 4 Authentication You can choose between Basic authentication or Keypair authentication for validating and securing the connection to your Snowflake instance.  Basic Authentication: This method uses a username and password combination for authentication. It is a straightforward method where the user's credentials are directly used to access Snowflake. <ul><li>Type: Select the authentication type from the dropdown menu.</li><li>User: Enter the username that Qualytics will use to connect to Snowflake.</li><li>Password: Enter the password associated with the specified user account.</li></ul> Keypair Authentication: This method uses a combination of a private key and a corresponding public key for authentication. This is a more secure method compared to basic authentication, as it involves asymmetric cryptography. <ul> <li>Type: Select \"Keypair\" from the dropdown menu.</li><li>User: Enter the username that Qualytics will use to connect to Snowflake.</li> <li>Private Key: Upload the private key file that will be used for authentication. This key is part of a public-private key pair used to securely authenticate the user.</li> <li>Private Key Password (Optional): Enter the password associated with the private key, if any. </li> </ul> 5. Database (Required) Specify the database name to be accessed. 6. Schema (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 8. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p>"},{"location":"add-datastores/snowflake/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection  button to check and verify the source data connection. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Info</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/snowflake/#add-enrichment-datastore-connection","title":"Add Enrichment Datastore Connection","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1 Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2 Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3 Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/snowflake/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window - Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>Once HashiCorp Vault is set up, use the ${key} format in the Connection form to reference a Vault secret.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 3: The configuration form, requesting credential details after selecting the enrichment datastore connector.</p> <p></p> REF. FIELDS ACTIONS 1. Account (Required) Define the account identifier to be used for accessing the Snowflake. 2. Role (Required) Specify the user role that grants appropriate access and permissions. 3. Warehouse (Required) Provide the warehouse name that will be used for computing resources. 4. Authentication You can choose between Basic authentication or Keypair authentication for validating and securing the connection to your Snowflake instance.  Basic Authentication: This method uses a username and password combination for authentication. It is a straightforward method where the user's credentials are directly used to access Snowflake. <ul><li>Type: Select the authentication type from the dropdown menu.</li><li>User: Enter the username that Qualytics will use to connect to Snowflake.</li><li>Password: Enter the password associated with the specified user account.</li></ul> Keypair Authentication: This method uses a combination of a private key and a corresponding public key for authentication. This is a more secure method compared to basic authentication, as it involves asymmetric cryptography. <ul> <li>Type: Select \"Keypair\" from the dropdown menu.</li><li>User: Enter the username that Qualytics will use to connect to Snowflake.</li> <li>Private Key: Upload the private key file that will be used for authentication. This key is part of a public-private key pair used to securely authenticate the user.</li> <li>Private Key Password (Optional): Enter the password associated with the private key, if any. </li> </ul> 5. Database (Required) Specify the database name to be accessed. 6. Schema (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. <p>Step 4: Click on the Test Connection button to verify the enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the enrichment datastore has been successfully verified.</p> <p></p> <p>Step 5: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 6: Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/snowflake/#option-ii-use-an-existing-datastore","title":"Option II: Use an Existing Datastore","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2:  A modal window - Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files for metadata. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore. Example - All users are assigned to the Public team, which means that this enrichment datastore is accessible to all users.</p> </li> <li> <p>Host: This is the host domain of the Snowflake instance.</p> </li> <li> <p>Database: Refers to the specific database within the Snowflake environment. This database is a logical grouping of schemas. Each database belongs to a single Snowflake account.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/snowflake/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/snowflake/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a Snowflake datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"snowflake_database\",\n    \"schema\": \"snowflake_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"snowflake\",\n        \"host\": \"snowflake_host\",\n        \"username\": \"snowflake_username\",\n        \"password\": \"snowflake_password\",\n        \"passphrase\": \"key_passphrase\",\n        \"parameters\": {\n            \"role\": \"snowflake_read_role\",\n            \"warehouse\": \"qualytics_wh\",\n            \"authentication_type\": \"KEYPAIR\"\n        }\n    }\n}\n</code></pre> <p>Note</p> <p>If the <code>authentication_type</code> parameter is removed, <code>BASIC</code> authentication will be used by default.</p> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"snowflake_database\",\n    \"schema\": \"snowflake_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/snowflake/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"snowflake_database\",\n    \"schema\": \"snowflake_schema\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"snowflake\",\n        \"host\": \"snowflake_host\",\n        \"username\": \"snowflake_username\",\n        \"password\": \"snowflake_password\",\n        \"parameters\": {\n            \"role\": \"snowflake_readwrite_role\",\n            \"warehouse\": \"qualytics_wh\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"snowflake_database\",\n    \"schema\": \"snowflake_schema\",\n    \"enrich_only\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/snowflake/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore:</p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/synapse/","title":"Synapse","text":"<p>Adding and configuring a Synapse connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on adding Synapse as a source datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Synapse environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/synapse/#add-the-source-datastore","title":"Add the Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Synapse is an example of such a datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the Synapse datastore allows the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name Specify the name of the datastore (e.g., The specified name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection 3. Connector Select Synapse from the dropdown list."},{"location":"add-datastores/synapse/#option-i-create-a-datastore-with-a-new-connection","title":"Option I: Create a Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Synapse connector from the dropdown list and add connection details such as Secret Management, host, port, username, etc.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get Hostname from your Synapse account and add it to this field. 2. Port Specify the Port number. 3. User Enter the User ID to connect. 4. Password Enter the Password to connect to the database. 5. Database Specify the database name. 6. Schema Define the schema within the database that should be used. 7. Teams Select one or more teams from the dropdown to associate with the source datastore. 8. Initial Cataloging Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/synapse/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Use an existing connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to check and verify the source data connection. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.  </p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/synapse/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>After successfully testing and verifying your source datastore connection, you have the option to add an enrichment datastore (recommended). This datastore is used to store analyzed results, including any anomalies and additional metadata in tables. This setup provides comprehensive visibility into your data quality, enabling you to manage and improve it effectively.</p> <p>Warning</p> <p>Qualytics does not support the Synapse connector as an enrichment datastore, but you can point to a different enrichment datastore.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/synapse/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for Add New Connection Toggle ON to create a new enrichment datastore from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p>If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 5: Close the Success dialogue and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/synapse/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example - Marked as Public means that this datastore is accessible to all the users.  </li> <li>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment.  </li> <li>Database: Refers to the specific database within the enrichment datastore environment where the data is stored.  </li> <li>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/synapse/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"add-datastores/synapse/#creating-a-datastore","title":"Creating a Datastore","text":"<p>This section provides a sample payload for creating a datastore. Replace the placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/synapse/#endpoint-post","title":"Endpoint (Post)","text":"<p><code>/api/datastores</code> (post)</p> Creating a datastore with a new connectionCreating a datastore with an existing connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"synapse_database\",\n        \"schema\": \"synapse_schema\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"synapse\",\n            \"host\": \"synapse_host\",\n            \"port\": \"synapse_port\",\n            \"username\": \"synapse_username\",\n            \"password\": \"synapse_password\"\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"synapse_database\",\n        \"schema\": \"synapse_schema\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/synapse/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":""},{"location":"add-datastores/synapse/#endpoint-post_1","title":"Endpoint (Post)","text":"<p><code>/api/datastores</code> (post)</p> <p>This section provides a sample payload for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> Creating an enrichment datastore with a new connectionCreating an enrichment datastore with an existing connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"synapse_database\",\n        \"schema\": \"synapse_schema\",\n        \"enrich_only\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"synapse\",\n            \"host\": \"synapse_host\",\n            \"port\": \"synapse_port\",\n            \"username\": \"synapse_username\",\n            \"password\": \"synapse_password\"\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"synapse_database\",\n        \"schema\": \"synapse_schema\",\n        \"enrich_only\": true,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/synapse/#linking-datastore-to-an-enrichment-datastore-through-api","title":"Linking Datastore to an Enrichment Datastore through API","text":""},{"location":"add-datastores/synapse/#endpoint-patch","title":"Endpoint (Patch)","text":"<p><code>/api/datastores/{datastore-id}/enrichment/{enrichment-id}</code> (patch)</p>"},{"location":"add-datastores/teradata/","title":"Teradata","text":"<p>Adding and configuring a Teradata connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.  </p> <p>This documentation provides a step-by-step guide on adding Teradata as a source datastore in Qualytics. It covers the entire process from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Teradata environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/teradata/#add-the-source-datastore","title":"Add the Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Teradata is an example of such a datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the Teradata datastore allows the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name Specify the name of the datastore (e.g., the specified name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection 3. Connector Select Teradata from the dropdown list."},{"location":"add-datastores/teradata/#option-i-create-a-datastore-with-a-new-connection","title":"Option I: Create a Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Teradata connector from the dropdown list and add connection details such as Secret Management, host, port, username, etc.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get the Hostname from your Teradata account and add it to this field. 2. Port Specify the Port number. 3. User Enter the User ID to connect. 4. Password Enter the password to connect to the database. 5. Database Specify the database name. 6. Teams Select one or more teams from the dropdown to associate with this source datastore. 7. Initial Cataloging Check the checkbox to automatically perform a catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/teradata/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Use an existing connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit details such as Database, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to check and verify the source data connection. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/teradata/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>After successfully testing and verifying your source datastore connection, you have the option to add an enrichment datastore (recommended). This datastore is used to store analyzed results, including any anomalies and additional metadata in tables. This setup provides comprehensive visibility into your data quality, enabling you to manage and improve it effectively.</p> <p>Warning</p> <p>Qualytics does not support the Teradata connector as an enrichment datastore, but you can point to a different enrichment datastore.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window- Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1 Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2 Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3 Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/teradata/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Note</p> <p>Qualytics does not support Teradata as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using BigQuery as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4:  Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 5: Close the success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/teradata/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Note</p> <p>Qualytics does not support Teradata as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using BigQuery as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. For example, marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment.</p> </li> <li> <p>Database: This refers to the specific database within the enrichment datastore environment where the data is stored.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/teradata/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"add-datastores/teradata/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides a sample payload for creating a datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint (Post): <code>/api/datastores (post)</code></p> Creating a source datastore with a new connectionCreating a datastore with an existing connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"schema\": \"schema_name\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"host\": \"teradata_host\",\n        \"port\": \"teradata_port\",\n        \"username\": \"teradata_user\",\n        \"password\": \"teradata_password\",\n        \"type\": \"teradata\"\n        }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"schema\": \"schema_name\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection_id\n}\n</code></pre>"},{"location":"add-datastores/teradata/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Endpoint Patch:</p> <p><code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/timescale-db/","title":"TimescaleDB","text":"<p>Adding and configuring a TimescaleDB connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on adding TimescaleDB as a source datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their TimescaleDB environment is properly connected with Qualytics, unlocking the platform\u2019s potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/timescale-db/#add-the-source-datastore","title":"Add the Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. TimescaleDB is an example of such a datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the TimescaleDB datastore allows the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore. (e.g., The specified name will appear on the datastore cards.) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector Select TimescaleDB from the dropdown list."},{"location":"add-datastores/timescale-db/#option-i-create-a-datastore-with-a-new-connection","title":"Option I: Create a Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the TimescaleDB connector from the dropdown list and add connection details such as Secrets Management, host, port, username, database, and schema.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get Hostname from your TimescaleDB account and add it to this field. 2. Port Specify the Port number. 3. User Enter the User ID to connect. 4. Password Enter the password to connect to the database. 5. Database Specify the database name. 6. Schema Define the schema within the database that should be used. 7. Teams Select one or more teams from the dropdown to associate with this source datastore. 8. Initial cataloging Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/timescale-db/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New Connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Step 2: Click on the Test Connection button to check and verify the source data connection. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next  button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/timescale-db/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>After successfully testing and verifying your source datastore connection, you have the option to add an enrichment datastore (recommended). This datastore is used to store analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, enabling you to manage and improve it effectively.</p> <p>Warning</p> <p>Qualytics does not support the TimescaleDB connector as an enrichment datastore, but you can point to a different enrichment datastore.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1 Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2 Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3 Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/timescale-db/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add New Connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Enter a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment datastore from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Note</p> <p>Qualytics does not support TimescaleDB as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using Microsoft SQL Server as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 5: Close the Success dialogue and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p>"},{"location":"add-datastores/timescale-db/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p>Note</p> <p>Qualytics does not support Timescale as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using Bank Enrichment as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example - Marked as Public means that this datastore is accessible to all the users.</li> <li>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment.</li> <li>Database: Refers to the specific database within the enrichment datastore environment where the data is stored.</li> <li>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal window will display and a success flash message stating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/timescale-db/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"add-datastores/timescale-db/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides a sample payload for creating a TimescaleDB datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint (Post): <code>/api/datastores (post)</code></p> Creating a source datastore with a new connectionCreating a source datastore with an existing connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"timescale_database\",\n    \"schema\": \"timescale_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"timescale\",\n        \"host\": \"timescale_host\",\n        \"port\": \"timescale_port\",\n        \"username\": \"timescale_username\",\n        \"password\": \"timescale_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"timescale_database\",\n    \"schema\": \"timescale_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/timescale-db/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/trino/","title":"Trino","text":"<p>Adding and configuring a Trino connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on adding Trino as a source datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Trino environment is properly connected with Qualytics, unlocking the platform\u2019s potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/trino/#add-the-source-datastore","title":"Add the Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Trino is an example of such a datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the Trino datastore allows the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELD ACTIONS 1. Name Specify the name of the datastore (e.g., the specified name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection 3. Connector Select Trino from the dropdown list."},{"location":"add-datastores/trino/#option-i-create-a-datastore-with-a-new-connection","title":"Option I: Create a Datastore with a new Connection","text":"<p>If the toggle for Add new connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Trino connector from the dropdown list and add connection details such as Secret Management, host, port, username, etc.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get Hostname from your Trino account and add it to this field. 2. Port Specify the Port number. 3. User Enter the User ID to connect. 4. Password Enter the Password to connect to the database. 5. Catalog Add a Catalog to fetch data structures and metadata from Trino. 6. Schema Define the schema within the database that should be used. 7. Teams Select one or more teams from the dropdown to associate with this source datastore. 8. Initial Cataloging Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/trino/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add new connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to check and verify the source datastore connection. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/trino/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>After successfully testing and verifying your source datastore connection, you have the option to add an enrichment datastore (recommended). This datastore is used to store analyzed results, including any anomalies and additional metadata in tables. This setup provides comprehensive visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/trino/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment datastore from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 5: Close the success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/trino/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example - Marked as Public means that this datastore is accessible to all the users.  </li> <li>Host: This is the server address where the Trino instance is hosted. It is the endpoint used to connect to the Trino environment.  </li> <li>Database: Refers to the specific database within the Trino environment where the data is stored.  </li> <li>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/trino/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"add-datastores/trino/#creating-a-datastore","title":"Creating a Datastore","text":"<p>This section provides a sample payload for creating a datastore. Replace the placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/trino/#endpoint-post","title":"Endpoint (Post)","text":"<p><code>/api/datastores</code> (post)</p> Creating a datastore with a new connectionCreating a datastore with an existing connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"trino_database\",\n        \"schema\": \"trino_schema\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"trino\",\n            \"host\": \"trino_host\",\n            \"port\": \"trino_port\",\n            \"username\": \"trino_username\",\n            \"password\": \"trino_password\",\n            \"parameters\":{\n                \"ssl_truststore\":\"truststore.jks\"\n            }\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"trino_database\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/trino/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":""},{"location":"add-datastores/trino/#endpoint-post_1","title":"Endpoint (Post)","text":"<p><code>/api/datastores</code> (post)</p> <p>This section provides a sample payload for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> Creating an enrichment datastore with a new connectionCreating an enrichment datastore with an existing connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"trino_database\",\n        \"schema\": \"trino_schema\",\n        \"enrich_only\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"trino\",\n            \"host\": \"trino_host\",\n            \"port\": \"trino_port\",\n            \"username\": \"trino_username\",\n            \"password\": \"trino_password\",\n            \"parameters\":{\n                \"ssl_truststore\":\"truststore.jks\"\n            }\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"trino_database\",\n        \"schema\": \"trino_schema\",\n        \"enrich_only\": true,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/trino/#linking-datastore-to-an-enrichment-datastore-through-api","title":"Linking Datastore to an Enrichment Datastore through API","text":""},{"location":"add-datastores/trino/#endpoint-patch","title":"Endpoint (Patch)","text":"<p><code>/api/datastores/{datastore-id}/enrichment/{enrichment-id}</code> (patch)</p>"},{"location":"anomalies/acknowledge-anomalies/","title":"Acknowledge Anomalies","text":"<p>By acknowledging anomalies, you indicate that they have been reviewed or recognized. This can be done either individually or in bulk, depending on your workflow. Acknowledging anomalies helps you keep track of issues that have been addressed, even if further action is still required.</p> <p>Warning</p> <p>Once an anomaly is acknowledged, it remains acknowledged and never reverts to the active state.</p>"},{"location":"anomalies/acknowledge-anomalies/#acknowledge-specific-anomaly","title":"Acknowledge Specific Anomaly","text":"<p>You can acknowledge individual anomalies either directly or through the action menu, giving you precise control over each anomaly's status.</p> <p>Step 1: Log in to your Qualytics account and select the datastore from the left menu on which you want to manage your anomalies.</p> <p></p> <p>Step 2: Click on the \u201cAnomalies\u201d from the Navigation Tab.</p> <p></p> <p>1. Acknowledge Directly</p> <p>Step 1: Locate the active anomaly you want to acknowledge.</p> <p></p> <p>Step 2: Click on the vertical ellipsis (\u22ee) located on the right side of the anomaly and select \u201cAcknowledge\u201d from the dropdown menu.</p> <p></p> <p>After clicking on the Acknowledge button your anomaly is successfully moved to the acknowledged state and a confirmation message appears on the screen.</p> <p>2. Acknowledge via Action Menu</p> <p>Step 1: Click on the active anomaly from the list of available anomalies that you want to acknowledge.</p> <p></p> <p>Step 2: You will be directed to the anomaly details page. Click on the Acknowledge button located at the top-right corner of the interface.</p> <p></p> <p>After clicking on the Acknowledge button your anomaly is successfully moved to the acknowledged state and a confirmation message appears on the screen.</p>"},{"location":"anomalies/acknowledge-anomalies/#acknowledge-anomalies-in-bulk","title":"Acknowledge Anomalies in Bulk","text":"<p>By acknowledging anomalies in bulk, you can quickly mark multiple anomalies as reviewed at once, saving time and ensuring that all relevant issues are addressed simultaneously.</p> <p>Step 1: Hover over the active anomalies and click on the checkbox to select multiple anomalies.</p> <p></p> <p>When multiple anomalies are selected, an action toolbar appears, displaying the total number of selected anomalies along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click on the vertical ellipsis (\u22ee) and choose \"Acknowledge\" from the dropdown menu to acknowledge the selected anomalies.</p> <p></p> <p>A modal window titled \u201cAcknowledge Anomalies\u201d will appear, confirming that this action acknowledges the anomalies as a legitimate data quality concern.</p> <p>You also have the option to leave a comment in the provided field to provide additional context or details.</p> <p></p> <p>Step 3: Click on the Acknowledge button to acknowledge the anomalies.</p> <p></p> <p>After clicking on the Acknowledge button your anomalies are successfully moved to the acknowledged state and a confirmation message appears on the screen.</p>"},{"location":"anomalies/anomalies/","title":"Anomalies","text":"<p>Anomalies in Qualytics represent data points that deviate from expected patterns or violate defined quality rules, often highlighting issues such as missing values, structural inconsistencies, or incorrect data. These anomalies are detected during scan operations through system-inferred or user-authored checks. </p>"},{"location":"anomalies/anomalies/#anomaly-types","title":"Anomaly Types","text":"<p>Qualytics classifies anomalies into two types: Record Anomalies and Shape Anomalies. Record anomalies flag rows with data issues like missing or invalid values, while shape anomalies detect structural problems such as missing columns or schema changes. Together, they ensure thorough data quality coverage at both the value and structure levels.</p> <p>Note</p> <p>For more information, please refer to the Anomaly Types Documentation.</p>"},{"location":"anomalies/anomalies/#anomaly-detection-process","title":"Anomaly Detection Process","text":"<p>The anomaly detection process in Qualytics ensures data quality by identifying deviations from expected patterns through a structured workflow. It starts with configuring datastores, cataloging metadata, and profiling data to understand its structure. Users then apply quality checks\u2014either authored or inferred\u2014during Scan operations. Any failures are flagged as anomalies, enabling timely detection and resolution of data issues to maintain overall data integrity.</p> <p>Note</p> <p>For more information, please refer to the Anomaly Detection Process Documentation.</p>"},{"location":"anomalies/anomaly-detection/","title":"Anomaly Detection Process","text":"<p>The anomaly detection process in Qualytics ensures data quality by identifying deviations from expected patterns through a structured workflow. It starts with configuring datastores, cataloging metadata, and profiling data to understand its structure. Users then apply quality checks\u2014either authored or inferred\u2014during Scan operations. Any failures are flagged as anomalies, enabling timely detection and resolution of data issues to maintain overall data integrity.</p> <p>Let\u2019s get started \ud83d\ude80</p> <p>1. Create a Datastore and Connection</p> <p>By setting up a datastore and establishing a connection to your data source (database or file system), you create a robust foundation for effective data management and analysis in Qualytics. This setup enables you to access, manipulate, and utilize your data efficiently, paving the way for advanced data quality checks, profiling, scanning, anomaly surveillance, and other analytics tasks.</p> <p>Note</p> <p>For more information, please refer to the Configuring Source Datastores documentation.</p> <p>2. Catalog Operation</p> <p>The Catalog operation involves systematically collecting data structures along with their corresponding metadata. This process also includes a thorough analysis of the existing metadata within the datastore. This ensures a solid foundation for the subsequent Profile and Scan operations.</p> <p>Note</p> <p>For more information, please refer to the Catalog Operation Documentation.</p> <p>3. Profile Operation</p> <p>The Profile operation enables training of the collected data structures and their associated metadata values. This is crucial for gathering comprehensive aggregated statistics on the selected data, providing deeper insights, and preparing the data for quality assessment.</p> <p>Note</p> <p>For more information, please refer to the documentation Profile Operation.</p> <p>4. Create Authored Checks</p> <p>Authored Checks are manually created data quality checks in Qualytics, defined by users either through the user interface (UI) or via API. These checks encapsulate a specific data quality check, along with additional context such as associated notifications, tags, filters, and tolerances. Authored checks can range from simple, template-based checks to more complex rules implemented through SQL or user-defined functions (UDFs) in Scala. By allowing users to define precise criteria for data quality, authored checks enable detailed monitoring and validation of data within the datastore, ensuring that it meets the specified standards and requirements.</p> <p>Note</p> <p>For more information, please refer to the documentation Authored Checks. </p> <p>5. Scan Operation</p> <p>The Scan operation asserts rigorous quality checks to identify any anomalies within the data. This step ensures data integrity and reliability by recording the analyzed data in your configured enrichment datastore, facilitating continuous data quality improvement.</p> <p>Note</p> <p>For more information, please refer to the documentation Scan Operation. </p> <p>6. Anomaly Analysis</p> <p>An Anomaly is a data record or column that fails a data quality check during a Scan Operation. These anomalies are identified through both Inferred and Authored Checks and are grouped together to highlight data quality issues. This process ensures that any deviations from expected data quality standards are promptly identified and addressed.</p> <p>Note</p> <p>For more information, please refer to the documentation Anomalies Overview.</p>"},{"location":"anomalies/anomaly-fingerprints/","title":"Anomaly Fingerprints","text":"<p>Anomaly fingerprints are unique identifiers generated for each detected anomaly to help the system recognize and manage duplicates effectively. By comparing these fingerprints, Qualytics can determine whether a newly detected anomaly matches a previously identified one. This mechanism reduces redundancy, ensures consistency in anomaly tracking, and simplifies decision-making during data quality operations.</p> <p>Info</p> <p>Fingerprinting works only when Incremental Row tracking is enabled.</p>"},{"location":"anomalies/anomaly-fingerprints/#duplicate-handling-configuration","title":"Duplicate Handling Configuration","text":"<p>Once anomalies are fingerprinted, Qualytics can use these unique identifiers to determine whether a newly detected anomaly matches any existing one. This fingerprint-based recognition powers the duplicate handling configuration during scan setup.</p> <p>When configuring a scan operation, you can define how the system should respond to anomalies that share fingerprints with previously detected ones:</p> <ul> <li> <p>Duplicate Status: You can instruct the system to automatically mark newly detected anomalies as \u201cDuplicate\u201d if their fingerprints match those of any open anomalies. These duplicates are then archived, ensuring focus remains on the original issue without creating redundant records.</p> </li> <li> <p>Re-opening Option: If a new anomaly matches an archived one, you can configure the system to automatically re-open the earlier anomaly. This ensures that reoccurring issues are not overlooked simply because they were resolved or dismissed in a prior scan.</p> </li> </ul>"},{"location":"anomalies/anomaly-fingerprints/#fingerprinting-criteria","title":"Fingerprinting Criteria","text":"<p>To determine whether anomalies are identical, Qualytics generates unique fingerprints based on specific criteria. These criteria differ depending on the type of anomaly being evaluated. This approach ensures that anomalies are only considered duplicates when they are truly the same in both nature and context.</p>"},{"location":"anomalies/anomaly-fingerprints/#record-anomalies","title":"Record Anomalies","text":"<p>For record-level anomalies, fingerprinting is based on the specific check that identified the issue and the complete source data of the anomalous record. This ensures that every unique row is evaluated precisely:</p> <ul> <li> <p>Identifying check: The check responsible for detecting the anomaly (e.g., a null value or out-of-range check).</p> </li> <li> <p>Source record data: All field values within the affected row.</p> </li> </ul> <p>Note</p> <p>Identical records across multiple scans will generate the same fingerprint and therefore be flagged as duplicates.</p>"},{"location":"anomalies/anomaly-fingerprints/#shape-anomalies","title":"Shape Anomalies","text":"<p>For shape anomalies\u2014which refer to patterns or distributions of data rather than individual records, the fingerprint is derived from a broader set of attributes:</p> <ul> <li> <p>Identifying check(s): The rule(s) that triggered the anomaly at the dataset level.</p> </li> <li> <p>Failure percentage: The proportion of records that failed the check(s) within the scanned batch.</p> </li> <li> <p>Maximum incremental identifier: The highest value of a designated incremental field (e.g., timestamp, ID) in the scanned dataset.</p> </li> </ul> <p>Tip</p> <p>Shape anomalies can only be fingerprinted if the data asset includes an incremental identifier. This field anchors the fingerprint to a specific range of data, ensuring accurate comparisons across different scans.</p> <p>This fingerprinting mechanism ensures consistent anomaly tracking by minimizing false duplicates and keeping historical issues relevant when they reoccur.</p>"},{"location":"anomalies/anomaly-fingerprints/#use-case-handling-daily-truncate-and-reload-tables","title":"Use Case: Handling Daily Truncate-and-Reload Tables","text":""},{"location":"anomalies/anomaly-fingerprints/#scenario","title":"Scenario","text":"<p>Many data pipelines use staging tables that follow a truncate-and-reload pattern daily. These tables present a unique challenge:</p> <ul> <li>No last update timestamp for incremental strategy  </li> <li>Table is completely truncated and reloaded each day  </li> <li>Same data anomalies appear repeatedly across scans  </li> <li>Standard incremental detection cannot identify \"already seen\" records  </li> </ul>"},{"location":"anomalies/anomaly-fingerprints/#problem","title":"Problem","text":"<p>Without fingerprinting, each daily scan treats truncated-and-reloaded data as entirely new:</p> <ul> <li>Day 1: Scan identifies 127 anomalies \u2192 Team acknowledges all 127  </li> <li>Day 2: Table truncated, data reloaded \u2192 Same 127 anomalies flagged as \"new\"  </li> <li>Day 3: Process repeats \u2192 Team faces anomaly fatigue from duplicate alerts  </li> </ul> <p>The lack of a persistent identifier means Qualytics cannot distinguish between truly new anomalies and recurring issues from reloaded data.</p>"},{"location":"anomalies/anomaly-fingerprints/#solution","title":"Solution","text":"<p>To handle recurring anomalies in truncate-and-reload tables, configure your scan to use fingerprint-based duplicate handling.</p> <p>Follow the steps in the scan operation configuration to reach the correct settings. Then, under Step 8 \u2192 Scan Settings, open the anomaly options section and enable both duplicate-handling options:</p> <ul> <li>Archive Duplicate Anomalies: When the same 127 anomalies appear again after the table reload, Qualytics recognizes their fingerprints and automatically marks them as duplicates rather than new anomalies.  </li> <li>Reactivate Recurring Anomalies: If an anomaly was previously archived or resolved but reappears in subsequent scans, Qualytics reactivates the original anomaly record, maintaining full historical context.  </li> </ul>"},{"location":"anomalies/anomaly-fingerprints/#benefits","title":"Benefits","text":"<ul> <li>Eliminates daily re-acknowledgment of the same known issues  </li> <li>Maintains clean anomaly counts reflecting only truly new problems  </li> <li>Preserves audit trail through anomaly reactivation  </li> <li>Reduces alert fatigue while ensuring genuine recurrences are tracked  </li> </ul>"},{"location":"anomalies/anomaly-fingerprints/#configuration","title":"Configuration","text":"<p>Enable these settings in Scan Settings of your Scan Operation:  </p> <ul> <li>Archive Duplicate Anomalies </li> <li>Reactivate Recurring Anomalies </li> </ul> <p>Set an appropriate Anomaly Rollup Threshold based on your data volume and tolerance for grouped anomalies.</p>"},{"location":"anomalies/anomaly-insights/","title":"Anomaly Insights","text":"<p>Anomaly Insights provides key insights into a specific data anomaly, including its status, anomalous record count, failed checks, and weight. It also shows when the anomaly was detected, the triggering scan, and the related datastore, table, and location. This view helps users quickly understand the scope and source of the anomaly for easier investigation and resolution.</p> <p>Let\u2019s get started \ud83d\ude80</p> <p>Step 1: Click on the anomaly that you want to see the details of.</p> <p></p> <p>You will be navigated to the details section, where you can view the Summary, Failed Checks, Source Records and Activity information.</p> <p></p>"},{"location":"anomalies/anomaly-insights/#summary-section","title":"Summary Section","text":"<p>The Summary section provides a quick overview of the anomaly's key attributes. It includes the anomaly\u2019s status, total anomalous records, failed checks, weight, detection time, scan information, and the corresponding datastore and table. This section helps users quickly understand where the anomaly occurred and its potential impact.</p> <p></p> No. Field Description 1 Status and Type Shows the current state and category of the anomaly. In this case, the anomaly is Active and of type Shape, indicating it relates to the structure or distribution of the data. 2 Anomalous Records Indicates the total number of records affected by the anomaly. Here, 102 records were identified as anomalous. 3 Failed Check Displays the number of data quality checks that were violated and triggered this anomaly. In this instance, 1 check failed. 4 Weight Represents the significance or impact of the anomaly. A higher weight value implies a more critical issue. This anomaly has a weight of 8. 5 Detected Shows how long ago the anomaly was first detected. When you hover over the time the anomaly was detected, a pop-up appears displaying the complete date and time. 6 Scan Indicates the scan operation that detected the anomaly. Scan ID #21379 is shown here, and it was an incremental scan. When you click on the expand icon, you will be directed to the Scan Results page where you can view the specific scan that detected the anomaly. 7 Source Datastore Identifies the dataset that contains the anomaly. This anomaly occurred in the \"Qualytics Databricks POC\" datastore. Clicking the expand icon opens a detailed view and navigates to the dataset\u2019s page for more information about the source datastore. 8 Table Points to the specific table involved in the anomaly. The affected table is raw_order. Clicking on the expand icon navigates to the table\u2019s page, providing more in-depth information about the table structure and contents. 9 Location Displays the full path of the table in the datastore. This helps users trace the exact location of the anomaly within the data pipeline. You can click on the copy icon to copy the full location path of the table where the anomaly was detected. 10 Tags Highlights the severity or categorization of the anomaly. The tag High indicates a high-priority issue. You can add or remove tags from the anomaly by clicking on the tag badge. <p></p>"},{"location":"anomalies/anomaly-insights/#failed-checks","title":"Failed Checks","text":"<p>The Failed Checks section lists the data quality checks that were violated and subsequently triggered the anomaly. Each listed item displays the check ID, type of violation, and a summarized description of the failure condition.</p> <p></p> <p>Click on a failed check to view the corresponding quality check information.</p> <p></p> <p>A right-side panel will open, allowing you to view the details without navigating to a different page.</p> <p></p>"},{"location":"anomalies/anomaly-insights/#source-records","title":"Source Records","text":"<p>The Source Records section displays all the data and fields related to the detected anomaly from the dataset. It is an Enrichment Datastore that is used to store the analyzed results, including any anomalies and additional metadata in files; therefore, it is recommended to add/link an enrichment datastore with your connected source datastore.</p> <p></p> <p>For more information on Source Records, please refer to the Source Records section in the documentation.</p>"},{"location":"anomalies/anomaly-insights/#activity-section","title":"Activity Section","text":"<p>The Activity section provides a complete timeline of actions and events related to the anomaly. It helps users track how the anomaly has been handled and by whom, ensuring better collaboration and accountability.</p> <p></p> <p>Users can leave comments to discuss the issue, add context, or communicate decisions. All comments are timestamped and attributed to the respective user.</p> <p>Note</p> <p>Users can\u2019t add, edit, or delete comments in the Activity section when an anomaly is archived (Duplicated, Invalid, or Resolved). Restore the anomaly to make updates, then revert its status if needed.</p> <p></p>"},{"location":"anomalies/anomaly-status/","title":"Anomaly Status","text":"<p>Anomaly status provides a structured way to track the lifecycle of data quality issues\u2014from detection to resolution. Each anomaly is assigned a status that indicates its current state, helping teams prioritize actions and maintain oversight. These statuses are divided into two main categories: Open, for anomalies that still need attention, and Archived, for those that have been resolved, dismissed, or categorized for reference.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"anomalies/anomaly-status/#open-anomalies","title":"Open Anomalies","text":"<p>Open anomalies are data quality issues that have been detected but not yet resolved or archived. This category is divided into three sub-statuses that help track the progress and handling of each anomaly.</p> <p></p> <p>1. Active: By clicking on the Active button, the user can see anomalies that are currently unresolved and have not been acknowledged, archived, or resolved. It may require immediate attention.</p> <p>2. Acknowledged:  By clicking on the Acknowledged button, the user can see an anomalies that has been reviewed and marked as acknowledged, though it may still need further action.</p> <p>3. All:  By clicking on the All button, the user can view all open anomalies, including those marked as Active and Acknowledged, providing a complete view of ongoing issues.</p>"},{"location":"anomalies/anomaly-status/#archived-anomalies","title":"Archived Anomalies","text":"<p>Archived anomalies are issues that have already been reviewed and moved out of the active monitoring flow. These anomalies are categorized based on how they were resolved or classified, helping maintain a clear historical record without cluttering ongoing monitoring efforts.</p> <p></p> <p>1. Resolved: This indicates that the anomaly was a legitimate data quality concern and has been addressed.</p> <p>2. Duplicate: This indicates that the anomaly is a duplicate of an existing record and has already been addressed.</p> <p>3. Invalid: This indicates that the anomaly is not a legitimate data quality concern and does not require further action.</p> <p>4. All: Displays all archived anomalies, including those marked as Resolved, Duplicate, and Invalid, giving a comprehensive view of all past issues.</p> <p>Note</p> <p>For more information refer to the Archived Anomalies Documentation.</p>"},{"location":"anomalies/anomaly-types/","title":"Anomaly Types","text":"<p>Anomalies in Qualytics are classified into two primary types, Record Anomalies and Shape Anomalies, each targeting different aspects of data integrity. Record anomalies flag individual rows that fail specific quality checks, such as missing or invalid values. Shape anomalies, on the other hand, detect structural issues in the dataset, like missing columns or schema mismatches. Together, these types provide a comprehensive approach to identifying both value-level and schema-level data quality issues.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"anomalies/anomaly-types/#record-anomaly","title":"Record Anomaly","text":"<p>A record anomaly identifies a single record (row) as anomalous and provides specific details regarding why it is considered anomalous. The simplest form of a record anomaly is a row that lacks an expected value for a field.</p>"},{"location":"anomalies/anomaly-types/#example-use-case","title":"Example Use Case","text":"<p>Scenario</p> <p>We have an Employee dataset used for payroll.</p> <p>Rules:</p> <ul> <li>Every employee must have a Salary greater than 40,000.  </li> <li>The dataset must contain these four columns: <code>id</code>, <code>name</code>, <code>age</code>, <code>salary</code>.  </li> <li>The <code>name</code> must follow the <code>\"First Last\"</code> format.</li> </ul> <p>Rule Checked: Salary &gt; 40,000</p>"},{"location":"anomalies/anomaly-types/#input-table","title":"Input Table","text":"id name age salary 1 John Doe 28 50,000 2 Jane Smith 35 75,000 3 Bob Johnson 22 30,000"},{"location":"anomalies/anomaly-types/#detection-result-record-anomaly","title":"Detection Result (Record Anomaly)","text":"id name age salary anomaly_reason 3 Bob Johnson 22 30,000 Salary is less than the required 40,000"},{"location":"anomalies/anomaly-types/#why-this-is-a-record-anomaly","title":"Why this is a Record Anomaly:","text":"<p>The table structure is correct. Only one row\u2019s value violates the rule.</p>"},{"location":"anomalies/anomaly-types/#shape-anomaly","title":"Shape Anomaly","text":"<p>A shape anomaly identifies an anomalous structure within the analyzed data. The simplest shape anomaly is a dataset that doesn't match the expected schema because it lacks one or more fields. Some shape anomalies only apply to a subset of the data being analyzed and can therefore produce a count of the number of rows that reflect the anomalous concern. Where that is possible, the shape anomaly's anomalous_record_count is populated.</p> <p>Note</p> <p>Sometimes, shape anomalies only affect a subset of the dataset. This means that only certain rows exhibit the structural issue, rather than the entire dataset.</p>"},{"location":"anomalies/anomaly-types/#example-use-case_1","title":"Example Use Case","text":"<p>Scenario</p> <p>We have a Sales Orders dataset.</p> <p>Rules:</p> <ul> <li>Required columns: <code>order_id</code>, <code>customer_id</code>, <code>order_date</code>, <code>total_amount</code>.  </li> <li><code>order_date</code> must be in YYYY-MM-DD format.</li> </ul>"},{"location":"anomalies/anomaly-types/#input-table_1","title":"Input Table","text":"order_id customer_id order_date 101 C001 2025-08-10 102 C002 08/11/2025 103 C003 2025-08-12"},{"location":"anomalies/anomaly-types/#detection-result-shape-anomalies","title":"Detection Result (Shape Anomalies)","text":"order_id customer_id order_date total_amount anomaly_reason 101 C001 2025-08-10 \u2013 Missing total_amount column 102 C002 08/11/2025 \u2013 Missing total_amount column; Date format incorrect 103 C003 2025-08-12 \u2013 Missing total_amount column"},{"location":"anomalies/anomaly-types/#why-this-is-a-shape-anomaly","title":"Why this is a Shape Anomaly:","text":"<ul> <li>A required column (<code>total_amount</code>) is completely missing from the structure.  </li> <li>A field format (<code>order_date</code> in row 102) does not match the required YYYY-MM-DD pattern.  </li> <li>The problem is with the shape/structure of the dataset, not just a wrong value.</li> </ul> <p>Note</p> <p>When a shape anomaly affects only a portion of the dataset, Qualytics can count the number of rows that have the structural problem. This count is stored in the anomalous_record_count field, providing a clear measure of how widespread the issue is within the dataset. Example: Imagine a dataset that is supposed to have columns for id, name, age, and salary. If some rows are missing the salary column, this would be flagged as a shape anomaly. If this issue only affects 50 out of 1,000 rows, the anomalous_record_count would be 50, indicating that 50 rows have a structural issue. </p>"},{"location":"anomalies/archive-anomalies/","title":"Archive Anomalies","text":"<p>By archiving anomalies, you move them to an inactive state, while still keeping them available for future reference or analysis. Archiving helps keep your active anomaly list clean without permanently deleting the records.</p>"},{"location":"anomalies/archive-anomalies/#archive-specific-anomalies","title":"Archive Specific Anomalies","text":"<p>You can archive individual anomalies either directly or through the action menu.</p>"},{"location":"anomalies/archive-anomalies/#1-archive-directly","title":"1. Archive Directly","text":"<p>Step 1: Locate the anomaly (whether Active or Acknowledged) you want to archive.</p> <p></p> <p>Step 2: Click on the vertical ellipsis (\u22ee) located on the right side of the anomaly and select \u201cArchive\u201d from the dropdown menu.</p> <p></p> <p>Step 3: A modal window titled \u201cArchive Anomaly\u201d will appear, providing you with the following archive options:</p> <ul> <li>Resolved: Choose this option if the anomaly was a legitimate data quality concern and has been addressed. This helps maintain a record of resolved issues while ensuring that they are no longer active.  </li> <li>Duplicate: Choose this option if the anomaly is a duplicate of an existing record and has already been addressed. No further action is required as the issue has been previously resolved.  </li> <li>Invalid: Select this option if the anomaly is not a legitimate data quality concern and does not require further action. Archiving anomalies as invalid helps differentiate between real issues and those that can be dismissed, improving overall data quality management.</li> </ul> <p></p> <p>You also have the option to leave a comment in the provided field to provide additional context or details.</p> <p></p> <p>Step 4: Once you've made your selection, click the Archive button to proceed.</p> <p></p> <p>After clicking on the Archive button your anomaly is moved to the archive and a confirmation message appears on the screen.</p>"},{"location":"anomalies/archive-anomalies/#2-archive-via-action-menu","title":"2. Archive via Action Menu","text":"<p>Step 1: Click on the anomaly from the list of available (whether Active or Acknowledged) anomalies that you want to archive.</p> <p></p> <p>Step 2: You will be directed to the anomaly details page. Click on the Settings icon located at the top right corner of the interface and select \u201cArchive\u201d from the dropdown menu.</p> <p></p> <p>Step 3: A modal window titled \u201cArchive Anomaly\u201d will appear, providing you with the following archive options:</p> <ul> <li>Resolved: Choose this option if the anomaly was a legitimate data quality concern and has been addressed. This helps maintain a record of resolved issues while ensuring that they are no longer active.  </li> <li>Duplicate: Choose this option if the anomaly is a duplicate of an existing record and has already been addressed. No further action is required as the issue has been previously resolved.  </li> <li>Invalid: Select this option if the anomaly is not a legitimate data quality concern and does not require further action. Archiving anomalies as invalid helps differentiate between real issues and those that can be dismissed, improving overall data quality management.</li> </ul> <p></p> <p>You also have the option to leave a comment in the provided field to provide additional context or details.</p> <p></p> <p>Step 4: Once you've made your selection, click the Archive button to proceed.</p> <p></p> <p>After clicking on the Archive button your anomaly is moved to the archive and a confirmation message appears on the screen.</p>"},{"location":"anomalies/archive-anomalies/#archive-anomalies-in-bulk","title":"Archive Anomalies in Bulk","text":"<p>To handle multiple anomalies efficiently, you can archive them in bulk, allowing you to quickly move large volumes of anomalies into the archived state.</p> <p>Step 1: Hover over the anomaly (whether Active or Acknowledged) and click on the checkbox to select multiple anomalies.</p> <p></p> <p>When multiple anomalies are selected, an action toolbar appears, displaying the total number of selected anomalies along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click on the vertical ellipsis (\u22ee) and choose \"Archive\" from the dropdown menu to archive the selected anomalies.</p> <p></p> <p>Step 3: A modal window titled \u201cArchive Anomaly\u201d will appear, providing you with the following archive options:</p> <ul> <li>Resolved: Choose this option if the anomaly was a legitimate data quality concern and has been addressed. This helps maintain a record of resolved issues while ensuring that they are no longer active.  </li> <li>Duplicate: Choose this option if the anomaly is a duplicate of an existing record and has already been addressed. No further action is required as the issue has been previously resolved.  </li> <li>Invalid: Select this option if the anomaly is not a legitimate data quality concern and does not require further action. Archiving anomalies as invalid helps differentiate between real issues and those that can be dismissed, improving overall data quality management.</li> </ul> <p></p> <p>You also have the option to leave a comment in the provided field to provide additional context or details.</p> <p></p> <p>Step 4: Once you've made your selection, click on the Archive button to proceed.</p> <p></p> <p>After clicking on the Archive button your anomaly is moved to the archive and a confirmation message appears on the screen.</p>"},{"location":"anomalies/delete-anomalies/","title":"Delete Anomalies","text":"<p>Deleting anomalies allows you to permanently remove records that are no longer relevant or were logged in error. This can be done individually or for multiple anomalies at once, ensuring that your anomaly records remain clean and up to date.</p> <p>Note</p> <p>You can only delete archived anomalies, not active or acknowledged anomalies. If you want to delete an active or acknowledged anomaly, you must first move it to the archive, and then you can delete it. </p> <p>Warning</p> <p>Deleting an anomaly is a one-time action. It cannot be restored after deletion.</p>"},{"location":"anomalies/delete-anomalies/#delete-specific-anomaly","title":"Delete Specific Anomaly","text":"<p>You can delete individual anomalies using one of two methods:</p>"},{"location":"anomalies/delete-anomalies/#1-delete-directly","title":"1. Delete Directly","text":"<p>Step 1: Click on Archived from the navigation bar in the Anomalies section to view all archived anomalies.</p> <p></p> <p>Step 2: Locate the anomaly that you want to delete and click on the Delete icon located on the right side of the anomaly.</p> <p></p> <p>Step 3: A confirmation modal window will appear, click on the Delete button to permanently remove the anomaly from the system.</p> <p></p> <p>After clicking on the Delete button, your anomaly is successfully deleted and a confirmation message appears on the screen.</p>"},{"location":"anomalies/delete-anomalies/#2-delete-via-action-menu","title":"2. Delete via Action Menu","text":"<p>Step 1: Click on the archived anomaly from the list of archived anomalies that you want to delete.</p> <p></p> <p>Step 2: You will be directed to the anomaly details page. Click on the Settings icon located at the top right corner of the page and select \u201cDelete\u201d from the dropdown menu.</p> <p></p> <p>Step 3: A confirmation modal window will appear, click on the Delete button to permanently remove the anomaly from the system.</p> <p></p> <p>After clicking on the Delete button, your anomaly is successfully deleted and a confirmation message appears on the screen.</p>"},{"location":"anomalies/delete-anomalies/#delete-anomalies-in-bulk","title":"Delete Anomalies in Bulk","text":"<p>For more efficient management, you can delete multiple anomalies at once using the bulk delete option, allowing for faster cleanup of unwanted records.</p> <p>Step 1: Hover over the archived anomalies and click on the checkbox to select anomalies in bulk.</p> <p></p> <p>When multiple checks are selected, an action toolbar appears, displaying the total number of checks chosen along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click on the vertical ellipsis (\u22ee) and choose \"Delete\" from the dropdown menu to delete the selected anomalies.</p> <p></p> <p>Step 3: A confirmation modal window will appear, click on the Delete button to permanently remove the selected anomalies from the system.</p> <p></p> <p>After clicking on the Delete button, your anomalies are successfully deleted and a confirmation message appears on the screen.</p>"},{"location":"anomalies/edit-anomalies/","title":"Edit Anomalies","text":"<p>By editing anomalies, you can only update their tags, allowing you to categorize and organize anomalies more effectively without altering their core details.</p> <p>Note</p> <p>When editing multiple anomalies in bulk, only the tags can be modified.</p> <p>Step 1: Hover over the anomaly (whether Active or Acknowledged) and click on the checkbox.</p> <p></p> <p>You can edit multiple anomalies by selecting the checkboxes next to each anomaly to choose multiple anomalies at once.</p> <p></p> <p>When multiple anomalies are selected, an action toolbar appears, displaying the total number of selected anomalies along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click on the vertical ellipsis (\u22ee) and choose \"Edit\" from the dropdown menu to edit the selected anomalies.</p> <p></p> <p>A modal window titled \u201cBulk Edit Anomalies\u201d will appear. Here you can only modify the \u201ctags\u201d of the selected anomalies.</p> <p></p> <p>Step 3: Turn on the toggle and assign tags to the selected anomalies.</p> <p></p> <p>Step 4: Once you have assigned the tags, click on the \u201cSave\u201d button.</p> <p>After clicking the Save button, the selected anomalies will be updated with the assigned tags.</p> <p></p>"},{"location":"anomalies/filter-and-sort/","title":"Filter and Sort","text":"<p>Filter and Sort options allow you to organize your anomalies by various criteria, such as Weight, Anomalous Records, and Created Date. You can also apply filters to refine your list of anomalies based on Timeframe, Type, and Rule etc.</p>"},{"location":"anomalies/filter-and-sort/#sort","title":"Sort","text":"<p>You can sorts your anomalies by Anomalous Records, Created Date, and Weight to easily organize and prioritize them according to your needs.</p> <p></p> No. Sort By Option Description 1 Anomalous Records Sorts anomalies based on the number of anomalous records identified. 2 Created Date Sorts anomalies according to the date they were detected. 3 Weight Sorts anomalies by their assigned weight or importance level. <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.</p> <p></p>"},{"location":"anomalies/filter-and-sort/#filter","title":"Filter","text":"<p>You can filter your anomalies based on values like Timeframe, Type, Rule, and Tags, etc.</p> <p></p> No. Field Description 1 Timeframe Filtering anomalies detected within specific time ranges (e.g., anomalies detected in the last week or year). 2 Type Filter anomalies based on anomaly type (Record or Shape). 3 Rule Filter anomalies based on specific rules applied to the anomaly. By clicking on the caret down button next to the Rule field, the available rule types will be dynamically populated based on the rule types present in the results. The rules displayed are based on the current dataset and provide more granular control over filtering. Each rule type will show a counter next to it, displaying the total number of occurrences for that rule in the dataset. For example, the rule type Between is displayed with a total of 3 occurrences. 4 Table Filters anomalies based on the table where they occurred. 5 Field Filters anomalies based on the column in the table where the issue was found. 6 Check Filters anomalies based on the check that generated them. <p></p> No. Filter Description 7 Tags Tag Filter displays only the tags associated with the currently visible items, along with their color icon, name, type, and the number of matching records. Selecting one or more tags refines the list based on your selection. If no matching items are found, a No options found message is displayed."},{"location":"anomalies/restore-anomalies/","title":"Restore Anomalies","text":"<p>By restoring archived anomalies, you can bring them back into the acknowledged state for further investigation or review. These anomalies will not return to the active state once they have been acknowledged.</p> <p>Step 1: Click on the anomaly that you want to restore from the list of archived anomalies.</p> <p></p> <p>Step 2: You will be directed to the anomaly details page. Click on the Settings icon located at the top right corner of the page and select \u201cRestore\u201d from the drop down menu.</p> <p></p> <p>After clicking on the Restore button, the selected anomaly is now restored in an acknowledged state and a confirmation message appears on the screen.</p>"},{"location":"anomalies/source-record/","title":"Source Records","text":"<p>The Source Records page provides a detailed view of all records from your dataset that have failed data quality checks and been identified as anomalies. It serves as the primary interface for reviewing anomalous data at the row level, with visual highlights indicating the specific fields that triggered the anomalies. All displayed records are sourced from the linked Enrichment Datastore, which stores the results of data quality scans along with relevant metadata.</p> <p>If the Anomaly Type is Shape, you will find the highlighted column(s) having anomalies in the source record.</p> <p></p> <p>If the Anomaly Type is Record, you will find the highlighted row(s) in the source record indicating failed checks. </p> <p></p> <p>Note</p> <p>In anomaly detection, source records are displayed as part of the Anomaly Details. For a Record anomaly, the specific record is highlighted. For a Shape anomaly, 10 samples from the underlying anomalous records are highlighted.</p>"},{"location":"anomalies/source-record/#source-record-visualization","title":"Source Record Visualization","text":"<p>Users can view source records with selectable display limits of 10, 100, 1,000, or 10,000 records for comprehensive dataset analysis. The interface includes sticky headers that remain visible when scrolling through large datasets, making navigation easier during data review.</p> <p></p>"},{"location":"anomalies/source-record/#comparison-source-records","title":"Comparison Source Records","text":"<p>Anomalies identified by the Data Diff rule type, configured with Row Identifiers, are displayed with a detailed source record comparison. This visualization highlights differences between rows, making it easier to identify specific discrepancies.</p> <p></p>"},{"location":"checks/after-date-check/","title":"After Date Time","text":""},{"location":"checks/after-date-check/#definition","title":"Definition","text":"<p>Asserts that the field is a timestamp later than a specific date and time.</p>"},{"location":"checks/after-date-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code>"},{"location":"checks/after-date-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/after-date-check/#specific-properties","title":"Specific Properties","text":"<p>Specify a particular date and time to act as the threshold for the rule.</p> Name Description Date The timestamp used as the lower boundary. Values in the selected field should be after this timestamp."},{"location":"checks/after-date-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/after-date-check/#example","title":"Example","text":"<p>Objective: Ensure that all O_ORDERDATE entries in the ORDERS table are later than 10:30 AM on December 31st, 1991.</p> <p>Sample Data</p> O_ORDERKEY O_ORDERDATE 1 1991-12-31 10:30:00 2 1992-01-02 09:15:00 3 1991-12-14 10:25:00 Payload example <pre><code>{\n    \"description\": \"Ensure that all O_ORDERDATE entries in the ORDERS table are later than 10:30 AM on December 31st, 1991.\",\n    \"coverage\": 1,\n    \"properties\":  {\n        \"datetime\": \"1991-12-31 10:30:00\"\n    },\n    \"tags\": [],\n    \"fields\": [\"O_ORDERDATE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"afterDateTime\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>O_ORDERKEY</code> 1 and 3 do not satisfy the rule because their <code>O_ORDERDATE</code> values are not after 1991-12-31 10:30:00.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_ORDERDATE]\nB --&gt; C{Is O_ORDERDATE &gt; '1991-12-31 10:30:00'?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s)\nselect\n    o_orderkey\n    , o_orderdate\nfrom orders \nwhere\n    o_orderdate &lt;= '1991-12-31 10:30:00'\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>O_ORDERDATE</code> value of <code>1991-12-14 10:30:00</code> is not later than 1991-12-31 10:30:00</p> <p>Shape Anomaly</p> <p>In <code>O_ORDERDATE</code>, 66.667% of 3 filtered records (2) are not later than 1991-12-31 10:30:00</p>"},{"location":"checks/aggregation-comparison-check/","title":"Aggregation Comparison","text":""},{"location":"checks/aggregation-comparison-check/#definition","title":"Definition","text":"<p>Verifies that the specified comparison operator evaluates true when applied to two aggregation expressions.</p>"},{"location":"checks/aggregation-comparison-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>Aggregation Comparison</code> is a rule that allows for the dynamic analysis of aggregations across different datasets. It empowers users to establish data integrity by ensuring that aggregate values meet expected comparisons, whether they are totals, averages, counts, or any other aggregated metric.</p> <p>By setting a comparison between aggregates from potentially different tables or even source datastores, this rule confirms that relationships between data points adhere to business logic or historical data patterns. This is particularly useful when trying to validate interrelated financial reports, summary metrics, or when monitoring the consistency of data ingestion over time.</p>"},{"location":"checks/aggregation-comparison-check/#field-scope","title":"Field Scope","text":"<p>Calculated: The rule automatically identifies the fields involved, without requiring explicit field selection.</p>"},{"location":"checks/aggregation-comparison-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/aggregation-comparison-check/#specific-properties","title":"Specific Properties","text":"<p>Facilitates the comparison between a <code>target</code> aggregate metric and a <code>reference</code> aggregate metric across different datasets.</p> Name Description Target Aggregation Specifies the aggregation expression to evaluate Comparison Select the comparison operator (e.g., greater than, less than, etc.) Datastore Identifies the source datastore for the reference aggregation Table/File Specifies the table or file for the reference aggregation Reference Aggregation Defines the reference aggregation expression to compare against Reference Filter Applies a filter to the reference aggregation if necessary <p>Details</p> <p>It's important to understand that each aggregation must result in a single row. Also, similar to Spark expressions, the aggregation expressions must be written in a valid format for DataFrames.</p> <p>Examples</p> <p>Simple Aggregations <pre><code>SUM(O_TOTALPRICE)\n</code></pre></p> <p>Combining with SparkSQL Functions <pre><code>ROUND(SUM(O_TOTALPRICE))\n</code></pre></p> <p>Complex Aggregations <pre><code>ROUND(SUM(L_EXTENDEDPRICE * (1 - L_DISCOUNT) * (1 + L_TAX)))\n</code></pre></p> <p>Aggregation Expressions <pre><code>COUNT(CATEGORY) * MAX(VALUE) - FIRST(VALUE)\n</code></pre></p> <p>Here are some common aggregate functions used in SparkSQL:</p> <ul> <li><code>SUM</code>: Calculates the sum of all values in a column.</li> <li><code>AVG</code>: Calculates the average of all values in a column.</li> <li><code>MAX</code>: Returns the maximum value in a column.</li> <li><code>MIN</code>: Returns the minimum value in a column.</li> <li><code>COUNT</code>: Counts the number of rows in a column.</li> </ul> <p>For a detailed list of valid SparkSQL aggregation functions, refer to the Apache Spark SQL documentation.</p> Payload example <pre><code>{\n    \"description\": \"Assert that O_ORDERDATE is after the defined date time\",\n    \"coverage\": 1,\n    \"properties\":  {\n        \"datetime\": \"1991-12-31 10:30:00\"\n    },\n    \"tags\": [],\n    \"fields\": fields,\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"afterDateTime\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n}\n</code></pre>"},{"location":"checks/aggregation-comparison-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/aggregation-comparison-check/#example","title":"Example","text":"<p>Objective: Ensure that the aggregated sum of <code>total_price</code> from the <code>ORDERS</code> table matches the aggregated and rounded sum of <code>calculated_price</code> from the <code>LINEITEM</code> table.</p> <p>Info</p> <p>The <code>calculated_price</code> in this example is represented by the sum of each product's extended price, adjusted for discount and tax.</p> <p>Sample Data</p> <p>Aggregated data from ORDERS (Target)</p> TOTAL_PRICE 5000000 <p>Aggregated data from LINEITEM (Reference)</p> CALCULATED_PRICE 4999800 Inputs <ul> <li>Target Aggregation: ROUND(SUM(O_TOTALPRICE))</li> <li>Comparison: eq (Equal To), lt (Less Than), lte (Less Than or Equal to), gte (Greater Than or Equal To), gt (Greater Than)</li> <li>Reference Aggregation: ROUND(SUM(L_EXTENDEDPRICE * (1 - L_DISCOUNT) * (1 + L_TAX)))</li> </ul> Payload example <pre><code>{\n    \"description\": \"Ensure that the aggregated sum of total_price from the ORDERS table matches the aggregated and sum of l_totalprice from the LINEITEM table\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"comparison\": \"eq\",\n        \"expression\": f\"SUM(O_TOTALPRICE)\",\n        \"ref_container_id\": ref_container_id,\n        \"ref_datastore_id\": ref_datastore_id,\n        \"ref_expression\": f\"SUM(L_TOTALPRICE)\",\n        \"ref_filter\": \"1=1\",\n    },\n    \"tags\": [],\n    \"fields\": [\"O_TOTALPRICE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"aggregationComparison\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the aggregated <code>TOTAL_PRICE</code> from the <code>ORDERS</code> table is 5000000, while the aggregated and rounded <code>CALCULATED_PRICE</code> from the <code>LINEITEM</code> table is 4999800. The difference between these totals indicates a potential anomaly, suggesting issues in data calculation or recording methods.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Aggregated Values]\nB --&gt; C{Do Aggregated Totals Match?}\nC --&gt;|Yes| D[End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query related to the rule using TPC-H tables.\nwith orders_agg as (\n    select \n        round(sum(o_totalprice)) as total_order_price\n    from \n        orders\n),\nlineitem_agg as (\n    select \n        round(sum(l_extendedprice * (1 - l_discount) * (1 + l_tax))) as calculated_price\n    from \n        lineitem\n),\ncomparison as (\n    select\n        o.total_order_price,\n        l.calculated_price\n    from\n        orders_agg o\n        cross join lineitem_agg l\n)\nselect * from comparison\nwhere comparison.total_order_price != comparison.calculated_price;\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p><code>ROUND(SUM(O_TOTALPRICE))</code> is not equal to <code>ROUND(SUM(L_EXTENDEDPRICE * (1 - L_DISCOUNT) * (1 + L_TAX)))</code>.</p>"},{"location":"checks/any-not-null-check/","title":"Any Not Null","text":""},{"location":"checks/any-not-null-check/#definition","title":"Definition","text":"<p>Asserts that at least one of the selected fields must hold a value.</p>"},{"location":"checks/any-not-null-check/#field-scope","title":"Field Scope","text":"<p>Multiple: The rule evaluates multiple specified fields.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/any-not-null-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/any-not-null-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/any-not-null-check/#example","title":"Example","text":"<p>Objective: Ensure that for every record in the ORDERS table, at least one of the fields (O_COMMENT, O_ORDERSTATUS) isn't null.</p> <p>Sample Data</p> O_ORDERKEY O_COMMENT O_ORDERSTATUS 1 NULL NULL 2 Good product NULL 3 NULL Shipped Payload example <pre><code>{\n    \"description\": \"Ensure that for every record in the ORDERS table, at least one of the fields (O_COMMENT, O_ORDERSTATUS) isn't null\",\n    \"coverage\": 1,\n    \"properties\": {},\n    \"tags\": [],\n    \"fields\": [\"O_ORDERSTATUS\",\"O_COMMENT\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"anyNotNull\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"_PARITY = 'odd'\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>O_ORDERKEY</code> 1 does not satisfy the rule because both <code>O_COMMENT</code> and <code>O_ORDERSTATUS</code> does not hold a value.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_COMMENT and O_ORDERSTATUS]\nB --&gt; C{Is Either Field Not Null?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    o_orderkey\n    , o_comment\n    , o_orderstatus\nfrom orders \nwhere\n    o_comment is null\n    and o_orderstatus is null\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>There is no value set for any of <code>O_COMMENT, O_ORDERSTATUS</code></p> <p>Shape Anomaly</p> <p>In <code>O_COMMENT, O_ORDERSTATUS</code>, 33.333% of 3 filtered records (1) have no value set for any of <code>O_COMMENT, O_ORDERSTATUS</code></p>"},{"location":"checks/apply-check-template-for-quality-checks/","title":"Apply Check Template for Quality Checks","text":"<p>You can export check templates to make quality checks easier and more consistent. Using a set template lets you quickly verify that your data meets specific standards, reducing mistakes and improving data quality. Exporting these templates simplifies the process, making finding and fixing errors more efficient, and ensuring your quality checks are applied across different projects or systems without starting from scratch.</p> <p>Let\u2019s get started \ud83d\ude80</p> <p>Step 1:  Log in to your Qualytics account and click the \u201cLibrary\u201d button on the left side panel of the interface.</p> <p></p> <p>Here you can view the list of all the customer data validation templates. </p> <p></p> <p>Step 2: Locate the template, click on the vertical ellipsis (three dots) next to it, and select \u201cAdd Check\u201d from the dropdown menu to create a Quality Check based on this template</p> <p>For demonstration purposes, we have selected the \u201cAfter Date Time\u201d template.</p> <p></p> <p>A modal window titled \u201cAuthored Check Template\u201d will appear, displaying all the details of the Quality Check Template.</p> <p></p> <p>Step 3: Enter the following details: </p> <p>1. Associate with a Check Template: </p> <ul> <li> <p>If you toggle ON the \"Associate with a Check Template\" option, the check will be linked to a specific template.</p> </li> <li> <p>If you toggle OFF the \"Associate with a Check Template\" option, the check will not be linked to any template, which allows you full control to modify the properties independently.</p> </li> </ul> <p>Since we are applying a check template to create quality checks, it's important to keep the toggle on to ensure the template is applied as a quality check.</p> <p></p> <p>2. Template: Choose a Template from the dropdown menu that you want to associate with the quality check. The check will inherit properties from the selected template.</p> <ul> <li> <p>Locked: If the template is locked, it will automatically sync with any future updates made to the template. However, you won't be able to modify the check's properties directly, except for specific fields like Datastore, Table, and Fields, which can still be updated while maintaining synchronization with the template.</p> </li> <li> <p>Unlocked: If the template is unlocked, you are free to modify the check's properties as needed. However, any future updates to the template will no longer affect this check, as it will no longer be synced with the template.</p> </li> </ul> <p></p> <p>3. Datastore: Select the Datastore, Table and Field where you want to apply the check template. This ensures that the template is linked to the correct data source, allowing the quality checks to be performed on the specified datastore.</p> <p>For demonstration purposes, we have selected the \u201cMIMIC II\u201d datastore, with the \u201cADMISSIONS\u201d table and the \u201cADMITTIME\u201d field.</p> <p></p> <p>Step 4: After completing all the check details, click on the \"Validate\" button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct. It ensures that the check will work as expected by running it against the data without committing any changes.</p> <p></p> <p>If the validation is successful, a green message will appear saying \"Validation Successful\". </p> <p></p> <p>If the validation fails, a red message will appear saying \"Failed Validation\". This typically occurs when the check logic or parameters do not match the data properly.</p> <p></p> <p>Step 5: Once you have a successful validation, click the \"Save\" button. </p> <p>Info</p> <p>You can create as many Quality checks as you want for a specific template.</p> <p></p> <p>After clicking on the \u201cSave\u201d button your check is successfully created and a success flash message will appear saying \u201cCheck successfully created\u201d.</p> <p></p>"},{"location":"checks/authored-check/","title":"Authored Check","text":"<p>Authored checks are manually created by users within the Qualytics platform or API. You can author many types of checks, ranging from simple templates for common checks to complex rules using Spark SQL and User-Defined Functions (UDF) in Scala.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"checks/authored-check/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and select the datastore from the left menu.</p> <p></p> <p>Step 2: Click on the \"Checks\" from the Navigation Tab.</p> <p></p> <p>Step 3: In the top-right corner, click on the \"Add\" button, then select \"Check\" from the dropdown menu.</p> <p></p> <p>A modal window titled \u201cAuthored Check Details\u201d will appear, providing you the options to add the authored check details.</p> <p></p> <p>Step 4: Enter the following details to add the authored check:</p> <p>1. Associate with a Check Template:</p> <ul> <li> <p>If you toggle ON the \"Associate with a Check Template\" option, the check will be linked to a specific template.</p> </li> <li> <p>If you toggle OFF the \"Associate with a Check Template\" option, the check will not be linked to any template, which allows you full control to modify the properties independently.</p> </li> </ul> <p></p> <p>2. Rule Type (Required): Select a Rule from the dropdown menu, such as checking for non-null values, matching patterns, comparing numerical values, or verifying date-time constraints. Each rule type defines the specific validation logic to be applied.</p> <p>For demonstration purposes we have selected the After Date Time rule type.</p> <p></p> <p>For more details about the available rule types, refer to the \"Rule Types Overview\" documentation.</p> <p>Note</p> <p>Different rule types have different sets of fields and options appearing when selected. </p> <p>3. File (Required): Select a file from the dropdown menu on which the check will be performed.</p> <p></p> <p>4. Field (Required): Select a field from the dropdown menu on which the check will be performed.</p> <p></p> <p>5. Filter Clause: Specify a valid Spark SQL WHERE expression to filter the data on which the check will be applied.</p> <p>The filter clause defines the conditions under which the check will be applied. It typically includes a WHERE statement that specifies which rows or data points should be included in the check.</p> <p></p> <p>6. Date (Required): Enter the reference date for the rule. For the After Date Time rule, records in the selected field must have a timestamp later than this specified date.</p> <p></p> <p>Note</p> <p>Spark SQL expressions used in calculated fields are editable, enabling greater flexibility in configuration.</p> <p>7. Coverage: Adjust the Coverage setting to specify the percentage of records that must comply with the check.</p> <p>Note</p> <p>The Coverage setting applies to most rule types and allows you to specify the percentage of records that must meet the validation criteria.</p> <p></p> <p>8. Description (Required): Enter a detailed description of the check template, including its purpose, applicable data, and relevant information to ensure clarity for users. If you're unsure of what to include, click on the \"\ud83d\udca1\" lightbulb icon to apply a suggested description based on the rule type.</p> <p>Example: The Date of Birth must be a timestamp later than &lt; date_time &gt;.</p> <p>This description specifies that the Date of Birth field must have a timestamp later than the specified &lt; date_time &gt;.</p> <p></p> <p>9. Tag: Assign relevant tags to your check to facilitate easier searching and filtering based on categories like \"data quality,\" \"financial reports,\" or \"critical checks.\"</p> <p></p> <p>10. Additional Metadata: Add key-value pairs as additional metadata to enrich your check. Click the plus icon (+) next to this section to open the metadata input form, where you can add key-value pairs.</p> <p></p> <p>Enter the desired key-value pairs (e.g., DataSourceType: SQL Database and PriorityLevel: High). After entering the necessary metadata, click \"Confirm\" to save the custom metadata.</p> <p></p> <p>Step 4: After completing all the check details, click on the \"Validate\" button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct. It ensures that the check will work as expected by running it against the data without committing any changes.</p> <p></p> <p>If the validation is successful, a green message will appear saying \"Validation Successful\".</p> <p></p> <p>If the validation fails, a red message will appear saying \"Failed Validation\". This typically occurs when the check logic or parameters do not match the data properly.</p> <p></p> <p>Step 5: Once you have a successful validation, click the \"Save\" button.</p> <p></p> <p>After clicking on the \u201cSave\u201d button your check is successfully created and a success flash message will appear saying \u201cCheck successfully created\u201d.</p> <p></p>"},{"location":"checks/authored-check/#author-a-check-via-api","title":"Author a Check via API","text":"<p>Users are able to author and interact with Checks through the API by passing JSON Payloads. Please refer to the API documentation on details: <code>qualytics.io/api/docs</code></p> <p></p>"},{"location":"checks/before-date-time-check/","title":"Before Date Time","text":""},{"location":"checks/before-date-time-check/#definition","title":"Definition","text":"<p>Asserts that the field is a timestamp earlier than a specific date and time.</p>"},{"location":"checks/before-date-time-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code>"},{"location":"checks/before-date-time-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/before-date-time-check/#specific-properties","title":"Specific Properties","text":"<p>Specify a particular date and time to act as the threshold for the rule.</p> Name Description Date The timestamp used as the upper boundary. Values in the selected field should be before this timestamp."},{"location":"checks/before-date-time-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/before-date-time-check/#example","title":"Example","text":"<p>Objective: Ensure that all L_SHIPDATE entries in the LINEITEM table are earlier than 3:00 PM on December 1st, 1998.</p> <p>Sample Data</p> L_ORDERKEY L_SHIPDATE 1 1998-12-01 15:30:00 2 1998-11-02 12:45:00 3 1998-08-01 10:20:00 Payload example <pre><code>{\n    \"description\": \"Make sure datetime values are earlier than 3:00 PM, Dec 01, 1998\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"datetime\":\"1998-12-01T15:00:00Z\"\n    },\n    \"tags\": [],\n    \"fields\": [\"L_SHIPDATE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"beforeDateTime\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>L_ORDERKEY</code> 1 does not satisfy the rule because its <code>L_SHIPDATE</code> value is not before 1998-12-01 15:00:00.</p> FlowchartsSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_SHIPDATE]\nB --&gt; C{Is L_SHIPDATE &lt; '1998-12-01 15:00:00'?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey\n    , l_shipdate\nfrom lineitem \nwhere\n    l_shipdate &gt;= '1998-12-01 15:00:00'\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>L_SHIPDATE</code> value of <code>1998-12-01 15:30:00</code> is not earlier than 1998-12-01 15:00:00.</p> <p>Shape Anomaly</p> <p>In <code>L_SHIPDATE</code>, 33.33% of 3 filtered records (1) are not earlier than 1998-12-01 15:00:00.</p>"},{"location":"checks/between-check/","title":"Between","text":""},{"location":"checks/between-check/#definition","title":"Definition","text":"<p>Asserts that values are equal to or between two numbers.</p>"},{"location":"checks/between-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/between-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/between-check/#specific-properties","title":"Specific Properties","text":"<p>Specify both minimum and maximum boundaries, and determine if these boundaries should be inclusive.</p> Name Explanation Max The upper boundary of the range. Inclusive (Max) If true, the upper boundary is considered a valid value within the range. Otherwise, it's exclusive. Min The lower boundary of the range. Inclusive (Min) If true, the lower boundary is considered a valid value within the range. Otherwise, it's exclusive."},{"location":"checks/between-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/between-check/#example","title":"Example","text":"<p>Objective: Ensure that all L_QUANTITY entries in the LINEITEM table are between 5 and 20 (inclusive).</p> <p>Sample Data</p> L_ORDERKEY L_QUANTITY 1 4 2 15 3 21 Payload example <pre><code>{\n    \"description\": \"Ensure that all L_QUANTITY entries in the LINEITEM table are between 5 and 20 (inclusive)\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"min\":5\n        \"inclusive_min\":true,\n        \"max\":20,\n        \"inclusive_max\":true,\n    },\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"between\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 1 and 3 do not satisfy the rule because their <code>L_QUANTITY</code> values are not between 5 and 20 inclusive.</p> FlowchartsSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_QUANTITY]\nB --&gt; C{Is 5 &lt;= L_QUANTITY &lt;= 20?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey\n    , l_quantity\nfrom lineitem \nwhere\n    l_quantity &lt; 5\n    or l_quantity &gt; 20\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The value for <code>L_QUANTITY</code> of 4 is not between 5.000 and 20.000.</p> <p>Shape Anomaly</p> <p>In <code>L_QUANTITY</code>, 66.67% of 3 filtered records (2) are not between 5.000 and 20.000.</p>"},{"location":"checks/between-times-check/","title":"Between Times","text":""},{"location":"checks/between-times-check/#definition","title":"Definition","text":"<p>Asserts that values are equal to or between two dates or times.</p>"},{"location":"checks/between-times-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code>"},{"location":"checks/between-times-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/between-times-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the range of dates or times that values in the selected field should fall between.</p> Name Description Min The timestamp used as the lower boundary. Values in the selected field should be after this timestamp. Max The timestamp used as the upper boundary. Values in the selected field should be before this timestamp."},{"location":"checks/between-times-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/between-times-check/#example","title":"Example","text":"<p>Objective: Ensure that all O_ORDERDATE entries in the ORDERS table are between 10:30 AM on January 1st, 1991 and 3:00 PM on December 31st, 1991.</p> <p>Sample Data</p> O_ORDERKEY O_ORDERDATE 1 1990-12-31 10:30:00 2 1991-06-02 09:15:00 3 1992-01-01 01:25:00 Payload example <pre><code>{\n    \"description\": \"Ensure that all O_ORDERDATE entries in the ORDERS table are between 10:30 AM on January 1st, 1991 and 3:00 PM on December 31st, 1991\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"min_time\":\"1991-01-01T10:30:00Z\",\n        \"max_time\":\"1991-12-31T15:00:00Z\"\n    },\n    \"tags\": [],\n    \"fields\": [\"O_ORDERDATE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"betweenTimes\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"_PARITY = 'odd'\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>O_ORDERKEY</code> 1 and 3 do not satisfy the rule because their <code>O_ORDERDATE</code> values are not between 1991-01-01 10:30:00 and 1991-12-31 15:00:00.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_ORDERDATE]\nB --&gt; C{Is '1991-01-01 10:30:00' &lt;= O_ORDERDATE &lt;= '1991-12-31 15:00:00'?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    o_orderkey\n    , o_orderdate\nfrom orders \nwhere\n    o_orderdate &lt; '1991-01-01 10:30:00'\n    or o_orderdate &gt; '1991-12-31 15:00:00'\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The value for <code>O_ORDERDATE</code> of <code>1990-12-31 10:30:00</code> is not between 1991-01-01 10:30:00 and 1991-12-31 15:00:00.</p> <p>Shape Anomaly</p> <p>In <code>O_ORDERDATE</code>, 66.667% of 3 filtered records (2) are not between 1991-01-01 10:30:00 and 1991-12-31 15:00:00.</p>"},{"location":"checks/checks-template/","title":"Check Templates","text":"<p>Check Templates empower users to efficiently create, manage, and apply standardized checks across various datastores, acting as blueprints that ensure consistency and data integrity across different datasets and processes.</p> <p>Check Templates streamline the validation process by enabling check management independently of specific data assets such as datastores, containers, or fields. These templates reduce manual intervention, minimize errors, and provide a reusable framework that can be applied across multiple datasets, ensuring all relevant data adheres to defined criteria. This not only saves time but also enhances the reliability of data quality checks within an organization.</p> <p>Let's get started \ud83d\ude80</p> <p>Step 1: Log in to your Qualytics account and click the \u201cLibrary\u201d button on the left side panel of the interface.</p> <p></p> <p>Step 2: Click on the \u201cAdd Check Template\u201d button located in the top right corner.</p> <p></p> <p>A modal window titled \u201cCheck Template Details\u201d will appear, providing you with the options to add the check template details.</p> <p></p> <p>Step 3: Enter the following details to add the check template:</p> <ul> <li>Rule Type (Required) </li> <li>Filter Clause</li> <li>Template Locked</li> <li>Description (Required) </li> <li>Tags </li> <li>Additional Metadata </li> </ul> <p>1. Rule Type (Required): Select a Rule Type from the dropdown menu for data validation, such as checking for non-null values, matching patterns, comparing numerical values, or verifying datetime constraints. Each rule type defines the specific validation logic to be applied.</p> <p>For more details about the available rule types, refer to the \"Check Rule Types\" section.</p> <p>Note</p> <p>Different rule types have different sets of fields and options appearing when selected.</p> <p></p> <p>2. Filter Clause: Specify a valid Spark SQL <code>WHERE</code> expression to filter the data on which the check will be applied.</p> <p>The filter clause defines the conditions under which the check will be applied. It typically includes a <code>WHERE</code> statement that specifies which rows or data points should be included in the check.</p> <p>Example: A filter clause might be used to apply the check only to rows where a certain column meets a specific condition, such as <code>WHERE status \\= 'active'</code>.</p> <p></p> <p>Adjust the Coverage setting to specify the percentage of records that must comply with the check.</p> <p>Note</p> <p>The Coverage setting applies to most rule types and allows you to specify the percentage of records that must meet the validation criteria.</p> <p></p> <p>3. Template Locked: Check or uncheck the \"Template Locked\" option to determine whether all checks created from this template will have their properties automatically synced to any changes made to the template.</p> <p>For more information about the template state, jump to the \"Template State\" section below.  </p> <p></p> <p>4. Description (Required): Enter a detailed description of the check template, including its purpose, applicable data, and relevant information to ensure clarity for users. If you're unsure of what to include, click on the \"\ud83d\udca1\" lightbulb icon to apply a suggested description based on the rule type.</p> <p>Example: \"The &lt; field &gt; must exist in <code>bank_transactions_*.csv.Total_Transaction_Amount</code> (Bank Dataset - Staging)\".</p> <p>This description clarifies that the specified field must be present in a particular file (<code>bank_transactions_*.csv</code>) and column (<code>Total_Transaction_Amount</code>) within the Bank Dataset.</p> <p></p> <p>5. Tags: Assign relevant tags to your check template to facilitate easier searching and filtering based on categories like \"data quality,\" \"financial reports,\" or \"critical checks.\"</p> <p></p> <p>6. Additional Metadata: Add key-value pairs as additional metadata to enrich your check. Click the plus icon (+) next to this section to open the metadata input form, where you can add key-value pairs.</p> <p></p> <p>Enter the desired key-value pairs (e.g., DataSourceType: SQL Database and PriorityLevel: High). After entering the necessary metadata, click \"Confirm\" to save the custom metadata.  </p> <p></p> <p>Step 4: Once you have entered all the required fields, click the \u201cSave\u201d button to finalize the template.</p> <p>Warning</p> <p>Once a template is saved, the selected rule type becomes locked and cannot be changed.</p> <p></p> <p>After clicking the Save button, a success notification appears on the screen confirming that the check template was created successfully.</p> <p>After saving the check template, you can now Apply a Check Template to create Quality Checks, which will enforce the validation rules defined in the template across your datastores. This ensures consistent data quality and compliance with the criteria you\u2019ve established.</p> <p>Once a check template is created, you can view its details by clicking on it, where three tabs are displayed at the top: Overview, Checks, and Anomalies.</p>"},{"location":"checks/checks-template/#overview","title":"Overview","text":"<p>The Overview tab gives a complete view of a check template, showing its key details, configuration, and recent activities. In the Summary section, users can also use redirect buttons to quickly navigate to related tabs like Checks and Active Anomalies. Information is divided into three sections: Summary, Activity, and Definition.</p> <p></p>"},{"location":"checks/checks-template/#summary","title":"Summary","text":"<p>The Summary section provides a quick overview of the check template\u2019s key details, including its name, type, priority, coverage, associated checks, active anomalies, description, and tags \u2014 helping users quickly understand its purpose and current status.</p> <p></p> REF. Field Description 1 Template The rule type of the check template, indicating its purpose (e.g., \u201cAfter Date Time\u201d). 2 Type Indicates whether the template is Locked (cannot be edited) or Unlocked (editable). 3 Weight A numeric value representing the importance or priority level of the template in scoring or decision-making. 4 Coverage The percentage of relevant dataset elements to which this template is applied. 5 Checks The total number of checks currently using this template. You can click on the (\u2197) button to navigate directly to the Checks tab. 6 Active Anomalies The count of unresolved anomalies detected by checks associated with this template. You can click on the (\u2197) button to navigate directly to the Anomalies tab. 7 Description A short statement explaining the logic or purpose of the check template. 8 Tags Used to categorize and filter templates (e.g., \u201cSandbox\u201d). Users can change the tags by clicking the tag badge. <p></p>"},{"location":"checks/checks-template/#definition","title":"Definition","text":"<p>The Definition section displays the configuration details of a check template. It outlines the target conditions, specific properties, and any additional metadata associated with the template, providing clarity on how and where it is applied.</p> <p></p> REF. Field Description 1 Target Defines the filter condition applied to the dataset. If no filter is specified, the check template applies to all data in the target scope. 2 Properties Displays configuration details specific to the check type. Content varies based on the selected check: <ul><li> Field Count checks: Shows \"Number of Fields\". </li><li> Metric checks: Shows \"Comparison\", \"Min Value\", and \"Max Value\".</li> 3 Metadata Displays any custom metadata properties linked to the template. If none are defined, this section remains empty. <p></p>"},{"location":"checks/checks-template/#activity","title":"Activity","text":"<p>The Activity section provides a chronological log of all actions and updates related to this template. It tracks key events such as creation, modifications, and other relevant activities, along with timestamps to show when they occurred.</p> <p></p> <p>You can hover over a timestamp to view the full date and last modified time.</p> <p></p>"},{"location":"checks/checks-template/#checks","title":"Checks","text":"<p>The Checks tab provides a comprehensive view of all checks linked to the chosen datastore, container, or field, along with their source details such as computed table and field information. By clicking options such as Active, Important, Favorite, Draft, Archived (Invalid and Discarded), or All, users can instantly view checks based on their status. This categorization helps in organizing, reviewing, and managing checks more effectively for consistent data quality oversight.</p> <p></p> <p>Alternatively, users can navigate to the Checks tab directly from the Overview tab by clicking the redirect button in the Checks section of the Summary panel.</p> <p></p>"},{"location":"checks/checks-template/#anomalies","title":"Anomalies","text":"<p>The Anomalies tab displays all anomalies detected for the selected check template, along with details such as source datastore, computed table, field, rule, and the number of anomalous records. Users can view anomalies based on their status: Open, Active, Acknowledged, Archived, or All and sort them based on specific parameters.</p> <p></p> <p>Alternatively, users can navigate to the Anomalies tab directly from the Overview tab by clicking the redirect button in the Active Anomalies section of the Summary panel.</p> <p></p>"},{"location":"checks/checks-template/#multiple-checks-creation","title":"Multiple Checks Creation","text":"<p>Users can create multiple checks at once by selecting a template and adding multiple targets. Each target will generate its own check.</p> <p>Step 1: Click on the Add button located in the top right corner and select Multiple Checks from the dropdown.</p> <p></p> <p>A Bulk Add Quality Checks modal window will appear. Fill in the details:</p> No. Field Description 1. Datastore Select the datastore where the check should be applied. 2. File/Table Choose the file/table within the selected datastore. 3. Field Select the field to apply the check on. 4. Filter Clause Specify a valid Spark SQL <code>WHERE</code> expression to filter the data on which the check will be applied. <p></p> <p>Step 2: Click on the Add Target button to create another check. You can keep adding targets to create as many checks as you need within the same template.</p> <p></p>"},{"location":"checks/checks-template/#template-state","title":"Template State","text":"<p>Any changes to a template may or may not impact its related checks, depending on whether the template state is locked or unlocked. Managing the template state allows you to control whether updates automatically apply to all related checks or let them function independently.</p> <p>Unlocked</p> <ul> <li>Quality Checks can evolve independently of the template. Subsequent updates to an unlocked Check Template do not affect its related quality checks.</li> </ul> <p>Locked</p> <ul> <li>Quality Checks from a locked Check Template will inherit changes made to the template. Subsequent updates to a locked Check Template do affect its related quality checks.</li> </ul> <p>Info</p> <p>Tags will be synced independently of unlocked and locked Check Templates, while Description and Additional Metadata will not be synced. This behavior is general for Check Templates.</p> Template State <pre><code>graph TD\nA[Start] --&gt;|Is `Template Locked` enabled?| B{Yes/No}\nB --&gt;|No| E[The quality check can evolve independently]\nB --&gt;|Yes| C[They remain synchronized with the template]\nC --&gt; D[End]\nE --&gt; D[End]</code></pre>"},{"location":"checks/contains-credit-card-check/","title":"Contains Credit Card","text":""},{"location":"checks/contains-credit-card-check/#definition","title":"Definition","text":"<p>Asserts that the values contain a credit card number.</p>"},{"location":"checks/contains-credit-card-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/contains-credit-card-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/contains-credit-card-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/contains-credit-card-check/#example","title":"Example","text":"<p>Objective: Ensure that every O_PAYMENT_DETAILS in the ORDERS table contains a credit card number to confirm the payment method used for each order.</p> <p>Sample Data</p> O_ORDERKEY O_PAYMENT_DETAILS 1 {\"date\": \"2023-09-25\", \"amount\": 250.50, \"credit_card\": \"5105105105105100\"} 2 {\"date\": \"2023-09-25\", \"amount\": 150.75, \"credit_card\": \"ABC12345XYZ\"} 3 {\"date\": \"2023-09-25\", \"amount\": 200.00, \"credit_card\": \"4111-1111-1111-1111\"} Payload example <pre><code>{\n    \"description\": \"Ensure that every O_PAYMENT_DETAILS in the ORDERS table contains a credit card number to confirm the payment method used for each order\",\n    \"coverage\": 1,\n    \"properties\": {},\n    \"tags\": [],\n    \"fields\": [\"C_CCN_JSON\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"containsCreditCard\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>O_ORDERKEY</code> 2 violates the rule as the <code>O_PAYMENT_DETAILS</code> does not contain a credit card number, indicating an incomplete order record.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_PAYMENT_DETAILS]\nB --&gt; C{Contains Credit Card Number?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query to identify order records that don't contain a credit card number in the payment details.\nselect\n    o_orderkey,\n    o_payment_details\nfrom orders\nwhere\n    not (regexp_like(o_payment_details, '[0-9]{16}'))\n    or not (regexp_like(o_payment_details, '\\d{4}-\\d{4}-\\d{4}-\\d{4}'))\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>O_PAYMENT_DETAILS</code> value of <code>{\"date\": \"2023-09-25\", \"amount\": 150.75, \"credit_card\": \"ABC12345XYZ\"}</code> does not contains a credit card number.</p> <p>Shape Anomaly</p> <p>In <code>O_PAYMENT_DETAILS</code>, 33.33% of 3 order records (1) do not contain a credit card number.</p>"},{"location":"checks/contains-email-check/","title":"Contains Email","text":""},{"location":"checks/contains-email-check/#definition","title":"Definition","text":"<p>Asserts that the values contain email addresses.</p>"},{"location":"checks/contains-email-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/contains-email-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/contains-email-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/contains-email-check/#example","title":"Example","text":"<p>Objective: .</p> <p>Sample Data</p> C_CUSTKEY C_CONTACT_DETAILS 1 {\"name\": \"John Doe\", \"email\": \"john.doe@example.com\"} 2 {\"name\": \"Amy Lu\", \"email\": \"amy.lu@\"} 3 {\"name\": \"Jane Smith\", \"email\": \"jane.smith@domain.org\"} Payload example <pre><code>{\n    \"description\": \"Ensure that all C_CONTACT_DETAILS entries in the CUSTOMER table contain valid email addresses\",\n    \"coverage\": 1,\n    \"properties\": {},\n    \"tags\": [],\n    \"fields\": [\"C_EMAIL_JSON\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"containsEmail\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>C_CUSTKEY</code> 2 does not satisfy the rule because its <code>C_CONTACT_DETAILS</code> value does not follow a typical email format.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve C_CONTACT_DETAILS]\nB --&gt; C{Does C_CONTACT_DETAILS contain an email address?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    c_custkey,\n    c_contact_details\nfrom customer \nwhere\n    not regexp_like(c_contact_details, '^[a-za-z0-9._%-]+@[a-za-z0-9.-]+\\.[a-za-z]{2,4}$')\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>C_CONTACT_DETAILS</code> value of <code>{\"name\": \"Amy Lu\", \"email\": \"amy.lu@\"}</code> does not contain an email address.</p> <p>Shape Anomaly</p> <p>In <code>C_CONTACT_DETAILS</code>, 33.333% of 3 filtered records (1) do not contain email addresses.</p>"},{"location":"checks/contains-social-security-number-check/","title":"Contains Social Security Number","text":""},{"location":"checks/contains-social-security-number-check/#definition","title":"Definition","text":"<p>Asserts that the values contain a social security number.</p>"},{"location":"checks/contains-social-security-number-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/contains-social-security-number-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/contains-social-security-number-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/contains-social-security-number-check/#example","title":"Example","text":"<p>Objective: Ensure that all C_CONTACT_DETAILS entries in the CUSTOMER table contain valid social security numbers.</p> <p>Sample Data</p> C_CUSTKEY C_CONTACT_DETAILS 1 {\"name\": \"John Doe\", \"ssn\": \"234567890\"} 2 {\"name\": \"Amy Lu\", \"ssn\": \"666-12-3456\"} 3 {\"name\": \"Jane Smith\", \"ssn\": \"429-14-2216\"} Payload example <pre><code>{\n    \"description\": \"Ensure that all C_CONTACT_DETAILS entries in the CUSTOMER table contain valid social security numbers\",\n    \"coverage\": 1,\n    \"properties\": {},\n    \"tags\": [],\n    \"fields\": [\"C_CONTACT_DETAILS\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"containsSocialSecurityNumber\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>C_CUSTKEY</code> 2 does not satisfy the rule because its <code>C_CONTACT_DETAILS</code> value does not contain the typical social security number format.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve C_CONTACT_DETAILS]\nB --&gt; C{Does C_CONTACT_DETAILS contain a valid SSN format?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    c_custkey,\n    c_contact_details\nfrom customer \nwhere\n    not regexp_like(ssn, '^[0-9]{3}-[0-9]{2}-[0-9]{4}$')\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>C_CONTACT_DETAILS</code> value of <code>{\"name\": \"Amy Lu\", \"ssn\": \"666-12-3456\"}</code> does not contain a social security number.</p> <p>Shape Anomaly</p> <p>In <code>C_CONTACT_DETAILS</code>, 33.333% of 3 filtered records (1) do not contain social security numbers.</p>"},{"location":"checks/contains-url/","title":"Contains URL","text":""},{"location":"checks/contains-url/#definition","title":"Definition","text":"<p>Asserts that the values contain valid URLs.</p>"},{"location":"checks/contains-url/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/contains-url/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/contains-url/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/contains-url/#example","title":"Example","text":"<p>Objective: Ensure that all S_DETAILS entries in the SUPPLIER table contain valid URLs.</p> <p>Sample Data</p> S_SUPPKEY S_DETAILS 1 {\"name\": \"Tech Parts\", \"website\": \"www.techparts.com\"} 2 {\"name\": \"Hardwarepro\", \"website\": \"https://www.hardwarepro.com\"} 3 {\"name\": \"Smith's Tools\", \"website\": \"ftp:server:8080\"} Payload example <pre><code>{\n    \"description\": \"Ensure that all S_DETAILS entries in the SUPPLIER table contain valid URLs\",\n    \"coverage\": 1,\n    \"properties\": {},\n    \"tags\": [],\n    \"fields\": [\"S_DETAILS\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"containsUrl\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>S_SUPPKEY</code> 1 and 3 do not satisfy the rule because their <code>S_DETAILS</code> values do not contain a valid URL pattern.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve S_DETAILS]\nB --&gt; C{Does S_DETAILS contain a valid URL?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    s_suppkey,\n    s_details\nfrom supplier \nwhere\n    not regexp_like(url, '^https?://.+')\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>S_DETAILS</code> value of <code>{\"name\": \"Tech Parts\", \"website\": \"www.techparts.com\"}</code> does not contain a valid URL.</p> <p>Shape Anomaly</p> <p>In <code>S_DETAILS</code>, 66.667% of 3 filtered records (2) do not contain a valid URL.</p>"},{"location":"checks/data-diff-check/","title":"Data Diff","text":"<p>Recommended Check</p> <p>Qualytics recommends using the <code>dataDiff</code> rule type instead of the <code>isReplicaOf</code>.</p> <p>The <code>isReplicaOf</code> check is sunsetting and will no longer be maintained, while <code>dataDiff</code> provides the same functionality with enhanced performance and additional capabilities.</p>"},{"location":"checks/data-diff-check/#definition","title":"Definition","text":"<p>Asserts that the dataset created by the targeted field(s) matches the referred field(s) for data comparison.</p>"},{"location":"checks/data-diff-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>DataDiff</code> rule ensures that data integrity is maintained when comparing data between different sources. This involves checking not only the data values themselves but also ensuring that the structure and relationships are preserved.</p> <p>In a distributed data ecosystem, data comparison often occurs to validate consistency across systems, verify data transfers, or ensure data quality between sources. However, discrepancies might arise due to various reasons such as network glitches, software bugs, or human errors. The <code>DataDiff</code> rule serves as a safeguard against these issues by:</p> <ol> <li>Preserving Data Structure: Ensuring that the structure of the compared data matches between sources.</li> <li>Checking Data Values: Ensuring that every piece of data in the source matches the reference data.</li> </ol>"},{"location":"checks/data-diff-check/#field-scope","title":"Field Scope","text":"<p>Multi: The rule evaluates multiple specified fields.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/data-diff-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/data-diff-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the datastore and table/file where the reference data for the targeted fields is located for comparison.</p> Name Description Row Identifiers The list of fields defining the compound key to identify rows in the comparison analysis. Datastore The source datastore where the reference data for the targeted field(s) is located. Table/file The table, view or file in the source datastore that should serve as the reference for comparison. Comparators Specifies how variations are handled, allowing for slight deviations within a defined margin of error. <p>Info</p> <p>The <code>DataDiff</code> rule supports editing of <code>Row Identifiers</code> and <code>Passthrough Fields</code>, allowing for more tailored configuration.</p> <p>Details</p> <p>This optional input allows row comparison analysis by defining a list of fields as row identifiers, it enables a more detailed comparison between tables/files, where each row compound key is used to identify its presence or absence in the reference table/file compared to the target table/file.  Qualytics can inform if the row exists or not and distinguish which field values differ in each row present in the reference table/file, helping to determine if it is a data diff.</p> <p>Info</p> <p>Anomalies produced by a <code>DataDiff</code> quality check making use of <code>Row Identifiers</code> have their source records presented in a different visualization.  See more at: Comparison Source Records</p> <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"checks/data-diff-check/#row-identifiers","title":"Row Identifiers","text":""},{"location":"checks/data-diff-check/#comparators","title":"Comparators","text":""},{"location":"checks/data-diff-check/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"checks/data-diff-check/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"checks/data-diff-check/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"checks/data-diff-check/#duration","title":"Duration","text":"<p>Duration comparators support time-based comparisons, allowing for flexibility in how duration differences are managed. This flexibility is crucial for datasets where time measurements are essential but can vary slightly.</p>"},{"location":"checks/data-diff-check/#unit","title":"Unit","text":"<p>The unit of time you select determines how granular the comparison is:</p> <ul> <li>Millis: Measures time in milliseconds, ideal for high-precision needs.</li> <li>Seconds: Suitable for most general purposes where precision is important but doesn't need to be to the millisecond.</li> <li>Days: Best for longer durations.</li> </ul>"},{"location":"checks/data-diff-check/#value","title":"Value","text":"<p>Value sets the maximum acceptable difference in time to consider two values as equal. It serves to define the margin of error, accommodating small discrepancies that naturally occur over time.</p> Illustration using Duration Comparator Unit Value A Value B Difference Threshold Are equal? Millis 500 ms 520 ms 20 ms 25 ms True Seconds 30 sec 31 sec 1 sec 2 sec True Days 5 days 7 days 2 days 1 day False Millis 1000 ms 1040 ms 40 ms 25 ms False Seconds 45 sec 48 sec 3 sec 2 sec False"},{"location":"checks/data-diff-check/#string","title":"String","text":"<p>String comparators facilitate comparisons of textual data by allowing variations in spacing. This capability is essential for ensuring data consistency, particularly where minor text inconsistencies may occur.</p>"},{"location":"checks/data-diff-check/#ignore-whitespace","title":"Ignore Whitespace","text":"<p>When enabled, this setting allows the comparator to ignore differences in whitespace. This means sequences of whitespace are collapsed into a single space, and any leading or trailing spaces are removed. This can be particularly useful in environments where data entry may vary in formatting but where those differences are not relevant to the data's integrity.</p> Illustration <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined string comparison to <code>ignore whitespace</code> as <code>True</code>.</p> Value A Value B Are equal? Has whitespace? <code>Leonidas</code> <code>Leonidas</code> True No <code>Beth</code> <code>Beth</code> True Yes <code>Ana</code> <code>Anna</code> False Yes <code>Joe</code> <code>Joel</code> False No"},{"location":"checks/data-diff-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/data-diff-check/#example","title":"Example","text":"<p>Scenario: Consider that the fields N_NATIONKEY and N_NATIONNAME in the NATION table need to be compared with a backup database for data validation purposes. The data engineering team wants to ensure that both fields in the backup accurately match the original.</p> <p>Objective: Ensure that N_NATIONKEY and N_NATIONNAME from the NATION table match the data in the NATION_BACKUP table.</p> <p>Sample Data from NATION</p> N_NATIONKEY N_NATIONNAME 1 Australia 2 United States 3 Uruguay <p>Reference Sample Data from NATION_BACKUP</p> N_NATIONKEY N_NATIONNAME 1 Australia 2 USA 3 Uruguay Payload example <pre><code>{\n    \"description\": \"Ensure that N_NATIONKEY and N_NATIONNAME from the NATION table match the data in the NATION_BACKUP table\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"ref_container_id\": {ref_container_id},\n        \"ref_datastore_id\": {ref_datastore_id}\n    },\n    \"tags\": [],\n    \"fields\": [\"N_NATIONKEY\", \"N_NATIONNAME\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"dataDiff\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>The datasets representing the fields <code>N_NATIONKEY</code> and <code>N_NATIONNAME</code> in the original and the reference data are not completely identical, indicating a possible discrepancy in the data or an unintended change.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Original Data]\nB --&gt; C[Retrieve Reference Data]\nC --&gt; D{Do datasets match for both fields?}\nD --&gt;|Yes| E[End]\nD --&gt;|No| F[Mark as Anomalous]\nF --&gt; E</code></pre> <pre><code>-- An illustrative SQL query comparing original to reference data for both fields.\nselect\n    orig.n_nationkey as original_key,\n    orig.n_nationname as original_name,\n    ref.n_nationkey as reference_key,\n    ref.n_nationname as reference_name\nfrom nation as orig\nleft join nation_backup as ref on orig.n_nationkey = ref.n_nationkey\nwhere\n    orig.n_nationname &lt;&gt; ref.n_nationname\nor\n    orig.n_nationkey &lt;&gt; ref.n_nationkey\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>There is 1 record that differs between <code>NATION_BACKUP</code> (3 records) and <code>NATION</code> (3 records) in <code>&lt;datastore_name&gt;</code></p>"},{"location":"checks/distinct-count-check/","title":"Distinct Count","text":""},{"location":"checks/distinct-count-check/#definition","title":"Definition","text":"<p>Asserts on the approximate count distinct of the given column.</p>"},{"location":"checks/distinct-count-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/distinct-count-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/distinct-count-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the distinct count expectation for the values in the field.</p> Name Description Value The exact count of distinct values expected in the selected field."},{"location":"checks/distinct-count-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/distinct-count-check/#example","title":"Example","text":"<p>Objective: Ensure that there are exactly 3 distinct O_ORDERSTATUS in the ORDERS table: 'O' (Open), 'F' (Finished), and 'P' (In Progress).</p> <p>Sample Data</p> O_ORDERKEY O_ORDERSTATUS 1 O 2 F ... ... 20 X 21 O Payload example <pre><code>{\n    \"description\": \"Ensure that there are exactly 3 distinct O_ORDERSTATUS in the ORDERS table: 'O' (Open), 'F' (Finished), and 'P' (In Progress)\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\":3\n    },\n    \"tags\": [],\n    \"fields\": [\"O_ORDERSTATUS\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"distinctCount\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the rule is violated because the <code>O_ORDERSTATUS</code> contains 4 distinct values and not 3: 'O' (Open), 'F' (Finished), and 'P' (In Progress).</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve all O_ORDERSTATUS entries and count distinct values]\nB --&gt; C{Is distinct count of O_ORDERSTATUS != 3?}\nC --&gt;|Yes| D[Mark as Anomalous]\nC --&gt;|No| E[End]</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    count(distinct o_orderstatus)\nfrom orders\nhaving count(distinct o_orderstatus) &lt;&gt; 3\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>In <code>O_ORDERSTATUS</code>, the distinct count of the records is not 3.</p>"},{"location":"checks/entity-resolution/","title":"Entity Resolution","text":""},{"location":"checks/entity-resolution/#definition","title":"Definition","text":"<p>Asserts that every distinct entity is appropriately represented once and only once</p>"},{"location":"checks/entity-resolution/#in-depth-overview","title":"In-Depth Overview","text":"<p>This check performs automated entity name clustering to identify entities with similar names that likely represent the same entity. It then assigns each cluster a unique entity identifier and asserts that every row with the same  entity identifier shares the same value for the designated <code>distinction field</code></p>"},{"location":"checks/entity-resolution/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/entity-resolution/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/entity-resolution/#specific-properties","title":"Specific Properties","text":"Name Description Distinction Field The field that must hold a distinct value for every distinct entity Pair Substrings Considers entities a match if one entity is part of the other Pair Homophones Considers entities a match if they sound alike, even if spelled differently Spelling Similarity The minimum similarity required for clustering two entity names"},{"location":"checks/entity-resolution/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/entity-resolution/#example","title":"Example","text":"<p>Objective: If you have a <code>businesses</code> table with an <code>id</code> field and a <code>name</code> field, this check can be configured to resolve <code>name</code> and use <code>id</code> as the <code>distinction field</code>.  During each scan, similar names will be grouped and assigned the same <code>entity identifier</code> and any rows that share the same <code>entity identifier</code> but have different values for <code>id</code> will be identified as anomalies.</p> <p>Sample Data</p> BUSINESS_ID BUSINESS_NAME 1 ACME Boxing 2 Frank's Flowers 3 ACME Boxes Payload example <pre><code>{\n    \"description\": \"Ensure a `businesses` table with an `BUSINESS_ID` field and a `BUSINESS_NAME` field shares the same `entity identifier`\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"distinct_field_name\":\"BUSINESS_ID\",\n        \"pair_substrings\":true,\n        \"pair_homophones\":true,\n        \"spelling_similarity_threshold\":0.6\n    },\n    \"tags\": [],\n    \"fields\": [\"BUSINESS_NAME\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"entityResolution\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>BUSINESS_ID</code> 1 and 3 do not satisfy the rule because their <code>BUSINESS_NAME</code> values will be marked as similar yet they do not share the same <code>BUSINESS_ID</code> </p> Flowchart <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Original Data]\nB --&gt; C{Which entities are similar?}\nC --&gt; D[Assign each record an entity identifier]\nD --&gt; E[Cluster records by entity identifier]\nE --&gt; F{Do records with same&lt;br/&gt;entity identifier share the&lt;br/&gt;same distinction field value?}\nF --&gt;|Yes| I[End]\nF --&gt;|No| H[Mark as Anomalous]\nH --&gt; I</code></pre>"},{"location":"checks/equal-to-check/","title":"Equal To","text":""},{"location":"checks/equal-to-check/#definition","title":"Definition","text":"<p>Asserts that all of the selected fields' equal a value.</p>"},{"location":"checks/equal-to-check/#field-scope","title":"Field Scope","text":"<p>Multi: The rule evaluates multiple specified fields.</p> <p>Accepted Types</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/equal-to-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/equal-to-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the field to compare for equality with the selected field.</p> Name Description Value Specifies the value a field should be equal to. Comparators Specifies how variations are handled, allowing for slight deviations within a defined margin of error. <p>Details</p> <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"checks/equal-to-check/#comparators","title":"Comparators","text":""},{"location":"checks/equal-to-check/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"checks/equal-to-check/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"checks/equal-to-check/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"checks/equal-to-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/equal-to-check/#example","title":"Example","text":"<p>Objective: Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table is equal to a value of 10.</p> <p>Sample Data</p> L_ORDERKEY L_LINENUMBER L_QUANTITY 1 1 10 2 2 5 3 3 10 4 4 8 Payload example <pre><code>{\n    \"description\": \"Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table is equal to a value of 10\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\":\"10\",\n        \"inclusive\":true\n    },\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"equalTo\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 2 and 4 do not satisfy the rule because their <code>L_QUANTITY</code> values are below the specified minimum value of 10.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_QUANTITY]\nB --&gt; C{Is L_QUANTITY = 10?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_linenumber,\n    l_quantity\nfrom lineitem \nwhere\n    l_quantity &lt; 10;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>Not all of the fields equal are equal to the value of <code>10</code></p> <p>Shape Anomaly</p> <p>In <code>L_QUANTITY</code>, 2 of 4 filtered records (4) are not equal to the value of <code>10</code></p>"},{"location":"checks/equal-to-field-check/","title":"Equal To Field","text":""},{"location":"checks/equal-to-field-check/#definition","title":"Definition","text":"<p>Asserts that a field is equal to another field.</p>"},{"location":"checks/equal-to-field-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code>"},{"location":"checks/equal-to-field-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/equal-to-field-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the field to compare for equality with the selected field.</p> Name Description Field to compare The field name whose values should match those of the selected field. Comparators Specifies how variations are handled, allowing for slight deviations within a defined margin of error. <p>Details</p> <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"checks/equal-to-field-check/#comparators","title":"Comparators","text":""},{"location":"checks/equal-to-field-check/#string","title":"String","text":"<p>String comparators facilitate comparisons of textual data by allowing variations in spacing. This capability is essential for ensuring data consistency, particularly where minor text inconsistencies may occur.</p>"},{"location":"checks/equal-to-field-check/#ignore-whitespace","title":"Ignore Whitespace","text":"<p>When enabled, this setting allows the comparator to ignore differences in whitespace. This means sequences of whitespace are collapsed into a single space, and any leading or trailing spaces are removed. This can be particularly useful in environments where data entry may vary in formatting but where those differences are not relevant to the data's integrity.</p> Illustration <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined string comparison to <code>ignore whitespace</code> as <code>True</code>.</p> Value A Value B Are equal? Has whitespace? <code>Leonidas</code> <code>Leonidas</code> True No <code>Beth</code> <code>Beth</code> True Yes <code>Ana</code> <code>Anna</code> False Yes <code>Joe</code> <code>Joel</code> False No"},{"location":"checks/equal-to-field-check/#duration","title":"Duration","text":"<p>Duration comparators support time-based comparisons, allowing for flexibility in how duration differences are managed. This flexibility is crucial for datasets where time measurements are essential but can vary slightly.</p>"},{"location":"checks/equal-to-field-check/#unit","title":"Unit","text":"<p>The unit of time you select determines how granular the comparison is:</p> <ul> <li>Millis: Measures time in milliseconds, ideal for high-precision needs.</li> <li>Seconds: Suitable for most general purposes where precision is important but doesn't need to be to the millisecond.</li> <li>Days: Best for longer durations.</li> </ul>"},{"location":"checks/equal-to-field-check/#value","title":"Value","text":"<p>Value sets the maximum acceptable difference in time to consider two values as equal. It serves to define the margin of error, accommodating small discrepancies that naturally occur over time.</p> Illustration using Duration Comparator Unit Value A Value B Difference Threshold Are equal? Millis 500 ms 520 ms 20 ms 25 ms True Seconds 30 sec 31 sec 1 sec 2 sec True Days 5 days 7 days 2 days 1 day False Millis 1000 ms 1040 ms 40 ms 25 ms False Seconds 45 sec 48 sec 3 sec 2 sec False"},{"location":"checks/equal-to-field-check/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"checks/equal-to-field-check/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"checks/equal-to-field-check/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"checks/equal-to-field-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/equal-to-field-check/#example","title":"Example","text":"<p>Scenario: An e-commerce platform sells digital products. The shipping date (when the digital product link is sent) should always be the same as the delivery date (when the customer acknowledges receiving the product).</p> <p>Objective: Ensure that the O_SHIPDATE in the ORDERS table matches its delivery date O_DELIVERYDATE.</p> <p>Sample Data</p> O_ORDERKEY O_SHIPDATE O_DELIVERYDATE 1 1998-01-04 1998-01-04 2 1998-01-14 1998-01-15 3 1998-01-12 1998-01-12 Payload example <pre><code>{\n    \"description\": \"Ensure that the O_SHIPDATE in the ORDERS table matches its delivery date O_DELIVERYDATE\",\n    \"coverage\": 1,\n    \"properties\": {\"field_name\":\"O_DELIVERYDATE\", \"inclusive\":false},\n    \"tags\": [],\n    \"fields\": [\"O_SHIPDATE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"equalToField\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>O_ORDERKEY</code> 2 does not satisfy the rule because its <code>O_SHIPDATE</code> of 1998-01-14 does not match the <code>O_DELIVERYDATE</code> of 1998-01-15.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_SHIPDATE and O_DELIVERYDATE]\nB --&gt; C{Is O_SHIPDATE = O_DELIVERYDATE?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    o_orderkey,\n    o_shipdate,\n    o_deliverydate\nfrom orders \nwhere\n    o_shipdate != o_deliverydate\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>O_SHIPDATE</code> value of 1998-01-14 is not equal to the value of O_DELIVERYDATE which is 1998-01-15.</p> <p>Shape Anomaly</p> <p>In <code>O_SHIPDATE</code>, 33.333% of the filtered fields are not equal.</p>"},{"location":"checks/exists-in-check/","title":"Exists In","text":""},{"location":"checks/exists-in-check/#definition","title":"Definition","text":"<p>Asserts that values assigned to a field exist as values in another field.</p>"},{"location":"checks/exists-in-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>ExistsIn</code> rule allows you to cross-validate data between different sources, whether it\u2019s object storage systems or databases.</p> <p>Traditionally, databases might utilize foreign key constraints (if available) to enforce data integrity between related tables. The <code>ExistsIn</code> rule extends this concept in two powerful ways:</p> <ol> <li>Cross-System Integrity: it allows for integrity checks to span across different databases or even entirely separate systems. This is particularly advantageous in scenarios where data sources are fragmented across diverse platforms.</li> <li>Flexible Data Formats: Beyond just databases, this rule can validate values against various data formats, such as ensuring values in a file align with those in a table.</li> </ol> <p>These enhancements enable businesses to maintain data integrity even in complex, multi-system environments.</p>"},{"location":"checks/exists-in-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/exists-in-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/exists-in-check/#specific-properties","title":"Specific Properties","text":"<p>Define the datastore, table/file, and field where the rule should look for matching values.</p> Name Description Datastore The source datastore where the profile of the reference field is located. Table/file The profile (e.g. table, view or file) containing the reference field. Field The field name whose values should match those of the selected field."},{"location":"checks/exists-in-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/exists-in-check/#example","title":"Example","text":"<p>Objective: Ensure that all NATION_NAME entries in the NATION table match entries under the COUNTRY_NAME column in an external lookup file listing official country names.</p> <p>Sample Data</p> N_NATIONKEY N_NATIONNAME 1 Algeria 2 Argentina 3 Atlantida Payload example <pre><code>{\n    \"description\": \"Ensure that all NATION_NAME entries in the NATION table match entries under the COUNTRY_NAME column in an external lookup file listing official country names\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"field_name\":\"COUNTRY_NAME\",\n        \"ref_container_id\": {ref_container_id},\n        \"ref_datastore_id\": {ref_datastore_id}\n    },\n    \"tags\": [],\n    \"fields\": [\"NATION_NAME\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"existsIn\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Lookup File Sample</p> COUNTRY_NAME Algeria Argentina Brazil Canada ... Zimbabwe <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>N_NATIONKEY</code> 3 does not satisfy the rule because the <code>N_NATIONNAME</code> \"Atlantida\" does not match any <code>COUNTRY_NAME</code> in the official country names lookup file.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve COUNTRY_NAME]\nB --&gt; C[Retrieve N_NATIONNAME]\nC --&gt; D{Does N_NATIONNAME exists in COUNTRY_NAME?}\nD --&gt;|Yes| E[Move to Next Record/End]\nD --&gt;|No| F[Mark as Anomalous]\nF --&gt; E</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    n_nationkey\n    , n_nationname\nfrom nation \nwhere\n    n_nationname not in ('Algeria', 'Argentina', ... /* other valid countries */)\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>N_NATIONNAME</code> value of <code>'Atlantida'</code> does not exist in <code>COUNTRY_NAME</code>.</p> <p>Shape Anomaly</p> <p>In <code>N_NATIONNAME</code>, 33.333% of 3 filtered records (1) do not match any <code>COUNTRY_NAME</code>.</p>"},{"location":"checks/expected-schema-check/","title":"Expected Schema","text":""},{"location":"checks/expected-schema-check/#definition","title":"Definition","text":"<p>Asserts that all of the selected fields must be present in the datastore.</p>"},{"location":"checks/expected-schema-check/#behavior","title":"Behavior","text":"<p>The expected schema is the first check to be tested during a scan operation. If it fails, the scan operation will result as <code>Failure</code> with the following message:</p> <p><code>&lt;container-name&gt;</code>: Aborted because schema check anomalies were identified.</p>"},{"location":"checks/expected-schema-check/#general-properties","title":"General Properties","text":"Details Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions"},{"location":"checks/expected-schema-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the fields that must be present in the schema, and determine if a schema change caused by additional fields should fail or pass the assertion.</p> Name Description Fields List of fields that must be presented in the schema. Allow other fields If true, then new fields are allowed to be presented in the schema. Otherwise, the assertion will be stricter."},{"location":"checks/expected-schema-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/expected-schema-check/#example","title":"Example","text":"<p>Objective: Ensure that expected fields such as L_ORDERKEY, L_PARTKEY, and L_SUPPKEY are always present in the LINEITEM table.</p> <p>Sample Data</p> Valid FIELD_NAME FIELD_TYPE L_ORDERKEY NUMBER L_PARTKEY NUMBER L_SUPPKEY NUMBER L_LINENUMBER NUMBER L_QUANTITY NUMBER L_EXTENDEDPRICE NUMBER ... ... Invalid <p>L_SUPPKEY is missing from the schema</p> FIELD_NAME FIELD_TYPE L_ORDERKEY NUMBER L_PARTKEY NUMBER L_LINENUMBER NUMBER L_QUANTITY NUMBER L_EXTENDEDPRICE NUMBER ... ... Payload example <pre><code>{\n    \"description\": \"Ensure that expected fields such as L_ORDERKEY, L_PARTKEY, and L_SUPPKEY are always present in the LINEITEM table\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"allow_other_fields\":false,\n        \"list\":[\"L_ORDERKEY\",\"L_PARTKEY\",\"L_SUPPKEY\"]\n    },\n    \"tags\": [],\n    \"fields\": null,\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"expectedSchema\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>Among the presented sample schemas, the second one is missing one of the expected schema. Only the first schema has the correct expected schema.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B{Check for Field Presence}\nB -.-&gt;|Field is missing| C[Mark as Shape Anomaly]\nB -.-&gt;|All fields present| D[End]</code></pre> <pre><code>-- An illustrative SQL query to check the existence of columns.\nselect \n    column_name \nfrom \n    information_schema.columns \nwhere \n    table_name = 'LINEITEM' and \n    column_name in ('L_ORDERKEY', 'L_PARTKEY', 'L_SUPPKEY');\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>The required fields (<code>L_SUPPKEY</code>) are not present.</p>"},{"location":"checks/expected-values-check/","title":"Expected Values","text":""},{"location":"checks/expected-values-check/#definition","title":"Definition","text":"<p>Asserts that values are contained within a list of expected values.</p>"},{"location":"checks/expected-values-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/expected-values-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/expected-values-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the list of expected values for the data in the field.</p> Name Description List A predefined set of values against which the data is validated."},{"location":"checks/expected-values-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/expected-values-check/#example","title":"Example","text":"<p>Objective: Ensure that all O_ORDERSTATUS entries in the ORDERS table only contain expected order statuses: \"O\", \"F\", and \"P\".</p> <p>Sample Data</p> O_ORDERKEY O_ORDERSTATUS 1 F 2 O 3 P 4 X Payload example <pre><code>{\n    \"description\": \"Ensure that all O_ORDERSTATUS entries in the ORDERS table only contain expected order statuses: \"O\", \"F\", and \"P\"\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"list\":[\"O\",\"F\",\"P\"]\n    },\n    \"tags\": [],\n    \"fields\": [\"O_ORDERSTATUS\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"expectedValues\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>O_ORDERKEY</code> 4 does not satisfy the rule because the <code>O_ORDERSTATUS</code> \"X\" is not on the list of expected order statuses (\"O\", \"F\", \"P\").</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_ORDERSTATUS]\nB --&gt; C{Is O_ORDERSTATUS in 'O', 'F', 'P'?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    o_orderkey\n    , o_orderstatus\nfrom orders \nwhere\n    o_orderstatus not in ('O', 'F', 'P')\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>O_ORDERSTATUS</code> value of <code>'X'</code> does not appear in the list of expected values</p> <p>Shape Anomaly</p> <p>In <code>O_ORDERSTATUS</code>, 25.000% of 4 filtered records (1) do not appear in the list of expected values</p>"},{"location":"checks/export-check-templates/","title":"Export Check Templates","text":"<p>You can export check templates to easily share or reuse your quality check settings across different systems or projects. This saves time by eliminating the need to recreate the same checks repeatedly and ensures that your quality standards are consistently applied. Exporting templates helps maintain accuracy and efficiency in managing data quality across various environments.</p> <p>Let\u2019s get started \ud83d\ude80</p> <p>Step 1:  Log in to your Qualytics account and click the \u201cLibrary\u201d button on the left side panel of the interface.</p> <p></p> <p>Step 2: Click on the \u201cExport Check Template\u201d button located in the top right corner.</p> <p></p> <p>Step 3: A modal window titled \u201cExport Check Templates\u201d will appear, where you have to select the enrichment store to which the check templates will be exported.</p> <p></p> <p>Step 4:  Once you have selected the enrichment store, click on the \u201cExport\u201d button</p> <p></p> <p>After clicking \u201cExport,\u201d the process starts, and a message will confirm that the metadata will be available in your Enrichment Datastore shortly.</p> <p></p>"},{"location":"checks/export-check-templates/#review-exported-check-templates","title":"Review Exported Check Templates","text":"<p>Step 1: Once the checks have been exported, navigate to the \u201cEnrichment Datastores\u201d located on the left menu.</p> <p></p> <p>Step 2: In the \u201cEnrichment Datastores\u201d section, select the datastore where you exported the checks templates. The exported check templates will now be visible in the selected datastore.</p> <p>When you export check templates, you can reuse them for other datastores, share them with teams, or save them as a backup. Once exported, the templates can be imported and customized to fit different datasets, making them versatile and easy to adapt.</p> <p></p> <p>You also have the option to download them as a CSV file, allowing you to share or store them for future use.</p> <p></p>"},{"location":"checks/field-count-check/","title":"Field Count","text":""},{"location":"checks/field-count-check/#definition","title":"Definition","text":"<p>Asserts that there must be exactly a specified number of fields.</p>"},{"location":"checks/field-count-check/#general-properties","title":"General Properties","text":"Details Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions"},{"location":"checks/field-count-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the exact number of fields expected in the profile.</p> Name Description Number of Fields The exact number of fields that should be present in the profile."},{"location":"checks/field-count-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/field-count-check/#example","title":"Example","text":"<p>Objective: Ensure that the ORDERS profile contains exactly 9 fields.</p> <p>Sample Profile</p> Valid FIELD_NAME FIELD_TYPE O_ORDERKEY STRING O_CUSTKEY STRING O_ORDERSTATUS STRING O_TOTALPRICE FLOAT O_ORDERDATE DATE O_ORDERPRIORITY STRING O_CLERK STRING O_SHIPPRIORITY STRING O_COMMENT STRING Invalid <p>count (8) less than expected (9)</p> FIELD_NAME FIELD_TYPE O_ORDERKEY STRING O_CUSTKEY STRING O_ORDERSTATUS STRING O_TOTALPRICE FLOAT O_ORDERDATE DATE O_ORDERPRIORITY STRING O_CLERK STRING O_COMMENT STRING <p>count (10) greater than expected (9)</p> FIELD_NAME FIELD_TYPE O_ORDERKEY STRING O_CUSTKEY STRING O_ORDERSTATUS STRING O_TOTALPRICE FLOAT O_ORDERDATE DATE O_ORDERPRIORITY STRING O_CLERK STRING O_COMMENT STRING EXTRA_FIELD UNKNOWN Payload example <pre><code>{\n    \"description\": \"Ensure that the ORDERS profile contains exactly 9 fields\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\": 9\n    },\n    \"tags\": [],\n    \"fields\": null,\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"fieldCount\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>Among the presented sample profiles, the second one is missing a field, while the third one contains an extra field. Only the first profile has the correct number of fields, which is 9.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Profile Fields]\nB --&gt; C{Does the profile have exactly 9 fields?}\nC --&gt;|Yes| D[End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query related to the rule.\nselect\n    table_name, count(column_name) as number_of_fields\nfrom information_schema.columns \nwhere table_name = 'orders'\ngroup by table_name\nhaving count(column_name) &lt;&gt; 9\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>In <code>ORDERS</code>, the field count is not <code>9</code>.</p>"},{"location":"checks/freshness-check/","title":"Freshness Check","text":"<p>A Freshness Check ensures data stays up-to-date by monitoring its last update time. It prevents stale data from impacting reports and dashboards while detecting outdated information early. By setting a maximum age threshold, it helps identify pipeline failures and ensures accurate, real-time analytics for reliable business insights.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"checks/freshness-check/#importance-of-freshness-check","title":"Importance of Freshness Check","text":"<p>A Freshness Check ensures data is always up-to-date for real-time analytics. It prevents stale data from affecting reports and dashboards while detecting pipeline failures early.</p> <ul> <li> <p>Keeps data up-to-date for real-time analytics.</p> </li> <li> <p>Prevents stale data from affecting reports and dashboards.</p> </li> <li> <p>Detects data pipeline failures early.</p> </li> </ul>"},{"location":"checks/freshness-check/#how-it-works","title":"How It Works","text":"<p>A Freshness Check monitors when a dataset (table, file, or view) was last updated. If the data is older than the allowed limit, the system triggers an alert.</p>"},{"location":"checks/freshness-check/#process-flow","title":"Process Flow","text":"<ol> <li> <p>Data Update: System records the last update timestamp.</p> </li> <li> <p>Threshold Definition: A Maximum Age (e.g., 1 hour, 24 hours) is set.</p> </li> <li> <p>Scan: The system checks if the data is within the allowed time.</p> </li> <li> <p>Result Evaluation:</p> </li> <li> <p>Pass \u2192 Data is updated within the allowed time.</p> </li> <li> <p>Fail \u2192 Data is older than the limit, triggering an alert.  </p> </li> </ol>"},{"location":"checks/freshness-check/#configuring-freshness-check","title":"Configuring Freshness Check","text":"<p>Step 1: Log in into your Qualytics account and select the datastore from the left menu on which you want to add a volumetric check.</p> <p></p> <p>Step 2: Click the Add button and select Checks.</p> <p></p> <p>Step 3: A modal window appears. Enter the required details to configure the Freshness Check.</p> <p></p> <p>Step 4: Enter the details to configure the volumetric check:</p> No. Field Description 1. Rule Type Select the Freshness Rule type from the dropdown. 2. Table Select the table for the rule to apply. 3. Unit Select time unit (Hours, Minutes, Days) for freshness measurement. 4. Maximum Age Set the time limit for data freshness. If exceeded, the check fails. 5. Description Enter a description for the check. 6. Tag Add tags for categorizing the check. 7. Additional Metadata Add custom metadata for additional details. <p></p> <p>Step 5: After completing all the check details, click on the \"Validate\" button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct. It ensures that the check will work as expected by running it against the data without committing any changes.</p> <p></p> <p>If the validation is successful, a green message will appear saying \"Validation Successful\".</p> <p></p> <p>Step 6: Once you have a successful validation, click the \"Save\" button.</p> <p></p> <p>After clicking on the \u201cSave\u201d button your check is successfully created and a success flash message will appear saying \u201cCheck successfully created\u201d.</p> <p></p>"},{"location":"checks/freshness-check/#example","title":"Example","text":"<p>A company needs hourly updates on sales data to ensure real-time reports. A Freshness Check is set up with a 1-hour threshold.</p>"},{"location":"checks/freshness-check/#before-running-the-check-data-is-fresh","title":"Before Running the Check (Data is Fresh)","text":"No. Order ID Customer Amount ($) Last Updated 01 12345 John Doe 49.99 10:30 AM 02 12346 Jane Smith 89.50 10:35 AM <ul> <li> <p>Current Time: <code>11:00 AM</code></p> </li> <li> <p>Threshold: 1 Hour</p> </li> <li> <p>Pass (Data is fresh)</p> </li> </ul>"},{"location":"checks/freshness-check/#when-freshness-check-fails-data-is-stale","title":"When Freshness Check Fails (Data is Stale)","text":"No. Order ID Customer Amount($) Last Updated 01 12345 John Doe 49.99 09:30 AM 02 12346 Jane Smith 89.50 09:35 AM <ul> <li> <p>Current Time: <code>11:00 AM.</code></p> </li> <li> <p>Threshold: 1 Hour.</p> </li> <li> <p>Fail (Data is older than 1 hour).</p> </li> </ul>"},{"location":"checks/greater-than-check/","title":"Greater Than","text":""},{"location":"checks/greater-than-check/#definition","title":"Definition","text":"<p>Asserts that the field is a number greater than (or equal to) a value.</p>"},{"location":"checks/greater-than-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/greater-than-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/greater-than-check/#specific-properties","title":"Specific Properties","text":"<p>Allows specifying a numeric value that acts as the threshold.</p> Name Description Value The number to use as the base comparison. Inclusive If true, the comparison will also allow values equal to the threshold. Otherwise, it's exclusive. Comparators Specifies how variations are handled, allowing for slight deviations within a defined margin of error. <p>Details</p> <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"checks/greater-than-check/#comparators","title":"Comparators","text":""},{"location":"checks/greater-than-check/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"checks/greater-than-check/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"checks/greater-than-check/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"checks/greater-than-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/greater-than-check/#example","title":"Example","text":"<p>Objective: Ensure that all L_QUANTITY entries in the LINEITEM table are greater than 10.</p> <p>Sample Data</p> L_ORDERKEY L_QUANTITY 1 9 2 15 3 5 Payload example <pre><code>{\n    \"description\": \"Ensure that all L_QUANTITY entries in the LINEITEM table are greater than 10\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"inclusive\": true,\n        \"value\": 10\n    },\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"greaterThan\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 1 and 3 do not satisfy the rule because their <code>L_QUANTITY</code> values are not greater than 10.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_QUANTITY]\nB --&gt; C{Is L_QUANTITY &gt; 10?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_quantity\nfrom lineitem \nwhere\n    l_quantity &lt;= 10;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>L_QUANTITY</code> value of <code>5</code> is not greater than the value of 10.</p> <p>Shape Anomaly</p> <p>In <code>L_QUANTITY</code>, 66.667% of 3 filtered records (2) are not greater than 10.</p>"},{"location":"checks/greater-than-field-check/","title":"Greater Than Field","text":""},{"location":"checks/greater-than-field-check/#definition","title":"Definition","text":"<p>Asserts that the field is greater than another field.</p>"},{"location":"checks/greater-than-field-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code>"},{"location":"checks/greater-than-field-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/greater-than-field-check/#specific-properties","title":"Specific Properties","text":"<p>Allows specifying another field against which the value comparison will be performed.</p> Name Description Field to compare Specifies the name of the field against which the value will be compared. Inclusive If true, the comparison will also allow values equal to the value of the other field. Otherwise, it's exclusive. Comparators Specifies how variations are handled, allowing for slight deviations within a defined margin of error. <p>Details</p> <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"checks/greater-than-field-check/#comparators","title":"Comparators","text":""},{"location":"checks/greater-than-field-check/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"checks/greater-than-field-check/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"checks/greater-than-field-check/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"checks/greater-than-field-check/#duration","title":"Duration","text":"<p>Duration comparators support time-based comparisons, allowing for flexibility in how duration differences are managed. This flexibility is crucial for datasets where time measurements are essential but can vary slightly.</p>"},{"location":"checks/greater-than-field-check/#unit","title":"Unit","text":"<p>The unit of time you select determines how granular the comparison is:</p> <ul> <li>Millis: Measures time in milliseconds, ideal for high-precision needs.</li> <li>Seconds: Suitable for most general purposes where precision is important but doesn't need to be to the millisecond.</li> <li>Days: Best for longer durations.</li> </ul>"},{"location":"checks/greater-than-field-check/#value","title":"Value","text":"<p>Value sets the maximum acceptable difference in time to consider two values as equal. It serves to define the margin of error, accommodating small discrepancies that naturally occur over time.</p> Illustration using Duration Comparator Unit Value A Value B Difference Threshold Are equal? Millis 500 ms 520 ms 20 ms 25 ms True Seconds 30 sec 31 sec 1 sec 2 sec True Days 5 days 7 days 2 days 1 day False Millis 1000 ms 1040 ms 40 ms 25 ms False Seconds 45 sec 48 sec 3 sec 2 sec False"},{"location":"checks/greater-than-field-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/greater-than-field-check/#example","title":"Example","text":"<p>Objective: Ensure that all O_TOTALPRICE entries in the ORDERS table are greater than their respective O_DISCOUNT.</p> <p>Sample Data</p> O_ORDERKEY O_TOTALPRICE O_DISCOUNT 1 100 105 2 500 10 3 120 121 Payload example <pre><code>{\n    \"description\": \"Ensure that all O_TOTALPRICE entries in the ORDERS table are greater than their respective O_DISCOUNT\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"field_name\": \"O_DISCOUNT\",\n        \"inclusive\": true\n    },\n    \"tags\": [],\n    \"fields\": [\"O_TOTALPRICE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"greaterThanField\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>O_ORDERKEY</code> 1 and 3 do not satisfy the rule because their <code>O_TOTALPRICE</code> values are not greater than their respective <code>O_DISCOUNT</code> values.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_TOTALPRICE and O_DISCOUNT]\nB --&gt; C{Is O_TOTALPRICE &gt; O_DISCOUNT?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    o_orderkey,\n    o_totalprice,\n    o_discount\nfrom orders \nwhere\n    o_totalprice &lt;= o_discount;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>O_TOTALPRICE</code> value of <code>100</code> is not greater than the value of <code>O_DISCOUNT</code>.</p> <p>Shape Anomaly</p> <p>In <code>O_TOTALPRICE</code>, 66.667% of 3 filtered records (2) are not greater than <code>O_DISCOUNT</code>.</p>"},{"location":"checks/inferred-check/","title":"Inferred Check","text":"<p>Qualytics automatically generates inferred checks during a Profile operation. These checks typically cover 80-90% of the rules needed by users. They are created and maintained through profiling, which involves statistical analysis and machine learning methods.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"checks/inferred-check/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and select the datastore from the left menu.</p> <p></p> <p>Step 2: Click on the \"Checks\" from the Navigation Tab.</p> <p></p> <p>Step 3: In the top-right corner, click on the \"Run\" button, then select \"Profile\" from the dropdown menu. This action will initiate the profiling process that generates inferred checks.</p> <p></p> <p>Note</p> <p>Inferred checks will be automatically updated with the next Profiling run. Manually updating an inferred check will take it out of the automatic update workflow.</p> <p>To understand how Inferred checks work, you can follow the steps from the documentation Profile Operation.</p> <p>After the profiling run is complete, inferred checks will be automatically created based on the analysis of your data.</p> <p></p> <p>1. Check Summary: Provides a summary of the schema validation check, including its unique identifier, type, status, and associated warnings or information. It serves as a quick reference for users to assess the check's current state and access relevant actions.</p> <p>For demonstration purposes, the applied rule type is Expected Schema.</p> <p></p> <p>Check ID: A unique identifier assigned to this particular check.</p> <p></p> <p>Check Type: Indicates the nature of the validation being performed on the check.</p> <p></p> <p>Warnings: Indicates the presence of active anomalies detected in the dataset. Clicking on this icon opens a dropdown menu with the following options:</p> <ul> <li> <p>View: Displays detailed information about the detected anomalies.</p> </li> <li> <p>Acknowledge: Marks the anomaly as reviewed or acknowledged.</p> </li> <li> <p>Archive: Moves the anomaly record to the archive for future reference.</p> </li> </ul> <p></p> <p>Open Details: Provides additional details or guidance about the check. Clicking this icon typically displays more context or documentation related to schema validation.</p> <p></p> <p>Check Actions: Opens a dropdown menu with more actions related to managing or modifying the check.</p> <p></p> <p>2. Target: Specifies the dataset and file that the inferred check will be applied to. This section ensures that the validation rules are correctly assigned to the intended source datastore. Users can select a different file if needed by clicking the dropdown.</p> <p></p> <p>3. Fields List: Displays the fields that are expected to be present in the dataset.</p> <p></p> No. Fields Description 1 Date of Birth Stores date and time values, ensuring precise representation of birth dates. 2 Created Date Holds the record\u2019s creation date as a text value rather than a structured date format. <p>4. Allow Other Fields (Checkbox):</p> <ul> <li>If checked, the validation process allows additional fields beyond those explicitly listed.</li> <li>If unchecked, any unexpected field in the dataset will trigger an error.</li> </ul> <p></p> <p>5. Description: Enter a detailed description of the check template, including its purpose, applicable data, and relevant information to ensure clarity for users. If you're unsure of what to include, click on the \"\ud83d\udca1\" lightbulb icon to apply a suggested description based on the rule type.</p> <p></p> <p>6. Tags: Tags help categorize and manage checks efficiently. The tag \"test partition scan\" in this case suggests that this check is related to a specific test or partition scan process.</p> <p></p> <p>7. Additional Metadata: Add key-value pairs as additional metadata to enrich your check. Click the plus icon (+) next to this section to open the metadata input form, where you can add key-value pairs.</p> <p></p> <p>Enter the desired key-value pairs. After entering the necessary metadata, click \"Confirm\" to save the custom metadata.</p> <p></p> <p>Note</p> <p>The Target field is non-editable. It automatically reflects the selected dataset and cannot be modified manually.</p> <p>Step 4: After completing all the check details, click on the \"Validate\" button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct. It ensures that the check will work as expected by running it against the data without committing any changes.</p> <p></p> <p>If the validation is successful, a green message will appear saying \"Validation Successful\".</p> <p></p> <p>Step 5: Once you have a successful validation, click the \"Update\" button.</p> <p></p> <p>After clicking on the \u201cUpdate\u201d button, your check is successfully updated and a success flash message will appear saying \u201cCheck successfully updated\u201d.</p> <p></p>"},{"location":"checks/inferred-check/#inference-engine","title":"Inference Engine","text":"<ol> <li> <p>After metadata is generated by a Profile Operation, Inference Engine is initiated to kick off Inductive and Unsupervised learning methods.  </p> </li> <li> <p>Available data is partitioned into a training set and a testing set.  </p> </li> <li> <p>The engine applies numerous machine learning models and techniques to the training data in an effort to discover well-fitting data quality constraints.</p> </li> <li> <p>Those inferred constraints are then filtered by testing them against the held out testing set and only those that assert true above a certain threshold are converted and exposed to users as Inferred Checks.</p> </li> </ol>"},{"location":"checks/is-address/","title":"Is Address","text":""},{"location":"checks/is-address/#definition","title":"Definition","text":"<p>Asserts that the values contain the specified required elements of an address.</p>"},{"location":"checks/is-address/#in-depth-overview","title":"In-Depth Overview","text":"<p>This check leverages machine learning powered by the libpostal library to support multilingual street address parsing/normalization that can handle addresses all over the world. The underlying statistical NLP model was trained using data from OpenAddress and OpenStreetMap, a total of about 1.2 billion records of data from over 230 countries, in 100+ languages. The international address parser uses Conditional Random Fields, which can infer a globally optimal tag sequence instead of making local decisions at each word, and it achieves 99.45% full-parse accuracy on held-out addresses (i.e. addresses from the training set that were purposefully removed so we could evaluate the parser on addresses it hasn\u2019t seen before).</p>"},{"location":"checks/is-address/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/is-address/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/is-address/#specific-properties","title":"Specific Properties","text":"Name Description Required Labels The labels that must be identifiable in the value of each record <p>Info</p> <p>The address parser can technically use any string labels that are defined in the training data, but these are the ones currently supported:</p> <ul> <li>road: Street name(s)</li> <li>city: Any human settlement including cities, towns, villages, hamlets, localities, etc</li> <li>state: First-level administrative division. Scotland, Northern Ireland, Wales, and England in the UK are mapped to \"state\" as well (convention used in OSM, GeoPlanet, etc.)</li> <li>country: Sovereign nations and their dependent territories, anything with an ISO-3166 code</li> <li>postcode: Postal codes used for mail sorting</li> </ul> <p>This check allows the user to define any combination of these labels as required elements of the value held in each record. Any value these does not contain every required element will be identified as anomalous.</p>"},{"location":"checks/is-address/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/is-address/#example","title":"Example","text":"<p>Objective: Ensure that all values in O_MAILING_ADDRESS include the labels \"road\", \"city\", \"state\", and \"postcode\"</p> <p>Sample Data</p> O_ORDERKEY O_MAILING_ADDRESS 1 One-hundred twenty E 96th St, new york NY 14925 2 Quatre vingt douze R. de l'\u00c9glise, 75196 cedex 04 3 781 Franklin Ave Crown Heights Brooklyn NYC NY 11216 USA Payload example <pre><code>{\n    \"description\": \"Ensure that all values in O_MAILING_ADDRESS include the labels \"road\", \"city\", \"state\", and \"postcode\"\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"required_labels\": [\"road\",\"city\",\"state\",\"country\",\"postcode\"]\n        },\n    \"tags\": [],\n    \"fields\": [\"O_MAILING_ADDRESS\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"isAddress\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>O_ORDERKEY</code> 2 does not satisfy the rule because the <code>O_MAILING_ADDRESS</code> value includes only a road and postcode which violates the business logic that city and state also be present.</p> Flowchart <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_MAILING_ADDRESS]\nB --&gt; C[Infer address labels using ML]\nC --&gt; D{Are all required labels present?}\nD --&gt;|Yes| E[Move to Next Record/End]\nD --&gt;|No| F[Mark as Anomalous]\nF --&gt; E</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>O_MAILING_ADDRESS</code> value of <code>Quatre vingt douze R. de l'\u00c9glise, 75196 cedex 04</code> does not adhere to the required format.</p> <p>Shape Anomaly</p> <p>In <code>O_MAILING_ADDRESS</code>, 33.33% of 3 filtered records (1) do not adhere to the required format.</p>"},{"location":"checks/is-credit-card-check/","title":"Is Credit Card","text":""},{"location":"checks/is-credit-card-check/#definition","title":"Definition","text":"<p>Asserts that the values are credit card numbers.</p>"},{"location":"checks/is-credit-card-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/is-credit-card-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/is-credit-card-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/is-credit-card-check/#example","title":"Example","text":"<p>Objective: Ensure that all C_CREDIT_CARD entries in the CUSTOMER table are valid credit card numbers.</p> <p>Sample Data</p> C_CUSTKEY C_CREDIT_CARD 1 5105105105105100 2 ABC12345XYZ 3 4111111111111111 Payload example <pre><code>{\n    \"description\": \"Ensure that all C_CREDIT_CARD entries in the CUSTOMER table are valid credit card numbers\",\n    \"coverage\": 1,\n    \"properties\": {},\n    \"tags\": [],\n    \"fields\": [\"C_CREDIT_CARD\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"isCreditCard\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>C_CUSTKEY</code> 2 does not satisfy the rule because its <code>C_CREDIT_CARD</code> value is not a valid credit card number.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve C_CREDIT_CARD]\nB --&gt; C{Is C_CREDIT_CARD valid?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    c_custkey\n    , c_credit_card\nfrom customer \nwhere\n    not regexp_like(replace(c_credit_card, '-', ''), '^[0-9]{16}$')\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>C_CREDIT_CARD</code> value of <code>ABC12345XYZ</code> is not a valid credit card number.</p> <p>Shape Anomaly</p> <p>In <code>C_CREDIT_CARD</code>, 33.33% of 3 filtered records (1) are not valid credit card numbers.</p>"},{"location":"checks/is-replica-of-check/","title":"Is Replica Of (Is sunsetting)","text":"<p>Deprecation Notice</p> <p>The <code>isReplicaOf</code> check is being deprecated and will no longer be maintained. We strongly recommend using the Data Diff check, which offers the same functionality with improved performance and additional features.</p> <p>Our recommendation:</p> <ul> <li>Consider using <code>Data Diff</code> for new implementations</li> <li><code>dataDiff</code> provides enhanced performance and additional capabilities</li> <li>Both checks will continue to coexist in the system</li> </ul> <p>If you have questions about this change, please contact our support team</p>"},{"location":"checks/is-replica-of-check/#definition","title":"Definition","text":"<p>Asserts that the dataset created by the targeted field(s) is replicated by the referred field(s).</p>"},{"location":"checks/is-replica-of-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>IsReplicaOf</code> rule ensures that data integrity is maintained when data is replicated from one source to another. This involves checking not only the data values themselves but also ensuring that the structure and relationships are preserved.</p> <p>In a distributed data ecosystem, replication often occurs to maintain high availability, create backups, or feed data into analytical systems. However, discrepancies might arise due to various reasons such as network glitches, software bugs, or human errors. The <code>IsReplicaOf</code> rule serves as a safeguard against these issues by:</p> <ol> <li>Preserving Data Structure: Ensuring that the structure of the replicated data matches the original.</li> <li>Checking Data Values: Ensuring that every piece of data in the source exists in the replica.</li> </ol>"},{"location":"checks/is-replica-of-check/#field-scope","title":"Field Scope","text":"<p>Multi: The rule evaluates multiple specified fields.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/is-replica-of-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/is-replica-of-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the datastore and table/file where the replica of the targeted fields is located for comparison.</p> Name Description Row Identifiers The list of fields defining the compound key to identify rows in the comparison analysis. Datastore The source datastore where the replica of the targeted field(s) is located. Table/file The table, view or file in the source datastore that should serve as the replica. Comparators Specifies how variations are handled, allowing for slight deviations within a defined margin of error. <p>Info</p> <p>The <code>IsReplicaOf</code> rule supports editing of <code>Row Identifiers</code> and <code>Passthrough Fields</code>, allowing for more tailored configuration.</p> <p>Details</p> <p>This optional input allows row comparison analysis by defining a list of fields as row identifiers, it enables a more detailed comparison between tables/files, where each row compound key is used to identify its presence or absence in the reference table/file compared to the target table/file.  Qualytics can inform if the row exists or not and distinguish which field values differ in each row present in the reference table/file, helping to determine if it is a replica.</p> <p>Info</p> <p>Anomalies produced by a <code>IsReplicaOf</code> quality check making use of <code>Row Identifiers</code> have their source records presented in a different visualization.  See more at: Comparison Source Records</p> <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"checks/is-replica-of-check/#row-identifiers","title":"Row Identifiers","text":""},{"location":"checks/is-replica-of-check/#comparators","title":"Comparators","text":""},{"location":"checks/is-replica-of-check/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"checks/is-replica-of-check/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"checks/is-replica-of-check/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"checks/is-replica-of-check/#duration","title":"Duration","text":"<p>Duration comparators support time-based comparisons, allowing for flexibility in how duration differences are managed. This flexibility is crucial for datasets where time measurements are essential but can vary slightly.</p>"},{"location":"checks/is-replica-of-check/#unit","title":"Unit","text":"<p>The unit of time you select determines how granular the comparison is:</p> <ul> <li>Millis: Measures time in milliseconds, ideal for high-precision needs.</li> <li>Seconds: Suitable for most general purposes where precision is important but doesn't need to be to the millisecond.</li> <li>Days: Best for longer durations.</li> </ul>"},{"location":"checks/is-replica-of-check/#value","title":"Value","text":"<p>Value sets the maximum acceptable difference in time to consider two values as equal. It serves to define the margin of error, accommodating small discrepancies that naturally occur over time.</p> Illustration using Duration Comparator Unit Value A Value B Difference Threshold Are equal? Millis 500 ms 520 ms 20 ms 25 ms True Seconds 30 sec 31 sec 1 sec 2 sec True Days 5 days 7 days 2 days 1 day False Millis 1000 ms 1040 ms 40 ms 25 ms False Seconds 45 sec 48 sec 3 sec 2 sec False"},{"location":"checks/is-replica-of-check/#string","title":"String","text":"<p>String comparators facilitate comparisons of textual data by allowing variations in spacing. This capability is essential for ensuring data consistency, particularly where minor text inconsistencies may occur.</p>"},{"location":"checks/is-replica-of-check/#ignore-whitespace","title":"Ignore Whitespace","text":"<p>When enabled, this setting allows the comparator to ignore differences in whitespace. This means sequences of whitespace are collapsed into a single space, and any leading or trailing spaces are removed. This can be particularly useful in environments where data entry may vary in formatting but where those differences are not relevant to the data's integrity.</p> Illustration <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined string comparison to <code>ignore whitespace</code> as <code>True</code>.</p> Value A Value B Are equal? Has whitespace? <code>Leonidas</code> <code>Leonidas</code> True No <code>Beth</code> <code>Beth</code> True Yes <code>Ana</code> <code>Anna</code> False Yes <code>Joe</code> <code>Joel</code> False No"},{"location":"checks/is-replica-of-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/is-replica-of-check/#example","title":"Example","text":"<p>Scenario: Consider that the fields N_NATIONKEY and N_NATIONNAME in the NATION table are being replicated to a backup database for disaster recovery purposes. The data engineering team wants to ensure that both fields in the replica in the backup accurately reflect the original.</p> <p>Objective: Ensure that N_NATIONKEY and N_NATIONNAME from the NATION table are replicas in the NATION_BACKUP table.</p> <p>Sample Data from NATION</p> N_NATIONKEY N_NATIONNAME 1 Australia 2 United States 3 Uruguay <p>Replica Sample Data from NATION_BACKUP</p> N_NATIONKEY N_NATIONNAME 1 Australia 2 USA 3 Uruguay Payload example <pre><code>{\n    \"description\": \"Ensure that N_NATIONKEY and N_NATIONNAME from the NATION table are replicas in the NATION_BACKUP table\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"ref_container_id\": {ref_container_id},\n        \"ref_datastore_id\": {ref_datastore_id}\n    },\n    \"tags\": [],\n    \"fields\": [\"N_NATIONKEY\", \"N_NATIONNAME\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"isReplicaOf\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>The datasets representing the fields <code>N_NATIONKEY</code> and <code>N_NATIONNAME</code> in the original and the replica are not completely identical, indicating a possible discrepancy in the replication process or an unintended change.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Original Data]\nB --&gt; C[Retrieve Replica Data]\nC --&gt; D{Do datasets match for both fields?}\nD --&gt;|Yes| E[End]\nD --&gt;|No| F[Mark as Anomalous]\nF --&gt; E</code></pre> <pre><code>-- An illustrative SQL query comparing original to replica for both fields.\nselect\n    orig.n_nationkey as original_key,\n    orig.n_nationname as original_name,\n    replica.n_nationkey as replica_key,\n    replica.n_nationname as replica_name\nfrom nation as orig\nleft join nation_backup as replica on orig.n_nationkey = replica.n_nationkey\nwhere\n    orig.n_nationname &lt;&gt; replica.n_nationname\nor\n    orig.n_nationkey &lt;&gt; replica.n_nationkey\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>There is 1 record that differs between <code>NATION_BACKUP</code> (3 records) and <code>NATION</code> (3 records) in <code>&lt;datastore_name&gt;</code></p>"},{"location":"checks/is-type-check/","title":"Is Type","text":""},{"location":"checks/is-type-check/#definition","title":"Definition","text":"<p>Asserts that the data is of a specific type.</p>"},{"location":"checks/is-type-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/is-type-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/is-type-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the expected type for the data in the field.</p> Name Description Field Type The type that values in the selected field should conform to."},{"location":"checks/is-type-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/is-type-check/#example","title":"Example","text":"<p>Objective: Ensure that all L_QUANTITY entries in the LINEITEM table are of Integral type.</p> <p>Sample Data</p> L_ORDERKEY L_QUANTITY 1 \"10\" 2 \"15.5\" 3 \"Ten\" Payload example <pre><code>{\n    \"description\": \"Ensure that all L_QUANTITY entries in the LINEITEM table are of Integral type\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"field_type\":\"Integral\"\n    },\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"isType\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 2 and 3 do not satisfy the rule because their <code>L_QUANTITY</code> values are not of Integral type.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_QUANTITY]\nB --&gt; C{Is L_QUANTITY of Integral type?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_quantity\nfrom lineitem \nwhere\n    typeof(l_quantity) != 'INTEGER'\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>L_QUANTITY</code> value of <code>Ten</code> is not a valid Integral.</p> <p>Shape Anomaly</p> <p>In <code>L_QUANTITY</code>, 66.667% of 3 filtered records (2) are not a valid Integral.</p>"},{"location":"checks/less-than-check/","title":"Less Than","text":""},{"location":"checks/less-than-check/#definition","title":"Definition","text":"<p>Asserts that the field is a number less than (or equal to) a value.</p>"},{"location":"checks/less-than-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/less-than-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/less-than-check/#specific-properties","title":"Specific Properties","text":"<p>Allows specifying a numeric value that acts as the threshold.</p> Name Description Value The number to use as the base comparison. Inclusive If true, the comparison will also allow values equal to the threshold. Otherwise, it's exclusive. Comparators Specifies how variations are handled, allowing for slight deviations within a defined margin of error. <p>Details</p> <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"checks/less-than-check/#comparators","title":"Comparators","text":""},{"location":"checks/less-than-check/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"checks/less-than-check/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"checks/less-than-check/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"checks/less-than-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/less-than-check/#example","title":"Example","text":"<p>Objective: Ensure that all L_PRICE entries in the LINEITEM table are less than 20.</p> <p>Sample Data</p> L_ORDERKEY L_PRICE 1 18 2 25 3 23 Payload example <pre><code>{\n    \"description\": \"Ensure that all L_PRICE entries in the LINEITEM table are less than 20\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"inclusive\": true,\n        \"value\": 20\n    },\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"lessThan\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 2 and 3 do not satisfy the rule because their <code>L_PRICE</code> values are not less than 20.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_PRICE]\nB --&gt; C{Is L_PRICE &lt; 20?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_price\nfrom lineitem \nwhere\n    l_price &gt;= 20;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>L_PRICE</code> value of <code>23</code> is not less than the value of 20.</p> <p>Shape Anomaly</p> <p>In <code>L_PRICE</code>, 66.667% of 3 filtered records (2) are not less than 20.</p>"},{"location":"checks/less-than-field-check/","title":"Less Than Field","text":""},{"location":"checks/less-than-field-check/#definition","title":"Definition","text":"<p>Asserts that the field is less than another field.</p>"},{"location":"checks/less-than-field-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code>"},{"location":"checks/less-than-field-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/less-than-field-check/#specific-properties","title":"Specific Properties","text":"<p>Allows specifying another field against which the value comparison will be performed.</p> Name Description Field to compare Specifies the name of the field against which the value will be compared. Inclusive If true, the comparison will also allow values equal to the value of the other field. Otherwise, it's exclusive. Comparators Specifies how variations are handled, allowing for slight deviations within a defined margin of error. <p>Details</p> <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"checks/less-than-field-check/#comparators","title":"Comparators","text":""},{"location":"checks/less-than-field-check/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"checks/less-than-field-check/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"checks/less-than-field-check/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"checks/less-than-field-check/#duration","title":"Duration","text":"<p>Duration comparators support time-based comparisons, allowing for flexibility in how duration differences are managed. This flexibility is crucial for datasets where time measurements are essential but can vary slightly.</p>"},{"location":"checks/less-than-field-check/#unit","title":"Unit","text":"<p>The unit of time you select determines how granular the comparison is:</p> <ul> <li>Millis: Measures time in milliseconds, ideal for high-precision needs.</li> <li>Seconds: Suitable for most general purposes where precision is important but doesn't need to be to the millisecond.</li> <li>Days: Best for longer durations.</li> </ul>"},{"location":"checks/less-than-field-check/#value","title":"Value","text":"<p>Value sets the maximum acceptable difference in time to consider two values as equal. It serves to define the margin of error, accommodating small discrepancies that naturally occur over time.</p> Illustration using Duration Comparator Unit Value A Value B Difference Threshold Are equal? Millis 500 ms 520 ms 20 ms 25 ms True Seconds 30 sec 31 sec 1 sec 2 sec True Days 5 days 7 days 2 days 1 day False Millis 1000 ms 1040 ms 40 ms 25 ms False Seconds 45 sec 48 sec 3 sec 2 sec False"},{"location":"checks/less-than-field-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/less-than-field-check/#example","title":"Example","text":"<p>Objective: Ensure that all O_DISCOUNT entries in the ORDERS table are less than their respective O_TOTALPRICE.</p> <p>Sample Data</p> O_ORDERKEY O_TOTALPRICE O_DISCOUNT 1 105 100 2 500 10 3 121 125 Payload example <pre><code>{\n    \"description\": \"Ensure that all O_DISCOUNT entries in the ORDERS table are less than their respective O_TOTALPRICE\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"field_name\": \"O_TOTALPRICE\",\n        \"inclusive\":true\n    },\n    \"tags\": [],\n    \"fields\": [\"O_DISCOUNT\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"lessThanField\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>O_ORDERKEY</code> 3 does not satisfy the rule because its <code>O_DISCOUNT</code> value is not less than its respective <code>O_TOTALPRICE</code> value.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_TOTALPRICE and O_DISCOUNT]\nB --&gt; C{Is O_DISCOUNT &lt; O_TOTALPRICE?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    o_orderkey,\n    o_totalprice,\n    o_discount\nfrom orders \nwhere\n    o_discount &gt;= o_totalprice;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>O_DISCOUNT</code> value of <code>125</code> is not less than the value of <code>O_TOTALPRICE</code>.</p> <p>Shape Anomaly</p> <p>In <code>O_DISCOUNT</code>, 33.333% of 3 filtered records (1) is not less than <code>O_TOTALPRICE</code>.</p>"},{"location":"checks/matches-pattern-check/","title":"Matches Pattern","text":""},{"location":"checks/matches-pattern-check/#definition","title":"Definition","text":"<p>Asserts that a field must match a pattern.</p>"},{"location":"checks/matches-pattern-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>Patterns, typically expressed as regular expressions, allow for the enforcement of custom structural norms for data fields. For complex patterns, regular expressions offer a powerful tool to ensure conformity to the expected format.</p>"},{"location":"checks/matches-pattern-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/matches-pattern-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/matches-pattern-check/#specific-properties","title":"Specific Properties","text":"<p>Allows specifying a pattern against which the field will be checked.</p> Name Description Pattern Specifies the regular expression pattern the field must match."},{"location":"checks/matches-pattern-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/matches-pattern-check/#example","title":"Example","text":"<p>Objective: Ensure that all P_SERIAL entries in the PART table match the pattern for product serial numbers: <code>TPCH-XXXX-####</code>, where <code>XXXX</code> are uppercase alphabetic characters and <code>####</code> are numbers.</p> <p>Sample Data</p> P_PARTKEY P_SERIAL 1 TPCH-ABCD-1234 2 TPCH-1234-ABCD 3 TPCH-WXYZ-9876 Payload example <pre><code>{\n    \"description\": \"Ensure that all P_SERIAL entries in the PART table match the pattern for product serial numbers: `TPCH-XXXX-####`, where `XXXX` are uppercase alphabetic characters and `####` are numbers\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"pattern\":\"^tpch-[a-z]{4}-[0-9]{4}$\"\n    },\n    \"tags\": [],\n    \"fields\": [\"P_SERIAL\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"matchesPattern\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>P_PARTKEY</code> 2 does not satisfy the rule because its <code>P_SERIAL</code> does not match the required pattern.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve P_SERIAL]\nB --&gt; C{Does P_SERIAL match TPCH-XXXX-#### format?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    p_partkey,\n    p_serial\nfrom part \nwhere\n    not regexp_like(p_serial, '^tpch-[a-z]{4}-[0-9]{4}$')\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>P_SERIAL</code> value of <code>TPCH-1234-ABCD</code> does not match the pattern <code>TPCH-XXXX-####</code>.</p> <p>Shape Anomaly</p> <p>In <code>P_SERIAL</code>, 33.333% of 3 filtered records (1) do not match the pattern <code>TPCH-XXXX-####</code>.</p>"},{"location":"checks/max-length-check/","title":"Max Length","text":""},{"location":"checks/max-length-check/#definition","title":"Definition","text":"<p>Asserts that a string has a maximum length.</p>"},{"location":"checks/max-length-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/max-length-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/max-length-check/#specific-properties","title":"Specific Properties","text":"<p>Determines the maximum acceptable length of the string.</p> Name Description Length Specifies the maximum number of characters a string in the field should have."},{"location":"checks/max-length-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/max-length-check/#example","title":"Example","text":"<p>Objective: Ensure that P_DESCRIPTION in the PART table do not exceed 50 characters in length.</p> <p>Sample Data</p> P_PARTKEY P_DESCRIPTION 1 Standard industrial widget 2 A product description that clearly goes way beyond the specified fifty characters limit. 3 Basic office equipment Payload example <pre><code>{\n    \"description\": \"Ensure that P_DESCRIPTION in the PART table do not exceed 50 characters in length\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\": 3\n    },\n    \"tags\": [],\n    \"fields\": [\"C_BLOOD_GROUP\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"maxLength\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>P_PARTKEY</code> 2 does not satisfy the rule because its <code>P_DESCRIPTION</code> exceeds 50 characters in length.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve P_DESCRIPTION]\nB --&gt; C{Is P_DESCRIPTION length &lt;= 50 characters?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    p_partkey,\n    p_description\nfrom part \nwhere\n    length(p_description) &gt; 50;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>P_DESCRIPTION</code> length of <code>A product description that clearly goes way beyond the specified fifty characters limit.</code> is greater than the max length of 50.</p> <p>Shape Anomaly</p> <p>In <code>P_DESCRIPTION</code>, 33.333% of 3 filtered records (1) have a length greater than 50.</p>"},{"location":"checks/max-partition-size-check/","title":"Max Partition Size","text":""},{"location":"checks/max-partition-size-check/#definition","title":"Definition","text":"<p>Asserts the maximum number of records that should be loaded from each file or table partition.</p>"},{"location":"checks/max-partition-size-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>Managing the volume of data in each partition is critical when dealing with partitioned datasets. This is especially pertinent when system limitations or data processing capabilities are considered, ensuring that no partition exceeds the system's ability to handle data efficiently.</p> <p>The Max Partition Size rule is designed to set an upper limit on the number of records each partition can contain.</p>"},{"location":"checks/max-partition-size-check/#general-properties","title":"General Properties","text":"Details Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions"},{"location":"checks/max-partition-size-check/#specific-properties","title":"Specific Properties","text":"<p>Specifies the maximum allowable record count for each data partition</p> Name Description Maximum partition size The maximum number of records that can be loaded from each partition."},{"location":"checks/max-partition-size-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/max-partition-size-check/#example","title":"Example","text":"<p>Objective: Ensure that no partition of the LINEITEM table contains more than 10,000 records to prevent data processing bottlenecks.</p> <p>Sample Data for Partition P3</p> Row Number L_ITEM 1 Data 2 Data ... ... 10,050 Data Payload example <pre><code>{\n    \"description\": \"Ensure that no partition of the LINEITEM table contains more than 10,000 records to prevent data processing bottlenecks\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\":10000\n    },\n    \"tags\": [],\n    \"fields\": null,\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"maxPartitionSize\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>In the sample data above, the rule is violated because partition P3 contains 10,050 records, which exceeds the set maximum of 10,000 records.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Number of Records for Each Partition]\nB --&gt; C{Does Partition have &lt;= 10,000 records?}\nC --&gt;|Yes| D[Move to Next Partition/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s). \nselect\n    subset_name, -- or any column indicating the partition or subset\n    count(*)\nfrom lineitem \ngroup by subset_name\nhaving count(*) &gt; 10000;\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>In <code>LINEITEM</code>, more than 10,000 records were loaded.</p>"},{"location":"checks/max-value-check/","title":"Max Value","text":""},{"location":"checks/max-value-check/#definition","title":"Definition","text":"<p>Asserts that a field has a maximum value.</p>"},{"location":"checks/max-value-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/max-value-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/max-value-check/#specific-properties","title":"Specific Properties","text":"<p>Determines the maximum allowable value for the field.</p> Name Description Value Specifies the maximum value a field should have."},{"location":"checks/max-value-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/max-value-check/#example","title":"Example","text":"<p>Objective: Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table does not exceed a value of 50.</p> <p>Sample Data</p> L_ORDERKEY L_LINENUMBER L_QUANTITY 1 1 40 1 2 55 2 1 20 3 1 60 Payload example <pre><code>{\n    \"description\": \"Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table does not exceed a value of 50\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\": 50\n    },\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"maxValue\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 1 and 3 do not satisfy the rule because their <code>L_QUANTITY</code> values exceed the specified maximum value of 50.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_QUANTITY]\nB --&gt; C{Is L_QUANTITY &lt;= 50?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_linenumber,\n    l_quantity\nfrom lineitem \nwhere\n    l_quantity &gt; 50;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>L_QUANTITY</code> value of <code>55</code> is greater than the max value of <code>50</code>.</p> <p>Shape Anomaly</p> <p>In <code>L_QUANTITY</code>, 50.000% of 4 filtered records (2) are greater than the max value of <code>50</code>.</p>"},{"location":"checks/metric-check/","title":"Metric","text":""},{"location":"checks/metric-check/#definition","title":"Definition","text":"<p>Records the value of the selected field during each scan operation and asserts limits based upon an expected change or absolute range (inclusive).</p>"},{"location":"checks/metric-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>Metric</code> rule is designed to monitor the values of a selected field over time. It is particularly useful in a time-series context where values are expected to evolve within certain bounds or limits. This rule allows for tracking absolute values or changes, ensuring they remain within predefined thresholds.</p>"},{"location":"checks/metric-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/metric-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/metric-check/#specific-properties","title":"Specific Properties","text":"<p>Determines the evaluation method and allowable limits for field value comparisons over time.</p> Name Description Comparison Specifies the type of comparison: Absolute Change, Absolute Value, or Percentage Change. Min Value Indicates the minimum allowable increase in value. Use a negative value to represent an allowable decrease. Max Value Indicates the maximum allowable increase in value. <p>Details</p>"},{"location":"checks/metric-check/#comparison-options","title":"Comparison Options","text":"<p>Absolute Change</p> <p>The <code>Absolute Change</code> comparison works by comparing the change in a numeric field's value to a pre-set limit (Min / Max). If the field's value changes by more than this specified limit since the last relevant scan, an anomaly is identified.</p> Illustration <p>Any record with a value change smaller than 30 or greater than 70 compared to the last scan should be flagged as anomalous</p> <p>Thresholds: Min Change = 30, Max Change = 70</p> Scan Previous Value Current Value Absolute Change Anomaly Detected #1 - 100 - No #2 100 150 50 No #3 150 220 70 No #4 220 300 80 Yes <p>Absolute Value</p> <p>The <code>Absolute Value</code> comparison works by comparing the change in a numeric field's value to a pre-set limit <code>between</code> Min and Max values. If the field's value changes by more than this specified range since the last relevant scan, an anomaly is identified.</p> Illustration <p>The value of the record in each scan should be within 100 and 300 to be considered normal</p> <p>Thresholds: Min Value = 100, Max Value = 300</p> Scan Current Value Anomaly Detected #1 150 No #2 90 Yes #3 250 No #4 310 Yes <p>Percentage Change</p> <p>The <code>Percentage Change</code> comparison operates by tracking changes in a numeric field's value relative to its previous value. If the change exceeds the predefined percentage (%) limit since the last relevant scan, an anomaly is generated.</p> Illustration <p>An anomaly is identified if the record's value decreases by more than 20% or increases by more than 50% compared to the last scan.</p> <p>Thresholds: Min Percentage Change = -20%, Max Percentage Change = 50%</p> <p>Percentage Change Formula: ( (current_value - previous_value) / previous_value ) * 100</p> Scan Previous Value Current Value Percentage Change Anomaly Detected 1 - 100 - No 2 100 150 50% No 3 150 120 -20% No 4 120 65 -45.83% Yes 5 65 110 69.23% Yes"},{"location":"checks/metric-check/#thresholds","title":"Thresholds","text":"<p>At least the Min or Max value must be specified, and including both is optional. These values determine the acceptable range or limit of change in the field's value.</p> <p>Min Value</p> <ul> <li>Represents the minimum allowable increase in the field's value.</li> <li>A negative Min Value signifies an allowable decrease, determining the minimum value the field can drop to be considered valid.</li> </ul> <p>Max Value</p> <ul> <li>Indicates the maximum allowable increase in the field\u2019s value, setting an upper limit for the value's acceptable growth or change.</li> </ul>"},{"location":"checks/metric-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/metric-check/#example","title":"Example","text":"<p>Objective: Ensure that the total price in the ORDERS table does not fluctuate beyond a predefined percentage limit between scans.</p> <p>Thresholds: Min Percentage Change = -30%, Max Percentage Change = 30%</p> <p>Sample Scan History</p> Scan O_ORDERKEY Previous O_TOTALPRICE Current O_TOTALPRICE Percentage Change Anomaly Detected #1 1 - 100 - No #2 1 100 110 10% No #3 1 110 200 81.8% Yes #4 1 200 105 -47.5% Yes Payload example <pre><code>{\n    \"description\": \"Ensure that the total price in the ORDERS table does not fluctuate beyond a predefined percentage limit between scans\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"comparison\":\"Percentage Change\",\n        \"min\":-0.3,\n        \"max\":0.3\n    },\n    \"tags\": [],\n    \"fields\": [\"O_TOTALPRICE \"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"metric\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample scan history above, anomalies are identified in scans #3 and #4. The <code>O_TOTALPRICE</code> values in these scans fall outside the declared percentage change limits of -30% and 30%, indicating that something unusual might be happening and further investigation is needed.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_TOTALPRICE]\nB --&gt; C{Is Percentage Change in O_TOTALPRICE within -30% and 30%?}\nC --&gt;|Yes| D[End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s)\nselect \n    o_orderkey,\n    o_totalprice,\n    lag(o_totalprice) over (order by o_orderkey) as previous_o_totalprice\nfrom\n    orders\nhaving\n    abs((o_totalprice - previous_o_totalprice) / previous_o_totalprice) * 100 &gt; 30\n    or\n    abs((o_totalprice - previous_o_totalprice) / previous_o_totalprice) * 100 &lt; -30;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly (Percentage Change)</p> <p>The percentage change of <code>O_TOTALPRICE</code> from '110' to '200' falls outside the declared limits</p> <p>Record Anomaly (Absolute Change)</p> <p>using hypothetical numbers</p> <p>The absolute change of <code>O_TOTALPRICE</code> from '150' to '300' falls outside the declared limits</p> <p>Record Anomaly (Absolute Value)</p> <p>using hypothetical numbers</p> <p>The value for <code>O_TOTALPRICE</code> of '50' is not between the declared limits</p>"},{"location":"checks/min-length-check/","title":"Min Length","text":""},{"location":"checks/min-length-check/#definition","title":"Definition","text":"<p>Asserts that a string has a minimum length.</p>"},{"location":"checks/min-length-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/min-length-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/min-length-check/#specific-properties","title":"Specific Properties","text":"<p>Determines the minimum allowable length for the field.</p> Name Description Value Specifies the minimum length that the string field should have."},{"location":"checks/min-length-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/min-length-check/#example","title":"Example","text":"<p>Objective: Ensure that all C_COMMENT entries in the CUSTOMER table have a minimum length of 5 characters.</p> <p>Sample Data</p> C_CUSTKEY C_COMMENT 1 Ok 2 Excellent customer service, very satisfied! 3 Nice staff Payload example <pre><code>{\n    \"description\": \"Ensure that all C_COMMENT entries in the CUSTOMER table have a minimum length of 5 characters\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\": 5\n    },\n    \"tags\": [],\n    \"fields\": [\"C_COMMENT\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"minLength\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>C_CUSTKEY</code> 1 does not satisfy the rule because the length of its <code>C_COMMENT</code> values is below the required minimum length of 5 characters.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve C_COMMENT]\nB --&gt; C{Is C_COMMENT length &gt;= 5?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    c_custkey,\n    c_comment\nfrom customer \nwhere\n    length(c_comment) &lt; 5;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>C_COMMENT</code> length of <code>Ok</code> is less than the min length of 5.</p> <p>Shape Anomaly</p> <p>In <code>C_COMMENT</code>, 33.333% of 3 filtered records (1) have a length less than 5.</p>"},{"location":"checks/min-partition-size-check/","title":"Min Partition Size","text":""},{"location":"checks/min-partition-size-check/#definition","title":"Definition","text":"<p>Asserts the minimum number of records that should be loaded from each file or table partition.</p>"},{"location":"checks/min-partition-size-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>When working with large datasets that are often partitioned for better performance and scalability, ensuring a certain minimum number of records from each partition becomes crucial. This could be to ensure that each partition is well-represented in the analysis, to maintain data consistency or even to verify that data ingestion or migration processes are functioning properly.</p> <p>The Min Partition Size rule allows users to set a threshold ensuring that each partition has loaded at least the specified minimum number of records.</p>"},{"location":"checks/min-partition-size-check/#general-properties","title":"General Properties","text":"Details Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions"},{"location":"checks/min-partition-size-check/#specific-properties","title":"Specific Properties","text":"<p>Sets the required minimum record count for each data partition</p> Name Description Minimum partition size Specifies the minimum number of records that should be loaded from each partition"},{"location":"checks/min-partition-size-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/min-partition-size-check/#example","title":"Example","text":"<p>Objective: Ensure that each partition of the LINEITEM table has at least 1000 records.</p> <p>Sample Data for Partition P3</p> Row Number L_ITEM 1 Data 2 Data ... ... 900 Data Payload example <pre><code>{\n    \"description\": \"Ensure that each partition of the LINEITEM table has at least 1000 records\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\": 1000\n    },\n    \"tags\": [],\n    \"fields\": null,\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"minPartitionSize\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>The sample data above does not satisfy the rule because it contains only 900 records, which is less than the required minimum of 1000 records.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Number of Records for Each Partition]\nB --&gt; C{Does Partition have &gt;= 1000 records?}\nC --&gt;|Yes| D[Move to Next Partition/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s). \nselect\n    subset_name, -- or any column indicating the partition or subset\n    count(*)\nfrom lineitem \ngroup by subset_name\nhaving count(*) &lt; 1000;\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>In <code>LINEITEM</code>, fewer than 900 records were loaded.</p>"},{"location":"checks/min-value-check/","title":"Min Value","text":""},{"location":"checks/min-value-check/#definition","title":"Definition","text":"<p>Asserts that a field has a minimum value.</p>"},{"location":"checks/min-value-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/min-value-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/min-value-check/#specific-properties","title":"Specific Properties","text":"<p>Determines the minimum allowable value for the field.</p> Name Description Value Specifies the minimum value a field should have."},{"location":"checks/min-value-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/min-value-check/#example","title":"Example","text":"<p>Objective: Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table is not below a value of 10.</p> <p>Sample Data</p> L_ORDERKEY L_LINENUMBER L_QUANTITY 1 1 40 1 2 5 2 1 20 3 1 8 Payload example <pre><code>{\n    \"description\": \"Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table is not below a value of 10\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\": 10\n    },\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"minValue\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 1 and 3 do not satisfy the rule because their <code>L_QUANTITY</code> values are below the specified minimum value of 10.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_QUANTITY]\nB --&gt; C{Is L_QUANTITY &gt;= 10?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_linenumber,\n    l_quantity\nfrom lineitem \nwhere\n    l_quantity &lt; 10;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>L_QUANTITY</code> value of <code>5</code> is less than the min value of <code>10</code>.</p> <p>Shape Anomaly</p> <p>In <code>L_QUANTITY</code>, 50.000% of 4 filtered records (2) are less than the min value of <code>10</code>.</p>"},{"location":"checks/not-exists-in-check/","title":"Not Exists In","text":""},{"location":"checks/not-exists-in-check/#definition","title":"Definition","text":"<p>Asserts that values assigned to this field do not exist as values in another field.</p>"},{"location":"checks/not-exists-in-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>Not ExistsIn</code> rule allows you to ensure data exclusivity between different sources, whether it\u2019s object storage systems or databases.</p> <p>While databases might utilize unique constraints to maintain data distinctiveness between related tables, the <code>Not ExistsIn</code> rule extends this capability in two significant ways:</p> <ol> <li>Cross-System Exclusivity: it enables checks to ensure data does not overlap across different databases or even entirely separate systems. This can be essential in scenarios where data should be partitioned or isolated across platforms.</li> <li>Flexible Data Formats: Not just limited to databases, this rule can validate values against various data formats, such as ensuring values in a file do not coincide with those in a table.</li> </ol> <p>These functionalities enable businesses to maintain data exclusivity even in intricate, multi-system settings.</p>"},{"location":"checks/not-exists-in-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/not-exists-in-check/#specific-properties","title":"Specific Properties","text":"<p>Define the datastore, table/file, and field where the rule should look for non-matching values.</p> Name Description Datastore The source datastore where the profile of the reference field is located. Table/file The profile (e.g. table, view or file) containing the reference field. Field The field name whose values should not match those of the selected field."},{"location":"checks/not-exists-in-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/not-exists-in-check/#example","title":"Example","text":"<p>Scenario: A shipping company needs to ensure that all NATION_NAME entries in the NATION table aren't listed in an external unsupported regions file, which lists countries they don't ship to.</p> <p>Sample Data</p> N_NATIONKEY N_NATIONNAME 1 Antarctica 2 Argentina 3 Atlantida <p>Unsupported Regions File Sample</p> UNSUPPORTED_REGION Antarctica Mars ... Payload example <pre><code>{\n    \"description\": \"A shipping company needs to ensure that all NATION_NAME entries in the NATION table aren't listed in an external unsupported regions file, which lists countries they don't ship to\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"field_name\":\"UNSUPPORTED_REGION\",\n        \"ref_container_id\": {ref_container_id},\n        \"ref_datastore_id\": {ref_datastore_id}\n    },\n    \"tags\": [],\n    \"fields\": [\"NATION_NAME\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"notExistsIn\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>N_NATIONKEY</code> 1 does not satisfy the rule because the <code>N_NATIONNAME</code> \"Antarctica\" is listed as an <code>UNSUPPORTED_REGION</code> in the unsupported regions file, indicating the company doesn't ship there.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve UNSUPPORTED_REGION]\nB --&gt; C[Retrieve N_NATIONNAME]\nC --&gt; D{Is N_NATIONNAME listed in UNSUPPORTED_REGION?}\nD --&gt;|No| E[Move to Next Record/End]\nD --&gt;|Yes| F[Mark as Anomalous]\nF --&gt; E</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    n_nationkey\n    , n_nationname\nfrom nation \nwhere\n    n_nationname in ('Antarctica', 'Mars', ... /* other unsupported regions */)\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>N_NATIONNAME</code> value of '<code>Antarctica</code>' is an <code>UNSUPPORTED_REGION</code>.</p> <p>Shape Anomaly</p> <p>In <code>N_NATIONNAME</code>, 33.333% of 3 filtered records (1) do exist in <code>UNSUPPORTED_REGION</code>.</p>"},{"location":"checks/not-future-check/","title":"Not Future","text":""},{"location":"checks/not-future-check/#definition","title":"Definition","text":"<p>Asserts that the field's value is not in the future.</p>"},{"location":"checks/not-future-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Fields</p> Field <code>Date</code> <code>Timestamp</code>"},{"location":"checks/not-future-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/not-future-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/not-future-check/#example","title":"Example","text":"<p>Objective: Ensure that the delivery dates (O_DELIVERYDATE) in the ORDERS table are not set in the future.</p> <p>Sample Data</p> O_ORDERKEY O_DELIVERYDATE 1 2023-09-20 2 2023-10-25 (Future Date) 3 2023-10-10 Payload example <pre><code>{\n    \"description\": \"Ensure that the delivery dates (O_DELIVERYDATE) in the ORDERS table are not set in the future\",\n    \"coverage\": 1,\n    \"properties\": null,\n    \"tags\": [],\n    \"fields\": [\"O_DELIVERYDATE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"notFuture\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>O_ORDERKEY</code> 2 does not satisfy the rule because its <code>O_DELIVERYDATE</code> is set in the future.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_DELIVERYDATE]\nB --&gt; C{Is O_DELIVERYDATE &lt;= Current Date?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    o_orderkey,\n    o_deliverydate\nfrom orders \nwhere\n    o_deliverydate &gt; current_date;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The value for <code>O_DELIVERYDATE</code> of <code>2023-10-25</code> is in the future.</p> <p>Shape Anomaly</p> <p>In <code>O_DELIVERYDATE</code>, 33.333% of 3 filtered records (1) are future times.</p>"},{"location":"checks/not-negative-check/","title":"Not Negative","text":""},{"location":"checks/not-negative-check/#definition","title":"Definition","text":"<p>Asserts that this is a non-negative number.</p>"},{"location":"checks/not-negative-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Fields</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/not-negative-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/not-negative-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/not-negative-check/#example","title":"Example","text":"<p>Objective: Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table is a non-negative number.</p> <p>Sample Data</p> L_ORDERKEY L_LINENUMBER L_QUANTITY 1 1 40 2 2 -5 3 1 20 Payload example <pre><code>{\n    \"description\": \"Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table is a non-negative number\",\n    \"coverage\": 1,\n    \"properties\": null,\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"notNegative\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>L_ORDERKEY</code> 2 does not satisfy the rule because its <code>L_QUANTITY</code> value is a negative number.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_QUANTITY]\nB --&gt; C{Is L_QUANTITY &gt;= 0?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_linenumber,\n    l_quantity\nfrom lineitem \nwhere\n    l_quantity &lt; 0;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The value for <code>L_QUANTITY</code> of <code>-5</code> is a negative number.</p> <p>Shape Anomaly</p> <p>In <code>L_QUANTITY</code>, 33.333% of 3 filtered records (1) are negative numbers.</p>"},{"location":"checks/not-null-check/","title":"Not Null","text":""},{"location":"checks/not-null-check/#definition","title":"Definition","text":"<p>Asserts that none of the selected fields' values are explicitly set to nothing.</p>"},{"location":"checks/not-null-check/#field-scope","title":"Field Scope","text":"<p>Multi: The rule evaluates multiple specified fields.</p> <p>Accepted Fields</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/not-null-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/not-null-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/not-null-check/#example","title":"Example","text":"<p>Objective: Ensure that every record in the CUSTOMER table has an assigned value for the C_NAME and C_ADDRESS fields.</p> <p>Sample Data</p> C_CUSTKEY C_NAME C_ADDRESS 1 Alice 123 Oak St 2 Bob NULL 3 Charlie 789 Maple Ave 4 NULL 456 Pine Rd Payload example <pre><code>{\n    \"description\": \"Ensure that every record in the CUSTOMER table has an assigned value for the C_NAME and C_ADDRESS fields\",\n    \"coverage\": 1,\n    \"properties\": null,\n    \"tags\": [],\n    \"fields\": [\"C_ADDRESS\",\"C_NAME\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"notNull\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>C_CUSTKEY</code> 2 and 4 do not satisfy the rule because they have <code>NULL</code> values in the <code>C_NAME</code> or <code>C_ADDRESS</code> fields.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve C_NAME and C_ADDRESS]\nB --&gt; C{Are C_NAME and C_ADDRESS non-null?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    c_custkey,\n    c_name,\n    c_address\nfrom customer \nwhere\n    c_name is null or c_address is null;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>There is no assigned value for <code>C_NAME</code>.</p> <p>Shape Anomaly</p> <p>In <code>C_NAME</code> and <code>C_ADDRESS</code>, 50.000% of 4 filtered records (2) are not assigned values.</p>"},{"location":"checks/overview-of-a-check/","title":"Checks Overview","text":"<p>Checks in Qualytics are rules applied to data that ensure quality by validating accuracy, consistency, and integrity. Each check includes a data quality rule, along with filters, tags, tolerances, and notifications, allowing efficient management of data across tables and fields.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"checks/overview-of-a-check/#checks-types","title":"Checks Types","text":"<p>In Qualytics, you will come across two types of checks:</p>"},{"location":"checks/overview-of-a-check/#inferred-checks","title":"Inferred Checks","text":"<p>Qualytics automatically generates inferred checks during a Profile operation. These checks typically cover 80-90% of the rules needed by users. They are created and maintained through profiling, which involves statistical analysis and machine learning methods.</p> <p>For more details on Inferred Checks, please refer to the Inferred Check documentation.</p>"},{"location":"checks/overview-of-a-check/#authored-checks","title":"Authored Checks","text":"<p>Authored checks are manually created by users within the Qualytics platform or API. You can author many types of checks, ranging from simple templates for common checks to complex rules using Spark SQL and User-Defined Functions (UDF) in Scala.</p> <p>For more details on Authored Checks, please refer to the Authored Checks documentation.</p>"},{"location":"checks/overview-of-a-check/#view-manage-checks","title":"View &amp; Manage Checks","text":"<p>Checks tab in Qualytics provides users with an interface to view and manage various checks associated with their data. These checks are accessible through two different methods, as discussed below.</p>"},{"location":"checks/overview-of-a-check/#method-1-datastore-specific-checks","title":"Method 1: Datastore-Specific Checks","text":"<p>Step 1: Log in to your Qualytics account and select the datastore from the left menu.</p> <p></p> <p>Step 2: Click the \"Checks\" from the navigation tab.</p> <p></p> <p>You will see a list of all the checks that have been applied to the selected datastore.</p> <p></p> <p>You can switch between different types of checks to view them categorically (such as All, Active, Draft, and Archived).</p> <p></p>"},{"location":"checks/overview-of-a-check/#method-2-explore-section","title":"Method 2: Explore Section","text":"<p>Step 1: Log in to your Qualytics account and click the Explore button on the left side panel of the interface.</p> <p></p> <p>Step 2: Click the \"Checks\" from the navigation tab.</p> <p></p> <p>You'll see a list of all the checks that have been applied to various tables and fields across different source datastores.</p> <p></p>"},{"location":"checks/overview-of-a-check/#check-templates","title":"Check Templates","text":"<p>Check Templates empower users to efficiently create, manage, and apply standardized checks across various datastores, acting as blueprints that ensure consistency and data integrity across different datasets and processes.</p> <p>Check templates streamline the validation process by enabling check management independently of specific data assets such as datastores, containers, or fields. These templates reduce manual intervention, minimize errors, and provide a reusable framework that can be applied across multiple datasets, ensuring all relevant data adheres to defined criteria. This not only saves time but also enhances the reliability of data quality checks within an organization.</p> <p>For more details about check templates, please refer to the Check Templates documentation.</p>"},{"location":"checks/overview-of-a-check/#apply-check-template-for-quality-checks","title":"Apply Check Template for Quality Checks","text":"<p>You can export check templates to make quality checks easier and more consistent. Using a set template lets you quickly verify that your data meets specific standards, reducing mistakes and improving data quality. Exporting these templates simplifies the process, making finding and fixing errors more efficient, and ensuring your quality checks are applied across different projects or systems without starting from scratch.</p> <p>For more details on how to apply check templates for quality checks, please refer to the Apply Check Template for Quality Checks documentation.</p>"},{"location":"checks/overview-of-a-check/#export-check-templates","title":"Export Check Templates","text":"<p>You can export check templates to easily share or reuse your quality check settings across different systems or projects. This saves time by eliminating the need to recreate the same checks repeatedly and ensures that your quality standards are consistently applied. Exporting templates helps maintain accuracy and efficiency in managing data quality across various environments.</p> <p>For more details about export check templates, please refer to the Export Check Templates documentation.</p>"},{"location":"checks/overview-of-a-check/#manage-checks-in-datastore","title":"Manage Checks in Datastore","text":"<p>Managing your checks within a datastore is important to maintain data integrity and ensure quality. You can categorize, create, update, archive, restore, delete, and clone checks, making it easier to apply validation rules across the datastores. The system allows for checks to be set as active, draft, or archived based on their current state of use. You can also define reusable templates for quality checks to streamline the creation of multiple checks with similar criteria. With options for important and favorite, users have full flexibility to manage data quality efficiently.</p> <p>For more details on how to manage checks in datastores, please refer to the Manage Checks in Datastore documentation.</p>"},{"location":"checks/overview-of-a-check/#check-rule-types","title":"Check Rule Types","text":"<p>In Qualytics, a variety of check rule types are provided to maintain data quality and integrity. These rules define specific criteria that data must meet, and checks apply these rules during the validation process.</p> <p>For more details about check rule types, please refer to the Rule Types Overview documentation.</p> Rule Type Description After Date Time Asserts that the field is a timestamp later than a specific date and time. Aggregation Comparison Verifies that the specified comparison operator evaluates true when applied to two aggregation expressions. Any Not Null Asserts that one of the fields must not be null. Before Date Time Asserts that the field is a timestamp earlier than a specific date and time. Between Asserts that values are equal to or between two numbers. Between Times Asserts that values are equal to or between two dates or times. Contains Credit Card Asserts that the values contain a credit card number. Contains Email Asserts that the values contain email addresses. Contains Social Security Number Asserts that the values contain social security numbers. Contains Url Asserts that the values contain valid URLs. Data Diff Asserts that the dataset created by the targeted field(s) has differences compared to the referred field(s). Distinct Count Asserts on the approximate count distinct of the given column. Entity Resolution Asserts that every distinct entity is appropriately represented once and only once. Equal To Asserts that all of the selected fields equal a value. Equal To Field Asserts that this field is equal to another field. Exists in Asserts if the rows of a compared table/field of a specific Datastore exists in the selected table/field. Expected Schema Asserts that all selected fields are present and that all declared data types match expectations. Expected Values Asserts that values are contained within a list of expected values. Field Count Asserts that there must be exactly a specified number of fields. Freshness Check Asserts that data was added or updated in the data asset after a declared time. Greater Than Asserts that the field is a number greater than (or equal to) a value. Greater Than Field Asserts that this field is greater than another field. Is Address Asserts that the values contain the specified required elements of an address. Is Credit Card Asserts that the values are credit card numbers. Is Replica Of (is sunsetting) Asserts that the dataset created by the targeted field(s) is replicated by the referred field(s). Is Type Asserts that the data is of a specific type. Less Than Asserts that the field is a number less than (or equal to) a value. Less Than Field Asserts that this field is less than another field. Matches Pattern Asserts that a field must match a pattern. Max Length Asserts that a string has a maximum length. Max Partition Size Asserts the maximum number of records that should be loaded from each file or table partition. Max Value Asserts that a field has a maximum value. Metric Records the value of the selected field during each scan operation and asserts that the value is within a specified range (inclusive). Min Length Asserts that a string has a minimum length. Min Partition Size Asserts the minimum number of records that should be loaded from each file or table partition. Min Value Asserts that a field has a minimum value. Not Exists In Asserts that values assigned to this field do not exist as values in another field. Not Future Asserts that the field's value is not in the future. Not Negative Asserts that this is a non-negative number. Not Null Asserts that the field's value is not explicitly set to nothing. Positive Asserts that this is a positive number. Predicted By Asserts that the actual value of a field falls within an expected predicted range. Required Values Asserts that all of the defined values must be present at least once within a field. Satisfies Expression Evaluates the given expression (any valid <code>Spark SQL</code>) for each record. Sum Asserts that the sum of a field is a specific amount. Time Distribution Size Asserts that the count of records for each interval of a timestamp is between two numbers. Unique Asserts that the field's value is unique. Volumetric Asserts that the data volume (rows or bytes) remains within dynamically inferred thresholds based on historical trends (daily, weekly, monthly)."},{"location":"checks/positive-check/","title":"Positive","text":""},{"location":"checks/positive-check/#definition","title":"Definition","text":"<p>Asserts that this is a positive number.</p>"},{"location":"checks/positive-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Fields</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/positive-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/positive-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/positive-check/#example","title":"Example","text":"<p>Objective: Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table is a positive number.</p> <p>Sample Data</p> L_ORDERKEY L_LINENUMBER L_QUANTITY 1 1 40 2 1 0 3 1 -5 4 1 20 Payload example <pre><code>{\n    \"description\": \"Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table is a positive number\",\n    \"coverage\": 1,\n    \"properties\": null,\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"positive\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 2 and 3 do not satisfy the rule because their <code>L_QUANTITY</code> values are not positive numbers.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_QUANTITY]\nB --&gt; C{Is L_QUANTITY Positive?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_linenumber,\n    l_quantity\nfrom lineitem \nwhere\n    l_quantity &lt;= 0;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The value for <code>L_QUANTITY</code> of <code>-5</code> is not a positive number.</p> <p>Shape Anomaly</p> <p>In <code>L_QUANTITY</code>, 50.000% of 4 filtered records (2) are not positive numbers.</p>"},{"location":"checks/predicted-by-check/","title":"Predicted By","text":""},{"location":"checks/predicted-by-check/#definition","title":"Definition","text":"<p>Asserts that the actual value of a field falls within an expected predicted range.</p>"},{"location":"checks/predicted-by-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>Predicted By</code> rule is used to verify whether the actual values of a specific field align with a set of expected values that are derived from a prediction expression. This expression could be a mathematical formula, statistical calculation, or any other valid predictive logic.</p>"},{"location":"checks/predicted-by-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Fields</p> Type <code>Integral</code> <code>Fractional</code> <code>Date</code> <code>Timestamp</code>"},{"location":"checks/predicted-by-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/predicted-by-check/#specific-properties","title":"Specific Properties","text":"<p>Determines if the actual value of a field falls within an expected predicted range.</p> Name Description Expression The prediction expression or formula for the field. Tolerance The allowed deviation from the predicted value. Note <p>The tolerance level must be defined to allow a permissible range of deviation from the predicted values.</p> <p>Here\u2019s a simple breakdown:</p> <ul> <li>An expression predicts what the value of a field should be.</li> <li>A tolerance value specifies how much deviation from the predicted value is acceptable.</li> <li>The actual value is then compared against the range defined by the predicted value \u00b1 tolerance.</li> </ul>"},{"location":"checks/predicted-by-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/predicted-by-check/#example","title":"Example","text":"<p>Objective: Ensure that the discount (L_DISCOUNT) in the LINEITEM table is calculated correctly based on the actual price (L_EXTENDEDPRICE). A correct discount should be approximately 8% less than the actual price, within a tolerance of \u00b12.</p> <p>Sample Data</p> L_ORDERKEY L_LINENUMBER L_EXTENDEDPRICE L_DISCOUNT 1 1 100 8 2 1 100 12 3 1 100 9 Inputs <ul> <li>Expression: L_EXTENDEDPRICE \u00d7 0.08</li> <li>Tolerance: 2</li> </ul> Payload example <pre><code>{\n    \"description\": \"Ensure that the discount (L_DISCOUNT) in the LINEITEM table is calculated correctly based on the actual price (L_EXTENDEDPRICE). A correct discount should be approximately 8% less than the actual price, within a tolerance of \u00b12\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"expression\": \"L_EXTENDEDPRICE \u00d7 0.08\",\n        \"tolerance\": 2\n    },\n    \"tags\": [],\n    \"fields\": [\"L_DISCOUNT\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"predictedBy\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>For the entry with <code>L_ORDERKEY</code> 2, the discount is 12, which is outside of the computed range. Based on an 8% expected discount with a tolerance of \u00b12, the discount should be between 6 and 10 (calculated from the actual price of 100). Therefore, this record is marked as anomalous.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_EXTENDEDPRICE and L_DISCOUNT]\nB --&gt; C{Is Discount within Predicted Range?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_linenumber,\n    l_extendedprice,\n    l_discount\nfrom lineitem \nwhere\n    l_discount not between l_extendedprice * 0.06 and l_extendedprice * 0.10;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>L_DISCOUNT</code> value of '12' is not within the predicted range defined by L_EXTENDEDPRICE * 0.08 +/- 2.0</p> <p>Shape Anomaly</p> <p>In <code>L_DISCOUNT</code>, 33.333% of 3 filtered records (1) are not within the predicted range defined by L_EXTENDEDPRICE * 0.08 +/- 2.0</p>"},{"location":"checks/required-values-check/","title":"Required Values","text":""},{"location":"checks/required-values-check/#definition","title":"Definition","text":"<p>Asserts that all of the defined values must be present at least once within a field.</p>"},{"location":"checks/required-values-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/required-values-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/required-values-check/#specific-properties","title":"Specific Properties","text":"<p>Ensures that a specific set of values is present within a field.</p> Name Description Values Specifies the list of values that must exist in the field."},{"location":"checks/required-values-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/required-values-check/#example","title":"Example","text":"<p>Objective: Ensure that orders have priorities labeled as '1-URGENT', '2-HIGH', '3-MEDIUM', '4-LOW', and '5-NOT URGENT'.</p> <p>Sample Data</p> O_ORDERKEY O_ORDERPRIORITY 1 1-URGENT 2 2-HIGH 3 3-MEDIUM 4 3-MEDIUM Payload example <pre><code>{\n    \"description\": \"Ensure that orders have priorities labeled as '1-URGENT', '2-HIGH', '3-MEDIUM', '4-LOW', and '5-NOT URGENT'\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"list\":[\"1-URGENT\",\"2-HIGH\",\"3-MEDIUM\",\"4-LOW\",\"5-NOT URGENT\"]\n    },\n    \"tags\": [],\n    \"fields\": [\"O_ORDERPRIORITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"requiredValues\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the rule is violated because the values '4-LOW' and '5-NOT URGENT' are not present in the <code>O_ORDERPRIORITY</code> field of the ORDERS table.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B{Check if all specified values exist in the field}\nB --&gt;|Yes| C[End: No Anomalies]\nB --&gt;|No| D[Mark as Anomalous: Missing Values]\nD --&gt; C</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect distinct\n    o_orderpriority\nfrom orders\nwhere o_orderpriority in ('1-URGENT', '2-HIGH', '3-MEDIUM', '4-LOW', '5-NOT URGENT');\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>In <code>O_ORDERPRIORITY</code>, required values are missing in 40.000% of filtered records.</p>"},{"location":"checks/rule-types-overview/","title":"Rule Types Overview","text":"<p>In Qualytics, a variety of rule types are provided to maintain data quality and integrity.These rules define specific criteria that data must meet, and checks apply these rules during the validation process.</p> <p>Here\u2019s an overview of the rule types and their purposes:</p>"},{"location":"checks/rule-types-overview/#check-rule-types","title":"Check Rule Types","text":"Rule Type Description After Date Time Asserts that the field is a timestamp later than a specific date and time. Any Not Null Asserts that one of the fields must not be null. Before DateTime Asserts that the field is a timestamp earlier than a specific date and time. Between Asserts that values are equal to or between two numbers. Between Times Asserts that values are equal to or between two dates or times. Contains Credit Card Asserts that the values contain a credit card number. Contains Email Asserts that the values contain email addresses. Contains Social Security Number Asserts that the values contain social security numbers. Contains Url Asserts that the values contain valid URLs. Data Diff Asserts that the dataset created by the targeted field(s) has differences compared to the referred field(s). Distinct Count Asserts on the approximate count distinct of the given column. Entity Resolution Asserts that every distinct entity is appropriately represented once and only once Equal To Field Asserts that this field is equal to another field. Exists in Asserts if the rows of a compared table/field of a specific Datastore exists in the selected table/field. Expected Schema Asserts that all selected fields are present and that all declared data types match expectations. Expected Values Asserts that values are contained within a list of expected values. Field Count Asserts that there must be exactly a specified number of fields. Greater Than Asserts that the field is a number greater than (or equal to) a value. Greater Than Field Asserts that this field is greater than another field. Is Address Asserts that the values contain the specified required elements of an address. Is Credit Card Asserts that the values are credit card numbers. Is Replica Of (is sunsetting) Asserts that the dataset created by the targeted field(s) is replicated by the referred field(s). Is Type Asserts that the data is of a specific type. Less Than Asserts that the field is a number less than (or equal to) a value. Less Than Field Asserts that this field is less than another field. Matches Pattern Asserts that a field must match a pattern. Max Length Asserts that a string has a maximum length. Max Value Asserts that a field has a maximum value. Metric Records the value of the selected field during each scan operation and asserts that the value is within a specified range (inclusive). Min Length Asserts that a string has a minimum length. Min Partition Size Asserts the minimum number of records that should be loaded from each file or table partition. Min Value Asserts that a field has a minimum value. Not Exists In Asserts that values assigned to this field do not exist as values in another field. Not Future Asserts that the field's value is not in the future. Not Negative Asserts that this is a non-negative number. Not Null Asserts that the field's value is not explicitly set to nothing. Positive Asserts that this is a positive number. Predicted By Asserts that the actual value of a field falls within an expected predicted range. Required Values Asserts that all of the defined values must be present at least once within a field. Satisfies Expression Evaluates the given expression (any valid <code>Spark SQL</code>) for each record. Sum Asserts that the sum of a field is a specific amount. Time Distribution Size Asserts that the count of records for each interval of a timestamp is between two numbers. Unique Asserts that the field's value is unique. Volumetric Check Asserts that the volume of the data asset has not changed by more than an inclusive percentage amount for the prescribed moving daily average."},{"location":"checks/satisfies-expression-check/","title":"Satisfies Expression","text":""},{"location":"checks/satisfies-expression-check/#definition","title":"Definition","text":"<p>Evaluates the given expression (any valid Spark SQL) for each record.</p>"},{"location":"checks/satisfies-expression-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>Satisfies Expression</code> rule allows for a wide range of custom validations on the dataset. By defining a Spark SQL expression, you can create customized conditions that the data should meet.</p> <p>This rule will evaluate an expression against each record, marking those that do not satisfy the condition as anomalies. It provides the flexibility to create complex validation logic without being restricted to predefined rule structures.</p>"},{"location":"checks/satisfies-expression-check/#field-scope","title":"Field Scope","text":"<p>Calculated: The rule automatically identifies the fields involved, without requiring explicit field selection.</p>"},{"location":"checks/satisfies-expression-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/satisfies-expression-check/#specific-properties","title":"Specific Properties","text":"<p>Evaluates each record against a specified Spark SQL expression to ensure it meets custom validation conditions.</p> Name Description Expression Defines the Spark SQL expression that each record should meet. <p>Info</p> <p>Refers to the Filter Guide in the General Properties topic for examples of valid Spark SQL expressions.</p>"},{"location":"checks/satisfies-expression-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/satisfies-expression-check/#example-1-satisfies-expression-using-a-case-statement","title":"Example 1: Satisfies Expression Using a <code>CASE</code> Statement","text":"<p>Let's assume you want to ensure that for orders with a priority of '1-URGENT' or '2-HIGH', the <code>orderstatus</code> must be 'O' (for open), and for orders with a priority of '3-MEDIUM', the <code>orderstatus</code> must be either 'O' or 'P' (for pending).</p> <pre><code> CASE\n    WHEN o_orderpriority IN ('1-URGENT', '2-HIGH') AND o_orderstatus != 'O' THEN FALSE\n    WHEN o_orderpriority = '3-MEDIUM' AND o_orderstatus NOT IN ('O', 'P') THEN FALSE\n    ELSE TRUE\n END\n</code></pre>"},{"location":"checks/satisfies-expression-check/#example-2-satisfies-expression-using-a-relatively-complex-cte-statement","title":"Example 2: Satisfies Expression Using a <code>Relatively Complex CTE</code> Statement","text":"<p>Objective:: To ensure that the overall effect of discounts on item prices remains within acceptable limits, we validate whether the average discounted price of all items is greater than the maximum discount applied to any single item.</p> <p>Background:</p> <p>In pricing analysis, it\u2019s important to monitor how discounts affect the final prices of products. By comparing the average price after discounts with the maximum discount applied, we can assess whether the discounts are having an overly significant impact or if they are within a reasonable range.</p> <pre><code>CASE \n        WHEN (SELECT AVG(l_extendedprice * (1 - l_discount)) FROM lineitem) &gt; \n             (SELECT MAX(l_discount) FROM {{ _qualytics_self }}) \n        THEN TRUE \n        ELSE FALSE \nEND AS is_discount_within_limits\n</code></pre>"},{"location":"checks/satisfies-expression-check/#use-case","title":"Use Case","text":"<p>Objective: Ensure that the total tax applied to each item in the LINEITEM table is not more than 10% of the extended price.</p> <p>Sample Data</p> L_ORDERKEY L_LINENUMBER L_EXTENDEDPRICE L_TAX 1 1 10000 900 2 1 15000 2000 3 1 20000 1800 4 1 10000 1500 Inputs <ul> <li>Expression: L_TAX &lt;= L_EXTENDEDPRICE * 0.10</li> </ul> Payload example <pre><code>{\n    \"description\": \"Ensure that the total tax applied to each item in the LINEITEM table is not more than 10% of the extended price\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"expression\":\"L_TAX &lt;= L_EXTENDEDPRICE * 0.10\"\n        },\n    \"tags\": [],\n    \"fields\": [\"L_TAX\", \"L_EXTENDEDPRICE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"satisfiesExpression\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 2 and 4 do not satisfy the rule because the <code>L_TAX</code> values are more than 10% of their respective <code>L_EXTENDEDPRICE</code> values.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_EXTENDEDPRICE and L_TAX]\nB --&gt; C{Is L_TAX &lt;= L_EXTENDEDPRICE * 0.10?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_linenumber,\n    l_extendedprice,\n    l_tax\nfrom\n    lineitem \nwhere\n    l_tax &gt; l_extendedprice * 0.10;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The record does not satisfy the expression: <code>L_TAX &lt;= L_EXTENDEDPRICE * 0.10</code></p> <p>Shape Anomaly</p> <p>50.000% of 4 filtered records (2) do not satisfy the expression: <code>L_TAX &lt;= L_EXTENDEDPRICE * 0.10</code></p>"},{"location":"checks/sum-check/","title":"Sum","text":""},{"location":"checks/sum-check/#definition","title":"Definition","text":"<p>Asserts that the sum of a field is a specific amount.</p>"},{"location":"checks/sum-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/sum-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/sum-check/#specific-properties","title":"Specific Properties","text":"<p>Ensures that the total sum of values in a specified field matches a defined amount.</p> Name Description Sum Specifies the expected sum of the values in the field."},{"location":"checks/sum-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/sum-check/#example","title":"Example","text":"<p>Objective: Ensure that the total discount value in the LINEITEM table does not exceed $2000.</p> <p>Sample Data</p> L_ORDERKEY L_LINENUMBER L_EXTENDEDPRICE L_DISCOUNT L_DISCOUNT_VALUE 1 1 10000 0.05 500 2 1 8000 0.10 800 3 1 7000 0.05 350 4 1 5000 0.10 500 Payload example <pre><code>{\n    \"description\": \"Ensure that the total discount value in the LINEITEM table does not exceed $2000\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\": \"2000\"\n    },\n    \"tags\": [],\n    \"fields\": [\"L_DISCOUNT_VALUE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"sum\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the total of the <code>L_DISCOUNT_VALUE</code> column is (500 + 800 + 350 + 500 = 2150), which exceeds the specified maximum total discount value of $2000.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_DISCOUNT_VALUE]\nB --&gt; C{Sum of L_DISCOUNT_VALUE &lt;= 2000?}\nC --&gt;|Yes| D[End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect \n    sum(l_discount_value) as total_discount_value\nfrom \n    lineitem \nhaving \n    sum(l_discount_value) &gt; 2000;\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>In <code>L_DISCOUNT_VALUE</code>, the sum of the 4 records is not 2000.000</p>"},{"location":"checks/time-distribution-size-check/","title":"Time Distribution Size","text":""},{"location":"checks/time-distribution-size-check/#definition","title":"Definition","text":"<p>Asserts that the count of records for each interval of a timestamp is between two numbers.</p>"},{"location":"checks/time-distribution-size-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>Time Distribution Size</code> rule helps in identifying irregularities in the distribution of records over time intervals such as hours, days, or months.</p> <p>For instance, in a retail context, it could ensure that there\u2019s a consistent number of orders each month to meet business targets. A sudden drop in orders might highlight operational issues or shifts in market demand that require immediate attention.</p>"},{"location":"checks/time-distribution-size-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Timestamp</code> <code>Date</code>"},{"location":"checks/time-distribution-size-check/#specific-properties","title":"Specific Properties","text":"Name Description Interval Defines the time interval for segmentation. Min Count Specifies the minimum count of records in each segment. Max Count Specifies the maximum count of records in each segment."},{"location":"checks/time-distribution-size-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/time-distribution-size-check/#example","title":"Example","text":"<p>Objective: Ensure that the number of orders for each month is consistently between 5 and 10.</p> <p>Sample Data</p> O_ORDERKEY O_ORDERDATE 1 2023-01-01 2 2023-01-15 3 2023-01-20 4 2023-01-25 5 2023-02-01 6 2023-02-05 7 2023-02-10 8 2023-02-15 9 2023-02-20 10 2023-02-25 11 2023-02-28 Payload example <pre><code>{\n    \"description\": \"Ensure that the number of orders for each month is consistently between 5 and 10\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"interval_name\": \"Monthly\",\n        \"min_size\": 5,\n        \"max_size\": 10\n    },\n    \"tags\": [],\n    \"fields\": [\"O_ORDERDATE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"timeDistributionSize\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the January segment fails the rule because there are only 4 orders, which is below the specified minimum count of 5.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_ORDERDATE]\nB --&gt; C{Segment data by month}\nC --&gt; D{Is count of records in each segment between 5 and 10?}\nD --&gt;|Yes| E[End]\nD --&gt;|No| F[Mark as Anomalous]\nF --&gt; E</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect \n    extract(month from o_orderdate) as month,\n    count(*) as order_count\nfrom \n    orders \ngroup by \n    extract(month from o_orderdate)\nhaving \n    count(*) &lt; 5\n    or count(*) &gt; 10;\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>50.000% of the monthly segments of <code>O_ORDERDATE</code> have record counts not between 5 and 10.</p>"},{"location":"checks/unique-check/","title":"Unique","text":""},{"location":"checks/unique-check/#definition","title":"Definition","text":"<p>Asserts that every value held by a field appears only once. If multiple fields are specified, then every combination of values of the fields should appear only once.</p>"},{"location":"checks/unique-check/#field-scope","title":"Field Scope","text":"<p>Multi: The rule evaluates multiple specified fields.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/unique-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.         <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre></p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage Incorrect usage <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage Incorrect usage <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage Incorrect usage <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage Incorrect usage <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/unique-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/unique-check/#example","title":"Example","text":"<p>Objective: Ensure that each combination of C_NAME and C_ADDRESS in the CUSTOMER table is unique.</p> <p>Sample Data</p> C_CUSTKEY C_NAME C_ADDRESS 1 Customer_A 123 Main St 2 Customer_B 456 Oak Ave 3 Customer_A 123 Main St 4 Customer_C 789 Elm St Payload example <pre><code>{\n    \"description\": \"Ensure that each combination of C_NAME and C_ADDRESS in the CUSTOMER table is unique\",\n    \"coverage\": 1,\n    \"properties\": null,\n    \"tags\": [],\n    \"fields\": [\"C_NAME\", \"C_ADDRESS\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"unique\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>C_CUSTKEY</code> 1 and 3 have the same <code>C_NAME</code> and <code>C_ADDRESS</code>, which violates the rule because this combination of keys should be unique.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve C_NAME and C_ADDRESS]\nB --&gt; C{Is the combination unique?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query to find non-unique C_NAME and C_ADDRESS combinations.\nselect\n    c_custkey,\n    c_name,\n    c_address\nfrom customer \ngroup by c_name, c_address\nhaving count(*) &gt; 1;\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>In <code>C_NAME</code> and <code>C_ADDRESS</code>, 25.000% of 4 filtered records (1) are not unique.</p>"},{"location":"checks/volumetric-check/","title":"Volumetric Check","text":"<p>Volumetric Check ensures data stability by monitoring dataset size fluctuations in rows or bytes. It detects anomalies by comparing current volumes against historical trends (daily, weekly, monthly). Users can configure rules for precise control, while automated threshold adjustments enhance accuracy over time.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"checks/volumetric-check/#configure-volumetric-check","title":"Configure Volumetric Check","text":"<p>Step 1: Login into your Qualytics account and select the datastore from the left menu on which you want to add a volumetric check.</p> <p></p> <p>Step 2: Click the Add button and select Checks.</p> <p></p> <p>Step 3: A modal window appears. Enter the required details to configure the Volumetric Check.</p> <p></p> <p>Step 4: Enter the details to configure the volumetric check:</p> No. Field Description 1. Rule Type Select the Volumetric Rule type from the dropdown. 2. Table Select the table for the rule to apply. <p></p> <p>3. Comparison:  Specifies the type of comparison: Absolute Change, Absolute Value, or Percentage Change:</p> <p></p> <p>Details</p> <p></p> <p>Details</p> <p></p> <p>Details</p> <p></p> <p>4. Measurement Period Days: Enter the number of days for measurement.</p> <p></p> <p>5. Threshold: At least the Min or Max value must be specified, and including both is optional. These values determine the acceptable range or limit of change in the field's value.</p> <p></p> <p>Min Value</p> <ul> <li> <p>Represents the minimum allowable increase in the field's value.</p> </li> <li> <p>A negative Min Value signifies an allowable decrease, determining the minimum value the field can drop to be considered valid.</p> </li> </ul> <p></p> <p>Max Value</p> <ul> <li>Indicates the maximum allowable increase in the field\u2019s value, setting an upper limit for the value's acceptable growth or change.</li> </ul> <p></p> No. Field Description 6. Description Enter a description for the check. 7. Tag Add tags for categorizing the check. 8. Additional Metadata Add custom metadata for additional details. <p></p> <p>Step 4: After completing all the check details, click on the \"Validate\" button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct. It ensures that the check will work as expected by running it against the data without committing any changes.</p> <p></p> <p>If the validation is successful, a green message will appear saying \"Validation Successful\".</p> <p></p> <p>Step 5: Once you have a successful validation, click the \"Save\" button.</p> <p></p> <p>After clicking on the \u201cSave\u201d button your check is successfully created and a success flash message will appear saying \u201cCheck successfully created\u201d.</p> <p></p>"},{"location":"checks/volumetric-check/#comparison-options","title":"Comparison Options","text":"<p>Absolute Value</p> <p>The <code>Absolute Value</code> comparison works by comparing the change in a numeric field's value to a pre-set limit <code>between</code> Min and Max values. If the field's value changes by more than this specified range since the last relevant scan, an anomaly is identified.</p> Illustration <p>The value of the record in each scan should be within 100 and 300 to be considered normal</p> <p>Thresholds: Min Value = 100, Max Value = 300</p> Scan Current Value Anomaly Detected #1 150 No #2 90 Yes #3 250 No #4 310 Yes"},{"location":"checks/volumetric-check/#comparison-options_1","title":"Comparison Options","text":"<p>Absolute Change</p> <p>The <code>Absolute Change</code> comparison works by comparing the change in a numeric field's value to a pre-set limit (Min / Max). If the field's value changes by more than this specified limit since the last relevant scan, an anomaly is identified.</p> Illustration <p>Any record with a value change smaller than 30 or greater than 70 compared to the last scan should be flagged as anomalous</p> <p>Thresholds: Min Change = 30, Max Change = 70</p> Scan Previous Value Current Value Absolute Change Anomaly Detected #1 - 100 - No #2 100 150 50 No #3 150 220 70 No #4 220 300 80 Yes"},{"location":"checks/volumetric-check/#comparison-options_2","title":"Comparison Options","text":"<p>Percentage Change</p> <p>The <code>Percentage Change</code> comparison operates by tracking changes in a numeric field's value relative to its previous value. If the change exceeds the predefined percentage (%) limit since the last relevant scan, an anomaly is generated.</p> Illustration <p>An anomaly is identified if the record's value decreases by more than 20% or increases by more than 50% compared to the last scan.</p> <p>Thresholds: Min Percentage Change = -20%, Max Percentage Change = 50%</p> <p>Percentage Change Formula: ( (current_value - previous_value) / previous_value ) * 100</p> Scan Previous Value Current Value Percentage Change Anomaly Detected 1 - 100 - No 2 100 150 50% No 3 150 120 -20% No 4 120 65 -45.83% Yes 5 65 110 69.23% Yes"},{"location":"checks/volumetric-check/#how-it-works","title":"How It Works","text":"<p>The system automatically infers and maintains volumetric checks based upon observed daily, weekly, and monthly averages. These checks enable proactive management of data volume trends, ensuring that any unexpected deviations are identified as anomalies for review.</p>"},{"location":"checks/volumetric-check/#automating-adaptive-volumetric-checks","title":"Automating Adaptive Volumetric Checks","text":"<p>The following Volumetric Checks are automatically inferred for data assets with automated volume measurements enabled:</p> <ul> <li> <p>Daily: the expected daily volume expressed as an absolute minimum and maximum threshold. The thresholds are calculated as standard deviations from the previous 7-day moving average.  </p> </li> <li> <p>Weekly: the expected weekly volume expressed as an absolute minimum and maximum threshold. The thresholds are calculated as standard deviations from the previous four weeks\u2019 weekly volume moving average.  </p> </li> <li> <p>Monthly: the expected 4-week volume expressed as an absolute minimum and maximum threshold. The thresholds are calculated as standard deviations from the previous sixteen weeks\u2019 4-week volume moving average.</p> </li> </ul>"},{"location":"checks/volumetric-check/#scan-assertion-and-anomaly-creation","title":"Scan Assertion and Anomaly Creation","text":"<p>Volumetric Checks are asserted during a Scan Operation just like all other check types and enrichment of volumetric check anomalies is fully supported. This enables full support for custom scheduling of volumetric checks and remediation workflows of volumetric anomalies.</p>"},{"location":"checks/volumetric-check/#adaptive-thresholds-and-manual-adjustments","title":"Adaptive Thresholds and Manual Adjustments","text":"<p>Each time data volume is measured for an asset, the system automatically updates the inferred Volumetric Checks.</p> <p>1.Automatic Threshold Adjustment:</p> <ul> <li> <p>The system sets initial thresholds at 2 standard deviations from the moving average.</p> </li> <li> <p>Over time, these thresholds adjust automatically using historical data trends to improve accuracy.</p> </li> </ul> <p>2.Continuous Learning:</p> <ul> <li>The system monitors past data and adapts thresholds to detect unusual data volume changes.</li> </ul> <p>3.Why It Matters:</p> <ul> <li> <p>Helps maintain data integrity by identifying unexpected volume changes.</p> </li> <li> <p>Ensures quick detection and response to potential data issues.</p> </li> </ul> <p></p>"},{"location":"cli/overview-of-qualytics-cli/","title":"Qualytics CLI","text":"<p>Qualytics CLI is a command-line tool designed to interact with the Qualytics API. With this tool, users can manage configurations, export and import checks, run operations and more.</p> <p>You can check more the latest version in Qualytics CLI</p>"},{"location":"cli/overview-of-qualytics-cli/#installation-and-upgrading","title":"Installation and Upgrading","text":"<p>You can install <code>Qualytics CLI</code> via pip: <pre><code>pip install qualytics-cli\n</code></pre></p> <p>You can upgrade the <code>Qualytics CLI</code> via pip:</p> <pre><code>pip install qualytics-cli --upgrade\n</code></pre>"},{"location":"cli/overview-of-qualytics-cli/#usage","title":"Usage","text":""},{"location":"cli/overview-of-qualytics-cli/#help","title":"Help","text":"<p>To view available commands and their usage:</p> <pre><code>qualytics --help\n</code></pre>"},{"location":"cli/overview-of-qualytics-cli/#initializing-configuration","title":"Initializing Configuration","text":"<p>To set up your Qualytics URL and token:</p> Bash ExamplePython Example <pre><code>    qualytics init \n        --url \"https://your-qualytics.qualytics.io/\" \n        --token \"YOUR_TOKEN_HERE\"\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n    CLI_TOKEN = \"&lt;your-token&gt;\"\n    AUDIENCE = \"https://your-qualytics.qualytics.io/\"\n\n    qualytics.init(AUDIENCE, CLI_TOKEN)\n</code></pre> <pre><code>    Configuration saved! \n</code></pre></p> <p>Options:</p> Option Type Description Default Required <code>--url</code> TEXT The URL of your Qualytics instance None Yes <code>--token</code> TEXT The personal access token for accessing Qualytics None Yes"},{"location":"cli/overview-of-qualytics-cli/#display-configuration","title":"Display Configuration","text":"<p>To view the currently saved configuration:</p> Bash ExamplePython Example <pre><code>qualytics show-config\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    qualytics.show_config()\n</code></pre> <pre><code>    Config file located in: /home/user/.qualytics/config.json\n    URL: (https://your-qualytics.qualytics.io/)\n    Token: &lt;your-token&gt;\n</code></pre></p>"},{"location":"cli/overview-of-qualytics-cli/#export-checks","title":"Export Checks","text":"<p>To export checks to a file:</p> Bash ExamplePython Example <pre><code>    qualytics checks export \n        --datastore DATASTORE_ID [--containers CONTAINER_IDS] \n        [--tags TAG_NAMES] \n        [--output LOCATION_TO_BE_EXPORTED]\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    DATASTORE_ID = 844\n    CONTAINERS = \"7504, 6657\"\n    qualytics.checks_export(\n        datastore=DATASTORE_ID, \n        containers=CONTAINERS, \n        tags=None, \n        output=\"/home/user/.qualytics/data_checks.json\"\n    )\n</code></pre> <pre><code>    Exporting quality checks... -------------------------------------- 100% 0:00:00\n    Total of Quality Checks = 27 \n    Total pages = 1 \n    Data exported to /home/user/.qualytics/data_checks.json\n</code></pre></p> <p>Options:</p> Option Type Description Default Required <code>--datastore</code> INTEGER Datastore ID None Yes <code>--containers</code> List of INTEGER Containers IDs None No <code>--tags</code> List of TEXT Tag names None No <code>--output</code> TEXT Output file path <code>$HOME/.qualytics/data_checks.json</code> No"},{"location":"cli/overview-of-qualytics-cli/#export-check-templates","title":"Export Check Templates","text":"<p>To export check templates:</p> Bash ExamplePython Example to enrichmentPython Example to local (output destination) <pre><code>qualytics checks export-templates \n    --enrichment_datastore_id 123 \n    [--check_templates \"1, 2, 3\" or \"[1,2,3]\"]\n    [--status `true` or `false`]\n    [--rules \"afterDateTime, aggregationComparison\" or \"[afterDateTime, aggregationComparison]\"]\n    [--tags \"tag1, tag2, tag3\" or \"[tag1, tag2, tag3]\"]\n    [--output \"/home/user/.qualytics/data_checks_template.json\"]\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    ENRICH_DATASTORE_ID = 597\n    CHECK_TEMPLATES = \"182716, 179514\"\n    qualytics.check_templates_export(\n        enrich_datastore_id=ENRICH_DATASTORE_ID,\n        check_templates=CHECK_TEMPLATES,\n        status=None,\n        rules=None,\n        tags=None\n    )\n</code></pre> <pre><code>    The check templates were exported to the table `_export_check_templates` to enrichment id: 597.\n</code></pre></p> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    ENRICH_DATASTORE_ID = 597\n    CHECK_TEMPLATES = \"182716, 179514\"\n    qualytics.check_templates_export(\n        enrich_datastore_id=None,\n        check_templates=CHECK_TEMPLATES,\n        status=None,\n        rules=None,\n        tags=None,\n        output=\"/home/user/.qualytics/data_checks_template.json\"\n    )\n</code></pre> <pre><code>    Exporting quality checks... -------------------------------------- 100% 0:00:01\n    Total of Check Templates = 123 \n    Total pages = 2 \n    Data exported to /home/user/.qualytics/data_checks_template.json\n</code></pre></p> <p>Options:</p> Option Type Description Default Required <code>--enrichment_datastore_id</code> INTEGER The ID of the enrichment datastore where check templates will be exported. Yes <code>--check_templates</code> TEXT IDs of specific check templates to export (comma-separated or array-like). No <code>--status</code> BOOL Check Template status send <code>true</code> if it's locked or <code>false</code> to unlocked. No No <code>--rules</code> TEXT Comma-separated list of check templates rule types or array-like format. Example: \"afterDateTime, aggregationComparison\" or \"[afterDateTime, aggregationComparison]\". No No <code>--tags</code> TEXT Comma-separated list of Tag names or array-like format. Example: \"tag1, tag2, tag3\" or \"[tag1, tag2, tag3]\". No No <code>--output</code> TEXT Output file path [example: <code>/home/user/.qualytics/data_checks_template.json</code>]. No No"},{"location":"cli/overview-of-qualytics-cli/#import-checks","title":"Import Checks","text":"<p>To import checks from a file:</p> Bash ExamplePython Example <pre><code>qualytics checks import \n    --datastore DATASTORE_ID_LIST \n    [--input LOCATION_FROM_THE_EXPORT]\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    TARGET_DATASTORE_ID = 1172\n    qualytics.checks_import(\n        datastore=TARGET_DATASTORE_ID, \n        input_file=\"/home/user/.qualytics/data_checks.json\"\n    )\n</code></pre> <pre><code>    Quality check id: 195646 for container: CUSTOMER created successfully\n    Quality check id: 195647 for container: CUSTOMER created successfully\n    Quality check id: 195648 for container: CUSTOMER created successfully\n    Quality check id: 195649 for container: CUSTOMER created successfully\n    Quality check id: 195650 for container: CUSTOMER created successfully\n    Quality check id: 195651 for container: CUSTOMER created successfully\n    Quality check id: 195652 for container: CUSTOMER created successfully\n    Quality check id: 195653 for container: CUSTOMER created successfully\n    Quality check id: 195654 for container: CUSTOMER created successfully\n</code></pre></p> <p>Options:</p> Option Type Description Default Required <code>--datastore</code> TEXT Datastore IDs to import checks into (comma-separated or array-like). None Yes <code>--input</code> TEXT Input file path HOME/.qualytics/data_checks.json No <p>Note: Errors during import will be logged in <code>$HOME/.qualytics/errors.log</code>.</p>"},{"location":"cli/overview-of-qualytics-cli/#run-a-catalog-operation-on-a-datastore","title":"Run a Catalog Operation on a Datastore","text":"<p>Allows you to trigger a catalog operation on any current datastore (datastore permission required by admin)</p> Bash ExamplePython Example <pre><code>    qualytics run catalog \n        --datastore \"DATSTORE_ID_LIST\" \n        --include \"INCLUDE_LIST\" \n        --prune \n        --recreate \n        --background\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    DATASTORE_ID = \"1172\"\n    qualytics.catalog_operation(\n        datastores=DATASTORE_ID, \n        include=None,\n        prune=None ,\n        recreate=None,\n        background=False\n    )\n</code></pre> <pre><code>    Started Catalog operation 29464 for datastore: 1172 \n    Waiting for operation to finish\n    Waiting for operation to finish\n    Waiting for operation to finish\n    Successfully Finished Catalog operation 29464for datastore: 1172 \n    Processing... ---------------------------------------- 100% 0:00:29\n</code></pre></p> <p>Options:</p> Option Type Description Required <code>--datastore</code> TEXT Comma-separated list of Datastore IDs or array-like format. Example: 1,2,3,4,5 or \"[1,2,3,4,5]\" Yes <code>--include</code> TEXT Comma-separated list of include types or array-like format. Example: \"table,view\" or \"[table,view]\" No <code>--prune</code> BOOL Prune the operation. Do not include if you want prune == false No <code>--recreate</code> BOOL Recreate the operation. Do not include if you want recreate == false No <code>--background</code> BOOL Starts the catalog but does not wait for the operation to finish No"},{"location":"cli/overview-of-qualytics-cli/#run-a-profile-operation-on-a-datastore","title":"Run a Profile Operation on a Datastore","text":"<p>Allows you to trigger a profile operation on any current datastore (datastore permission required by admin)</p> Bash ExamplePython Example <pre><code>qualytics run profile \n    --datastore \"DATSTORE_ID_LIST\" \n    --container_names \"CONTAINER_NAMES_LIST\" \n    --container_tags \"CONTAINER_TAGS_LIST\"\n    --infer_constraints \n    --max_records_analyzed_per_partition \"MAX_RECORDS_ANALYZED_PER_PARTITION\" \n    --max_count_testing_sample \"MAX_COUNT_TESTING_SAMPLE\"\n    --percent_testing_threshold \"PERCENT_TESTING_THRESHOLD\" \n    --high_correlation_threshold \"HIGH_CORRELATION_THRESHOLD\" \n    --greater_then_date \"GREATER_THAN_TIME\"\n    --greater_than_batch \"GREATER_THAN_BATCH\" \n    --histogram_max_distinct_values \"HISTOGRAM_MAX_DISTINCT_VALUES\" \n    --background\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    DATASTORE_ID = \"844\"\n    CONTAINER_NAMES = \"CUSTOMER, NATION\"\n    qualytics.profile_operation(\n        datastores=DATASTORE_ID,\n        container_names=CONTAINER_NAMES,\n        container_tags=None,\n        infer_constraints=True,\n        max_records_analyzed_per_partition=None,\n        max_count_testing_sample=None,\n        percent_testing_threshold=None,\n        high_correlation_threshold=None,\n        greater_than_time=None,\n        greater_than_batch=None,\n        histogram_max_distinct_values=None,\n        background=False\n    )\n</code></pre> <pre><code>    Successfully Started Profile 29466 for datastore: 844 \n    Waiting for operation to finish\n    Waiting for operation to finish\n    Waiting for operation to finish\n    Waiting for operation to finish\n    Waiting for operation to finish\n    Waiting for operation to finish\n    Successfully Finished Profile operation 29466 for datastore: 844 \n    Processing... ---------------------------------------- 100% 0:00:46\n</code></pre></p> <p>Options:</p> Option Type Description Required <code>--datastore</code> TEXT Comma-separated list of Datastore IDs or array-like format. Example: 1,2,3,4,5 or \"[1,2,3,4,5]\" Yes <code>--container_names</code> TEXT Comma-separated list of include types or array-like format. Example: \"container1,container2\" or \"[container1,container2]\" No <code>--container_tags</code> TEXT Comma-separated list of include types or array-like format. Example: \"tag1,tag2\" or \"[tag1,tag2]\" No <code>--infer_constraints</code> BOOL Infer quality checks in profile. Do not include if you want infer_constraints == false No <code>--max_records_analyzed_per_partition</code> INT Number of max records analyzed per partition No <code>--max_count_testing_sample</code> INT The number of records accumulated during profiling for validation of inferred checks. Capped at 100,000 No <code>--percent_testing_threshold</code> FLOAT Percent of testing threshold No <code>--high_correlation_threshold</code> FLOAT Number of Correlation Threshold No <code>--greater_than_time</code> DATETIME Only include rows where the incremental field's value is greater than this time. Use one of these formats %Y-%m-%dT%H:%M:%S or %Y-%m-%d %H:%M:%S No <code>--greater_than_batch</code> FLOAT Only include rows where the incremental field's value is greater than this number No <code>--histogram_max_distinct_values</code> INT Number of max distinct values of the histogram No <code>--background</code> BOOL Starts the catalog but does not wait for the operation to finish No"},{"location":"cli/overview-of-qualytics-cli/#run-a-scan-operation-on-a-datastore","title":"Run a Scan Operation on a Datastore","text":"<p>Allows you to trigger a scan operation on a datastore (datastore permission required by admin)</p> Bash ExamplePython Example <pre><code>qualytics run scan \n    --datastore \"DATSTORE_ID_LIST\"\n    --container_names \"CONTAINER_NAMES_LIST\" \n    --container_tags \"CONTAINER_TAGS_LIST\"\n    --incremental \n    --remediation \n    --max_records_analyzed_per_partition \"MAX_RECORDS_ANALYZED_PER_PARTITION\" \n    --enrichment_source_records_limit\n    --greater_then_date \"GREATER_THAN_TIME\" \n    --greater_than_batch \"GREATER_THAN_BATCH\" \n    --background\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    DATASTORE_ID = 1172\n    CONTAINER_NAMES = \"CUSTOMER, NATION\"\n    qualytics.scan_operation(\n        datastores=str(DATASTORE_ID),\n        container_names=None,\n        container_tags=None,\n        incremental=False,\n        remediation=\"none\",\n        enrichment_source_record_limit=10,\n        greater_than_batch=None,\n        greater_than_time=None,\n        max_records_analyzed_per_partition=10000,\n        background=False\n    )\n</code></pre> <pre><code>    Successfully Started Scan 29467 for datastore: 1172 \n    Waiting for operation to finish\n    Waiting for operation to finish\n    Waiting for operation to finish\n    Waiting for operation to finish\n    Waiting for operation to finish\n    Successfully Finished Scan operation 29467 for datastore: 1172 \n    Processing... ---------------------------------------- 100% 0:03:04\n</code></pre></p> <p>Options:</p> Option Type Description Required <code>--datastore</code> TEXT Comma-separated list of Datastore IDs or array-like format. Example: 1,2,3,4,5 or \"[1,2,3,4,5]\" Yes <code>--container_names</code> TEXT Comma-separated list of include types or array-like format. Example: \"container1,container2\" or \"[container1,container2]\" No <code>--container_tags</code> TEXT Comma-separated list of include types or array-like format. Example: \"tag1,tag2\" or \"[tag1,tag2]\" No <code>--incremental</code> BOOL Process only new or records updated since the last incremental scan No <code>--remediation</code> TEXT Replication strategy for source tables in the enrichment datastore. Either 'append', 'overwrite', or 'none' No <code>--max_records_analyzed_per_partition</code> INT Number of max records analyzed per partition. Value must be Greater than or equal to 0 No <code>--enrichment_source_record_limit</code> INT Limit of enrichment source records per . Value must be Greater than or equal to -1 No <code>--greater_than_date</code> DATETIME Only include rows where the incremental field's value is greater than this time. Use one of these formats %Y-%m-%dT%H:%M:%S or %Y-%m-%d %H:%M:%S No <code>--greater_than_batch</code> FLOAT Only include rows where the incremental field's value is greater than this number No <code>--background</code> BOOL Starts the catalog but does not wait for the operation to finish No <p>Note: Errors during any of the three operations will be logged in <code>$HOME/.qualytics/operation-error.log</code>.</p>"},{"location":"cli/overview-of-qualytics-cli/#check-operation-status","title":"Check Operation Status","text":"<p>To check the status of operations:</p> Bash ExamplePython Example <pre><code>qualytics operation check_status \n    --ids \"OPERATION_IDS\"\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    qualytics.operation_status(ids=\"29468\")\n</code></pre> <pre><code>    Operation: 29468 is still running \n    Processing... ---------------------------------------- 100% 0:00:00\n</code></pre></p> <p>Options:</p> Option Type Description Required <code>--ids</code> TEXT Comma-separated list of Operation IDs or array-like format. Example: 1,2,3,4,5 or \"[1,2,3,4,5]\" Yes"},{"location":"cli/python-installation/","title":"Installing Python on Windows","text":"<p>Installing Python on Windows is simple and requires a compatible system, administrative access, and an internet connection. It can be installed through the Microsoft Store or the official website, with options to configure settings for seamless use. Verifying the installation ensures Python is ready for development, and setting environment variables can help with advanced tasks.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"cli/python-installation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you meet the following requirements:</p> <ul> <li> <p>System Requirements: Windows 7 or later with sufficient disk space.</p> </li> <li> <p>Administrative Privileges: You need admin rights to install Python and make changes to system settings.</p> </li> <li> <p>Internet Connection: A stable internet connection is required for downloading the installer.</p> </li> </ul>"},{"location":"cli/python-installation/#method-1-install-python-from-the-microsoft-store","title":"Method 1: Install Python from the Microsoft Store","text":"<p>Step 1: Click the Windows icon in the bottom-left corner of the screen, type Microsoft Store in the search bar.</p> <p></p> <p>Step 2:  Click on the Microsoft Store app to open it.</p> <p></p> <p>A Microsoft Store window will open, displaying a home screen with featured apps, games, and promotions.</p> <p></p> <p>Step 3: Click the search bar, type Python, and press Enter to search.</p> <p></p> <p>Step 4: A list of available Python versions appears. Select the latest version published by the Python Software Foundation to open its installation page.</p> <p></p> <p>Step 5: Click on the Python version you wish to install.</p> <p>For demonstration purposes, we will install Python 3.12.</p> <p></p> <p>Step 6: Click the Get button to start the download and installation process.</p> <p></p> <p>Step 7: Once the download and installation are complete, click the Downloads button in the left panel of the Microsoft Store to view the downloaded application.</p> <p></p> <p>Step 8: Click the Open button next to the downloaded Python version.</p> <p> </p> <p>Step 9: A modal command prompt window will open. In the command prompt, type python --version and press Enter.</p> <p></p> <p>If the installed Python version appears, it confirms that Python has been successfully installed on your system.</p> <p></p>"},{"location":"cli/python-installation/#method-2-installing-python-from-the-official-website","title":"Method 2: Installing Python from the Official Website","text":"<p>Step 1: Open a web browser and navigate to the Downloads for Windows section of the official Python website.</p> <p></p> <p>In the Downloads section, you will see different Python versions listed under Stable Releases and Pre-releases. Each version includes multiple installer options, such as:</p> <ul> <li> <p>Windows installer (64-bit) \u2013 For 64-bit systems</p> </li> <li> <p>Windows installer (32-bit) \u2013 For 32-bit systems</p> </li> <li> <p>Windows installer (ARM64) \u2013 For ARM-based systems</p> </li> </ul> <p>Choose the appropriate installer based on your system requirements before proceeding with the download.</p> <p></p> <p>Step 2: Click the link to download the file. For demonstration purposes, we have selected the Download Windows installer (64-bit).</p> <p></p> <p>Step 3: Locate the downloaded Python installer on your system and click to open it.</p> <p></p> <p>Step 4: Once the Python installer opens, the installation window shows two checkboxes:</p> <p></p> <ul> <li>Admin privileges: Check the box labelled Admin Privileges parameter controls whether to install Python for the current or all system users. This option allows you to change the installation folder for Python.</li> </ul> <p></p> <ul> <li>Add Python to Path: Check the box labeled Add Python to PATH important for running Python from the command line.</li> </ul> <p></p> <p>Step 5: Click Install Now option for the recommended installation.</p> <p></p> <p>Once installation is complete, you\u2019ll see an option to Disable path length limit. Click this option if prompted, as it can prevent issues with long file paths during development.</p> <p></p> <p>Step 6: Click Close to exit the installer.</p> <p></p> <p>Step 7: Verify the installation by opening a Command Prompt and typing:</p> <p>Syntax</p> <p>python --version</p> <p></p> <p>If Python is installed correctly, it should display the installed version:</p> <p>Example</p> <p>C:\\Users\\user&gt;python --version Python 3.12.4</p> <p></p>"},{"location":"cli/python-installation/#setting-up-environment-variables","title":"Setting Up Environment Variables","text":"<p>If the Python installer does not include the Add Python to PATH checkbox or you have not selected that option, continue in this step. Otherwise, skip these steps.</p> <p>Step 1: Open File Explorer (Win +E) and navigate to where Python is installed. The default location is:</p> <p>Example</p> <p>C:\\Users\\win 10\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Python 3.12 </p> <p></p> <p>Step 2: Copy this path, then press Win + R on your keyboard, type sysdm.cpl, and press Enter.</p> <p></p> <p>Step 3: A modal window system properties will appear. Click on the Advanced Tab.</p> <p></p> <p>Step 4: Click on the Environment Variable button.</p> <p></p> <p>A modal window will appear. Under System Variables, select Path and click on Edit button.</p> <p></p> <p>Paste the copied Python installation path.</p> <p></p> <p>Step 5: Also, add the Scripts folder path:</p> <p>Example</p> <p>C:\\Users\\YourUsername\\AppData\\Local\\Programs\\Python\\Python3x\\Scripts </p> <p></p> <p>Step 6: Click OK to save the changes and restart your computer.</p> <p></p> <p>Step 7: Open the Command Prompt and type:</p> <p>Syntax</p> <p>python --version </p> <p></p> <p>If Python is installed correctly, it should display the installed version:</p> <p>Example</p> <p>C:\\Users\\user&gt;python --version Python 3.12.4 </p> <p></p>"},{"location":"connections/overview-of-a-connection/","title":"Connections Overview","text":"<p>In Qualytics, setting up datastore connections is simple and efficient. Enter the necessary details like datastore name, connector, and authentication credentials to connect your datastores. You can also enable Secrets Management for secure credential handling with HashiCorp Vault.</p> <p>Once verified, the Reuse Connection feature lets you use existing credentials for future datastores, saving time and ensuring consistency. Manage your connections easily by adding, editing, or deleting datastores as needed.</p>"},{"location":"connections/overview-of-a-connection/#setup-a-connection","title":"Setup a Connection","text":"<p>To configure a datastore connection in Qualytics, begin by entering the required details such as the datastore name, connector, and authentication credentials. Optionally, enable Secrets Management for secure credential handling. Once the connection is tested and confirmed, your datastore will be set up and ready for use.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore (e.g., the specified name will appear on the datastore cards.) 2. Toggle Button Toggle ON to create a new source datastore from scratch. 3. Connector (Required) Select a connector from the drop-down list. <p>For demonstration purposes, we have selected the BigQuery Connector.</p> <p>Step 3: Add connection details such as temp dataset ID, service account key, project ID, and dataset ID.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any connection property to reference a key from the configured Vault secret. Each time the connection is initiated, the corresponding secret value will be retrieved dynamically. </p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 4: The configuration form requests credential details before establishing a connection.</p> <p></p> <p>Note</p> <p>Different connectors have unique fields and parameters. For this demonstration, we have selected the BigQuery Connector, so the fields displayed are specific to the BigQuery configuration. </p> REF. FIELDS ACTIONS 1. Temp Dataset ID (Optional) Enter a temporary dataset ID for intermediate data storage during BigQuery operations. 2. Service Account Key (Required) Upload a JSON file that contains the credentials required for accessing BigQuery. 3. Project ID (Required) Enter the Project ID associated with BigQuery. 4. Dataset ID (Required) Enter the Dataset ID (schema name) associated with BigQuery. 5. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 6. Initiate Cataloging (Optional) Tick the checkbox to automatically perform a catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 5: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p> <p>Step 6: Once the connection is verified, click the Finish button to complete the process.</p> <p></p> <p>Step 7: A message will appear indicating that your datastore has been successfully added. Once the datastore is added, you can reuse the connection for future tasks without needing to re-enter the details.</p> <p></p>"},{"location":"connections/overview-of-a-connection/#reuse-a-connection","title":"Reuse a Connection","text":"<p>The Reuse Connection feature lets you use existing credentials to set up a new datastore, saving time and ensuring consistency in your Qualytics account. Simply toggle the option to reuse credentials instead of entering new ones.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore (e.g., the specified name will appear on the datastore cards). 2. Toggle Button Toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select a connector from the drop-down list. <p>For demonstration purposes, we have selected the BigQuery Connector.</p> <p>Step 3: Add connection details such as temp dataset ID, service account key, project ID, and dataset ID.</p> <p></p> <p>Step 4: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Step 5: Once the connection is verified, click the Finish button to complete the process.</p> <p></p> <p>A message will appear indicating that your datastore has been successfully added. </p> <p></p>"},{"location":"connections/overview-of-a-connection/#manage-connection","title":"Manage Connection","text":"<p>You can effectively manage your connections by editing, deleting, and adding datastores to maintain accuracy and efficiency.</p> <p>For more information on managing connections, refer to the Manage Connection section.</p>"},{"location":"connections/overview-of-a-connection/#conclusion","title":"Conclusion","text":"<p>Using Connections optimizes datastore management by enabling the reuse of connection parameters, making the process more streamlined and organized.</p>"},{"location":"container/computed-join/","title":"Computed Join","text":"<p>A Computed Join Container allows you to combine data from two containers, which can be from the same source datastore or different source datastores (e.g., a database table vs. a file system container). You can choose the join type (Inner, Left, Right, or Full Outer) and apply transformations, filters, and custom queries to the joined result.</p> <p>This feature is useful when you want to:</p> <ul> <li> <p>Merge information from multiple source datastores into a single dataset.</p> </li> <li> <p>Perform cross-datastore analysis (e.g., JDBC tables with DFS files).</p> </li> <li> <p>Apply Spark SQL transformations and filters on top of the joined data.</p> </li> </ul> <p>Let's get started \ud83d\ude80</p>"},{"location":"container/computed-join/#how-it-works","title":"How It Works","text":"<p>The Add Computed Join form consists of:</p> REF. FIELDS DESCRIPTION 1 Name The unique name for your computed join container. 2 Join Type Choose one of the following:\u2022 Inner Join: Keeps only rows with matching keys in both containers.\u2022 Left Join: Keeps all rows from the left container, matching rows from the right.\u2022 Right Join: Keeps all rows from the right container, matching rows from the left.\u2022 Full Outer Join: Keeps all rows from both containers. 3 Left Reference \u2022 Datastore: Source datastore where the computed join container will be created.\u2022 Container: The left container to join.\u2022 Field: The key (column) to join on.\u2022 Prefix: A label (e.g., <code>left</code>) applied to all columns from this container. 4 Right Reference \u2022 Datastore: Source datastore containing the second container.\u2022 Container: The right container to join.\u2022 Field: The key (column) to join on.\u2022 Prefix: A label (e.g., <code>right</code>) applied to all columns from this container. 5 Select Expression A list of columns to include in the result. Columns are automatically prefixed (e.g., <code>left_name</code>, <code>right_name</code>) to avoid conflicts. 6 Filter Clause (WHERE) Additional filters applied to the join result. <p></p>"},{"location":"container/computed-join/#example-use-case","title":"Example Use Case","text":"<p>Scenario</p> <p>We want to join:</p> <ul> <li>Left Container: customers</li> <li>Right Container: orders</li> <li>Join Key: customer_id</li> <li>Join Type: Left Join</li> <li>Prefixes: cust_ and order_</li> </ul>"},{"location":"container/computed-join/#input-tables","title":"Input Tables","text":""},{"location":"container/computed-join/#customers","title":"customers","text":"customer_id name city 1 Alice Berlin 2 Bob London 3 Charlie Paris"},{"location":"container/computed-join/#orders","title":"orders","text":"order_id customer_id product 101 1 Laptop 102 1 Mouse 103 2 Keyboard"},{"location":"container/computed-join/#joined-result-left-join","title":"Joined Result (Left Join)","text":"cust_customer_id cust_name cust_city order_order_id order_product 1 Alice Berlin 101 Laptop 1 Alice Berlin 102 Mouse 2 Bob London 103 Keyboard 3 Charlie Paris NULL NULL"},{"location":"container/computed-join/#visual-diagram","title":"Visual Diagram","text":"<pre><code>+------------+                 +--------+\n| customers  |    LEFT JOIN    | orders |\n+------------+ &lt;-------------&gt; +--------+\n    |                               |\n    +-- customer_id = customer_id --+\n</code></pre>"},{"location":"container/computed-join/#api-example","title":"API Example","text":""},{"location":"container/computed-join/#endpoint","title":"Endpoint","text":"<p>POST:<code>/api/containers</code> </p> <p>Expected response: <code>200 OK</code></p> Request Payload <pre><code>{\n    \"container_type\": \"computed_join\",\n    \"name\": \"customer_orders_join\",\n    \"select_clause\": \"cust_customer_id, cust_name, cust_city, order_order_id, order_product\",\n    \"where_clause\": null,\n    \"left_join_field_name\": \"customer_id\",\n    \"left_prefix\": \"cust\",\n    \"right_join_field_name\": \"customer_id\",\n    \"right_prefix\": \"order\",\n    \"join_type\": \"left\",\n    \"left_container_id\": 101,\n    \"right_container_id\": 202\n}\n</code></pre>"},{"location":"container/computed-join/#tips","title":"Tips","text":"<ul> <li>Always set prefixes to avoid column name collisions.</li> <li>Use Select Expression to choose only the columns you need.</li> <li>Apply a Filter Clause for better performance by reducing unnecessary data.</li> <li>Test the join type with sample data to verify expected behavior.</li> </ul>"},{"location":"container/computed-tables-and-files/","title":"Computed Tables &amp; Files","text":"<p>Computed Tables and Computed Files are powerful virtual tables within the Qualytics platform, each serving distinct purposes in data manipulation. Computed Tables are created using SQL queries on JDBC source datastores, enabling advanced operations like joins and where clauses. Computed Files, derived from Spark SQL transformations on DFS source datastores, allow for efficient data manipulation and transformation directly within the DFS environment.</p> <p>This guide explains how to add Computed Tables and Computed Files and discusses the differences between them.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"container/computed-tables-and-files/#computed-tables","title":"Computed Tables","text":"<p>Use Computed Tables when you want to perform the following operations on your selected source datastores:</p> <ul> <li>Data Preparation and Transformation: Clean, shape, and restructure raw data from JDBC source datastores.</li> <li>Complex Calculations and Aggregations: Perform calculations not easily supported by standard containers.</li> <li>Data Subsetting: Extract specific data subsets based on filters using SQL's WHERE clause.     </li> <li>Joining Data Across Source Datastores: Combine data from multiple JDBC source datastores using SQL joins.</li> </ul>"},{"location":"container/computed-tables-and-files/#add-computed-tables","title":"Add Computed Tables","text":"<p>Step 1: Log in to your Qualytics account and select a JDBC-type source datastore from the side menu on which you would like to add a computed table.</p> <p></p> <p>Step 2: After selecting your preferred source datastore, you will be redirected to the source datastore operations page. From this page, click on the Add button and select the Computed Table option from the dropdown menu.</p> <p></p> <p>Step 3: A modal window will appear prompting you to enter a name for your computed table, a valid SQL query that supports your selected source datastore, and optionally, additional metadata.</p> REF. FIELDS ACTIONS 1. Name (Required) Enter a name for your computed table. The name should be descriptive and meaningful to help you easily identify the table later (e.g., add a meaningful name like <code>Customer_Order_Statistics</code>). 2. Query (Required) Write a valid SQL query that supports your selected source datastore. The query helps to perform joins and aggregations on your selected source datastore. 3. Additional Metadata (Optional) Add custom metadata to enhance the definition of your computed table. Click the plus icon (+) next to this section to open the metadata input form, where you can add key-value pairs. <p></p> <p>Step 4: Click on the Validate button to instantly check the syntax and semantics of your SQL query. This ensures your query runs successfully and prevents errors before saving.</p> <p></p> <p>Step 5: Once validation is successful, click on the Save button to add the computed table to your selected source datastore.</p> <p></p>"},{"location":"container/computed-tables-and-files/#computed-files","title":"Computed Files","text":"<p>Use Computed Files when you want to perform the following operations on your selected source datastore:</p> <ul> <li>Data Preparation and Transformation: Efficiently clean and restructure raw data stored in a DFS.</li> <li>Column-Level Transformations: Utilize Spark SQL functions to manipulate and clean individual columns.</li> <li>Filtering Data: Extract specific data subsets within a DFS container using Spark SQL's WHERE clause.</li> </ul>"},{"location":"container/computed-tables-and-files/#add-computed-files","title":"Add Computed Files","text":"<p>Step 1: Log in to your Qualytics account and select a DFS-type source datastore from the side menu on which you would like to add a computed file.</p> <p></p> <p>Step 2: After clicking on your preferred source datastore, you will be redirected to the source datastore operations page. From this page, click on the Add button and select the Computed File option from the dropdown menu.</p> <p></p> <p>Step 3: A modal window will appear prompting you to enter a name for your computed file, select a source file pattern, choose the expression, and optionally define a filter clause and add additional metadata.</p> REF. FIELDS ACTION 1. Name (Required) Enter a name for your computed file. The name should be descriptive and meaningful to help you easily identify the file later (e.g., add a meaningful name like Customer_Order_Statistics). 2. Source File Pattern (Required) Select a source file pattern from the dropdown menu to match files that have a similar naming convention. 3. Select Expression (Required) Select the expression to define the data you want to include in the computed file. 4. Filter Clause (Optional) Add a WHERE clause to filter the data that meets certain conditions. 5. Additional Metadata (Optional) Enhance the computed file definition by setting custom metadata. Click the plus icon (+) next to this section to open the metadata input form, where you can add key-value pairs. <p></p> <p>Step 4: Click on the Validate button to quickly check your query or expression before saving.</p> <p></p> <p>Step 5: Once validation is successful, click on the Save button to add the computed file to your selected source datastore.</p> <p></p> <p>After clicking the Save button, a success notification appears on the screen showing the action was completed successfully.</p>"},{"location":"container/computed-tables-and-files/#computed-table-vs-computed-file","title":"Computed Table Vs. Computed File","text":"Feature Computed Table (JDBC) Computed File (DFS) Source Data JDBC source datastores DFS source datastores Query Language SQL (database-specific functions) Spark SQL Supported Operations Joins, where clauses, and database functions Column transforms, where clauses (no joins), Spark SQL functions <p>Note</p> <p>Computed tables and files function like regular tables. You can profile them, create checks, and detect anomalies.</p> <ul> <li>Updating a computed table's query will trigger a profiling operation.      </li> <li>Updating a computed file's select or where clause will trigger a profiling operation.  </li> <li>When you create a computed table or file, a basic profile of up to 1000 records is automatically generated.</li> </ul>"},{"location":"container/computed-tables-and-files/#view-assigned-teams","title":"View Assigned Teams","text":"<p>By hovering over the information icon, users can view the assigned teams for enhanced collaboration and data transparency.</p> <p></p>"},{"location":"container/container/","title":"Containers Overview","text":"<p>Containers are fundamental entities representing structured data sets. These containers could manifest as tables in JDBC datastores or as files within DFS datastores. They play a pivotal role in data organization, profiling, and quality checks within the Qualytics application.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"container/container/#container-types","title":"Container Types","text":"<p>There are two main types of containers in Qualytics:</p>"},{"location":"container/container/#jdbc-container","title":"JDBC Container","text":"<p>JDBC containers are virtual representations of database objects, making it easier to work with data stored in relational databases. These containers include tables, which organize data into rows and columns like a spreadsheet, views that provide customized displays of data from one or more tables, and other database objects such as indexes or stored procedures. Acting as a bridge between applications and databases, JDBC enables seamless interaction with these containers, allowing efficient data management and retrieval.</p>"},{"location":"container/container/#dfs-container","title":"DFS Container","text":"<p>DFS containers are used to represent files stored in distributed file systems, such as Hadoop or cloud storage. These files can include formats like CSV, JSON, or Parquet, which are commonly used for storing and organizing data. DFS containers make it easier for applications to work with these files by providing a structured way to access and process data in large-scale storage systems.</p>"},{"location":"container/container/#container-attributes","title":"Container Attributes","text":""},{"location":"container/container/#totals","title":"Totals","text":"<p>Note</p> <p>Totals are calculated from sampled data, not the full dataset. Values may differ from actual totals across all records.</p> <ol> <li> <p>Quality Score: This represents the overall health of the data based on various checks. A higher score indicates better data quality and fewer issues detected.</p> </li> <li> <p>Sampling: Displays the percentage of data sampled during profiling. A 100% sampling rate means the entire dataset was analyzed for the quality report.</p> </li> <li> <p>Completeness: Indicates the percentage of records that are fully populated without missing or incomplete data. Lower percentages may suggest that some fields have missing values.</p> </li> <li> <p>Records Profiled: Shows the number or percentage of records that have been analyzed during the profiling process.</p> </li> <li> <p>Fields Profiled: This shows the number of fields or attributes within the dataset that have undergone data profiling, which helps identify potential data issues in specific columns.</p> </li> <li> <p>Active Checks: Represents the number of ongoing checks applied to the dataset. These checks monitor data quality, consistency, and correctness.</p> </li> <li> <p>Active Anomalies: Displays the total number of anomalies found during the data profiling process. Anomalies can indicate inconsistencies, outliers, or potential data quality issues that need resolution.  </p> </li> </ol> <p></p>"},{"location":"container/container/#observability","title":"Observability","text":"<p>1. Volumetric Measurement </p> <p>Volumetric measurement allows users to track the size of data stored within the table over time. This helps in monitoring how the data grows or changes, making it easier to detect sudden spikes that may impact system performance. Users can visualize data volume trends and manage the table's efficiency. This helps in optimizing storage, adjusting resource allocation, and improving query performance based on the size and growth of the computed table.</p> <p></p> <p>2. Anomalies Measurement</p> <p>The Anomalies section helps users track any unusual data patterns or issues within the computed tables. It shows a visual representation of when anomalies occurred over a specific time period, making it easy to spot unusual activity. This allows users to quickly identify when something might have gone wrong and take action to fix it, ensuring the data stays accurate and reliable.</p> <p></p>"},{"location":"container/container/#actions-on-container","title":"Actions on Container","text":"<p>Users can perform various operations on containers to manage datasets effectively. The actions are divided into three main sections: Settings, Add, and Run. Each section contains specific options to perform different tasks.</p> <p></p>"},{"location":"container/container/#settings","title":"Settings","text":"<p>The Settings button allows users to configure the container. By clicking on the Settings button, users can access the following options:</p> <p></p> No. Options Description 1. Settings Configure incremental strategy, partitioning fields, and exclude specific fields from analysis. 2. Score Score allowing you to adjust the decay period and factor weights for metrics like completeness, accuracy, and consistency. 3. Observability Enables or disables tracking for data volume and freshness.Volume Tracking: Monitors daily volume metrics to identify trends and detect anomalies over time.Freshness Tracking: Records the last update timestamp to ensure data timeliness and detect pipeline delays. 4. Migrate Migrate authored quality checks from one container to another (even across datastores) to quickly reuse, standardize, and avoid recreating rules. 5. Export Export quality checks, field profiles, and anomalies to an enrichment datastore for further action or analysis. 6. Materialize Captures snapshots of data from a source datastore and exports it to an enrichment datastore for faster access and analysis. 7. Delete Delete the selected container from the system."},{"location":"container/container/#add","title":"Add","text":"<p>The Add button allows users to add checks or computed fields. By clicking on the Add button, users can access the following options:</p> <p></p> No. Options Description 1. Checks Checks allow you to add new checks or validation rules for the container. 2. Computed Field Allows you to add a computed field."},{"location":"container/container/#run","title":"Run","text":"<p>The Run button provides options to execute operations on datasets, such as profiling, scanning, and external scans. By clicking on the Run button, users can access the following options:</p> <p></p> No. Options Descriptions 1. Profile Profile allows you to run a profiling operation to analyze the data structure, gather metadata, set thresholds, and define record limits for comprehensive dataset profiling. 2. Scan Scan allows you to perform data quality checks, configure scan strategies, and detect anomalies in the dataset. 3. External Scan External Scan allows you to upload a file and validate its data against predefined checks in the selected table."},{"location":"container/container/#field-profiles","title":"Field Profiles","text":"<p>After profiling a container, individual field profiles offer granular insights:</p>"},{"location":"container/container/#totals_1","title":"Totals","text":"<p>1. Quality Score: This provides a comprehensive assessment of the overall health of the data, factoring in multiple checks for accuracy, consistency, and completeness. A higher score, closer to 100, indicates optimal data quality with minimal issues or errors detected. A lower score may highlight areas that require attention and improvement.</p> <p>2. Sampling: This shows the percentage of data that was evaluated during profiling. A sampling rate of 100% indicates that the entire dataset was analyzed, ensuring a complete and accurate representation of the data\u2019s quality across all records, rather than just a partial sample.</p> <p>3. Completeness: This metric measures how fully the data is populated without missing or null values. A higher completeness percentage means that most fields contain the necessary information, while a lower percentage indicates data gaps that could negatively impact downstream processes or analysis.</p> <p>4. Active Checks: This refers to the number of ongoing quality checks being applied to the dataset. These checks monitor aspects such as format consistency, uniqueness, and logical correctness. Active checks help maintain data integrity and provide real-time alerts about potential issues that may arise.</p> <p>5. Active Anomalies: This tracks the number of anomalies or irregularities detected in the data. These could include outliers, duplicates, or inconsistencies that deviate from expected patterns. A count of zero indicates no anomalies, while a higher count suggests that further investigation is needed to resolve potential data quality issues.</p> <p></p>"},{"location":"container/container/#profile","title":"Profile","text":"<p>This provides detailed insights into the characteristics of the field, including its type, distinct values, and length. You can use this information to evaluate the data's uniqueness, length consistency, and complexity.</p> No Profile Description 1 Declared Type Indicates whether the type is declared by the source or inferred. 2 Distinct Values Count of distinct values observed in the dataset. 3 Min Length Shortest length of the observed string values or lowest value for numerics. 4 Max Length Greatest length of the observed string values or highest value for numerics. 5 Mean Mathematical average of the observed numeric values. 6 Median The median of the observed numeric values. 7 Standard Deviation Measure of the amount of variation in observed numeric values. 8 Kurtosis Measure of the \u2018tailedness\u2019 of the distribution of observed numeric values. 9 Skewness Measure of the asymmetry of the distribution of observed numeric values. 10 Q1 The first quartile; the central point between the minimum and the median. 11 Q3 The third quartile; the central point between the median and the maximum. 12 Sum Total sum of all observed numeric values. <p></p>"},{"location":"container/container/#last-profile","title":"Last Profile","text":"<p>The Last Profile timestamp helps users understand how up-to-date the field is. When you hover over the time indicator shown on the right side of the Last Profile label (e.g., \"1 week ago\"), a tooltip displays the complete date and time the field was last profiled.</p> <p></p> <p>This visibility ensures better context for interpreting profile metrics like mean, completeness, and anomalies.</p>"},{"location":"container/container/#compare-profile","title":"Compare Profile","text":"<p>You can compare the current field profile with earlier versions to spot changes over time. Visual indicators highlight modified metrics, interactive charts show numeric trends across profile history, and special badges identify data drift or field type changes.</p> <p>By clicking on the dropdown under Compare With, you can select an earlier profile run (for example, 1 day ago or 5 days ago).</p> <p></p> <p>Once selected, the system highlights differences between profiles, marking metrics as Changed or Unchanged. It compares data quality (Sampling, Completeness) and statistical measures (mean, median, standard deviation, skewness, kurtosis, min, max, distinct values, etc.), making it easy to track shifts in data quality and distribution.</p> <p></p>"},{"location":"container/container/#view-metric-chart","title":"View Metric Chart","text":"<p>You can access detailed metric charts by clicking the View Metric Chart button. This will display variations across the last 10 profiles. By hovering over points on the chart, you can see additional details such as profile dates, measured values, and sampling percentages for deeper analysis.</p> <p></p>"},{"location":"container/data-preview/","title":"Data Preview","text":"<p>Data Preview in Qualytics makes it easy for users to view and understand their container data. It provides a clear snapshot of the data's structure and contents, showing up to 100 rows from the source. With options to filter specific data, refresh for the latest updates, and download records, it helps users focus on the most relevant information, troubleshoot issues, and analyze data effectively. The simple grid view ensures a smooth and efficient way to explore and work with your data.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"container/data-preview/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and select the source datastore (JDBC or DFS) from the left menu that contains the data you want to preview.</p> <p></p> <p>Step 2: Select Tables (if a JDBC datastore is connected) or File Patterns (if a DFS datastore is connected) from the Navigation tab at the top.</p> <p></p> <p>Step 3: You will view the full list of tables or files belonging to the selected source datastore. Select the specific table or file whose data you want to preview.</p> <p></p> <p>Alternatively, you can access the tables or files by clicking the drop-down arrow on the selected datastore. This will display the full list of tables or files associated with the selected source datastore. From there, select the specific table or file whose data you want to preview.</p> <p></p> <p>Step 4: After selecting the specific table or file, click on the Data Preview tab.</p> <p></p> <p>You will see a tabular view of the data, displaying the field names (columns) and their corresponding data values, allowing you to review the data's structure, types, and sample records.</p> <p></p>"},{"location":"container/data-preview/#ui-caching","title":"UI Caching","text":"<p>Upon initial access to the Data Preview section, the data may not be stored (cached) yet, which can cause longer loading times. How long it takes to load depends on the type of datastore being used (like DFS or JDBC) and whether the data warehouse is serverless. However, the next time you access the same data, it will load faster because it will be cached, meaning the data is stored temporarily for quicker access.</p>"},{"location":"container/data-preview/#filter-clause-and-refresh","title":"Filter Clause and Refresh","text":"<p>The Data Preview tab includes filter functionality that enables users to focus on specific fields by applying filter clauses. This refines the displayed rows based on specific criteria, enhancing data analysis and providing more targeted insights and includes a Refresh button to update the data view with the latest data.</p>"},{"location":"container/data-preview/#filter-clause","title":"Filter Clause","text":"<p>Use the Filter Clause to narrow down the displayed rows by applying specific criteria, allowing for focused and precise data analysis.</p> <p></p>"},{"location":"container/data-preview/#refresh","title":"Refresh","text":"<p>Click the Refresh button to update the data view with the latest information, ensuring accuracy and relevance.</p> <p></p>"},{"location":"container/data-preview/#select-specific-fields","title":"Select Specific Fields","text":"<p>Select specific fields to display, allowing you to focus on the most relevant data for analysis. Click on the Select Fields to Show dropdown and choose specific fields you want to review by checking or unchecking options.</p> <p></p>"},{"location":"container/data-preview/#download-records","title":"Download Records","text":"<p>The Download Records feature in Qualytics allows users to easily export all source records from the selected enrichment dataset. This functionality is essential for performing deeper analysis outside the platform or for sharing data with external tools and teams.</p> <p></p>"},{"location":"container/data-preview/#use-cases","title":"Use Cases","text":""},{"location":"container/data-preview/#debugging-checks","title":"Debugging Checks","text":"<p>One of the primary use cases of the Data Preview tab is for debugging checks. Users can efficiently inspect the first 100 rows of container data to identify any anomalies, inconsistencies, or errors, facilitating the debugging process and improving data quality.</p>"},{"location":"container/data-preview/#data-analysis","title":"Data Analysis","text":"<p>The Data Preview tab also serves as a valuable tool for data analysis tasks. Users can explore the dataset, apply filters to focus on specific subsets of data, and gain insights into patterns, trends, and correlations within the container data.</p>"},{"location":"container/data-preview/#examples","title":"Examples","text":""},{"location":"container/data-preview/#example-1-debugging-data-import","title":"Example 1: Debugging Data Import","text":"<p>Suppose a user encounters issues with importing data into a container. By utilizing the Data Preview tab, the user can quickly examine the first 100 rows of imported data, identify any formatting errors or missing values, and troubleshoot the data import process effectively.</p>"},{"location":"container/data-preview/#example-2-filtering-data-by-date-range","title":"Example 2: Filtering Data by Date Range","text":"<p>In another scenario, a user needs to analyze sales data within a specific date range. The user can leverage the filter support feature of the Data Preview tab to apply date range filters, displaying only the sales records that fall within the specified timeframe. This allows for targeted analysis and informed decision-making.</p>"},{"location":"container/export-operation/","title":"Export Operation","text":"<p>Qualytics metadata export feature lets you capture the changing states of your data. You can export metadata for Quality Checks, Field Profiles, and Anomalies from selected profiles into an enrichment datastore so that you can perform deeper analysis, identify trends, detect issues, and make informed decisions based on your data.</p> <p>To keep things organized, the exported files use specific naming patterns:</p> <ul> <li>Anomalies: Saved as <code>_&lt;enrichment_prefix&gt;_anomalies_export</code>.  </li> <li>Quality Checks: Saved as <code>_&lt;enrichment_prefix&gt;_checks_export</code>.  </li> <li>Field Profiles: Saved as <code>_&lt;enrichment_prefix&gt;_field_profiles_export</code>.</li> </ul> <p>Note</p> <p>Ensure that an enrichment datastore is already set up and properly configured to accommodate the exported data. This setup is essential for exporting anomalies, quality checks, and field profiles successfully. </p> <p>Let\u2019s get started \ud83d\ude80</p> <p>Step 1: Select a source datastore from the side menu from which you would like to export the metadata.</p> <p> </p> <p>For demonstration purposes, we have selected the \u201cCOVID-19 Data\u201d Snowflake source datastores.</p> <p> </p> <p>Step 2: After selecting a datastore, a bottom-up menu appears on the right side of the interface. Click Enrichment Operations next to the Enrichment Datastore and select Export.</p> <p> </p> <p>Step 3: After clicking Export, the Export Operation modal window appears, allowing metadata extraction from the selected source datastore to the enrichment datastore. </p> <p> </p> <p>Step 4: Select the tables you wish to export. All, Specific, or Tag and click Next to proceed.</p> <p> </p> <p>Step 5: After clicking Next, select the assets you want to export to your Enrichment Datastore: Anomalies, Quality Checks, or Field Profiles, and click Export to proceed with the export process.</p> <p> </p> <p>After clicking Export, a confirmation message appears stating \"Export in motion.\" In a couple of minutes, the metadata will be available in your Enrichment Datastore.</p> <p> </p>"},{"location":"container/export-operation/#schedule-operation","title":"Schedule Operation","text":"<p>Step 1: Click Schedule to configure scheduling options for the Export Operation.</p> <p> </p> <p>Step 2: Configure the scheduling preferences for the Export Operation.</p> <ul> <li> <p>Hourly: Runs every set number of hours at a specified minute. (e.g., Every 1 hour at 00 minutes).</p> </li> <li> <p>Daily: Runs once per day at a specific UTC time. (e.g., Every day at 00:00 UTC).</p> </li> <li> <p>Weekly: Runs on selected weekdays at a set time. (e.g., Every Sunday and Friday at 00:00 UTC).</p> </li> <li> <p>Monthly: Runs on a specific day of the month at a set time. (e.g., 1st day of every month at 00:00 UTC).</p> </li> <li> <p>Advanced: Use Cron expressions for custom scheduling. (e.g., <code>0 12 * * 1-5</code> runs at 12 PM, Monday to Friday).</p> </li> </ul> <p> </p> <p>Step 3: Define the Schedule Name to identify the scheduled Export Operation when it runs.</p> <p> </p> <p>Step 4: Click Schedule to finalize and schedule the Export Operation.</p> <p> </p> <p>After clicking Schedule, a confirmation message appears stating \"Operation Scheduled\". Go to the Activity tab to see the progress of export operation.</p> <p> </p>"},{"location":"container/export-operation/#review-exported-data","title":"Review Exported Data","text":"<p>Step 1: Once the metadata has been exported, navigate to the \u201cEnrichment Datastores\u201d located on the left menu.</p> <p> </p> <p>Step 2: In the \u201cEnrichment Datastores\u201d section, select the datastore where you exported the metadata. The exported metadata will now be visible in the selected datastore.</p> <p> </p> <p>Step 3: Click on the exported files to view the metadata. For demonstration purposes, we have selected the \u201cexport_field_profiles\u201d file to review the metadata.</p> <p>The exported metadata is displayed in a table format, showing key details about the field profiles from the datastore. It typically includes columns that indicate the uniqueness of data, the completeness of the fields, and the data structure. You can use this metadata to check data quality, prepare for analysis, ensure compliance, and manage your data.</p> <p> </p>"},{"location":"container/manage-tables-and-files/","title":"Manage Tables &amp; Files","text":"<p>Managing JDBC \u201ctables\u201d and DFS \u201cfiles\u201d in a connected source datastore allows you to perform actions such as adding validation checks, running scans, monitoring data changes, exporting, or deleting them. For JDBC tables, you can also handle metadata, configure partitions, and manage incremental data for optimized processing. However, for DFS datastores, the default incremental field is the file\u2019s last modified timestamp, and users cannot configure incremental or partition fields manually. </p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"container/manage-tables-and-files/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and select the source datastore (JDBC or DFS) from the left menu that you want to manage.</p> <p> </p> <p>Step 2: Select Tables (if JDBC datastore is connected) or File Patterns (if DFS datastore is connected) from the Navigation tab on the top. </p> <p> </p> <p>Step 3: You will view the full list of tables or files belonging to the selected source datastore.</p> <p> </p>"},{"location":"container/manage-tables-and-files/#settings-for-jdbc-table","title":"Settings For JDBC Table","text":"<p>Settings allow you to edit how data is processed and analyzed for a specific table in your connected source datastore. This includes selecting fields for incremental and partitioning strategies, grouping data, excluding certain fields from scans, and adjusting general behaviors.</p> <p>Step 1: Click on the vertical ellipse next to the table of your choice and select Settings from the dropdown list.</p> <p> </p> <p>A modal window will appear for \u201cTable Settings\u201d.</p> <p> </p> <p>Step 2: Modify the table setting based on:</p> <ul> <li> <p>Identifiers</p> </li> <li> <p>Group Criteria</p> </li> <li> <p>Excluding</p> </li> <li> <p>General</p> </li> </ul>"},{"location":"container/manage-tables-and-files/#identifiers","title":"Identifiers","text":"<p>An Identifier is a field that can be used to help load the desired data from a Table in support of analysis. For more details about identifiers, you can refer to the documentation on Identifiers.</p>"},{"location":"container/manage-tables-and-files/#incremental-strategy","title":"Incremental Strategy","text":"<p>This is crucial for tracking changes at the row level within tables. This approach is essential for efficient data processing, as it is specifically used to track which records have already been scanned. This allows for scan operations to focus exclusively on new records that have not been previously scanned, thereby optimizing the scanning process and ensuring that only the most recent and relevant data is analyzed.</p> <p>Note</p> <p>If you have connected a DFS datastore, no manual setup is needed for the incremental strategy, the system automatically tracks and processes the latest data changes.</p> <p> </p> <p>For information about incremental strategy, you can refer to the Incremental Strategy section in the Identifiers documentation.   </p>"},{"location":"container/manage-tables-and-files/#incremental-field","title":"Incremental Field","text":"<p>Incremental Field lets you select a field that tracks changes in your data. This ensures only new or updated records are scanned, improving efficiency and reducing unnecessary processing.</p> <p> </p>"},{"location":"container/manage-tables-and-files/#partition-field","title":"Partition Field","text":"<p>Partition Field is used to divide the data in a table into distinct segments, or dataframes. These partitions allow for parallel analysis, improving efficiency and performance. By splitting the data, each partition can be processed independently. This approach helps optimize large-scale data operations.</p> <p> </p> <p>For information about Partition Field, you can refer to the Partition Field section in the Identifiers documentation. </p>"},{"location":"container/manage-tables-and-files/#group-criteria","title":"Group Criteria","text":"<p>Group Criteria allow you to organize data into specific groups for more precise analysis. By grouping fields, you can gain better insights and enhance the accuracy of your profiling. </p> <p> </p> <p>For information about Group Criteria, you can refer to the documentation on Grouping.</p>"},{"location":"container/manage-tables-and-files/#excluding","title":"Excluding","text":"<p>Excluding allows you to choose specific fields from a table that you want to exclude from data checks. This helps focus on the fields that matter most for validation while ignoring others that are not relevant to the current analysis.</p> <p> </p> <p>For information about Excluding, you can refer to the documentation on Excluding Settings.</p>"},{"location":"container/manage-tables-and-files/#general","title":"General","text":"<p>You can control the default behavior of the specific table by checking or unchecking the option to infer the data type for each field. When checked, the system will automatically determine and cast the data types as needed for accurate data processing.</p> <p> </p> <p>For information about General, you can refer to the documentation on General Settings.</p> <p>Step 3: Once you have configured the table settings, click on the Save button.</p> <p> </p> <p>After clicking on the Save button, your table is successfully updated and a success flash message will appear stating \"Table has been successfully updated\".</p> <p> </p>"},{"location":"container/manage-tables-and-files/#settings-for-dfs-files-pattern","title":"Settings For DFS Files Pattern","text":"<p>Settings allow you to edit how data is processed and analyzed for a specific file patterns in your connected source datastore. This includes selecting fields for incremental and partitioning strategies, grouping data, excluding certain fields from scans, and adjusting general behaviors.</p> <p>Step 1: Click on the vertical ellipse next to the file pattern of your choice and select Settings from the dropdown list.</p> <p> </p> <p>A modal window will appear for \u201cFile Pattern Settings\u201d.</p> <p> </p> <p>Step 2: Modify the table setting based on:</p> <ul> <li> <p>Group Criteria</p> </li> <li> <p>Excluding</p> </li> <li> <p>General</p> </li> </ul>"},{"location":"container/manage-tables-and-files/#group-criteria_1","title":"Group Criteria","text":"<p>Group Criteria allow you to organize data into specific groups for more precise analysis. By grouping fields, you can gain better insights and enhance the accuracy of your profiling. </p> <p> </p> <p>For information about Group Criteria, you can refer to the documentation on Grouping.</p>"},{"location":"container/manage-tables-and-files/#excluding_1","title":"Excluding","text":"<p>Excluding allows you to choose specific fields from a file pattern that you want to exclude from data checks. This helps focus on the fields that matter most for validation while ignoring others that are not relevant to the current analysis.</p> <p> </p> <p>For information about Excluding, you can refer to the documentation on Excluding Settings.</p>"},{"location":"container/manage-tables-and-files/#general_1","title":"General","text":"<p>You can control how file patterns behave by checking or unchecking options to make data processing easier and more consistent. These settings help the system automatically adjust file structures for better integration and analysis.</p> <p> </p> <ul> <li>Inferring Data Types: When enabled, the system figures out the correct data type for each field and applies it automatically. This keeps data consistent and reduces errors, saving you time on manual fixes.</li> </ul> <p> </p> <ul> <li>First Row as Field Names: Turning this on uses the first row of a file as headers, making it simple to map and organize data in the right format.</li> </ul> <p> </p> <ul> <li>Treating Empty Values as Nulls: The Treat empty values as null setting controls how empty fields in files like Excel and CSV are handled. If enabled (true), empty fields are treated as NULL (missing data). If disabled (false), they are stored as empty strings (\"\"), meaning the field exists but is blank. This affects reporting, calculations, and data processing, as NULL values are ignored while empty strings may still be counted.</li> </ul> <p> </p> <p>Step 3: Once you have configured the file pattern settings, click on the Save button.</p> <p> </p> <p>After clicking on the Save button, your table is successfully updated and a success flash message will appear stating \"File Pattern has been successfully updated\".</p> <p> </p>"},{"location":"container/manage-tables-and-files/#add-checks","title":"Add Checks","text":"<p>Add Check allows you to create rules to validate the data within a particular table. You can choose the type of rule, link it directly to the selected table, and add descriptions or tags. This ensures that the table's data remains accurate and compliant with the required standards.</p> <p>Step 1: Click on the vertical ellipse next to the table name and select Add Checks.</p> <p> </p> <p>A modal window will appear to add checks against the selected table.</p> <p> </p> <p>To understand how to add checks, you can follow the remaining steps from the documentation Checks Template.</p>"},{"location":"container/manage-tables-and-files/#run","title":"Run","text":"<p>Execute various operations like profiling or scanning your table or file. It helps validate data quality and ensures that the table meets the defined checks and rules, providing insights into any anomalies or data issues that need attention.</p> <p>Step 1: Click on the vertical ellipse next to the table name and select Run.</p> <p> </p> <p>Under Run, choose the type of operation you want to perform:</p> <ul> <li> <p>Profile: To collect metadata and profile the table's contents.</p> </li> <li> <p>Scan: To validate the data against defined rules and checks.</p> </li> </ul> <p> </p> <p>To understand how a profile operation is performed, you can follow the remaining steps from the documentation Profile Operation..</p> <p>To understand how a scan operation is performed, you can follow the remaining steps from the documentation Scan Operation.</p>"},{"location":"container/manage-tables-and-files/#observability-settings","title":"Observability Settings","text":"<p>Observability helps you track and monitor data performance in your connected source datastore\u2019s tables and files. It provides insights into data volume, detects anomalies, and ensures smooth data processing by identifying potential issues early. This makes it easier to manage and maintain data quality over time.</p> <p>Step 1: Select the table in your JDBC datastore that you would like to monitor, then click on Observability. </p> <p> </p> <p>A modal window \u201cObservability Settings\u201d will appear. Here you can view the details of the table and datastore where actions have been applied.</p> <p> </p> <p>Step 2: Check the \"Volume Tracking\" to enable trend analysis and anomaly detection in data volumes over time and check the \"Freshness Tracking\" to ensure data timeliness and to identify pipeline delays.</p> <p>Volume Tracking monitors and records daily volume metrics for this data asset. This feature enables trend analysis and anomaly detection in data volumes over time. Freshness Tracking measures and records the last time data was added or updated in the data asset. This feature helps ensure data timeliness and identifies pipeline delays.</p> <p> </p> <p>Step 3: Click on the Save button.</p> <p> </p> <p>After clicking on the Save button, a success flash message will appear stating \"Profile has been successfully updated\".</p> <p> </p>"},{"location":"container/manage-tables-and-files/#export","title":"Export","text":"<p>Export feature lets you capture changes in your tables. You can export metadata for Quality Checks, Field Profiles, and Anomalies from selected tables to an enrichment datastore. This helps you analyze data trends, find issues, and make better decisions based on the table data.</p> <p>Step 1: Select the tables in your JDBC datastore that you would like to export, then click on Export.</p> <p> </p> <p>A modal window will appear with the Export Operation setting.</p> <p> </p> <p>For the next steps, detailed information on the export operation is available in the Export Operation section of the documentation.</p>"},{"location":"container/manage-tables-and-files/#materialize","title":"Materialize","text":"<p>Materialize Operation captures snapshots of selected containers from a source datastore and exports them to an enrichment datastore for seamless data loading. Users can run it instantly or schedule it at set intervals, ensuring structured data is readily available for analysis and integration.</p> <p>Step 1: Select the tables in your JDBC datastore that you would like to capture and export containers for the Materialize Operation, then click on Materialize.</p> <p> </p> <p>A modal window will appear with the Materialize Operation setting.</p> <p> </p> <p>For the next steps, detailed information on the materialize operation is available in the Materialize Operation section of the documentation.</p>"},{"location":"container/manage-tables-and-files/#delete","title":"Delete","text":"<p>Delete allows you to remove a table from the connected source datastore. While the table and its associated data will be deleted, it is not permanent, as the table can be recreated if you run a catalog with the \"recreate\" option. </p> <p>Note</p> <p>Deleting a table is a reversible action if a catalog with the \"recreate\" option is run later. </p> <p>Step 1: Select the tables in your connected source datastore that you would like to delete, then click on Delete.</p> <p> </p> <p>Step 2: A confirmation modal window will appear, click on the Delete button to remove the table from the system.</p> <p> </p> <p>Step 3: After clicking on the delete button, your table is successfully deleted and a success flash message will appear saying \"Profile has been successfully deleted\"</p> <p> </p>"},{"location":"container/manage-tables-and-files/#mark-tables-files-as-favorite","title":"Mark Tables &amp; Files as Favorite","text":"<p>Marking a tables and files as a favorite allows you to quickly access important items. This feature helps you prioritize and manage the tables and files you use frequently, making data management more efficient.</p> <p>Step 1: Locate the table and file you want to mark as a favorite and click on the bookmark icon to mark the table and file as a favorite.</p> <p> </p> <p>After Clicking on the bookmark icon your table and file is successfully marked as a favorite and a success flash message will appear stating \u201cThe Table has been favorited\u201d.</p> <p> </p> <p>Step 2: To unmark a tables and files, simply click on the bookmark icon of the marked tables and files. This will remove it from your favorites.</p> <p> </p>"},{"location":"container/materialize-operation/","title":"Materialize Operation","text":"<p>Materialize Operation captures snapshots of selected containers from a source datastore and exports them to an enrichment datastore for seamless data loading. Users can run it instantly or schedule it at set intervals, ensuring structured data is readily available for analysis and integration.</p>"},{"location":"container/materialize-operation/#materialize-naming-conventions","title":"Materialize Naming Conventions","text":"<p>To keep materialized data organized and compatible across different enrichment datastores, the system applies specific naming conventions. These conventions ensure that the resulting container names remain valid, readable, and free of conflicts.</p>"},{"location":"container/materialize-operation/#default-naming-convention","title":"Default Naming Convention","text":"<p>Used when the container name is safe to use as-is.</p> <p><code>&lt;enrichment_prefix&gt;_mat_&lt;container_name&gt;</code>.</p> <p>This naming format is applied when:</p> <ul> <li>The container name length is 60 characters or less.</li> <li>The container name does not contain special characters that may cause invalid table or file names.</li> </ul> <p>Example:</p> <p>If the enrichment prefix is <code>sales</code> and the container name is <code>orders_2024</code>:</p> <p><code>sales_mat_orders_2024</code>.</p>"},{"location":"container/materialize-operation/#fallback-naming-convention","title":"Fallback Naming Convention","text":"<p>If the container name contains characters that may cause issues in downstream systems, the system switches to a safer naming structure by using the container ID instead.</p> <p><code>&lt;enrichment_prefix&gt;_materialize_&lt;container_id&gt;</code>.</p> <p>This fallback is used when:</p> <ul> <li>The container name exceeds 60 characters.</li> <li>The container name includes restricted or special characters.    (e.g., symbols, glob patterns when moving from DFS to JDBC).</li> </ul> <p>Example:</p> <p>If the enrichment prefix is <code>sales</code> and the container ID is <code>1023456</code>:</p> <p><code>sales_materialize_1023456</code>.</p> <p>Note</p> <p>The fallback naming ensures successful loading into the enrichment datastore by preventing invalid or non-compliant table names.</p> <p>Let\u2019s get started \ud83d\ude80</p> <p>Step 1: Select a source datastore from the side menu to capture and export containers for the Materialize Operation.</p> <p></p> <p>For demonstration purposes, we have selected the \u201cCOVID-19 Data\u201d Snowflake source datastore.</p> <p></p> <p>Step 2: After selecting a datastore, a bottom-up menu appears on the right side of the interface. Click Enrichment Operations next to the Enrichment Datastore and select Materialize.</p> <p></p> <p>Step 3:  After clicking Materialize a modal window appears, allowing you to configure the data export settings for the Materialize Operation.</p> <p></p> <p>Step 4:  Select tables to materialize all tables, specific tables, or tables by tag, then click Next.</p> <p></p> <p>Step 5: Configure Record Limit: set the maximum number of records to be materialized per table.</p> <p></p>"},{"location":"container/materialize-operation/#run-now","title":"Run Now","text":"<p>Click Run Now to instantly materialize selected containers.</p> <p></p> <p>After clicking Run Now, a confirmation message appears stating \"Operation Triggered\". Go to the Activity tab to see the progress of materialize operation.</p> <p></p>"},{"location":"container/materialize-operation/#schedule","title":"Schedule","text":"<p>Step 1: Click Schedule to configure scheduling options for the Materialize Operation.</p> <p></p> <p>Step 2: Configure the scheduling preferences for the Materialize Operation.</p> <ul> <li> <p>Hourly: Runs every set number of hours at a specified minute. (e.g., Every 1 hour at 00 minutes).</p> </li> <li> <p>Daily: Runs once per day at a specific UTC time. (e.g., Every day at 00:00 UTC).</p> </li> <li> <p>Weekly: Runs on selected weekdays at a set time. (e.g., Every Sunday and Friday at 00:00 UTC).</p> </li> <li> <p>Monthly: Runs on a specific day of the month at a set time. (e.g., 1st day of every month at 00:00 UTC).</p> </li> <li> <p>Advanced: Use Cron expressions for custom scheduling. (e.g., <code>0 12 * * 1-5</code> runs at 12 PM, Monday to Friday).</p> </li> </ul> <p></p> <p>Step 3: Define the Schedule Name to identify the scheduled Materialize Operation when it runs.</p> <p></p> <p>Step 4: Click Schedule to finalize and schedule the Materialize Operation.</p> <p></p> <p>After clicking Schedule, a confirmation message appears stating \"Operation Scheduled\". Go to the Activity tab to see the progress of materialize operation.</p> <p></p>"},{"location":"container/materialize-operation/#review-materialized-data","title":"Review Materialized Data","text":"<p>Step 1: Once the selected containers are materialized, go to Enrichment Datastores from the left menu. </p> <p></p> <p>Step 2: In the Enrichment Datastores section, select the datastore where you materialized the snapshot. The materialized containers will now be visible.</p> <p></p> <p>Step 3: Click on the materialized files to review the snapshot. For demonstration, we have selected the \"materialized_field_profiles\" file.</p> <p>The materialized data is displayed in a table format, showing key details about the selected containers. It typically includes columns indicating data structure, completeness, and uniqueness. You can use this data for analysis, validation, and integration.</p> <p></p>"},{"location":"container/overview-of-a-computed-field/","title":"Computed Fields","text":"<p>Computed Fields allow you to enhance data analysis by applying dynamic transformations directly to your data. These fields let you create new data points, perform calculations, and customize data views based on your specific needs, ensuring your data is both accurate and actionable.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"container/overview-of-a-computed-field/#add-computed-fields","title":"Add Computed Fields","text":"<p>Step 1: Log in to Your Qualytics Account, navigate to the side menu, and select the source datastore where you want to create a computed field.</p> <p> </p> <p>Step 2: Select the Container within the chosen datastore where you want to create the computed field. This container holds the data to which the new computed field will be applied, enabling you to enhance your data analysis within that specific datastore.</p> <p>For demonstration purposes, we have selected the Bank Dataset-Staging source datastore and the bank_transactions_.csv container within it to create a computed field.</p> <p> </p> <p>Step 3: After selecting the container, click on the Add button and select Computed Field from the dropdown menu to create a new computed field. </p> <p> </p> <p>A modal window will appear, allowing you to enter the details for your computed field. </p> <p> </p> <p>Step 4: Enter the Name for the computed field, select Transformation Type from the dropdown menu, and optionally add Additional Metadata.</p> REF. FIELDS ACTION 1. Field Name (Required) Add a unique name for your computed field. 2. Transformation Type (Required) The type of transformation you want to apply from the available options. 3. Additional Metadata (Optional) Enhance the computed field definition by setting custom metadata. Click the plus icon (+) to open the metadata input form and add key-value pairs. <p> </p> <p>Info</p> <p>Transformations are changes made to data, like converting formats, doing calculations, or cleaning up fields. In Qualytics, you can use transformations to meet specific needs, such as cleaning entity names, converting formatted numbers, or applying custom expressions. With various transformation types available, Qualytics enables you to customize your data directly within the platform, ensuring it\u2019s accurate and ready for analysis.</p> Transformation Types Purpose Reference Cleaned Entity Name Removes business signifiers (such as 'Inc.' or 'Corp') from an entity name. See here Convert Formatted Numeric Removes formatting (such as parentheses for denoting negatives or commas as delimiters) from values that represent numeric data, converting them into a numerically typed field. See here Custom Expression Allows you to create a new field by applying any valid Spark SQL expression to one or more existing fields. See here <p> </p> <p>Step 5: After selecting the appropriate Transformation Type, click the Save button.</p> <p> </p> <p>Step 6: After clicking on the Save button, your computed field is created and a success flash message will display saying The computed field has been successfully created.</p> <p> </p> <p>You can find your computed field by clicking on the dropdown arrow next to the container you selected when creating the computed field.</p> <p> </p>"},{"location":"container/overview-of-a-computed-field/#computed-fields-details","title":"Computed Fields Details","text":""},{"location":"container/overview-of-a-computed-field/#totals","title":"Totals","text":"<p>1. Quality Score: This provides a comprehensive assessment of the overall health of the data, factoring in multiple checks for accuracy, consistency, and completeness. A higher score, closer to 100, indicates optimal data quality with minimal issues or errors detected. A lower score may highlight areas that require attention and improvement.</p> <p>2. Sampling: This shows the percentage of data that was evaluated during profiling. A sampling rate of 100% indicates that the entire dataset was analyzed, ensuring a complete and accurate representation of the data\u2019s quality across all records, rather than just a partial sample.  </p> <p>3. Completeness: This metric measures how fully the data is populated without missing or null values. A higher completeness percentage means that most fields contain the necessary information, while a lower percentage indicates data gaps that could negatively impact downstream processes or analysis.</p> <p>4. Active Checks: This refers to the number of ongoing quality checks being applied to the dataset. These checks monitor aspects such as format consistency, uniqueness, and logical correctness. Active checks help maintain data integrity and provide real-time alerts about potential issues that may arise.</p> <p>5. Active Anomalies: This tracks the number of anomalies or irregularities detected in the data. These could include outliers, duplicates, or inconsistencies that deviate from expected patterns. A count of zero indicates no anomalies, while a higher count suggests that further investigation is needed to resolve potential data quality issues.</p> <p> </p>"},{"location":"container/overview-of-a-computed-field/#profile","title":"Profile","text":"<p>This provides detailed insights into the characteristics of the field, including its type, distinct values, and length. You can use this information to evaluate the data's uniqueness, length consistency, and complexity.</p> No Profile Description 1 Declared Type Indicates whether the type is declared by the source or inferred. 2 Distinct Values Count of distinct values observed in the dataset. 3 Min Length Shortest length of the observed string values or lowest value for numerics. 4 Max Length Greatest length of the observed string values or highest value for numerics. 5 Mean Mathematical average of the observed numeric values. 6 Median The median of the observed numeric values. 7 Standard Deviation Measure of the amount of variation in observed numeric values. 8 Kurtosis Measure of the \u2018tailedness\u2019 of the distribution of observed numeric values. 9 Skewness Measure of the asymmetry of the distribution of observed numeric values. 10 Q1 The first quartile; the central point between the minimum and the median. 11 Q3 The third quartile; the central point between the median and the maximum. 12 Sum Total sum of all observed numeric values. <p> </p> <p>You can hover over the (i) button to view the native field properties, which provide detailed information such as the field's type (numeric), size, decimal digits, and whether it allows null values.</p> <p> </p>"},{"location":"container/overview-of-a-computed-field/#last-profile","title":"Last Profile","text":"<p>The Last Profile timestamp helps users understand how up to date the field is. When you hover over the time indicator shown on the right side of the Last Profile label (e.g., \"8 months ago\"), a tooltip displays the complete date and time the field was last profiled.</p> <p> </p> <p>This visibility ensures better context for interpreting profile metrics like mean, completeness, and anomalies.</p>"},{"location":"container/overview-of-a-computed-field/#manage-tags-in-field-details","title":"Manage Tags in field details","text":"<p>Tags can now be directly managed in the field profile within the Explore section. Simply access the Field Details panel to create, add, or remove tags, enabling more efficient and organized data management.</p> <p>Step 1: Log in to your Qualytics account and click the Explore button on the left side panel of the interface.</p> <p> </p> <p>Step 2: Click on the Profiles tab and select fields.</p> <p> </p> <p>Step 3: Click on the specific field that you want to manage tags.</p> <p> </p> <p>A Field Details modal window will appear. Click on the plus button (+) to assign tags to the selected field.</p> <p> </p> <p>Step 4: You can also create the new tag by clicking on the \u2795 button.</p> <p> </p> <p>A modal window will appear, providing the options to create the tag. Enter the required values to get started.</p> <p> </p> <p>For more information on creating tags, refer to the Add Tag section.</p>"},{"location":"container/overview-of-a-computed-field/#view-team","title":"View Team","text":"<p>By hovering over the information icon, users can view the assigned teams for enhanced collaboration and data transparency.</p> <p> </p>"},{"location":"container/overview-of-a-computed-field/#filter-and-sort-fields","title":"Filter and Sort Fields","text":"<p>Filter and Sort options allow you to organize your fields by various criteria, such as Name, Checks, Completeness, Created Date, and Tags. You can also apply filters to refine your list of fields based on Type and Tags</p>"},{"location":"container/overview-of-a-computed-field/#sort","title":"Sort","text":"<p>You can sort your checks by Active Anomalies, Active Checks, Completeness,  Created Date,  Name, Quality Score, and Type to easily organize and prioritize them according to your needs.</p> <p> </p> No Sort By Description 1 Active Anomalies Sorts fields based on the number of currently active anomalies detected. 2 Active Checks Sorts fields by the number of active validation checks applied. 3 Completeness Sorts fields based on their data completeness percentage. 4 Created Date Sorts fields by the date they were created, showing the newest or oldest fields first. 5 Name Sorts fields alphabetically by their names. 6 Quality Score Sorts fields based on their quality score, indicating the reliability of the data in the field. 7 Type Sorts fields based on their data type (e.g., string, boolean, etc.). <p> </p> <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.</p> <p> </p>"},{"location":"container/overview-of-a-computed-field/#filter","title":"Filter","text":"<p>You can filter your fields based on values like Type and Tag to easily organize and prioritize them according to your needs.</p> <p> </p> No Filter Description 1 Type Filters fields based on the data type (e.g., string, boolean, date, etc.). 2 Tag Tag Filter displays only the tags associated with the currently visible items, along with their color icon, name, type, and the number of matching records. Selecting one or more tags refines the list based on your selection. If no matching items are found, a No option found message is displayed."},{"location":"container/overview-of-a-computed-field/#types-of-transformations","title":"Types of Transformations","text":""},{"location":"container/overview-of-a-computed-field/#cleaned-entity-name","title":"Cleaned Entity Name","text":"<p>This transformation removes common business signifiers from entity names, making your data cleaner and more uniform.</p>"},{"location":"container/overview-of-a-computed-field/#options-for-cleaned-entity-name","title":"Options for Cleaned Entity Name","text":"REF. FIELDS ACTIONS 1. Drop from Suffix Add a unique name for your computed field. 2. Drop from Prefix Removes specified terms from the beginning of the entity name. 3. Drop from Interior Removes specified terms from the beginning of the entity name. 4. Additional Terms to Drop (Custom) Allows you to specify additional terms that should be dropped from the entity name. 5. Terms to Ignore (Custom) Designate terms that should be ignored during the cleaning process."},{"location":"container/overview-of-a-computed-field/#example-for-cleaned-entity-name","title":"Example for Cleaned Entity Name","text":"Example Input Transformation Output 1 \"TechCorp, Inc.\" Drop from Suffix: \"Inc.\" \"TechCorp\" 2 \"Global Services Ltd.\" Drop from Prefix: \"Global\" \"Services Ltd.\" 3 \"Central LTD &amp; Finance Co.\" Drop from Interior: \"LTD\" \"Central &amp; Finance Co.\" 4 \"Eat &amp; Drink LLC\" Additional Terms to Drop: \"LLC\", \"&amp;\" \"Eat Drink\" 5 \"ProNet Solutions Ltd.\" Terms to Ignore: \"Ltd.\" \"ProNet Solutions\""},{"location":"container/overview-of-a-computed-field/#convert-formatted-numeric","title":"Convert Formatted Numeric","text":"<p>This transformation converts formatted numeric values into a plain numeric format, stripping out any characters like commas or parentheses that are not numerically significant.</p>"},{"location":"container/overview-of-a-computed-field/#example-for-convert-formatted-numeric","title":"Example for Convert Formatted Numeric","text":"Example Input Transformation Output 1 \"$1,234.56\" Remove non-numeric characters: \",\", \"$\" \"1234.56\" 2 \"(2020)\" Remove non-numeric characters: \"(\", \")\" \"-2020\" 3 \"100%\" Remove non-numeric characters: \"%\" \"100\""},{"location":"container/overview-of-a-computed-field/#custom-expression","title":"Custom Expression","text":"<p>Enables the creation of a field based on a custom computation using Spark SQL. This is useful for applying complex logic or transformations that are not covered by other types.</p>"},{"location":"container/overview-of-a-computed-field/#using-custom-expression","title":"Using Custom Expression:","text":"<p>You can combine multiple fields, apply conditional logic, or use any valid Spark SQL functions to derive your new computed field.</p> <p>Example: To create a field that sums two existing fields, you could use the expression <code>SUM(field1, field2)</code>.</p> <p>Advanced Example: You need to ensure that a log of leases has no overlapping dates for an asset but your data only captures a single lease's details like </p> LeaseID AssetID Lease_Start Lease_End 1 42 1/1/2025 2/1/2026 2 43 1/1/2025 2/1/2026 3 42 1/1/2026 2/1/2026 4 43 2/2/2026 2/1/2027 <p>You can see in this example that Lease 1 has overlapping dates with Lease 3 for the same Asset. This can be difficult to detect without a full transformation of the data. However, we can accomplish our goal easily with a Computed Field. We'll simply add a Computed Field to our table named \"Next_Lease_Start\" and define it with the following custom expression so that our table will now hold the new field and render it as shown below.</p> <p><code>LEAD(Lease_Start, 1) OVER (PARTITION BY AssetID ORDER BY Lease_Start)</code> </p> LeaseID AssetID Lease_Start Lease_End Next_Lease_Start 1 42 1/1/2025 2/1/2026 1/1/2026 2 43 1/1/2025 2/1/2026 2/2/2026 3 42 1/1/2026 2/1/2026 4 43 2/2/2026 2/1/2027 <p>Now you can author a Quality Check stating that Lease_End should always be less than \"Next_Lease_Start\" to catch any errors of this type. In fact, Qualytics will automatically infer that check for you at Level 3 Inference!</p>"},{"location":"container/overview-of-a-computed-field/#more-examples-for-custom-expression","title":"More Examples for Custom Expression","text":"Example Input Fields Custom Expression Output 1 <code>field1 = 10</code>, <code>field2 = 20</code> <code>SUM(field1, field2)</code> <code>30</code> 2 <code>salary = 50000</code>, <code>bonus = 5000</code> <code>salary + bonus</code> <code>55000</code> 3 <code>hours = 8</code>, <code>rate = 15.50</code> <code>hours * rate</code> <code>124</code> 4 <code>status = 'active'</code>, <code>score = 85</code> <code>CASE WHEN status = 'active' THEN score ELSE 0 END</code> <code>85</code>"},{"location":"container/overview-of-grouping/","title":"Grouping Overview","text":"<p>Grouping is a fundamental aspect of data analysis, allowing users to organize data into meaningful categories for in-depth examination. With the ability to set grouping on Containers, users can define how data within a container should be grouped, facilitating more focused and efficient analysis.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"container/overview-of-grouping/#managing-an-grouping","title":"Managing an Grouping","text":"<p>Step 1: Log in to your Qualytics account and select the source datastore (JDBC or DFS) from the left menu that you want to manage.</p> <p></p> <p>Step 2: Select Tables (if JDBC datastore is connected) or File Patterns (if DFS datastore is connected) from the Navigation tab on the top.</p> <p></p> <p>Step 3: You will view the full list of tables or files belonging to the selected source datastore.</p> <p></p> <p>Step 4: Click on the vertical ellipse next to the table of your choice and select Settings from the dropdown list.</p> <p></p> <p>A modal window will appear for \u201cTable Settings\u201d, where you can manage grouping for the selected table.Use the Grouping section to organize fields, with a warning to avoid large row groupings to maintain performance. Add grouping logic via Group Criteria.</p> <p></p>"},{"location":"container/overview-of-grouping/#usage","title":"Usage","text":"<p>The <code>grouping</code> parameter accepts a list of lists of field names. Each inner list holds the field names in the order that they will be applied as grouping criteria. This flexibility allows users to customize the grouping behavior based on their specific analytical requirements.</p>"},{"location":"container/overview-of-grouping/#example","title":"Example","text":"<p>Consider the following examples of <code>grouping</code> configurations:</p> <ol> <li><code>[\"store_id\"]</code>: Groups data within the container by the <code>store_id</code> field.</li> <li><code>[\"store_id\", \"month\"]</code>: Groups data first by <code>store_id</code>, then by <code>month</code>.</li> <li><code>[\"store_id\", \"state\"]</code>: Groups data first by <code>store_id</code>, then by <code>state</code>.</li> </ol> <p>By specifying different combinations of fields in the <code>grouping</code> parameter, users can tailor the grouping behavior to suit their analytical needs.</p>"},{"location":"container/overview-of-grouping/#impact-on-data-profiles","title":"Impact on Data Profiles","text":"<p>The grouping has implications for various aspects of data profiling and analysis within Qualytics.</p>"},{"location":"container/overview-of-grouping/#field-profiles","title":"Field Profiles","text":"<p>Field Profiles are now produced with filters determined by the <code>grouping</code> specified on the Profile Operation. This means that the profiles generated will reflect the characteristics of data within each group defined by the grouping criteria.</p>"},{"location":"container/overview-of-grouping/#inferred-quality-checks","title":"Inferred Quality Checks","text":"<p>The inferred checks produced by the analytics engine will also hold the filter defined by the <code>grouping</code>. This ensures that data access controls and constraints are applied consistently across different groupings of data within the container.</p>"},{"location":"container/overview-of-grouping/#inferred-quality-check-filters","title":"Inferred Quality Check Filters","text":"<p>Quality Check filters, represented as Spark SQL where clauses, are set based on the <code>grouping</code> specified on the Profile Operation. This ensures that quality checks are applied appropriately to the data within each group, allowing for comprehensive data validation and quality assurance.</p>"},{"location":"container/overview-of-grouping/#conclusion","title":"Conclusion","text":"<p>The introduction of Grouping for Containers in Qualytics enhances data organization and analysis capabilities, allowing users to define custom grouping criteria and analyze data at a granular level. By leveraging <code>grouping</code>, users can gain deeper insights into their data and streamline the analytical process, ultimately driving more informed decision-making and improving overall data quality and reliability.</p>"},{"location":"container/overview-of-identifiers/","title":"Identifiers","text":"<p>An Identifier is a field that can be used to help load the desired data from a table in support of analysis. There are two types of identifiers that can be declared for a table:</p> <ul> <li> <p>Incremental Field: Track records in the table that have already been scanned in order to support Scan operations that only analyze new (not previously scanned) data.</p> </li> <li> <p>Partition Field: Divides the data in the table into distinct dataframes that can be analyzed in parallel.</p> </li> </ul>"},{"location":"container/overview-of-identifiers/#managing-an-identifier","title":"Managing an Identifier","text":"<p>Step 1: Log in to your Qualytics account and select the source datastore (JDBC or DFS) from the left menu that you want to manage.</p> <p></p> <p>Step 2: Select Tables (if JDBC datastore is connected) or File Patterns (if DFS datastore is connected) from the Navigation tab on the top.</p> <p></p> <p>Step 3: You will view the full list of tables or files belonging to the selected source datastore.</p> <p></p> <p>Step 4: Click on the vertical ellipse next to the table of your choice and select Settings from the dropdown list.</p> <p></p> <p>A modal window will appear for \u201cTable Settings\u201d, where you can manage identifiers for the selected table.</p> <p></p>"},{"location":"container/overview-of-identifiers/#incremental-strategy","title":"Incremental Strategy","text":"<p>The Incremental Strategy configuration in Qualytics is crucial for tracking changes at the row level within tables.</p> <p>This approach is essential for efficient data processing, as it is specifically used to track which records have already been scanned.</p> <p>This allows for scan operations to focus exclusively on new records that have not been previously scanned, thereby optimizing the scanning process and ensuring that only the most recent and relevant data is analyzed.</p> <p></p> No Strategy Option Description 1 None No incremental strategy, it will run full. 2 Last Modified - Available types are Date or Timestamp was last modified.- Uses a \"last modified column\" to track changes in the data set.- This column typically contains a timestamp or date value indicating when a record was last modified.- The system compares the \"last modified column\" to a previous timestamp or date, updating only the records modified since that time. 3 Batch Value - Available types are Integral or Fractional.- Uses a \"batch value column\" to track changes in the data set.- This column typically contains an incremental value that increases as new data is added.- The system compares the current \"batch value\" with the previous one, updating only records with a higher \"batch value\".- Useful when data comes from a system without a modification timestamp. 4 Postgres Commit Timestamp Tracking - Utilizes commit timestamps for change tracking. <p>Availability based on technologies: </p> Option Availability Last Modified All Batch Value All Postgres Commit Timestamp Tracking PostgreSQL <p>Info</p> <ul> <li>All options are useful for incremental strategy, it depends on the availability of the data and how it is modeled. </li> <li>The 3 options will allow you to track and process only the data that has changed since the last time the system was run, reducing the amount of data that needs to be read and processed, and increasing the efficiency of your system.</li> </ul>"},{"location":"container/overview-of-identifiers/#incremental-strategy-with-dfs-distributed-file-system","title":"Incremental Strategy with DFS (Distributed File System)","text":"<p>For DFS in Qualytics, the incremental strategy leverages the last modified timestamps from the file metadata. </p> <p>This automated process means that DFS users do not need to manually configure their incremental strategy, as the system efficiently identifies and processes the most recent changes in the data.</p>"},{"location":"container/overview-of-identifiers/#example","title":"Example","text":"<p>Objective: Identify and process new or modified records in the ORDERS table since the last scan using an Incremental Strategy.</p> <p>Sample Data</p> O_ORDERKEY O_PAYMENT_DETAILS LAST_MODIFIED 1 {\"date\": \"2023-09-25\", \"amount\": 250.50, \"credit_card\": \"5105105105105100\"} 2023-09-25 10:00:00 2 {\"date\": \"2023-09-25\", \"amount\": 150.75, \"credit_card\": \"4111-1111-1111-1111\"} 2023-09-25 10:30:00 3 {\"date\": \"2023-09-25\", \"amount\": 200.00, \"credit_card\": \"1234-5678-9012-3456\"} 2023-09-25 11:00:00 4 {\"date\": \"2023-09-25\", \"amount\": 175.00, \"credit_card\": \"5555-5555-5555-4444\"} 2023-09-26 09:00:00 5 {\"date\": \"2023-09-25\", \"amount\": 300.00, \"credit_card\": \"2222-2222-2222-2222\"} 2023-09-26 09:30:00"},{"location":"container/overview-of-identifiers/#incremental-strategy-explanation","title":"Incremental Strategy Explanation","text":"<p>In this example, an Incremental Strategy would focus on processing records that have a LAST_MODIFIED timestamp after a certain cutoff point. For instance, if the last scan was performed on 2023-09-25 at 11:00:00, then only records with O_ORDERKEY 4 and 5 would be considered for the current scan, as they have been modified after the last scan time.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Orders Since Last Scan]\nB --&gt; C{Record Modified After Last Scan?}\nC --&gt;|Yes| D[Process Record]\nC --&gt;|No| E[Skip Record]\nD --&gt; F[Move to Next Record/End]\nE --&gt; F</code></pre> <pre><code>-- An illustrative SQL query to identify and process new or modified records in the ORDERS table since the last scan.\nselect\n    o_orderkey,\n    o_payment_details,\n    last_modified\nfrom orders\nwhere\n    last_modified &gt; '2023-09-25 11:00:00'\n</code></pre>"},{"location":"container/overview-of-identifiers/#partition-field","title":"Partition Field","text":"<p>The Partition Field is a fundamental feature for organizing and managing large datasets. It is specifically designed to divide the data within a table into separate, distinct dataframes. </p> <p>This segmentation is a key strategy for handling and analyzing data more effectively. By creating these individual dataframes, Qualytics allows for parallel processing, which significantly accelerates the analysis.</p> <p>Each partition can be analyzed independently, enabling simultaneous examination of different segments of the dataset.</p> <p>This not only increases the efficiency of data processing but also ensures a more streamlined and scalable approach to handling large volumes of data, making it an indispensable tool in data analysis and management.</p> <p>The ideal Partition Identifier is an Incremental Identifier of type datetime such as a last-modified field, however, alternatives are automatically identified and set during a Catalog operation.</p> <p></p> <p>Info</p> <ul> <li>Partition Field Selection: When selecting a partition field for a table during catalog operation, we will attempt to select a field with no nulls where possible. </li> <li>User-Specified Partition Fields: Users are permitted to specify partition fields manually. While we ensure that the user selects a field of a supported data type, we do not currently enforce non-nullability or completeness. Care should be given to select partition fields with no or a low percentage of nulls in order to avoid unbalanced partitioning.</li> </ul> <p>Warning</p> <p>If no appropriate partition identifier can be selected, then repeatable ordering candidates (order by fields) are used for less efficient processing of containers with a very large number of rows.</p>"},{"location":"container/overview-of-identifiers/#example_1","title":"Example","text":"<p>Objective: Identify an efficient process and analyze the ORDERS table by partitioning the data based on the O_ORDERDATE field, allowing parallel processing of different date segments.</p> <p>Sample Data</p> O_ORDERKEY O_CUSTKEY O_ORDERSTATUS O_TOTALPRICE O_ORDERDATE 1 123 'O' 173665.47 2023-09-01 2 456 'O' 46929.18 2023-09-01 3 789 'F' 193846.25 2023-09-02 4 101 'O' 32151.78 2023-09-02 5 202 'F' 144659.20 2023-09-03"},{"location":"container/overview-of-identifiers/#partition-field-explanation","title":"Partition Field Explanation","text":"<p>In this example, the O_ORDERDATE field is used to partition the ORDERS table. Each partition represents a distinct date, allowing for the parallel processing of orders based on their order date. This strategy enhances the efficiency of data analysis by distributing the workload across different partitions.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Orders Data]\nB --&gt; C{Partition by O_ORDERDATE}\nC --&gt; D[Distribute Partitions for Parallel Processing]\nC --&gt; E[Identify Date Segments]\nD --&gt; F[Analyze Each Partition Independently]\nE --&gt; F\nF --&gt; G[Combine Results/End]</code></pre> <pre><code>-- An illustrative SQL query to partition the ORDERS table by the O_ORDERDATE field for parallel processing.\nSELECT\n    O_ORDERKEY,\n    O_CUSTKEY,\n    O_ORDERSTATUS,\n    O_TOTALPRICE,\n    O_ORDERDATE,\n    O_ORDERPRIORITY\nFROM\n    orders\nDISTRIBUTE BY\n    O_ORDERDATE;\n</code></pre>"},{"location":"container/overview-of-infer-data-type/","title":"General &amp; Excluding Overview","text":"<p>General and excluding fields in Qualytics simplify data analysis by organizing key information and removing irrelevant or sensitive data. This ensures efficient management, protects privacy, and supports customized configurations for specific needs.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"container/overview-of-infer-data-type/#manage-general-excluding","title":"Manage General &amp; Excluding","text":"<p>Step 1: Log in to your Qualytics account and select the source datastore (JDBC or DFS) from the left menu that you want to manage.</p> <p></p> <p>Step 2: Select Tables (if JDBC datastore is connected) or File Patterns (if DFS datastore is connected) from the Navigation tab on the top.</p> <p></p> <p>Step 3: You will view the full list of tables or files belonging to the selected source datastore.</p> <p></p> <p>Step 4: Click on the vertical ellipse next to the table of your choice and select Settings from the dropdown list.</p> <p></p> <p>A modal window will appear for \u201cTable Settings\u201d, where you can manage general and excluding for the selected table.</p> <p></p>"},{"location":"container/overview-of-infer-data-type/#excluding-fields","title":"Excluding Fields","text":"<p>This configuration allows you to selectively exclude specific fields from containers. These excluded fields will be omitted from check creation during profiling operations while also being hidden in data previews, without requiring a profile run.</p> <p>This can be helpful when dealing with sensitive data, irrelevant information, or large datasets where you want to focus on specific fields.</p> <p></p>"},{"location":"container/overview-of-infer-data-type/#benefits-of-excluding-fields","title":"Benefits of Excluding Fields","text":""},{"location":"container/overview-of-infer-data-type/#targeted-analysis","title":"Targeted Analysis","text":"<p>Focus your analysis on the fields that matter most by removing distractions from excluded fields.</p>"},{"location":"container/overview-of-infer-data-type/#data-privacy","title":"Data Privacy","text":"<p>Protect sensitive information by excluding fields that contain personal data or confidential information.</p>"},{"location":"container/overview-of-infer-data-type/#important-considerations","title":"Important Considerations","text":"<p>Excluding fields will permanently remove them from profile creation and data preview until you re-include them and re-profile the container.</p>"},{"location":"container/overview-of-infer-data-type/#infer-data-type","title":"Infer Data Type","text":"<p>The \"infer data type\" option in containers allows the system to automatically determine the appropriate data types (e.g., fractional, integer, date) for columns within your data containers. This setting is configurable for both JDBC and DFS containers.</p> <p></p>"},{"location":"container/overview-of-infer-data-type/#behavior-in-jdbc-datastores","title":"Behavior in JDBC Datastores","text":"<ul> <li>Default: Disabled  </li> <li>Reason: JDBC datastores provide inherent schema information from the database tables. Qualytics leverages this existing schema for accurate data typing.  </li> <li>Override: You can optionally enable this setting if encountering issues with automatic type detection from the source database.</li> </ul>"},{"location":"container/overview-of-infer-data-type/#behavior-in-dfs-datastores","title":"Behavior in DFS Datastores","text":"<ul> <li>Default: </li> <li>Enabled for CSV files  </li> <li> <p>Disabled for other file formats (Parquet, Delta, Avro, ORC, etc.)  </p> </li> <li> <p>Reason: </p> </li> <li>CSV files lack a defined schema. Data type inference helps ensure correct data interpretation.  </li> <li> <p>File formats like Parquet, Delta, Avro, and ORC have embedded schemas, making inference unnecessary.  </p> </li> <li> <p>Override: You can adjust the default behavior based on your specific data sources and requirements.</p> </li> </ul>"},{"location":"container/overview-of-infer-data-type/#rule-for-the-infer-data-type","title":"Rule for the \"Infer Data Type\"","text":""},{"location":"container/overview-of-infer-data-type/#schema-based-data-sources","title":"Schema-Based Data Sources","text":"<p>If the data source has a defined schema (JDBC, Delta, Parquet, Avro, ORC), the flag is set to \"False\".</p>"},{"location":"container/overview-of-infer-data-type/#schema-less-data-sources","title":"Schema-less Data Sources","text":"<p>If the data source lacks a defined schema (CSV), the flag is set to \"True\".</p>"},{"location":"container/overview-of-infer-data-type/#override-file-pattern-for-dfs-datastores","title":"Override file pattern for DFS datastores","text":"<p>Override the file pattern to include files with the same schema but don't match the automatically generated pattern from the initial cataloging.</p> <p>In some cases, you may have multiple files that share the same schema but don't match the automatically generated file pattern during the initial cataloging process. To address this, Qualytics has the ability to override file patterns in the UI. This allows you to specify a custom pattern that encompasses all files with the shared schema, ensuring they are properly included in profiling and analysis.</p>"},{"location":"container/overview-of-infer-data-type/#explore-deeper-knowledge","title":"Explore Deeper Knowledge","text":"<p>If you want to go deeper into the knowledge or if you are curious and want to learn more about DFS filename globbing, you can explore our comprehensive guide here: How DFS Filename Globbing Works.</p>"},{"location":"container/overview-of-infer-data-type/#important-considerations_1","title":"Important Considerations","text":"<p>Subsequent catalog operations without pruning (<code>Disabled</code>) will use the new pattern.</p>"},{"location":"datastore-checks/activate-archived-checks/","title":"Activate Archived Checks","text":"<p>You can activate archived checks individually or in bulk.</p>"},{"location":"datastore-checks/activate-archived-checks/#method-i-activate-specific-check","title":"Method I: Activate Specific Check","text":"<p>Step 1: Navigate to the Archived checks section and click the vertical ellipsis (\u22ee) next to the archived check you want to activate, and select \"Activate\"\u202ffrom the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Metric\" check.</p> <p></p> <p>Step 2: A confirmation modal window \u201cActivate Check\u201d will appear. Click the \u201cActivate\u201d button to activate the archived check.</p> <p></p> <p>Step 3: After clicking the Activate button, your check has been successfully moved to active checks, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/activate-archived-checks/#method-ii-activate-archived-checks-in-bulk","title":"Method II: Activate Archived Checks in Bulk","text":"<p>Step 1: Hover over the archived checks and click the checkbox to select multiple checks in bulk.</p> <p></p> <p>When multiple checks are selected, an action toolbar appears, displaying the total number of checks chosen along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click the vertical ellipsis (\u22ee) and choose \"Activate\" from the dropdown menu to activate the selected checks.</p> <p></p> <p>Step 3: A confirmation modal window \u201cBulk Activate Checks\u201d will appear. Click the \u201cActivate\u201d button to activate the archived checks.</p> <p></p> <p>After clicking the Activate button, your archived checks will be activated, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/activate-draft-check/","title":"Activate Draft Checks","text":"<p>There are two ways to activate draft checks: you can activate specific checks or activate multiple checks in bulk.</p>"},{"location":"datastore-checks/activate-draft-check/#method-i-activate-specific-check","title":"Method I. Activate Specific Check","text":"<p>Step 1: Navigate to the Draft check section and click the vertical ellipsis (\u22ee) next to the draft check you want to activate, and select Edit\u202ffrom the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Metric\" check.</p> <p></p> <p>A modal window will appear with the check details. If you want to make any changes to the check details, you can edit them.</p> <p></p> <p>Step 2: Click the down arrow icon with the Update button. A dropdown menu will appear. Click the Activate button.  </p> <p></p> <p>After clicking the Activate button, your check has been successfully moved to active checks, and a success message will appear on the screen.</p> <p>Alternatively, you can activate a draft check by clicking the vertical ellipsis (\u22ee) next to the draft check and selecting \"Activate\" from the dropdown menu.</p> <p></p> <p>A confirmation modal window \u201cActivate Check\u201d will appear. Click the \u201cActivate\u201d button to activate the draft check.</p> <p></p>"},{"location":"datastore-checks/activate-draft-check/#method-ii-activate-draft-checks-in-bulk","title":"Method II. Activate Draft Checks in Bulk","text":"<p>Step 1: Hover over the draft checks and click the checkbox to select multiple checks in bulk.</p> <p></p> <p>When multiple checks are selected, an action toolbar appears, displaying the total number of checks chosen along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click the vertical ellipsis (\u22ee) and choose \"Activate\" from the dropdown menu to activate the selected checks.</p> <p></p> <p>Step 3: A confirmation modal window \u201cBulk Activate Checks\u201d will appear. Click the \u201cActivate\u201d button to activate the draft checks.</p> <p></p> <p>After clicking the Activate button, your draft checks will be activated, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/archive-check/","title":"Archive Checks","text":"<p>There are two ways to archive checks: you can archive individual checks or archive multiple checks in bulk.</p>"},{"location":"datastore-checks/archive-check/#method-i-archive-specific-check","title":"Method I: Archive Specific Check","text":"<p>You can archive a specific check using two ways: either by directly clicking the archive button on the check or by opening the check and selecting the archive option from the action menu.</p>"},{"location":"datastore-checks/archive-check/#1-archive-directly","title":"1. Archive Directly","text":"<p>Step 1: Locate the check (whether Active or Draft) that you want to archive and click the vertical ellipsis (\u22ee) next to it and select \"Archive\" from the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Metric\" check.</p> <p></p> <p>Step 2: A modal window titled \"Archive Check\" will appear, providing you with the following archive options:</p> <ul> <li> <p>Discarded: Select this option if the check is no longer relevant or suitable for the current business rules or data requirements. This helps in archiving checks that are obsolete but still exist for historical reference.</p> </li> <li> <p>Invalid: Choose this option if the check is not valid and should be retired from future inference. This helps the system learn from invalid checks and improves its ability to infer valid checks in the future.</p> </li> </ul> <p></p> <p>Step 3: Once you've made your selection, click the Archive button to proceed.</p> <p></p> <p>After clicking the Archive button, your check is moved to the archive, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/archive-check/#2-archive-from-action-menu","title":"2. Archive from Action Menu","text":"<p>Step 1: Locate the check (whether Active or Draft) that you want to archive and click the vertical ellipsis (\u22ee) next to it and select \"Edit\" from the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Between\" check.</p> <p></p> <p>Step 2: A modal window will appear displaying the check details. Click the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window and click \"Archive\" from the dropdown menu.</p> <p></p> <p>Step 3: A modal window titled \u201cArchive Check\u201d will appear, providing you with the following archive options:</p> <ul> <li> <p>Discarded: Select this option if the check is no longer relevant or suitable for the current business rules or data requirements. This helps in archiving checks that are obsolete but still exist for historical reference.</p> </li> <li> <p>Invalid: Choose this option if the check is not valid and should be retired from future inference. This helps the system learn from invalid checks and improves its ability to infer valid checks in the future.</p> </li> </ul> <p></p> <p>Step 4: Once you've made your selection, click the Archive button to proceed.</p> <p></p> <p>After clicking the Archive button, your check is moved to the archive, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/archive-check/#method-ii-archive-checks-in-bulk","title":"Method II: Archive Checks in Bulk","text":"<p>You can archive multiple checks in a single step, deactivating and storing them for future reference or restoration while keeping your active checks uncluttered.</p> <p>Step 1: Hover over the checks (whether Active or Draft) and click the checkbox to select multiple checks.</p> <p></p> <p>When multiple checks are selected, an action toolbar appears, displaying the total number of selected checks along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click the vertical ellipsis (\u22ee) and choose \"Archive\" from the dropdown menu to archive the selected checks.</p> <p></p> <p>A modal window will appear, providing you with the following archive options:</p> <p>1. Delete all anomalies associated with the checks: Toggle this option \"On\" if you want to delete any anomalies related to the selected checks when archiving them.</p> <p>2. Archive Options: You are presented with two options to categorize why the checks are being archived:</p> <ul> <li> <p>Discarded: Select this option if the check is no longer relevant or suitable for the current business rules or data requirements. This helps in archiving checks that are obsolete but still exist for historical reference.</p> </li> <li> <p>Invalid: Choose this option if the check is not valid and should be retired from future inference. This helps the system learn from invalid checks and improves its ability to infer valid checks in the future.</p> </li> </ul> <p></p> <p>Step 3: Once you've made your selections, click the \"Archive\" button to confirm and archive the checks.  </p> <p></p> <p>After clicking the \"Archive\" button, your selected checks (whether Active or Draft) will be successfully archived, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/check-favorite/","title":"Mark Check as Favorite","text":"<p>Locate the check you want to mark as a favorite and click the bookmark icon located on the right side of the check.</p> <p></p> <p>After clicking the bookmark icon, your check is successfully marked as a favorite, and a success message will appear on the screen.</p> <p>To unmark a check, simply click the bookmark icon of the marked check. This will remove it from your favorites.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/","title":"Checks in Datastore","text":"<p>Checks are validation rules applied to datasets to ensure data quality and integrity. They can be categorized as Active, Draft, or Archived based on their status and usage. Each check includes detailed metadata such as importance, scan history, anomalies, and assertion results. This section guides you through viewing, managing, and analyzing these checks within your datastore.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"datastore-checks/checks-datastore/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and select the datastore from the left menu on which you want to manage your checks.</p> <p></p> <p>Step 2: Click on the \"Checks\" from the Navigation Tab.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#categories-checks","title":"Categories Checks","text":"<p>You can categorize your checks based on their status, such as Active, Draft, Archived (Invalid and Discarded), or All, according to your preference. This categorization offers a clear view of the data quality validation process, helping you manage checks efficiently and maintain data integrity.</p>"},{"location":"datastore-checks/checks-datastore/#all","title":"All","text":"<p>By selecting All Checks, you can view a comprehensive list of all the checks in the datastore, including both active and draft checks, allowing you to focus on the checks that are currently being managed or are in progress. However, archived checks are not displayed in this.  </p> <p></p>"},{"location":"datastore-checks/checks-datastore/#active","title":"Active","text":"<p>By selecting Active, you can view checks that are currently applied and being enforced on the data. These operational checks are used to validate data quality in real time, allowing you to monitor all active checks and their performance.</p> <p></p> <p>You can also categorize the active checks based on their importance and favorites to streamline your data quality monitoring.</p> <p>1. Important: Shows only checks that are marked as important. These checks are prioritized based on their significance, typically assigned a weight of 7 or higher.</p> <p>Note</p> <p>Important checks are prioritized based on a weight of 7 or higher.</p> <p></p> <p>2. Favorite: Displays checks that have been marked as favorites. This allows you to quickly access checks that you use or monitor frequently.</p> <p></p> <p>3. All: Displays a comprehensive view of all active checks, including important, favorite, and any checks that do not fall under these specific categories.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#draft-checks","title":"Draft Checks","text":"<p>By selecting Draft, you can view checks that have been created but have not yet been applied to the data. These checks are in the drafting stage, allowing for adjustments and reviews before activation. Draft checks provide flexibility to experiment with different validation rules without affecting the actual data.</p> <p></p> <p>You can also categorize the draft checks based on their importance and favorites to prioritize and organize them effectively during the review and adjustment process.</p> <p>1. Important: Shows only checks that are marked as important. These checks are prioritized based on their significance, typically assigned a weight of 7 or higher. </p> <p></p> <p>2. Favorite: Displays checks that have been marked as favorites. This allows you to quickly access checks that you use or monitor frequently.  </p> <p></p> <p>3. All: Displays a comprehensive view of all draft checks, including important, favorite, and any checks that do not fall under these specific categories.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#archived-checks","title":"Archived Checks","text":"<p>By selecting Archived, you can view checks that have been marked as discarded or invalid from use but are still stored for future reference or restoration. Although these checks are no longer active, they can be restored if needed.</p> <p></p> <p>You can also categorize the archived checks based on their status as Discarded, Invalid, or view All archived checks to manage and review them effectively.</p> <p>1. Discarded: Shows checks that have been marked as no longer useful or relevant and have been discarded from use.</p> <p></p> <p>2. Invalid: Displays checks that are deemed invalid due to errors or misconfigurations, requiring review or deletion.</p> <p></p> <p>3. All: Provides a view of all archived checks within this category, including discarded and invalid checks.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#check-info","title":"Check Info","text":"<p>Check Details provides important information about each check in the system. It shows when a check was last run, how often it has been used, when it was last updated, who made changes to it, and when it was created. This section helps users understand the status and history of the check, making it easier to manage and track its use over time.</p> <p>Step 1: Locate the check you want to review, then hover over the info icon to view the Check Details.</p> <p></p> <p>A popup will appear with additional details about the check.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#last-asserted","title":"Last Asserted","text":"<p>Last Asserted At shows the most recent time the check was run, indicating when the last validation occurred. For example, the check was last asserted on Oct 17, 2023, at 2:37 AM (GMT+5:30).</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#scans","title":"Scans","text":"<p>Scans show how many times the check has been used in different operations. It helps you track how often the check has been applied. For example, the check was used in 30 operations.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#updated-at","title":"Updated At","text":"<p>Updated At shows the most recent time the check was modified or updated. It helps you see when any changes were made to the check\u2019s configuration or settings. For example, the check was last updated on Sep 9, 2024, at 3:18 PM (GMT+5:30).</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#last-editor","title":"Last Editor","text":"<p>Last Editor indicates who most recently made changes to the check. It helps track who is responsible for the latest updates or modifications. This is useful for accountability and collaboration within teams.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#created-at","title":"Created At","text":"<p>Created At shows when the check was first made. It helps you know how long the check has been in use. This is useful for tracking its history. For example, the check was created on Oct 17, 2023, at 2:19 PM (GMT+5:30).</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#check-details","title":"Check Details","text":"<p>Check Detail View displays all key information related to a specific data quality check. It shows what the check is monitoring, how it's configured, where it's applied in the dataset, and whether any issues have been found. It also includes sections for viewing the check\u2019s recent performance, related activities, and any additional metadata. This view helps users easily understand the purpose and current state of the check.</p> <p>Step 1: Click on the check that you want to see the details of.</p> <p></p> <p>You will be navigated to the detail section, where you can view the Summary, Observability, Properties, Activity, and Metadata information.</p> <p></p> <p>Info</p> <p>In addition to viewing the check details, you can also monitor and manage any anomalies associated with this check \u2014 all from the same page, without needing to navigate elsewhere.</p>"},{"location":"datastore-checks/checks-datastore/#summary-section","title":"Summary Section","text":"<p>The Summary section shows that a data quality check is applied to a field and is currently active. It indicates whether the check was created automatically by the system or manually by a user and is being applied to the entire dataset and has a defined importance level. It also shows when the check last ran and whether there are any current issues found in the data.</p> <p>1. Check &amp; Status : The type of check applied to the data. In this case, it's a Volumetric check and the check is Active, meaning it's currently being applied.</p> <p></p> <p>2. Type : This check is Authored, meaning it was manually created by the users.</p> <p></p> <p>When you hover over the time period written below the type of the check, a pop-up appears displaying the complete date and time.</p> <p></p> <p>3. Last Asserted : Shows when the check was last run \u2013 3 months ago in this case.</p> <p></p> <p>When you hover over the time the check last ran, a pop-up appears displaying the complete date and time.</p> <p></p> <p>Last Asserted Details</p> <p>Click on the info icon to view the last asserted details.</p> <p></p> <p>A popup will appear with Scans details. Scans show how many times the check has been used in different operations. It helps you track how often the check has been applied. For example, the check was used in 19 operations.</p> <p></p> <p>4. Weight : Indicates the importance or priority of this check \u2013 the weight is 13.</p> <p></p> <p>5. Coverage : How much data this check applies to \u2013 here it's 100%, meaning it applies to the whole dataset.</p> <p></p> <p>6. Active Anomalies : Number of current issues found \u2013 0 anomalies are active right now.</p> <p></p> <p>7. Description : Explains the rule or condition that the check is validating.</p> <p></p> <p>8. Tags : Displays any tags linked to the check. Users can also add new tags by clicking on the tag area.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#copy-the-check-link","title":"Copy the Check Link","text":"<p>Click on the Copy Check Link icon(represented by share icon) located at the right corner of the summary section to copy a direct link to the selected check. This link can be shared with other users for quick access to the specific check within the platform.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#favorite-the-check","title":"Favorite the check","text":"<p>Click on the bookmark icon located at the right corner of the summary section to mark the check as favorite.</p> <p></p> <p>To unmark a check, simply click on the bookmark icon of the marked check. This will remove it from your favorites.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#observability-section","title":"Observability Section","text":"<p>Observability provides a visual overview of how a check performs over time by tracking assertion results. It helps identify trends, failures, or anomalies using daily status indicators across a selected timeframe.</p> <p></p> <p>Users can hover over any date in the timeline. It provides a comprehensive view of assertion statuses, including passed, failed, and anomalous results. By hovering over a specific date, users can access detailed information such as the result status, the number of asserted records, and any anomalies identified. Highlighting all available status types ensures a clearer understanding of the data quality over time.</p> <p></p> <p>Additionally, clicking the Latest Assertion Scan button (e.g., #48151) will navigate users directly to the Scan Results page for that specific assertion.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#selecting-report-date-and-timeframe","title":"Selecting Report Date and Timeframe","text":"<p>The Observability section helps you monitor how your check assertion metrics change over time. You can customize the view by selecting a specific report date and timeframe to analyze trends over different periods.</p>"},{"location":"datastore-checks/checks-datastore/#select-the-report-date","title":"Select the Report Date","text":"<p>Step 1: Locate the Report Date field at the top-right of the Observability section.</p> <p></p> <p>Step 2: Click on the calendar icon. A date picker will appear. Select the desired report date to update the Assertion Over Time graph accordingly.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#choose-the-timeframe","title":"Choose the Timeframe","text":"<p>Step 1: Locate the Timeframe field at the top-right of the Observability section.</p> <p></p> <p>Step 2: Choose a timeframe for your assertion data view:</p> <ul> <li> <p>Week \u2013 Shows assertion metrics distributed over a 7-day period.</p> </li> <li> <p>Month \u2013 Displays daily or weekly assertions throughout the selected month.</p> </li> <li> <p>Quarter \u2013 Covers a three-month range (e.g., Q1: Jan\u2013Mar, Q2: Apr\u2013Jun), useful for quarterly reporting and insights.</p> </li> <li> <p>Year \u2013 Presents assertion data trends for an entire calendar year, allowing for broad, high-level performance monitoring.</p> </li> </ul> <p></p> <p>Once a timeframe is selected, the Assertion Over Time chart below will automatically adjust to reflect assertion activity within the chosen window.</p>"},{"location":"datastore-checks/checks-datastore/#properties-section","title":"Properties Section","text":"<p>The Properties section explains where this check is applied. In this case, the check is applied to a table called supplier, specifically to the s_comment field of type String. There is no filter added, so the check is applied to all rows in the table. This helps maintain clean and trustworthy data, especially when phone numbers must be unique per customer.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#activity-section","title":"Activity Section","text":"<p>The Activity section displays a chronological history of all actions performed on the quality check, including creation, updates, and automated adjustments. It provides visibility into how the check has evolved over time, capturing the exact configuration, properties, and tags associated with each event.</p> <p></p> <p>You can view the exact version of the check as it existed at that point in time by clicking the check icon on the right side of the activity entry.</p> <p></p> <p>A right side panel will open with the historical configuration of the check.</p> <p></p> <p>The Version At field displays the exact date and time when that version of the check was created. For example, July 8, 2025, at 5:42 AM (GMT+5:30) indicates when the configuration shown was active in the system.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#metadata-section","title":"Metadata Section","text":"<p>Currently, there is no extra metadata added to this check. Metadata can include additional notes or properties, but in this case, it's left blank.</p> <p></p>"},{"location":"datastore-checks/clone-check/","title":"Clone Check","text":"<p>Step 1: Click the vertical ellipsis (\u22ee) next to the check (whether Active or Draft) that you want to clone and select \"Edit\" from the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Between\" check.</p> <p></p> <p>Step 2: A modal window will appear, displaying the check details. Click the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window and select \"Clone\" from the dropdown menu.</p> <p></p> <p>Step 3: After clicking the Clone button, a modal window will appear. This window allows you to adjust the cloned check's details.</p> <p></p> <p>1. If you toggle on the \"Associate with a Check Template\" option, the cloned check will be linked to a specific template.</p> <p></p> <p>Choose a Template from the dropdown menu that you want to associate with the cloned check. The check will inherit properties from the selected template.</p> <ul> <li> <p>Locked: The check will automatically sync with any future updates made to the template, but you won't be able to modify the check's properties directly.</p> </li> <li> <p>Unlocked: You can modify the check, but future updates to the template will no longer affect this check.</p> </li> </ul> <p></p> <p>2. If you toggle off the \"Associate with a Check Template\" option, the cloned check will not be linked to any template, which allows you full control to modify the properties independently.</p> <p></p> <p>Select the appropriate Rule Type for the check from the dropdown menu.</p> <p></p> <p>Step 4: Once you have selected the template or rule type, fill out the remaining check details as required. </p> <p></p> <p>Step 5: After completing all the check details, click the \"Validate\" button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct. It ensures that the check will work as expected by running it against the data without committing any changes.</p> <p></p> <p>If the validation is successful, a green message saying \"Validation Successful\" will appear. </p> <p></p> <p>If the validation fails, a red message saying \"Failed Validation\" will appear. This typically occurs when the check logic or parameters do not match the data properly.</p> <p></p> <p>Step 6: Once you have a successful validation, click the \"Save\" button. The system will save any modifications you've made to the check and create a clone of that check based on your changes.  </p> <p></p> <p>After clicking the \"Save\" button, your check is successfully created, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/delete-checks/","title":"Delete Checks","text":"<p>There are two methods for deleting checks: you can either delete individual checks or delete multiple checks in bulk.</p> <p>Note</p> <p>You can only delete archived checks. If you want to delete an active or draft check, you must first move it to the archive, and then you can delete it.</p> <p>Warning</p> <p>Deleting a check is a one-time action. It cannot be restored after deletion.</p>"},{"location":"datastore-checks/delete-checks/#method-i-delete-specific-check","title":"Method I. Delete Specific Check","text":"<p>Step 1: Click Archived from the navigation bar in the Checks section to view all archived checks.</p> <p></p> <p>Step 2: Locate the check that you want to delete and click the vertical ellipsis (\u22ee) and select Delete from the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Not Null\" check.</p> <p></p> <p>Step 3: A confirmation modal window will appear. Click the Delete button to permanently remove the check from the system.</p> <p></p> <p>After clicking the Delete button, your check is successfully deleted, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/delete-checks/#method-ii-delete-checks-in-bulk","title":"Method II. Delete Checks in Bulk","text":"<p>You can permanently delete multiple checks from the system in one action. This process is irreversible, so it should be used when you are certain that the checks are no longer needed.</p> <p>Step 1: Hover over the archived checks and click the checkbox to select checks in bulk.  </p> <p></p> <p>When multiple checks are selected, an action toolbar appears, displaying the total number of selected checks along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click the vertical ellipsis (\u22ee) and choose \"Delete\" from the dropdown menu to delete the selected checks.  </p> <p></p> <p>Step 3: A confirmation modal window will appear. Click the \"Delete\" button to permanently delete the selected checks.</p> <p></p> <p>After clicking the Delete button, your selected checks will be permanently deleted, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/draft-archived-checks/","title":"Draft Archived Checks","text":"<p>You can draft archived checks individually or in bulk.</p>"},{"location":"datastore-checks/draft-archived-checks/#method-i-draft-specific-check","title":"Method I: Draft Specific Check","text":"<p>Step 1: Navigate to the Archived checks section. Click the vertical ellipsis (\u22ee) next to the archived check you want to move to the draft state and select \"Draft\" from the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Not Null\" check.</p> <p></p> <p>After clicking \"Draft\", the check will be successfully moved to the draft state, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/draft-archived-checks/#method-ii-draft-archived-checks-in-bulk","title":"Method II: Draft Archived Checks in Bulk","text":"<p>Step 1: Hover over the archived checks and click the checkbox to select multiple checks in bulk.</p> <p></p> <p>When multiple checks are selected, an action toolbar appears, displaying the total number of checks chosen along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click the vertical ellipsis (\u22ee) and select \"Draft\" from the dropdown menu to move archived checks to the draft state.</p> <p></p> <p>Step 3: A confirmation modal window \"Bulk Update Checks to Draft\" will appear. Click the \"Update\" button to move the selected archived checks to draft.</p> <p></p> <p>After clicking the \"Update\" button, your archived checks will be moved to draft, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/draft-checks/","title":"Draft Checks","text":"<p>There are two methods to move your active checks to draft: you can either draft specific checks or draft multiple checks in bulk.</p>"},{"location":"datastore-checks/draft-checks/#method-i-draft-specific-check","title":"Method I: Draft Specific Check","text":"<p>Step 1: Click the vertical ellipsis (\u22ee) next to the active check you want to move to the draft state and select \"Edit\" from the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Between\" check.</p> <p></p> <p>Step 2: A modal window will appear displaying the check details. Click the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window and select \"Draft\" from the dropdown menu.</p> <p></p> <p>After clicking \"Draft\", the selected item will move to the draft state, and a success message will appear on the screen.</p> <p>Alternatively, you can move an active check to the draft state by clicking the vertical ellipsis (\u22ee) next to the check and selecting \u201cDraft\u201d from the dropdown menu.</p> <p></p>"},{"location":"datastore-checks/draft-checks/#method-ii-draft-checks-in-bulk","title":"Method II. Draft Checks in Bulk","text":"<p>You can move multiple checks into the draft state in one action, allowing you to pause or make adjustments to several checks without affecting your active validation process.</p> <p>Step 1: Hover over the active checks and click the checkbox to select multiple checks.</p> <p></p> <p>Step 2: Click the vertical ellipsis (\u22ee) and select \"Draft\" from the dropdown menu to move active checks to the draft state.</p> <p></p> <p>A confirmation modal window titled \"Bulk Update Checks to Draft\" will appear, indicating the number of checks being moved to draft. </p> <p></p> <p>Step 3: Click the \"Update\" button to move the selected active checks to draft.</p> <p></p> <p>After clicking the \"Update\" button, your selected checks will be moved to draft, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/dry-run/","title":"Dry Run","text":"<p>Step 1: Click the check you want to test using the Dry Run feature.</p> <p></p> <p>Step 2: Click the Settings icon located at the top-right corner of the interface and select \u201cDry Run\u201d from the dropdown menu.</p> <p></p> <p>A modal window titled Dry Run Results will appear.</p> <p></p> <p>This window enables you to confidently evaluate and refine data quality checks before running full scans, helping maintain high-quality standards without unnecessary noise or misconfiguration.</p> <p></p> No. Field Description 1 Status Indicates whether the dry run completed successfully. 2 Timing Displays the total time taken to execute the dry run. 3 Sampling Limit Shows the number of records sampled during the dry run (default is 10K records). 4 Check ID and Name The unique identifier and name of the data quality check. This provides both a reference ID and a descriptive label indicating the rule type. 5 Description A concise explanation of the check rule being tested. For example, \u201cPS_SUPPLYCOST is greater than PS_AVAILQTY.\u201d 6 Table The name of the table on which the check is being applied. 7 Field The specific column or field within the table that the rule targets."},{"location":"datastore-checks/dry-run/#anomalies","title":"Anomalies","text":"<p>Highlights any violations detected during the dry run, such as constraint breaches or unexpected value patterns.</p> <p></p> No. Field Description 1 Violation Clearly states the reason for failure. This message helps users quickly understand what went wrong and why the data didn't pass the quality check. 2 Asserted Records Displays the total number of records evaluated in the dry run. 3 Anomalous Records Shows how many of those records violated the constraint logic. <p></p>"},{"location":"datastore-checks/dry-run/#source-records","title":"Source Records","text":"<p>The Source Records section presents a detailed, tabular view of all records that were evaluated by the selected quality check. This section is designed to help users investigate the underlying data issues that may have led to anomalies, offering clear visibility into the records that failed to meet the defined constraint.</p> <p></p>"},{"location":"datastore-checks/dry-run/#sort-options","title":"Sort Options","text":"<p>Users can sort the records based on different fields using the Sort By dropdown.</p> <p></p> No. Sort By Description 1 Name Sorts the records alphabetically based on the field name. 2 Weight Sorts records based on the weight or severity of the failure. Higher-weighted issues appear first. 3 Quality Score Sorts records by their quality score, helping you prioritize records with the lowest data quality."},{"location":"datastore-checks/dry-run/#download-source-records","title":"Download Source Records","text":"<p>The Download Source Records option allows users to export the records evaluated during the Dry Run process for further offline analysis or documentation purposes. A file containing the asserted records and their anomaly status will be downloaded in CSV format.</p> <p></p> <p>Note</p> <p>When no issues are detected, users receive a clear confirmation message indicating no anomalies were identified.</p> <p>Info</p> <p>You can perform a Dry Run on draft checks to validate the logic before they are finalized and published.</p>"},{"location":"datastore-checks/edit-checks/","title":"Edit Check","text":"<p>There are two methods for editing checks: you can either edit specific checks or edit multiple checks in bulk.</p> <p>Note</p> <p>When editing multiple checks in bulk, only the filter clause, tags, and metadata can be modified. </p>"},{"location":"datastore-checks/edit-checks/#method-i-edit-specific-check","title":"Method I. Edit Specific Check","text":"<p>Step 1: Click the vertical ellipsis (\u22ee) next to the check you want to edit whether it is an active or draft check, and select \"Edit\" from the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Between\" check.</p> <p></p> <p>A modal window will appear with the check details. </p> <p></p> <p>Step 2: Modify the check details as needed based on your requirements.</p> <p></p> <p>Step 3: Once you have edited the check details, click the \"Validate\" button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct.</p> <p></p> <p>If the validation is successful, a green message saying \"Validation Successful\" will appear. </p> <p></p> <p>If the validation fails, a red message saying \"Failed Validation\" will appear. This typically occurs when the check logic or parameters do not match the data properly.</p> <p></p> <p>Step 4: Once you have a successful validation, click the \"Update\" button. The system will update the changes you've made to the check, including changes to the fields, filter clause, coverage, description, tags, or metadata.</p> <p></p> <p>After clicking the \"Update\" button, your check is successfully updated, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/edit-checks/#method-ii-edit-checks-in-bulk","title":"Method II. Edit Checks in Bulk","text":"<p>You can easily apply changes to multiple checks at once, saving time by editing several checks simultaneously without having to modify each one individually.</p> <p>Step 1: Hover over the checks (whether Active or Draft) and click the checkbox to select multiple checks.</p> <p></p> <p>When multiple checks are selected, an action toolbar appears, displaying the total number of selected checks along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click the vertical ellipsis (\u22ee) and select \"Edit\" from the dropdown menu to make changes to the selected checks.</p> <p></p> <p>Step 3: A modal window titled \"Bulk Edit Checks\" will appear. Here you can only modify the filter clause, tags, and metadata of the selected checks.</p> <p></p> <p>Step 4: Toggle on the options (Filter Clause, Tags, or Additional Metadata) that you want to modify for the selected checks, and make the necessary changes.</p> <p>Note</p> <p>This action will overwrite the existing data for the selected checks.</p> <p></p> <p>Step 5: Once you have made the changes, click the \"Save\" button.</p> <p></p> <p>After clicking the \"Save\" button, your selected checks will be updated with the new changes, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/filter-and-sort/","title":"Filter and Sort","text":"<p>You can easily organize your checks using the available sort and filter options.</p>"},{"location":"datastore-checks/filter-and-sort/#sort","title":"Sort","text":"<p>You can sort your checks by Active Anomalies, Coverage, Created Date, Last Asserted, Rules, and Weight to easily organize and prioritize them according to your needs.</p> <p></p> No Sort By Option Description 1 Active Anomalies Sort checks based on the number of active anomalies. 2 Coverage Sort checks by data coverage percentage. 3 Created Date Sort checks according to the date they were created. 4 Last Asserted Sorts by the last time the check was executed. 5 Rules Sort checks based on specific rules applied to the checks. 6 Weight Sort checks by their assigned weight or importance level. <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.</p> <p></p>"},{"location":"datastore-checks/filter-and-sort/#filter","title":"Filter","text":"<p>You can filter your checks based on values like Check Type, Asserted State, Rule, Tags, Table, Field, and Template.</p> <p></p> No Filter Filter Value Description 1 Check Type All Displays all types of checks, both inferred and authored. Inferred Shows system-generated checks that automatically validate data based on detected patterns or logic. Authored Displays user-created checks, allowing the user to focus on custom validations tailored to specific requirements. 2 Asserted State All Displays all checks, regardless of their asserted status. This provides a full overview of both passed, failed, and not asserted checks. Passed Shows checks that have been asserted successfully, meaning no active anomalies were found during the validation process. Failed Displays checks that have failed assertion, indicating active anomalies or issues that need attention. Not Asserted Filters out checks that have not yet been asserted, either because they haven\u2019t been processed or validated yet. 3 Rule N/A Select this to filter the checks based on a specific rule type for data validation, such as checking non-null values, matching patterns, comparing numerical ranges, or verifying date-time constraints. By clicking on the caret down button next to the Rule field, the available rule types will be dynamically populated based on the rule types present in the results. The rules displayed are based on the current dataset and provide more granular control over filtering. Each rule type will show a counter next to it, displaying the total number of occurrences for that rule in the dataset. For example, the rule type After Date Time is displayed with a total of 2 occurrences. <p></p> No Filter Filter Value Description 4 Tag N/A Tag Filter displays only the tags associated with the currently visible items, along with their color icon, name, type, and the number of matching records. Selecting one or more tags refines the list based on your selection. If no matching items are found, a 'No options found' message is displayed. 5 Table N/A Filters checks by the table to which they are applied. 6 Field N/A Filters checks by the specific field/column name within a table. 7 Template N/A This filter allows users to view and apply predefined check templates."},{"location":"datastore-checks/overview/","title":"Overview","text":"<p>Managing your checks within a datastore is important to maintain data integrity and ensure quality. You can categorize, create, update, archive, restore, delete, and clone checks, making it easier to apply validation rules across the datastores. The system allows for checks to be set as active, draft, or archived based on their current state of use. You can also define reusable templates for quality checks to streamline the creation of multiple checks with similar criteria. With options for important and favorite, users have full flexibility to manage data quality efficiently.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"datastore-checks/overview/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and select the datastore from the left menu on which you want to manage your checks.</p> <p></p> <p>Step 2: Click \"Checks\" from the navigation tab.</p> <p></p> <p>You will be navigated to the Checks section within the selected datastore. Here, you can view checks categorized as Active, Draft, Archived (including Invalid and Discarded), or All.</p> <p></p>"},{"location":"datastore-checks/overview/#status-management-of-checks","title":"Status Management of Checks","text":""},{"location":"datastore-checks/overview/#set-check-as-draft","title":"Set Check as Draft","text":"<p>You can move an active check into a draft state, allowing you to work on the check, make adjustments, and refine the validation rules without affecting live data. This is useful when you need to temporarily deactivate a check for review and updates.</p> <p>For more information on how to set a check as draft, please refer to the Draft Checks documentation.</p>"},{"location":"datastore-checks/overview/#activate-draft-check","title":"Activate Draft Check","text":"<p>You can activate the draft checks after you have worked on the check, made adjustments, and refined the validation rules. Activating the draft check and making it live ensures that the defined criteria are enforced on the data.</p> <p>For more information on how to activate a draft check, please refer to the Activate Draft Check documentation.</p>"},{"location":"datastore-checks/overview/#set-check-as-archived","title":"Set Check as Archived","text":"<p>You can move an active or draft check into the archive when it is no longer relevant but may still be needed for historical purposes or future use. Archiving helps keep your checks organized without permanently deleting them.</p> <p>For more information on how to set a check as archived, please refer to the Archive Checks documentation.</p>"},{"location":"datastore-checks/overview/#activate-archived-checks","title":"Activate Archived Checks","text":"<p>You can activate archived checks when you need to restore previously defined validation rules. This is useful if a check was archived temporarily and is now relevant again for data quality enforcement.</p> <p>For more information on how to activate archived checks, please refer to the Activate Archived Checks documentation.</p>"},{"location":"datastore-checks/overview/#draft-archived-checks","title":"Draft Archived Checks","text":"<p>You can move archived checks to the draft state when you want to update or refine them before activation. This is useful if a check is no longer archived but needs adjustments before going live.</p> <p>For more information on how to draft archived checks, please refer to the Draft Archived Checks documentation.</p>"},{"location":"datastore-checks/overview/#restore-archived-checks","title":"Restore Archived Checks","text":"<p>If a check has been archived, then you can restore it back to an active state or in a draft state. This allows you to reuse your checks that were previously archived without having to recreate them from scratch.</p> <p>For more information on how to restore archived checks, please refer to the Restore Archived Checks documentation.</p>"},{"location":"datastore-checks/overview/#edit-check","title":"Edit Check","text":"<p>You can edit an existing check to modify its properties, such as the rule type, coverage, filter clause, or description. Updating a check ensures that it stays aligned with evolving data requirements and maintains data quality as conditions change.</p> <p>For more information on how to edit a check, please refer to the Edit Check documentation.</p>"},{"location":"datastore-checks/overview/#delete-checks","title":"Delete Checks","text":"<p>You can delete a check permanently, removing it from the system, and this is an irreversible action. Once you delete it, the check cannot be restored. By deleting the check, you ensure it will no longer appear in active or archived lists, making the system more streamlined and organized.</p> <p>For more information on how to delete checks, please refer to the Delete Checks documentation.</p>"},{"location":"datastore-checks/overview/#dry-run","title":"Dry Run","text":"<p>The Dry Run feature allows users to simulate the behavior of a Data Quality Check before enforcing it during a scan. This helps validate the check logic and preview potential anomalies without persisting the results or affecting any data.</p> <p>For more information on dry run, please refer to the Dry Run documentation.</p>"},{"location":"datastore-checks/overview/#clone-check","title":"Clone Check","text":"<p>You can clone both active and draft checks to create a duplicate copy of an existing check. This is useful when you want to create a new check based on the structure of an existing one, allowing you to make adjustments without affecting the original check.</p> <p>For more information on how to clone a check, please refer to the Clone Check documentation.</p>"},{"location":"datastore-checks/overview/#create-a-quality-check-template","title":"Create a Quality Check Template","text":"<p>You can add checks as templates, which allows you to create a reusable framework for quality checks. By using templates, you standardize the validation process, enabling the creation of multiple checks with similar rules and criteria across different datastores. This ensures consistency and efficiency in managing data quality checks.</p> <p>For more information on how to create a quality check template, please refer to the Quality Check Template documentation.</p>"},{"location":"datastore-checks/overview/#mark-check-as-favorite","title":"Mark Check as Favorite","text":"<p>Marking a check as a favorite helps you quickly access and prioritize important checks during your data validation process. Favorited checks appear in the \"Favorite\" category, making them easier to manage and monitor.</p> <p>For more information on how to mark a check as favorite, please refer to the Mark Check as Favorite documentation.</p>"},{"location":"datastore-checks/overview/#filter-and-sort","title":"Filter and Sort","text":"<p>Filter and Sort options allow you to organize your checks by various criteria, such as Weight, Active Anomalies, Coverage, Created Date, and Rules. You can also apply filters to refine your list of checks based on Check Type, Asserted State (Passed, Failed, Not Asserted), Tags, Tables, and Fields.</p> <p>For more information on how to filter and sort, please refer to the Filter and Sort documentation.</p>"},{"location":"datastore-checks/overview/#quality-check-migration","title":"Quality Check Migration","text":"<p>Quality Check Migration allows you to transfer authored quality checks from one container to another, even across different datastores. This feature helps you reuse existing quality rules without manually recreating them in the target container. </p> <p>For more information about Quality Check Migration, please refer to the Quality Check Migration documentation.</p>"},{"location":"datastore-checks/quality-check-migration/","title":"Quality Check Migration","text":"<p>Quality Check Migration allows you to transfer authored quality checks from one container to another, even across different datastores. This feature helps you reuse existing quality rules without manually recreating them in the target container. This feature is useful when you want to:</p> <ul> <li>Reuse existing authored quality checks in another container or datastore.  </li> <li>Quickly set up quality checks for similar datasets without starting from scratch.  </li> <li>Standardize quality rules across multiple data stores.</li> </ul> <p>Note</p> <p>Archived and inferred checks are excluded from migration to ensure only active, relevant, authored checks are moved. All migrated checks are set to Draft status, allowing you to review and activate them in the new container.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"datastore-checks/quality-check-migration/#how-it-works","title":"How It Works","text":"<p>The Migrate Quality Checks process consists of two main steps:</p>"},{"location":"datastore-checks/quality-check-migration/#1-select-checks","title":"1. Select Checks","text":"<p>Choose which authored quality checks to migrate:</p> REF. FIELDS DESCRIPTION 1. All Migrates all authored quality checks available in the source container, excluding archived ones. 2. Specific Lets you manually select individual authored quality checks from a list. Useful when you only need certain checks in the target container. 3. Tag Migrates all authored quality checks that match the selected tags, allowing for automated grouping. <p></p>"},{"location":"datastore-checks/quality-check-migration/#2-destination-settings","title":"2. Destination Settings","text":"<p>Define where the selected checks will be migrated:</p> REF. FIELDS DESCRIPTION 1. Source Datastore The datastore where the selected quality checks will be migrated. 2. Table The specific target container (e.g., table) within the datastore where the checks will be added. 3. Assign Additional Tags Lets you add tags to migrated checks to help with categorization and filtering in the target datastore. <p></p> <p>Note</p> <p>Migrated checks are set to Draft. Field(s) will be automatically matched by name when possible; unmatched fields remain unassigned. </p>"},{"location":"datastore-checks/quality-check-migration/#example-use-case","title":"Example Use Case","text":"<p>Scenario</p> <p>You have two tables in the COVID-19 Data datastore:</p> <ul> <li> <p>CDC_INPATIENT_BEDS_ALL \u2013 contains all hospital inpatient bed records.</p> </li> <li> <p>CDC_INPATIENT_BEDS_COVID \u2013 contains only records related to COVID-19 cases.</p> </li> </ul> <p>The first table already has 12 authored quality checks to verify important fields like hospital_id, report_date, and available_beds.</p> <p>The second table doesn\u2019t have these checks yet, but it uses the same structure and fields.</p> <p>Instead of creating all 12 checks again, you migrate them from CDC_INPATIENT_BEDS_ALL to CDC_INPATIENT_BEDS_COVID.</p> <p>This way, both tables follow the same validation rules, saving time and keeping data quality consistent.</p> <p>Before and After Migration</p> Item Before Migration After Migration Source Table CDC_INPATIENT_BEDS_ALL (authored checks already exist). CDC_INPATIENT_BEDS_ALL (unchanged). Destination Table CDC_INPATIENT_BEDS_COVID (no authored checks). CDC_INPATIENT_BEDS_COVID (authored checks in Draft status). Check Count 12 authored checks. 12 authored checks (copied). Status of Checks Active in source. Draft in destination. Benefit N/A Saves time, ensures consistency, and avoids manual recreation."},{"location":"datastore-checks/quality-check-migration/#visual-diagram","title":"Visual Diagram","text":"Flowchart <pre><code>graph TD\nA[Start] --&gt; B[CDC_INPATIENT_BEDS_ALL Source: 12 Authored Checks]\nB --&gt;|Migrate| C[CDC_INPATIENT_BEDS_COVID Destination: Draft Checks]\nC --&gt; D[End]</code></pre>"},{"location":"datastore-checks/quality-check-migration/#tips","title":"Tips","text":"<ul> <li> <p>Review before activation: Migrated checks are saved as Draft, so you can make adjustments before using them.</p> </li> <li> <p>Use tags for tracking: Assign a tag like Migrated_Aug2025 to easily find migrated checks later.</p> </li> <li> <p>Keep field names the same: The system only assigns fields when names match. If a field name differs during migration, it will not be mapped\u2014another reason checks start in Draft status and require user review.</p> </li> </ul>"},{"location":"datastore-checks/quality-check-template/","title":"Create a Quality Check template","text":"<p>Step 1: Locate the check (whether Active or Draft) that you want to convert into a template and click that check.</p> <p>For demonstration purposes, we have selected the \"Between\" check.</p> <p></p> <p>Step 2: A modal window will appear displaying the check details. Click the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window and select \"Template\" from the dropdown menu.</p> <p></p> <p>After clicking the \"Template\" button, the check will be saved and created as a template in the library, and a success message will appear on the screen. This allows you to reuse the template for future checks, streamlining the validation process.</p>"},{"location":"datastore-checks/restore-archived-checks/","title":"Restore Archived Checks","text":"<p>Step 1: Click Archived from the navigation bar in the Checks section to view all archived checks.</p> <p></p> <p>Step 2: Click the archived check that you want to restore.</p> <p></p> <p>Step 3: You will be directed to the check details page. Click the Settings icon located at the top-right corner of the interface and select \u201cEdit\u201d from the dropdown menu.</p> <p></p> <p>A modal window will appear with the check details.</p> <p></p> <p>Step 4: If you want to make any changes to the check, you can edit it. Otherwise, click the Restore button to restore it as an active check.</p> <p></p> <p>To restore the check as a draft, click the arrow icon next to the Restore button. A dropdown menu will appear\u2014select Restore as Draft from the options.</p> <p></p> <p>After clicking the Restore button, the check will be successfully restored as either an active or draft check, depending on your selection, and a success message will appear on the screen.</p>"},{"location":"deployments/databricks-deployment/","title":"Dataplane Deployment Guide for Databricks","text":"<p>This guide will walk you through deploying the Qualytics dataplane in your Databricks environment.</p> <p> </p> Deployment Architecture with Databricks"},{"location":"deployments/databricks-deployment/#prerequisites","title":"Prerequisites","text":"<p>Before starting the deployment, ensure you have: - Databricks CLI installed and configured - Access to your Databricks workspace with job creation permissions</p>"},{"location":"deployments/databricks-deployment/#step-1-create-secrets-scope","title":"Step 1: Create Secrets Scope","text":"<p>First, create a secrets scope to securely store sensitive information:</p> <pre><code>databricks secrets create-scope qualytics\n</code></pre>"},{"location":"deployments/databricks-deployment/#step-2-add-required-secrets","title":"Step 2: Add Required Secrets","text":"<p>Add the following secrets to your Databricks secrets scope:</p>"},{"location":"deployments/databricks-deployment/#rabbitmq-password","title":"RabbitMQ Password","text":"<p><pre><code>databricks secrets put-secret qualytics rabbitmq-password\n</code></pre> When prompted, enter the RabbitMQ password: <code>[RABBIT_PASSWORD_TO_BE_PROVIDED]</code></p>"},{"location":"deployments/databricks-deployment/#docker-hub-token","title":"Docker Hub Token","text":"<p><pre><code>databricks secrets put-secret qualytics docker-token\n</code></pre> When prompted, enter the Docker Hub token: <code>[DOCKER_TOKEN_TO_BE_PROVIDED]</code></p>"},{"location":"deployments/databricks-deployment/#step-3-deploy-the-job","title":"Step 3: Deploy the Job","text":"<p>Create a file named <code>databricks.yml</code> with the following configuration:</p> <pre><code>resources:\n  jobs:\n    QualyticsDataplane:\n      name: QualyticsDataplane\n      continuous:\n        pause_status: PAUSED\n      tasks:\n        - task_key: QualyticsDataplane\n          spark_jar_task:\n            jar_uri: \"\"\n            main_class_name: io.qualytics.dataplane.SparkMothership\n            run_as_repl: true\n          job_cluster_key: QualyticsJobCluster\n          libraries:\n            - jar: file:///opt/qualytics/qualytics-dataplane.jar\n      job_clusters:\n        - job_cluster_key: QualyticsJobCluster\n          new_cluster:\n            spark_version: 17.1.x-scala2.13\n            spark_conf:\n              spark.driver.extraJavaOptions: -Dconfig.resource=prod.conf\n                -Djava.library.path=/databricks/libs\n                --add-opens=java.base/java.lang=ALL-UNNAMED\n                --add-opens=java.base/java.util=ALL-UNNAMED\n                --add-opens=java.base/java.lang.invoke=ALL-UNNAMED\n                --add-opens=java.base/java.nio=ALL-UNNAMED\n                --add-opens=java.base/sun.nio.ch=ALL-UNNAMED\n                --add-opens=java.management/sun.management=ALL-UNNAMED\n                --add-exports=java.management/sun.management=ALL-UNNAMED\n                -Djava.security.manager=allow\n              spark.executor.extraJavaOptions: -Djava.library.path=/databricks/libs\n                --add-opens=java.base/java.lang=ALL-UNNAMED\n                --add-opens=java.base/java.util=ALL-UNNAMED\n                --add-opens=java.base/java.lang.invoke=ALL-UNNAMED\n                --add-opens=java.base/java.nio=ALL-UNNAMED\n                --add-opens=java.base/sun.nio.ch=ALL-UNNAMED\n                --add-opens=java.management/sun.management=ALL-UNNAMED\n                --add-exports=java.management/sun.management=ALL-UNNAMED\n                -Djava.security.manager=allow\n              spark.databricks.r.command: /bin/false\n              spark.executorEnv.PYSPARK_PYTHON: /bin/false\n              spark.executorEnv.PYSPARK_DRIVER_PYTHON: /bin/false\n              spark.databricks.driverNfs.clusterWidePythonLibsEnabled: \"false\"\n              spark.databricks.driverNfs.enabled: \"false\"\n              spark.databricks.sql.externalUDF.env.enabled: \"false\"\n            aws_attributes:\n              first_on_demand: 1\n              availability: SPOT_WITH_FALLBACK\n              zone_id: auto\n              spot_bid_price_percent: 100\n            node_type_id: r6id.2xlarge\n            spark_env_vars:\n              MOTHERSHIP_NUM_CORES_PER_EXECUTOR: \"8\"\n              MOTHERSHIP_MAX_MEMORY_PER_EXECUTOR: \"50000\"\n              MOTHERSHIP_MAX_EXECUTORS: \"20\"\n              MOTHERSHIP_RABBIT_HOST: rabbitmq.us-east-1.elb.amazonaws.com\n              JNAME: zulu21-ca-amd64\n              MOTHERSHIP_RABBIT_USER: user\n              MOTHERSHIP_RABBIT_PASS: \"{{secrets/qualytics/rabbitmq-password}}\"\n            enable_elastic_disk: false\n            docker_image:\n              url: qualyticsai/dataplane-databricks:latest\n              basic_auth:\n                username: qualyticsai\n                password: \"{{secrets/qualytics/docker-token}}\"\n            data_security_mode: DATA_SECURITY_MODE_DEDICATED\n            runtime_engine: PHOTON\n            kind: CLASSIC_PREVIEW\n            is_single_node: false\n            autoscale:\n              min_workers: 1\n              max_workers: 20\n      queue:\n        enabled: true\n</code></pre>"},{"location":"deployments/databricks-deployment/#step-4-deploy-the-configuration","title":"Step 4: Deploy the Configuration","text":"<p>Deploy the job using the Databricks CLI:</p> <pre><code>databricks bundle deploy\n</code></pre>"},{"location":"deployments/databricks-deployment/#step-5-start-the-job","title":"Step 5: Start the Job","text":"<p>Once deployed, you can start the job from the Databricks UI or using the CLI:</p> <pre><code>databricks jobs start --job-id &lt;job-id&gt;\n</code></pre>"},{"location":"deployments/databricks-deployment/#configuration-notes","title":"Configuration Notes","text":"<ul> <li>RabbitMQ Connection: The dataplane connects to <code>rabbitmq.us-east-1.elb.amazonaws.com</code> with user <code>user</code></li> <li>Cluster Configuration: Uses <code>r6id.2xlarge</code> instances with autoscaling from 1-20 workers</li> <li>Docker Image: Uses <code>qualyticsai/dataplane-databricks:latest</code> with provided authentication</li> </ul>"},{"location":"deployments/databricks-deployment/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues:</p> <ul> <li>Verify secrets are properly configured: <code>databricks secrets list-secrets qualytics</code></li> <li>Check job logs in the Databricks UI</li> <li>Ensure the JAR file is uploaded to the correct location</li> <li>Verify network connectivity to the RabbitMQ endpoint</li> </ul>"},{"location":"deployments/databricks-deployment/#support","title":"Support","text":"<p>For additional support or questions, please contact the Qualytics team.</p>"},{"location":"deployments/overview/","title":"Deployment Options","text":""},{"location":"deployments/overview/#introduction","title":"Introduction","text":"<p>This document serves as a primer for organizations looking to decide which deployment model of Qualytics is right for them. It provides an overview of the two primary deployment models and considerations when making this decision.</p>"},{"location":"deployments/overview/#overview","title":"Overview","text":"<p>The following two deployment models are supported for the Qualytics platform:</p> <ul> <li>Model 1: Platform as a Service Deployment: to a single-tenant virtual private cloud (VPC) provisioned by Qualytics on infrastructure that Qualytics manages</li> <li>Model 2: Customer-Managed Deployment: to a CNCF compliant Kubernetes control plane on Customer managed infrastructure (including on-premises options)</li> </ul>"},{"location":"deployments/overview/#across-both-models-the-following-is-true","title":"Across both models, the following is true:","text":"<ul> <li>Raw customer data is not stored at-rest but derivative data and select values may be held in the dedicated VPC</li> <li>READ access is required to connect a datastore to the Qualytics Platform</li> <li>WRITE access to Customer chosen datastore is required for enrichment data</li> </ul>"},{"location":"deployments/overview/#databricks-deployment","title":"Databricks Deployment","text":"<p>For organizations using Databricks as their data processing platform, Qualytics provides a specialized dataplane deployment option. This allows the Qualytics dataplane to run directly within your Databricks environment as a continuous job.</p> <p>For detailed instructions on setting up the Qualytics dataplane in Databricks, see our Databricks Deployment Guide.</p>"},{"location":"deployments/overview/#model-1-platform-as-a-service-paas-deployment","title":"Model 1: Platform as a Service (PaaS) Deployment","text":""},{"location":"deployments/overview/#overview_1","title":"Overview","text":"<p>In this model, the Qualytics platform is deployed to a single-tenant virtual private cloud provisioned by Qualytics and with the provider and in the region of Customer's choosing. This VPC is not shared (single-tenant) and contains a single Customer Qualytics deployment.</p> <p> </p> PaaS Deployment Architecture"},{"location":"deployments/overview/#supported-cloud-providers","title":"Supported Cloud Providers","text":"<p>Depending on Customer's cloud infrastructure, this option uses one of the following:</p> <ul> <li>EKS (Elastic Kubernetes Service)</li> <li>AKS (Azure Kubernetes Service)</li> <li>GKE (Google Kubernetes Engine)</li> <li>Oracle OKE (Oracle Container Engine for Kubernetes)</li> </ul>"},{"location":"deployments/overview/#network-requirements","title":"Network Requirements","text":"<p>This model requires that the provisioned VPC have the ability to access Customer's datastore(s). In the case of publicly routable datastores such as Snowflake or S3, no extra configuration is required. In the case of private datastore(s) with no public IP address or route, the hosted VPC will require private routing using: PrivateLink, Transit Gateway peering, point to point VPN, or similar support to enable network access to that private datastore.</p>"},{"location":"deployments/overview/#considerations","title":"Considerations","text":"<p>This is Qualytics' preferred model of deployment. In this model, Qualytics is fully responsible for the provisioning and operation of the Qualytics platform. Customer is only responsible for granting the Qualytics platform necessary access.</p>"},{"location":"deployments/overview/#model-2-customer-managed-deployment","title":"Model 2: Customer-Managed Deployment","text":""},{"location":"deployments/overview/#overview_2","title":"Overview","text":"<p>In this model, the Qualytics platform is deployed to a CNCF compliant Kubernetes control plane on Customer managed infrastructure, which can include on-premises deployments. This chart will deploy a single-tenant instance of the qualytics platform to a CNCF compliant kubernetes control plane.</p> <p> </p> Customer-Managed Deployment Architecture"},{"location":"deployments/overview/#system-requirements","title":"System Requirements","text":"<p>This option supports deployments to any Kubernetes control plane that meets the following system requirements:</p> <ul> <li>Any Kubernetes version that is officially supported for patches running any CNCF compliant control plane</li> <li>A minimum 16 cores and 80 gigabytes of memory available for workload allocation</li> <li>Assigned a Customer resolvable fully-qualified domain name for the https ingress to the Qualytics UI</li> <li>(optional) Grant Qualytics an admin-level ServiceAccount to the cluster for pushing automated upgrades</li> </ul>"},{"location":"deployments/overview/#network-requirements_1","title":"Network Requirements","text":"<p>This model requires that the Kubernetes nodes supporting Qualytics' analytics engine have the ability to access Customer's datastore(s). Because Customer hosts the Qualytics deployment, Customer is solely responsible for ensuring the necessary network configuration and support.</p>"},{"location":"deployments/overview/#considerations_1","title":"Considerations","text":"<p>This model supports organizations that due to regulatory or other restrictions cannot permit READ access to their datastore(s) from a third-party hosted product. This model requires Customer to manage and operate the appropriate infrastructure and ensure it is granted all necessary access to the targeted datastore(s).</p> <p>For deployments to supported commercial Kubernetes control planes (EKS, AKS, GKE, OKE) and at the Customer's discretion, Qualytics will provision the deployment and transfer ownership of the applicable infrastructure to the Customer. Otherwise, the Customer shall be responsible for both the provisioning of a cluster meeting the requisite system requirements and the deployment of the Qualytics platform via Qualytics provided Helm chart.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/","title":"Filename Globbing","text":""},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#overview","title":"Overview","text":"<p>Our data quality product offers a sophisticated feature that facilitates the organization and categorization of files on a distributed filesystem. This feature, known as Multi-Token Filename Globbing, enables the system to recursively scan files and intelligently group them based on shared filename conventions. It achieves this through a combination of filename pattern analysis and globbing techniques.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#process","title":"Process","text":"<p>Delimiter Identification: The first step involves identifying a common delimiter in filenames, such as an underscore (_) or dash (-). This delimiter is used to split the filenames into tokens. Tokenization and Grouping: Once the filenames are tokenized, the system groups them based on shared tokens. This is achieved through a method called applyMultiTokenGlobbing. Glob Pattern Formation: The core of this feature lies in forming glob patterns that represent groups of files sharing a schema. These patterns are created using the tokens derived from the filenames.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#methodology","title":"Methodology","text":"<ul> <li>Initial Token Grouping: The method begins by grouping filenames based on each token. It considers the number of tokens and processes each token index separately.</li> <li> <p>Left or Right Side Grouping Decision: The system decides whether to group tokens starting from the left side or the right side of the filename, based on the distribution of tokens.</p> </li> <li> <p>Pattern Creation Logic:</p> </li> <li> <p>For filenames with a single token, the system avoids globbing and keeps the filenames as they are.</p> </li> <li>For multi-token filenames, the method constructs a container name (glob pattern) by iterating through each token.</li> <li> <p>At each token, the method decides whether to include the token as-is or replace it with a wildcard (*). This decision is based on several factors, such as:</p> <ul> <li>The uniqueness of the token in the context of other filenames.</li> <li>The nature of the token (e.g., all letters).</li> <li>The comparison of token counts in adjacent indexes.</li> </ul> </li> <li> <p>Special Cases Handling: The method includes logic to handle special cases, such as all-letter tokens, tokens at the beginning or end of a filename, and unique tokens.</p> </li> <li>Glob Pattern Optimization: Finally, the system optimizes the glob patterns, ensuring that each pattern uniquely represents a group of files with a shared schema. This is done by comparing new patterns with existing ones and updating them based on the latest file modifications.</li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#detailed-methodology-multi-token-filename-globbing","title":"Detailed Methodology: Multi-Token Filename Globbing","text":""},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#step-by-step-process","title":"Step-by-Step Process","text":""},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#delimiter-identification-and-tokenization","title":"Delimiter Identification and Tokenization","text":"<p>The system identifies a common delimiter in the filenames, typically an underscore (_) or dash (-), and splits the filenames into tokens.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#token-grouping-and-indexing","title":"Token Grouping and Indexing","text":"<ul> <li>Each token in a filename is indexed (0, 1, 2, ...).</li> <li>Filenames are grouped based on the value of tokens at each index.</li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#determining-grouping-strategy","title":"Determining Grouping Strategy","text":"<ul> <li>The system decides whether to group tokens from the left (start of filename) or right (end of filename) based on the distribution and variation of tokens at each index.</li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#pattern-creation-logic","title":"Pattern Creation Logic","text":"<ul> <li>Single-Token Filenames: No globbing is applied to filenames with only one token.</li> <li>Multi-Token Filenames: The method constructs glob patterns by analyzing each token. It considers factors like token uniqueness, commonality, and special cases like all-letter tokens.</li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#uniqueness-vs-commonality","title":"Uniqueness vs. Commonality:","text":"<ul> <li>Unique tokens (unique in their position across all filenames) are replaced with a wildcard \"*\".</li> <li>Common tokens across many files are kept as they are in the pattern.</li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#special-considerations-for-all-letter-tokens","title":"Special Considerations for All-Letter Tokens:","text":"<ul> <li>Tokens comprising entirely of letters are often grouped together, unless they are unique identifiers.</li> <li>Tokens at the start or end of a filename are treated with contextual logic, considering their potential roles (like identifiers or file types).</li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#adjacent-token-group-sizes","title":"Adjacent Token Group Sizes:","text":"<p>The method compares the group sizes of adjacent tokens to determine if a token leads to a tighter grouping, influencing whether it's kept as literal or replaced with a wildcard.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#constructing-container-names-glob-patterns","title":"Constructing Container Names (Glob Patterns)","text":"<ul> <li> <p>For each token index, the method constructs a container name, deciding whether to include the token as-is or replace it with \"*\".</p> </li> <li> <p>This decision is influenced by factors like the uniqueness of the token, the nature of the token (all letters or not), and the comparison of token counts in adjacent indexes.</p> </li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#optimization-and-finalization","title":"Optimization and Finalization","text":"<ul> <li>The system optimizes the glob patterns to ensure each pattern uniquely represents a group of files with a shared schema.</li> <li>It compares new patterns with existing ones and updates them based on the latest file modifications.</li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#example-scenarios","title":"Example Scenarios","text":"<ul> <li>Filename: <code>\"project_data_2023_v1.csv\"</code><ul> <li>Potential Pattern: <code>\"project_data_*_*.csv\"</code> (if \"2023\" and \"v1\" vary across files).</li> </ul> </li> <li>Filename: <code>\"user_123_profile_2023-06-01.json\"</code><ul> <li>Potential Pattern: <code>\"user_*_profile_*.json\"</code> (if \"123\" and dates vary, and \"user\" and \"profile\" are consistent).</li> </ul> </li> <li>Filename: <code>\"log2023-06_error.txt\"</code><ul> <li>Potential Pattern: <code>\"*_error.txt\"</code> (if dates vary but \"error\" is a constant token).</li> </ul> </li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#limitations","title":"Limitations","text":""},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#context","title":"Context","text":"<p>While the Multi-Token Filename Globbing feature is a powerful tool for organizing files in distributed filesystems, including object storage systems like AWS S3, Google Cloud Storage (GCS), and Azure Blob Storage, it's important to understand the limitations of using glob patterns with wildcards in these environments.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#wildcard-mechanics-in-directory-listings","title":"Wildcard Mechanics in Directory Listings","text":""},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#wildcard-character","title":"Wildcard Character (*):","text":"<p>In glob patterns, the asterisk (*) is used as a wildcard that matches any character, any number of times. This flexibility is powerful for grouping a wide range of file patterns but has limitations in precision.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#behavior-in-object-storage-systems","title":"Behavior in Object Storage Systems:","text":"<ul> <li>Systems like AWS S3, GCS, and Azure Blob interpret the wildcard in a glob pattern to match any sequence of characters in a filename.</li> <li>This means a pattern with a wildcard can encompass a broad range of filenames, potentially grouping files that were not intended to be grouped together.</li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#specific-limitation-example","title":"Specific Limitation Example","text":"<p>Consider the following scenario to illustrate this limitation:</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#intended-file-grouping-patterns","title":"Intended File Grouping Patterns:","text":"<ul> <li>Pattern A: <code>project_data_*.txt</code></li> <li>Pattern B: <code>project_data_*_*.txt</code></li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#example-filenames","title":"Example Filenames:","text":"<ul> <li><code>project_data_1234.txt</code></li> <li><code>project_data_1234_suffix.txt</code></li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#limitation-in-practice","title":"Limitation in Practice:","text":"<ul> <li> <p>In this case, Pattern A (<code>project_data_*.txt</code>) is intended to match files like project_data_1234.txt. However, due to the nature of the wildcard, this pattern will also inadvertently match <code>project_data_1234_suffix.txt</code>.</p> </li> <li> <p>The wildcard in Pattern A extends to any length of characters following project_data_, making it impossible to exclusively group files that strictly follow the <code>project_data_1234.txt</code> format without including those with additional suffixes like <code>project_data_1234_suffix.txt</code>.</p> </li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#addressing-the-limitations","title":"Addressing the Limitations:","text":"<p>Understanding the inherent limitations of glob patterns, particularly when dealing with wildcards in object storage systems, is crucial for effective file management. </p> <p>When users encounter scenarios where filenames within a folder are incompatible due to these limitations, several practical options are available.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#ensure-appropriate-file-grouping","title":"Ensure appropriate file grouping:","text":""},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#separation-into-distinct-folders","title":"Separation into Distinct Folders:","text":"<p>One effective strategy is to organize files with conflicting name formats into separate folders. </p> <p>By doing so, the resultant glob patterns within each folder will be distinct and won\u2019t overlap, ensuring precise file grouping.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#leveraging-folder-globbing-feature","title":"Leveraging Folder-Globbing Feature:","text":"<p>For added flexibility, users can also utilize our folder-globbing feature. </p> <p>This feature simplifies the grouping process by aggregating all files in the same folder, regardless of their filename patterns. This approach is particularly useful in scenarios where filename-based grouping is less critical or when dealing with a wide variety of filename formats within the same directory.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#customized-filename-conventions","title":"Customized Filename Conventions:","text":"<p>Users are encouraged to adopt filename conventions that align better with the capabilities and limitations of glob patterns. By designing filenames with clear, distinct segments and predictable structures, users can more effectively leverage the globbing feature for accurate file categorization.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#conclusion","title":"Conclusion","text":"<p>The Multi-Token Filename Globbing feature stands out as a powerful and efficient tool for organizing and categorizing files within a distributed filesystem. </p> <p>By astutely analyzing filename patterns and forming optimized glob patterns, this feature significantly streamlines the process of managing files that share common schemas, thereby elevating the overall data quality and accessibility within the system.</p>"},{"location":"enrichment/api-payload-examples/","title":"API Payload Examples","text":""},{"location":"enrichment/api-payload-examples/#retrieving-enrichment-datastore-tables","title":"Retrieving Enrichment Datastore Tables","text":""},{"location":"enrichment/api-payload-examples/#endpoint-get","title":"Endpoint (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/listing</code> (get)</p> Example result response <pre><code>    [\n        {\n            \"name\":\"_datastore_prefix_scan_operations\",\n            \"label\":\"scan_operations\",\n            \"datastore\":{\n                \"id\":123,\n                \"name\":\"My Datastore\",\n                \"store_type\":\"jdbc\",\n                \"type\":\"postgresql\",\n                \"enrich_only\":false,\n                \"enrich_container_prefix\":\"_datastore_prefix\",\n                \"favorite\":false\n            }\n        },\n        {\n            \"name\":\"_datastore_prefix_source_records\",\n            \"label\":\"source_records\",\n            \"datastore\":{\n                \"id\":123,\n                \"name\":\"My Datastore\",\n                \"store_type\":\"jdbc\",\n                \"type\":\"postgresql\",\n                \"enrich_only\":false,\n                \"enrich_container_prefix\":\"_datastore_prefix\",\n                \"favorite\":false\n            }\n        },\n        {\n            \"name\":\"_datastore_prefix_failed_checks\",\n            \"label\":\"failed_checks\",\n            \"datastore\":{\n                \"id\":123,\n                \"name\":\"My Datastore\",\n                \"store_type\":\"jdbc\",\n                \"type\":\"postgresql\",\n                \"enrich_only\":false,\n                \"enrich_container_prefix\":\"_datastore_prefix\",\n                \"favorite\":false\n            }\n        },\n        {\n            \"name\": \"_datastore_prefix_remediation_container_id\",\n            \"label\": \"table_name\",\n            \"datastore\": {\n                \"id\": 123,\n                \"name\": \"My Datastore\",\n                \"store_type\": \"jdbc\",\n                \"type\": \"postgresql\",\n                \"enrich_only\": false,\n                \"enrich_container_prefix\": \"_datastore_prefix\",\n                \"favorite\": false\n            }\n        }\n    ]\n</code></pre>"},{"location":"enrichment/api-payload-examples/#retrieving-enrichment-datastore-source-records","title":"Retrieving Enrichment Datastore Source Records","text":""},{"location":"enrichment/api-payload-examples/#endpoint-get_1","title":"Endpoint (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?path={_source-record-table-prefix}</code> (get)</p>"},{"location":"enrichment/api-payload-examples/#endpoint-with-filters-get","title":"Endpoint With Filters (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?filter=anomaly_uuid='{uuid}'&amp;path={_source-record-table-prefix}</code> (get)</p> Example result response <pre><code>    {\n        \"source_records\": \"[{\\\"source_container\\\":\\\"table_name\\\",\\\"source_partition\\\":\\\"partition_name\\\",\\\"anomaly_uuid\\\":\\\"f11d4e7c-e757-4bf1-8cd6-d156d5bc4fa5\\\",\\\"context\\\":null,\\\"record\\\":\\\"{\\\\\\\"P_NAME\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"strategize intuitive systems\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"P_TYPE\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"Radiographer, therapeutic\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"P_RETAILPRICE\\\\\\\":\\\\\\\"-24.69\\\\\\\",\\\\\\\"LAST_MODIFIED_TIMESTAMP\\\\\\\":\\\\\\\"2023-09-29 11:17:19.048\\\\\\\",\\\\\\\"P_MFGR\\\\\\\":null,\\\\\\\"P_COMMENT\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"Other take so.\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"P_PARTKEY\\\\\\\":\\\\\\\"845004850\\\\\\\",\\\\\\\"P_SIZE\\\\\\\":\\\\\\\"4\\\\\\\",\\\\\\\"P_CONTAINER\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"MED BOX\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"P_BRAND\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"PLC\\\\\\\\\\\\\\\"\\\\\\\"}\\\"}]\"\n    }\n</code></pre>"},{"location":"enrichment/api-payload-examples/#retrieving-enrichment-datastore-remediation","title":"Retrieving Enrichment Datastore Remediation","text":""},{"location":"enrichment/api-payload-examples/#endpoint-get_2","title":"Endpoint (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?path={_remediation-table-prefix}</code> (get)</p>"},{"location":"enrichment/api-payload-examples/#endpoint-with-filters-get_1","title":"Endpoint With Filters (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?filter=anomaly_uuid='{uuid}'&amp;path={_remediation-table-prefix}</code> (get)</p> Example result response <pre><code>    {\n        \"source_records\": \"[{\\\"source_container\\\":\\\"table_name\\\",\\\"source_partition\\\":\\\"partition_name\\\",\\\"anomaly_uuid\\\":\\\"f11d4e7c-e757-4bf1-8cd6-d156d5bc4fa5\\\",\\\"context\\\":null,\\\"record\\\":\\\"{\\\\\\\"P_NAME\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"strategize intuitive systems\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"P_TYPE\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"Radiographer, therapeutic\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"P_RETAILPRICE\\\\\\\":\\\\\\\"-24.69\\\\\\\",\\\\\\\"LAST_MODIFIED_TIMESTAMP\\\\\\\":\\\\\\\"2023-09-29 11:17:19.048\\\\\\\",\\\\\\\"P_MFGR\\\\\\\":null,\\\\\\\"P_COMMENT\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"Other take so.\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"P_PARTKEY\\\\\\\":\\\\\\\"845004850\\\\\\\",\\\\\\\"P_SIZE\\\\\\\":\\\\\\\"4\\\\\\\",\\\\\\\"P_CONTAINER\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"MED BOX\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"P_BRAND\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"PLC\\\\\\\\\\\\\\\"\\\\\\\"}\\\"}]\"\n    }\n</code></pre>"},{"location":"enrichment/api-payload-examples/#retrieving-enrichment-datastore-failed-checks","title":"Retrieving Enrichment Datastore Failed Checks","text":""},{"location":"enrichment/api-payload-examples/#endpoint-get_3","title":"Endpoint (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?path={_failed-checks-table-prefix}</code> (get)</p>"},{"location":"enrichment/api-payload-examples/#endpoint-with-filters-get_2","title":"Endpoint With Filters (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?filter=anomaly_uuid='{uuid}'&amp;path={_failed-checks-table-prefix}</code> (get)</p> Example result response <pre><code>    {\n        \"source_records\": \"[{\\\"quality_check_id\\\":155481,\\\"anomaly_uuid\\\":\\\"1a937875-6bce-4bfe-8701-075ba66be364\\\",\\\"quality_check_message\\\":\\\"{\\\\\\\"SNPSHT_TIMESTAMP\\\\\\\":\\\\\\\"2023-09-03 10:26:15.0\\\\\\\"}\\\",\\\"suggested_remediation_field\\\":null,\\\"suggested_remediation_value\\\":null,\\\"suggested_remediation_score\\\":null,\\\"quality_check_rule_type\\\":\\\"greaterThanField\\\",\\\"quality_check_tags\\\":\\\"Time-Sensitive\\\",\\\"quality_check_parameters\\\":\\\"{\\\\\\\"field_name\\\\\\\":\\\\\\\"SNPSHT_DT\\\\\\\",\\\\\\\"inclusive\\\\\\\":false}\\\",\\\"quality_check_description\\\":\\\"Must have a value greater than the value of SNPSHT_DT\\\",\\\"operation_id\\\":28162,\\\"detected_time\\\":\\\"2024-03-29T15:08:07.585Z\\\",\\\"source_container\\\":\\\"ACTION_TEST_CLIENT_V3\\\",\\\"source_partition\\\":\\\"ACTION_TEST_CLIENT_V3\\\",\\\"source_datastore\\\":\\\"DB2 Dataset\\\"}]\"\n    }\n</code></pre>"},{"location":"enrichment/api-payload-examples/#retrieving-enrichment-datastore-scan-operations","title":"Retrieving Enrichment Datastore Scan Operations","text":""},{"location":"enrichment/api-payload-examples/#endpoint-get_4","title":"Endpoint (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?path={_scan-operations-table-prefix}</code> (get)</p>"},{"location":"enrichment/api-payload-examples/#endpoint-with-filters-get_3","title":"Endpoint With Filters (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?filter=operation_id='{operation-id}'&amp;path={_scan-operations-table-prefix}</code> (get)</p> Example result response <pre><code>    {\n        \"source_records\": \"[{\\\"operation_id\\\":22871,\\\"datastore_id\\\":850,\\\"container_id\\\":7239,\\\"container_scan_id\\\":43837,\\\"partition_name\\\":\\\"ACTION_TEST_CLIENT_V3\\\",\\\"incremental\\\":true,\\\"records_processed\\\":0,\\\"enrichment_source_record_limit\\\":10,\\\"max_records_analyzed\\\":-1,\\\"anomaly_count\\\":0,\\\"start_time\\\":\\\"2023-12-04T20:35:54.194Z\\\",\\\"end_time\\\":\\\"2023-12-04T20:35:54.692Z\\\",\\\"result\\\":\\\"success\\\",\\\"message\\\":null}]\"\n    }\n</code></pre>"},{"location":"enrichment/api-payload-examples/#retrieving-enrichment-datastore-exported-metadata","title":"Retrieving Enrichment Datastore Exported Metadata","text":""},{"location":"enrichment/api-payload-examples/#endpoint-get_5","title":"Endpoint (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?path={_export-metadata-table-prefix}</code> (get)</p>"},{"location":"enrichment/api-payload-examples/#endpoint-with-filters-get_4","title":"Endpoint With Filters (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?filter=container_id='{container-id}'&amp;path={_export-metadata-table-prefix}</code> (get)</p> Example result of export anomalies responseExample result of export checks responseExample result of field profiles response <pre><code>    {\n        \"source_records\": \"[{\\\"container_id\\\":13511,\\\"created\\\":\\\"2024-06-10T17:07:20.751438Z\\\",\\\"datastore_id\\\":1198,\\\"generated_at\\\":\\\"2024-06-11 18:42:31+0000\\\",\\\"global_tags\\\":\\\"\\\",\\\"id\\\":224818,\\\"source_container\\\":\\\"PARTSUPP-FORMATTED.csv\\\",\\\"source_datastore\\\":\\\"TPCH GCS\\\",\\\"status\\\":\\\"Active\\\",\\\"type\\\":\\\"shape\\\",\\\"uuid\\\":\\\"f2d4fae3-982b-45a1-b289-5854b7af4b03\\\"}]\"\n    }\n</code></pre> <pre><code>    {\n        \"source_records\": \"[{\\\"additional_metadata\\\":null,\\\"container_id\\\":13515,\\\"coverage\\\":1.0,\\\"created\\\":\\\"2024-06-10T16:27:05.600041Z\\\",\\\"datastore_id\\\":1198,\\\"deleted_at\\\":null,\\\"description\\\":\\\"Must have a numeric value above &gt;= 0\\\",\\\"fields\\\":\\\"L_QUANTITY\\\",\\\"filter\\\":null,\\\"generated_at\\\":\\\"2024-06-11 18:42:38+0000\\\",\\\"global_tags\\\":\\\"\\\",\\\"has_passed\\\":false,\\\"id\\\":196810,\\\"inferred\\\":true,\\\"is_new\\\":false,\\\"is_template\\\":false,\\\"last_asserted\\\":\\\"2024-06-11T18:04:24.480899Z\\\",\\\"last_editor\\\":null,\\\"last_updated\\\":\\\"2024-06-10T17:07:43.248644Z\\\",\\\"num_container_scans\\\":4,\\\"properties\\\":null,\\\"rule_type\\\":\\\"notNegative\\\",\\\"source_container\\\":\\\"LINEITEM-FORMATTED.csv\\\",\\\"source_datastore\\\":\\\"TPCH GCS\\\",\\\"template_id\\\":null,\\\"weight\\\":7.0}]\"\n    }\n</code></pre> <pre><code>    {\n        \"source_records\": \"[{\\\"approximate_distinct_values\\\":106944.0,\\\"completeness\\\":0.7493389459,\\\"container_container_type\\\":\\\"file\\\",\\\"container_id\\\":13509,\\\"created\\\":\\\"2024-06-10T16:23:48.457907Z\\\",\\\"datastore_id\\\":1198,\\\"datastore_type\\\":\\\"gcs\\\",\\\"entropy\\\":null,\\\"field_global_tags\\\":\\\"\\\",\\\"field_id\\\":145476,\\\"field_name\\\":\\\"C_ACCTBAL\\\",\\\"field_profile_id\\\":882170,\\\"field_quality_score\\\":\\\"{\\\\\\\"total\\\\\\\": 81.70052209952111, \\\\\\\"completeness\\\\\\\": 74.93389459101233, \\\\\\\"coverage\\\\\\\": 66.66666666666666, \\\\\\\"conformity\\\\\\\": null, \\\\\\\"consistency\\\\\\\": 100.0, \\\\\\\"precision\\\\\\\": 100.0, \\\\\\\"timeliness\\\\\\\": null, \\\\\\\"volumetrics\\\\\\\": null, \\\\\\\"accuracy\\\\\\\": 100.0}\\\",\\\"field_type\\\":\\\"Fractional\\\",\\\"field_weight\\\":1,\\\"generated_at\\\":\\\"2024-06-11 18:42:32+0000\\\",\\\"histogram_buckets\\\":null,\\\"is_not_normal\\\":true,\\\"kll\\\":null,\\\"kurtosis\\\":-1.204241522,\\\"max\\\":9999.99,\\\"max_length\\\":null,\\\"mean\\\":4488.8079264033,\\\"median\\\":4468.34,\\\"min\\\":-999.99,\\\"min_length\\\":null,\\\"name\\\":\\\"C_ACCTBAL\\\",\\\"q1\\\":1738.87,\\\"q3\\\":7241.17,\\\"skewness\\\":0.0051837205,\\\"source_container\\\":\\\"CUSTOMER-FORMATTED.csv\\\",\\\"source_datastore\\\":\\\"TPCH GCS\\\",\\\"std_dev\\\":3177.3005493585,\\\"sum\\\":5.0501333575999904E8,\\\"type_declared\\\":false,\\\"unique_distinct_ratio\\\":null}]\"\n    }\n</code></pre>"},{"location":"enrichment/delete-enrichment/","title":"Delete Enrichment","text":"<p>Step 1: Click on the Delete icon.</p> <p></p> <p>A modal window Delete Enrichment Datastore will appear.</p> <p></p> <p>When deleting an enrichment datastore, the confirmation dialog displays the number of linked source datastores. </p> <p></p> <p>Step 2: Enter the name of the enrichment datastore in the given field (confirmation check) and then click on the I\u2019M SURE, DELETE THIS ENRICHMENT DATASTORE button to delete the enrichment datastore.</p> <p></p> <p>After clicking the I\u2019M SURE, DELETE THIS ENRICHMENT DATASTORE button, a success notification appears confirming the deletion.</p>"},{"location":"enrichment/edit-enrichment/","title":"Edit Enrichment","text":"<p>Step 1: Click on the Edit option.</p> <p></p> <p>Step 2: After selecting the Edit option, a modal window will appear, displaying the connection details. This window allows you to modify any specific connection details.</p> <p></p> <p>Step 3: After editing the connection details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided connection details are verified, a success message will be displayed indicating that the connection has been verified.</p> <p>Step 4: Click on the Save button.</p> <p></p> <p>After clicking the Save button, a success notification appears on the screen showing the action was completed successfully.</p>"},{"location":"enrichment/enrichment-actions/","title":"Enrichment Actions","text":"<p>Enrichment Actions in Qualytics help you manage enrichment datastores efficiently\u2014whether you're adding a new source, updating existing settings, or removing. These actions keep your enrichment workflows accurate, current, and easy to maintain.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"enrichment/enrichment-actions/#navigation","title":"Navigation","text":"<p>Log in to your Qualytics account and click the Enrichment Datastores button on the left side panel of the interface.</p> <p></p>"},{"location":"enrichment/enrichment-actions/#add-enrichment","title":"Add Enrichment","text":"<p>Use this action to create a new enrichment datastore by entering details such as name, connector type, and authentication credentials.</p> <p>For more information, refer to the Add Enrichment Documentation.</p>"},{"location":"enrichment/enrichment-actions/#edit-delete-enrichment","title":"Edit &amp; Delete Enrichment","text":"<p>Use Edit to update existing configuration details, and Delete to permanently remove a datastore and its linked components when no longer needed.</p> <p>Step 1: Select the specific enrichment datastore you want to edit or delete.</p> <p></p> <p>Step 2: Click the Settings icon located at the top right corner of the interface, then choose Edit or Delete depending on the action you want to perform.</p> <p></p>"},{"location":"enrichment/enrichment-actions/#edit","title":"Edit","text":"<p>Use this action to modify an existing enrichment datastore\u2014update its connection details or reconfigure any required fields.</p> <p>For more information, refer to the Edit Enrichment Documentation.</p>"},{"location":"enrichment/enrichment-actions/#delete-enrichment","title":"Delete Enrichment","text":"<p>Use this action to permanently delete an enrichment datastore that is no longer required.</p> <p>For more information, refer to the Delete Enrichment Documentation.</p>"},{"location":"enrichment/enrichment-datastore-creation/","title":"Add Enrichment Datastore","text":"<p>Step 1: Click on the Add Enrichment Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window- Add Enrichment Datastore will appear, providing you with the options to add enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name Specify the name of the enrichment datastore 2. Toggle Button Toggle ON to create a new enrichment datastore from scratch, or toggle OFF to reuse credentials from an existing connection 3. Connector Select connector from the dropdown list."},{"location":"enrichment/enrichment-datastore-creation/#option-i-add-enrichment-datastore-with-a-new-connection","title":"Option I: Add Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using existing connection details.</p> <p>Step 1: Select the connector from the dropdown list and add connection details such as Secrets Management, temp dataset ID, service account key, project ID, and dataset ID.</p> <p>For demonstration purposes we have selected the Snowflake connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.  </p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form, requesting credential details before add the enrichment datastore.</p> <p>Note</p> <p>Different connectors have different sets of fields and options appearing when selected. </p> <p></p> REF FIELDS ACTIONS 1. Account (Required) Define the account identifier to be used for accessing the Snowflake. 2. Role (Required) Specify the user role that grants appropriate access and permissions. 3. Warehouse (Required) Provide the warehouse name that will be used for computing resources. 4. Authentication (Required) You can choose between Basic authentication or Keypair authentication for validating and securing the connection to your Snowflake instance.  Basic Authentication: This method uses a username and password combination for authentication. It is a straightforward method where the user's credentials are directly used to access Snowflake. <ul><li>Type: Select the authentication type from the dropdown menu.</li><li>User: Enter the username that Qualytics will use to connect to Snowflake.</li><li>Password: Enter the password associated with the specified user account.</li></ul> Keypair Authentication: This method uses a combination of a private key and a corresponding public key for authentication. This is a more secure method compared to basic authentication, as it involves asymmetric cryptography <ul> <li>Type: Select \"Keypair\" from the dropdown menu.</li><li>User: Enter the username that Qualytics will use to connect to Snowflake.</li> <li>Private Key: Upload the private key file that will be used for authentication. This key is part of a public-private key pair used to securely authenticate the user.</li> <li>Private Key Password (Optional): Enter the password associated with the private key, if any </li> </ul> 5. Database Specify the database name to be accessed. 6. Schema Define the schema within the database that should be used. 7. Teams Select one or more teams from the dropdown to associate with this source datastore. <p>Step 3: After adding the details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p> <p>Step 4: Click on the Save button.</p> <p></p> <p>A modal window appears and shows a success message that the enrichment was updated successfully.</p> <p>Step 5: Close the success dialog. Here, you can view a list of all the enrichment datastores you have added to the system. For demonstration purposes, we have created an enrichment datastore named Snowflake_demo, which is visible in the list.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-creation/#option-ii-use-an-existing-enrichment-datastore","title":"Option II: Use an Existing Enrichment Datastore","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to add and configure the enrichment datastore using existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, and Teams.  </p> <p></p> <p>Step 2: After adding the details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p> <p>Step 3:  Click on the Save button.</p> <p></p> <p>A modal window appears and shows a success message that the enrichment was updated successfully.</p> <p>Step 4: Close the success dialog. Here, you can view a list of all the enrichment datastores you have added to the system. For demonstration purposes, we have created an enrichment datastore named Snowflake_demo, which is visible in the list.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/","title":"Data Preview","text":"<p>Data Preview in Qualytics makes it simple to explore data tables and fields within a selected enrichment dataset. It supports filtering, field selection, and record downloads for deeper analysis, ensuring streamlined and efficient data management.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"enrichment/enrichment-datastore-preview/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the Enrichment Datastores button on the left side panel of the interface.</p> <p></p> <p>Step 2: You will see a list of available enrichment datastores. Click on the specific datastore you want to preview its details and data.</p> <p></p> <p>For Demonstration purposes, we have selected Netsuite Financials Enrich enrichment datastore.</p> <p>Step 3: After clicking on your selected enrichment datastore, you will be able to preview its enrichment, export, materialize, remediation, including all data tables, and unlinked data tables.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#data-preview-tab","title":"Data Preview Tab","text":"<p>Data Preview Tab provides a clear visualization of enriched dataset tables and fields like  <code>_FAILED_CHECKS</code>, <code>_SOURCE_RECORDS</code>, and <code>_SCAN_OPERATIONS</code>. Users can explore remediation data, export data, materialized datasets, and unlinked objects, refine data with filters, select specific fields, and download records for further analysis. This tab ensures efficient data review and management for enhanced insights.</p>"},{"location":"enrichment/enrichment-datastore-preview/#all","title":"All","text":"<p>By selecting All, users can access a comprehensive list of data tables associated with the selected enrichment datastore. This includes all relevant tables categorized under Enrichment, Remediation, Export, Materialize, and Unlinked sections, enabling users to efficiently explore and manage the data. Click on a specific table or dataset within the All section to access its detailed information.</p> <p></p> <p>After clicking on a specific table or dataset, a detailed view opens, displaying fields such as <code>_FAILED_CHECKS</code>, <code>_SOURCE_RECORDS</code>, <code>_SCAN_OPERATIONS</code>, remediation tables (e.g., <code>_ENRICHMENT_CONTAINER_PREFIX_REMEDIATION_CONTAINER_ID</code>), exported tables, materialized outputs, and unlinked objects (orphaned data) for review and action.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#enrichment","title":"Enrichment","text":"<p>By selecting Enrichment users can access a comprehensive view of the table or data associated with the selected enrichment datastore. Click on specific table or dataset within the Enrichment section to access its detailed information.</p> <p></p> <p>After clicking on a specific table or dataset, a detailed view opens, displaying fields of the selected table or dataset.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#remediation","title":"Remediation","text":"<p>By selecting Remediation users can access a comprehensive view of the table or data associated with the selected enrichment datastore. Click on specific table or dataset within the Remediation section to access its detailed information.</p> <p></p> <p>After clicking on a table or dataset, a detailed view opens, displaying all the fields and data associated with the selected table or dataset.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#export","title":"Export","text":"<p>By selecting Export, users can access a comprehensive view of the exported tables or data associated with the selected enrichment datastore. Click on a specific table or dataset within the Export section to access its detailed information.</p> <p></p> <p>After clicking on a table or dataset, a detailed view opens, displaying all the fields and data associated with the selected table or dataset.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#materialize","title":"Materialize","text":"<p>By selecting Materialize, users can access a comprehensive view of the materialized tables or data associated with the selected enrichment datastore. Click on a specific table or dataset within the Materialize section to access its detailed information.</p> <p></p> <p>After clicking on a table or dataset, a detailed view opens, displaying all the fields and data associated with the selected table or dataset.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#unlinked","title":"Unlinked","text":"<p>By selecting Unlinked users can access a comprehensive view of the table or data associated with the selected enrichment datastore. Click on specific table or dataset within the Unlinked section to access its detailed information.</p> <p></p> <p>After clicking on a table or dataset, a detailed view opens, displaying all the fields and data associated with the selected table or dataset.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#filter-clause-and-refresh","title":"Filter Clause and Refresh","text":"<p>Data Preview tab includes a filter functionality that enables users to focus on specific fields by applying filter clauses. This refines the displayed rows based on specific criteria, enhancing data analysis and providing more targeted insights and a Refresh button to update the data view with the latest data.</p>"},{"location":"enrichment/enrichment-datastore-preview/#filter-clause","title":"Filter Clause","text":"<p>Use the Filter Clause to narrow down the displayed rows by applying specific filter clauses, allowing for focused and precise data analysis.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#refresh","title":"Refresh","text":"<p>Click Refresh button to update the data view with the latest information, ensuring accuracy and relevance.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#select-specific-fields","title":"Select Specific Fields","text":"<p>Select specific fields to display, allowing you to focus on the most relevant data for analysis.To focus on relevant data for analysis, click on the Select Fields to Show dropdown. Choose specific fields you want to review by checking or unchecking options.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#download-records","title":"Download Records","text":"<p>Download Records feature in Qualytics allows users to easily export all source records from the selected enrichment dataset. This functionality is essential for performing deeper analysis outside the platform or for sharing data with external tools and teams.</p> <p></p>"},{"location":"enrichment/enrichment-tables/","title":"Enrichment Tables","text":"<p>When anomalies are detected, the platform writes metadata into four primary enrichment tables:</p> <ul> <li>&lt;enrichment_prefix&gt;_check_metrics</li> <li>&lt;enrichment_prefix&gt;_failed_checks</li> <li>&lt;enrichment_prefix&gt;_source_records</li> <li>&lt;enrichment_prefix&gt;_scan_operations</li> </ul>"},{"location":"enrichment/enrichment-tables/#_check_metrics_table","title":"_CHECK_METRICS_Table","text":"<p>Captures and logs detailed metrics for every data quality check performed within the Qualytics Platform, providing insights into asserted and anomalous records across datasets.</p> <p>Columns</p> Name Data Type Description OPERATION_ID NUMBER Unique Identifier for the check metric. CONTAINER_ID NUMBER Identifier for the container associated with the check metric. SOURCE_DATASTORE STRING Datastore where the source data resides. SOURCE_CONTAINER STRING Name of the source data container. SOURCE_PARTITION STRING Partition of the source data. ASSERTION_RESULT STRING Result of the check assertion: one of <code>passed</code>, <code>failed</code>, or <code>unasserted</code>. ASSERTION_DETAILS STRING Text description explaining any warnings, errors, or notes from the check. QUALITY_CHECK_ID NUMBER Unique identifier for the quality check performed. ASSERTED_RECORDS_COUNT NUMBER Count of records expected or asserted in the source. ANOMALOUS_RECORDS_COUNT NUMBER Count of records identified as anomalous. _QUALYTICS_SOURCE_PARTITION STRING Partition information specific to Qualytics metrics."},{"location":"enrichment/enrichment-tables/#_failed_checks-table","title":"_FAILED_CHECKS Table","text":"<p>Acts as an associative entity that consolidates information on failed checks, associating anomalies with their respective quality checks.</p> <p>Columns</p> Name Data Type Description QUALITY_CHECK_ID NUMBER Unique identifier for the quality check. ANOMALY_UUID STRING UUID for the anomaly detected. QUALITY_CHECK_MESSAGE STRING Message describing the quality check outcome. SUGGESTED_REMEDIATION_FIELD STRING Field suggesting remediation. SUGGESTED_REMEDIATION_VALUE STRING Suggested value for remediation. SUGGESTED_REMEDIATION_SCORE FLOAT Score indicating confidence in remediation. QUALITY_CHECK_RULE_TYPE STRING Type of rule applied for quality check. QUALITY_CHECK_TAGS STRING Tags associated with the quality check. QUALITY_CHECK_PARAMETERS STRING Parameters used for the quality check. QUALITY_CHECK_DESCRIPTION STRING Description of the quality check. OPERATION_ID NUMBER Identifier for the operation detecting anomaly. DETECTED_TIME TIMESTAMP Timestamp when the anomaly was detected. SOURCE_CONTAINER STRING Name of the source data container. SOURCE_PARTITION STRING Partition of the source data. SOURCE_DATASTORE STRING Datastore where the source data resides. FINGERPRINT INTEGER Unique identifier created when Reactivate Recurring Anomalies is enabled. <p>Info</p> <p>This table is not characterized by unique <code>ANOMALY_UUID</code> or <code>QUALITY_CHECK_ID</code> values alone. Instead, the combination of <code>ANOMALY_UUID</code> and <code>QUALITY_CHECK_ID</code> serves as a composite key, uniquely identifying each record in the table.</p>"},{"location":"enrichment/enrichment-tables/#_source_records-table","title":"_SOURCE_RECORDS Table","text":"<p>Stores source records in JSON format, primarily to enable the preview source record feature in the Qualytics App.</p> <p>Columns</p> Name Data Type Description SOURCE_CONTAINER STRING Name of the source data container. SOURCE_PARTITION STRING Partition of the source data. ANOMALY_UUID STRING UUID for the associated anomaly. CONTEXT STRING Contextual information for the anomaly. RECORD STRING JSON representation of the source record."},{"location":"enrichment/enrichment-tables/#_scan_operations-table","title":"_SCAN_OPERATIONS Table","text":"<p>Captures and stores the results of every scan operation conducted on the Qualytics Platform.</p> <p>Columns</p> Name Data Type Description OPERATION_ID NUMBER Unique identifier for the scan operation. DATASTORE_ID NUMBER Identifier for the source datastore associated with the operation. CONTAINER_ID NUMBER Identifier for the container associated with the operation. CONTAINER_SCAN_ID NUMBER Identifier for the container scan associated with the operation. PARTITION_NAME STRING Name of the source partition on which the scan operation is performed. INCREMENTAL BOOLEAN Boolean flag indicating whether the scan operation is incremental. RECORDS_PROCESSED NUMBER Total number of records processed during the scan operation. ENRICHMENT_SOURCE_RECORD_LIMIT NUMBER Maximum number of records written to the enrichment for each anomaly detected. MAX_RECORDS_ANALYZED NUMBER Maximum number of records analyzed in the scan operation. ANOMALY_COUNT NUMBER Total number of anomalies identified in the scan operation. START_TIME TIMESTAMP Timestamp marking the start of the scan operation. END_TIME TIMESTAMP Timestamp marking the end of the scan operation. RESULT STRING Textual representation of the scan operation's status. MESSAGE STRING Detailed message regarding the process of the scan operation."},{"location":"enrichment/metadata-tables/","title":"Metadata Tables","text":"<p>The Qualytics platform enables users to manually export metadata into the enrichment datastore, providing a structured approach to data analysis and management. These metadata tables are structured to reflect the evolving characteristics of data entities, primarily focusing on aspects that are subject to changes.</p> <p>Currently, the following assets are available for exporting:</p> <ul> <li>_&lt;enrichment_prefix&gt;_export_anomalies</li> <li>_&lt;enrichment_prefix&gt;_export_checks</li> <li>_&lt;enrichment_prefix&gt;_export_field_profiles</li> </ul> <p>Note</p> <p>The strategy used for managing these metadata tables employs a <code>create or replace</code> approach, meaning that the export process will create a new table if one does not exist, or replace it entirely if it does. This means that any previous data will be overwritten.</p> <p>For more detailed information on exporting metadata, please refer to the export operation documentation</p>"},{"location":"enrichment/metadata-tables/#_export_anomalies-table","title":"_EXPORT_ANOMALIES Table","text":"<p>Contains metadata from anomalies in a distinct normalized format. This table is specifically designed to capture the mutable states of anomalies, emphasizing their status changes.</p> <p>Columns</p> Name Data Type Description ID NUMBER Unique identifier for the anomaly. CREATED TIMESTAMP Timestamp of anomaly creation. UUID UUID Universal Unique Identifier of the anomaly. TYPE STRING Type of the anomaly (e.g., 'shape'). STATUS STRING Current status of the anomaly (e.g., 'Active'). GLOBAL_TAGS STRING Tags associated globally with the anomaly. CONTAINER_ID NUMBER Identifier for the associated container. SOURCE_CONTAINER STRING Name of the source container. DATASTORE_ID NUMBER Identifier for the associated datastore. SOURCE_DATASTORE STRING Name of the source datastore. GENERATED_AT TIMESTAMP Timestamp when the export was generated."},{"location":"enrichment/metadata-tables/#_export_checks-table","title":"_EXPORT_CHECKS Table","text":"<p>Contains metadata from quality checks.</p> <p>Columns</p> Name Data Type Description ADDITIONAL_METADATA STRING JSON-formatted string containing additional metadata for the check. COVERAGE FLOAT Represents the expected tolerance of the rule. CREATED STRING Created timestamp of the check. DELETED_AT STRING Deleted timestamp of the check. DESCRIPTION STRING Description of the check. FIELDS STRING Fields involved in the check separated by comma. FILTER STRING Criteria used to filter data when asserting the check. GENERATED_AT STRING Indicates when the export was generated. GLOBAL_TAGS STRING Represents the global tags of the check separated by comma. HAS_PASSED BOOLEAN Boolean indicator of whether the check has passed its last assertion . ID NUMBER Unique identifier for the check. INFERRED BOOLEAN Indicates whether the check was inferred by the platform. IS_NEW BOOLEAN Flags if the check is new. LAST_ASSERTED STRING Timestamp of the last assertion performed on the check. LAST_EDITOR STRING Represents the last editor of the check. LAST_UPDATED STRING Represents the last updated timestamp of the check. NUM_CONTAINER_SCANS NUMBER Number of containers scanned. PROPERTIES STRING Specific properties for the check in a JSON format. RULE_TYPE STRING Type of rule applied in the check. WEIGHT FLOAT Represents the weight of the check. DATASTORE_ID NUMBER Identifier of the datastore used in the check. CONTAINER_ID NUMBER Identifier of the container used in the check. TEMPLATE_ID NUMBER Identifier of the template id associated to the check. IS_TEMPLATE BOOLEAN Indicates whether the check is a template or not. SOURCE_CONTAINER STRING Name of the container used in the check. SOURCE_DATASTORE STRING Name of the datastore used in the check."},{"location":"enrichment/metadata-tables/#_export_check_templates-table","title":"_EXPORT_CHECK_TEMPLATES Table","text":"<p>Contains metadata from check templates.</p> <p>Columns</p> Name Data Type Description ADDITIONAL_METADATA STRING JSON-formatted string containing additional metadata for the check. COVERAGE FLOAT Represents the expected tolerance of the rule. CREATED STRING Created timestamp of the check. DELETED_AT STRING Deleted timestamp of the check. DESCRIPTION STRING Description of the check. FIELDS STRING Fields involved in the check separated by comma. FILTER STRING Criteria used to filter data when asserting the check. GENERATED_AT STRING Indicates when the export was generated. GLOBAL_TAGS STRING Represents the global tags of the check separated by comma. ID NUMBER Unique identifier for the check. IS_NEW BOOLEAN Flags if the check is new. IS_TEMPLATE BOOLEAN Indicates whether the check is a template or not. LAST_EDITOR STRING Represents the last editor of the check. LAST_UPDATED STRING Represents the last updated timestamp of the check. PROPERTIES STRING Specific properties for the check in a JSON format. RULE_TYPE STRING Type of rule applied in the check. TEMPLATE_CHECKS_COUNT NUMBER The count of associated checks to the template. TEMPLATE_LOCKED BOOLEAN Indicates whether the check template is locked or not. WEIGHT FLOAT Represents the weight of the check."},{"location":"enrichment/metadata-tables/#_export_field_profiles-table","title":"_EXPORT_FIELD_PROFILES Table","text":"<p>Contains metadata from field profiles.</p> <p>Columns</p> Name Data Type Description APPROXIMATE_DISTINCT_VALUES FLOAT Estimated number of distinct values in the field. COMPLETENESS FLOAT Ratio of non-null entries to total entries in the field. CONTAINER_ID NUMBER Identifier for the container holding the field. SOURCE_CONTAINER STRING Name of the container holding the field. CONTAINER_STORE_TYPE STRING Storage type of the container. CREATED STRING Date when the field profile was created. DATASTORE_ID NUMBER Identifier for the datastore containing the field. SOURCE_DATASTORE STRING Name of the datastore containing the field. DATASTORE_TYPE STRING Type of datastore. ENTROPY FLOAT Measure of randomness in the information being processed. FIELD_GLOBAL_TAGS STRING Global tags associated with the field. FIELD_ID NUMBER Unique identifier for the field. FIELD_NAME STRING Name of the field being profiled. FIELD_PROFILE_ID NUMBER Identifier for the field profile record. FIELD_QUALITY_SCORE FLOAT Score representing the quality of the field. FIELD_TYPE STRING Data type of the field. FIELD_WEIGHT NUMBER Weight assigned to the field for quality scoring. GENERATED_AT STRING Date when the field profile was generated. HISTOGRAM_BUCKETS STRING Distribution of data within the field represented as buckets. IS_NOT_NORMAL BOOLEAN Indicator of whether the field data distribution is not normal. KLL STRING Sketch summary of the field data distribution. KURTOSIS FLOAT Measure of the tailedness of the probability distribution. MAX FLOAT Maximum value found in the field. MAX_LENGTH FLOAT Maximum length of string entries in the field. MEAN FLOAT Average value of the field's data. MEDIAN FLOAT Middle value in the field's data distribution. MIN FLOAT Minimum value found in the field. MIN_LENGTH FLOAT Minimum length of string entries in the field. NAME STRING Descriptive name of the field. Q1 FLOAT First quartile in the field's data distribution. Q3 FLOAT Third quartile in the field's data distribution. SKEWNESS FLOAT Measure of the asymmetry of the probability distribution. STD_DEV FLOAT Standard deviation of the field's data. SUM FLOAT Sum of all numerical values in the field. TYPE_DECLARED BOOLEAN Indicator of whether the field type is explicitly declared. UNIQUE_DISTINCT_RATIO FLOAT Ratio of unique distinct values to the total distinct values."},{"location":"enrichment/overview-of-an-enrichment-datastore/","title":"Enrichment Datastore Overview","text":"<p>An Enrichment Datastore is a user-managed storage location where the Qualytics platform records and accesses metadata through a set of system-defined tables. It is purpose-built to capture metadata generated by the platform's profiling and scanning operations.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"enrichment/overview-of-an-enrichment-datastore/#key-points","title":"Key Points","text":"<ul> <li> <p>Metadata Storage: The Enrichment Datastore acts as a dedicated mechanism for writing and retaining metadata that the platform generates. This includes information about anomalies, quality checks, field profiling, and additional details that enrich the source data.</p> </li> <li> <p>Feature Enablement: By using the Enrichment Datastore, the platform unlocks certain features such as the previewing of source records. For instance, when an anomaly is detected, the platform typically previews a limited set of affected records. For a comprehensive view and persistent access, the Enrichment Datastore captures and maintains a complete snapshot of the source records associated with the anomalies.</p> </li> <li> <p>User-Managed Location: While the Qualytics platform handles the generation and processing of metadata, the actual storage is user-managed. This means that the user maintains control over the Enrichment Datastore, deciding where and how this data is stored, adhering to their governance and compliance requirements.</p> </li> <li> <p>Insight and Reporting: Beyond storing metadata, the Enrichment Datastore allows users to derive actionable insights and develop custom reports for a variety of use cases, from compliance tracking to data quality improvement initiatives.</p> </li> </ul>"},{"location":"enrichment/overview-of-an-enrichment-datastore/#navigation","title":"Navigation","text":"<p>Log in to your Qualytics account and click the Enrichment Datastores button on the left side panel of the interface.</p> <p></p>"},{"location":"enrichment/overview-of-an-enrichment-datastore/#table-types","title":"Table Types","text":"<p>The Enrichment Datastore contains several types of tables, each serving a specific purpose in the data enrichment and remediation process. </p> <p>Note</p> <p>For more information, please refer to the table types documentation.</p>"},{"location":"enrichment/overview-of-an-enrichment-datastore/#diagram","title":"Diagram","text":"<p>The diagram below provides a visual representation of the associations between various tables in the Enrichment Datastore. It illustrates how tables can be joined to track and analyze data across different processes.</p> <p> </p>"},{"location":"enrichment/overview-of-an-enrichment-datastore/#handling-json-and-string-splitting","title":"Handling JSON and string splitting","text":"SnowflakePostgreSQLMySQL <pre><code>SELECT\n    PARSE_JSON(ADDITIONAL_METADATA):metadata_1::string AS Metadata1_Key1,\n    PARSE_JSON(ADDITIONAL_METADATA):metadata_2::string AS Metadata2_Key1,\n    PARSE_JSON(ADDITIONAL_METADATA):metadata_3::string AS Metadata3_Key1,\n    -- Add more lines as needed up to MetadataN\n    CONTAINER_ID,\n    COVERAGE,\n    CREATED,\n    DATASTORE_ID,\n    DELETED_AT,\n    DESCRIPTION,\n    SPLIT_PART(FIELDS, ',', 1) AS Field1,\n    SPLIT_PART(FIELDS, ',', 2) AS Field2,\n    -- Add more lines as needed up to FieldN\n    FILTER,\n    GENERATED_AT,\n    SPLIT_PART(GLOBAL_TAGS, ',', 1) AS Tag1,\n    SPLIT_PART(GLOBAL_TAGS, ',', 2) AS Tag2,\n    -- Add more lines as needed up to TagN\n    HAS_PASSED,\n    ID,\n    INFERRED,\n    IS_NEW,\n    IS_TEMPLATE,\n    LAST_ASSERTED,\n    LAST_EDITOR,\n    LAST_UPDATED,\n    NUM_CONTAINER_SCANS,\n    PARSE_JSON(PROPERTIES):allow_other_fields::string AS Property_AllowOtherFields,\n    PARSE_JSON(PROPERTIES):assertion::string AS Property_Assertion,\n    PARSE_JSON(PROPERTIES):comparison::string AS Property_Comparison,\n    PARSE_JSON(PROPERTIES):datetime_::string AS Property_Datetime,\n    -- Add more lines as needed up to Property\n    RULE_TYPE,\n    SOURCE_CONTAINER,\n    SOURCE_DATASTORE,\n    TEMPLATE_ID,\n    WEIGHT\nFROM \"_EXPORT_CHECKS\";\n</code></pre> <pre><code>SELECT\n    (ADDITIONAL_METADATA::json -&gt;&gt; 'metadata_1') AS Metadata1_Key1,\n    (ADDITIONAL_METADATA::json -&gt;&gt; 'metadata_2') AS Metadata2_Key1,\n    (ADDITIONAL_METADATA::json -&gt;&gt; 'metadata_3') AS Metadata3_Key1,\n    -- Add more lines as needed up to MetadataN\n    CONTAINER_ID,\n    COVERAGE,\n    CREATED,\n    DATASTORE_ID,\n    DELETED_AT,\n    DESCRIPTION,\n    (string_to_array(FIELDS, ','))[1] AS Field1,\n    (string_to_array(FIELDS, ','))[2] AS Field2,\n    -- Add more lines as needed up to FieldN\n    FILTER,\n    GENERATED_AT,\n    (string_to_array(GLOBAL_TAGS, ','))[1] AS Tag1,\n    (string_to_array(GLOBAL_TAGS, ','))[2] AS Tag2,\n    -- Add more lines as needed up to TagN\n    HAS_PASSED,\n    ID,\n    INFERRED,\n    IS_NEW,\n    IS_TEMPLATE,\n    LAST_ASSERTED,\n    LAST_EDITOR,\n    LAST_UPDATED,\n    NUM_CONTAINER_SCANS,\n    (PROPERTIES::json -&gt;&gt; 'allow_other_fields') AS Property_AllowOtherFields,\n    (PROPERTIES::json -&gt;&gt; 'assertion') AS Property_Assertion,\n    (PROPERTIES::json -&gt;&gt; 'comparison') AS Property_Comparison,\n    (PROPERTIES::json -&gt;&gt; 'datetime_') AS Property_Datetime,\n    -- Add more lines as needed up to PropertyN\n    RULE_TYPE,\n    SOURCE_CONTAINER,\n    SOURCE_DATASTORE,\n    TEMPLATE_ID,\n    WEIGHT\nFROM \"_EXPORT_CHECKS\";\n</code></pre> <pre><code>SELECT\n    (ADDITIONAL_METADATA-&gt;&gt;'$.metadata_1') AS Metadata1_Key1,\n    (ADDITIONAL_METADATA-&gt;&gt;'$.metadata_2') AS Metadata2_Key1,\n    (ADDITIONAL_METADATA-&gt;&gt;'$.metadata_3') AS Metadata3_Key1,\n    -- Add more lines as needed up to MetadataN\n    CONTAINER_ID,\n    COVERAGE,\n    CREATED,\n    DATASTORE_ID,\n    DELETED_AT,\n    DESCRIPTION,\n    SUBSTRING_INDEX(FIELDS, ',', 1) AS Field1,\n    -- Add more lines as needed up to FieldN\n    SUBSTRING_INDEX(GLOBAL_TAGS, ',', 1) AS Tag1,\n    -- Add more lines as needed up to TagN\n    HAS_PASSED,\n    ID,\n    INFERRED,\n    IS_NEW,\n    IS_TEMPLATE,\n    LAST_ASSERTED,\n    LAST_EDITOR,\n    LAST_UPDATED,\n    NUM_CONTAINER_SCANS,\n    (PROPERTIES-&gt;&gt;'$.allow_other_fields') AS Property_AllowOtherFields,\n    (PROPERTIES-&gt;&gt;'$.assertion') AS Property_Assertion,\n    (PROPERTIES-&gt;&gt;'$.comparison') AS Property_Comparison,\n    (PROPERTIES-&gt;&gt;'$.datetime_') AS Property_Datetime,\n    -- Add more lines as needed up to PropertyN\n    RULE_TYPE,\n    SOURCE_CONTAINER,\n    SOURCE_DATASTORE,\n    TEMPLATE_ID,\n    WEIGHT\nFROM \"_EXPORT_CHECKS\";\n</code></pre>"},{"location":"enrichment/overview-of-an-enrichment-datastore/#usage-notes","title":"Usage Notes","text":"<ul> <li>Both metadata tables and remediation tables, are designed to be ephemeral and thus are recommended to be used as temporary datasets. Users are advised to move this data to a more permanent dataset for long-term storage and reporting.</li> <li>The anomaly UUID in the remediation tables acts as a link to the detailed data in the _anomaly enrichment table. This connection not only shows the number of failed checks but also provides insight into each one, such as the nature of the issue, the type of rule violated, and associated check tags. Additionally, when available, suggested remediation actions, including suggested field modifications and values, are presented alongside a score indicating the suggested action's potential effectiveness. This information helps users to better understand the specifics of each anomaly related to the remediation tables.</li> <li>The Qualytics platform is configured to capture and write a maximum of 10 rows of data per anomaly by default for both the _source_records enrichment table and the remediation tables. To adjust this limit, users can utilize the <code>enrichment_source_record_limit</code> parameter within the Scan Operation settings. This parameter accepts a minimum value of 10 but allows the specification of a higher limit, up to an unrestricted number of rows per anomaly. It is important to note that if an anomaly is associated with fewer than 10 records, the platform will only write the actual number of records where the anomaly was detected.</li> </ul>"},{"location":"enrichment/overview-of-an-enrichment-datastore/#api-payload-examples","title":"API Payload Examples","text":"<p>Note</p> <p>For more information, please refer to the API Payload Example documentation.</p>"},{"location":"enrichment/remediation-tables/","title":"Remediation Tables","text":"<p>When anomalies are detected in a container, the platform has the capability to create remediation tables in the Enrichment Datastore. These tables are detailed snapshots of the affected container, capturing the state of the data at the time of anomaly detection. They also include additional columns for metadata and remediation purposes. However, the creation of these tables depends upon the chosen remediation strategy during the scan operation.</p> <p>Currently, there are three types of remediation strategies:</p> <ul> <li>None: No remediation tables will be created, regardless of anomaly detection.</li> <li>Append: Replicate source containers using an append-first strategy.</li> <li>Overwrite: Replicate source containers using an overwrite strategy.</li> </ul> <p>Note</p> <p>The naming convention for the remediation tables follows the pattern of <code>&lt;enrichment_prefix&gt;_remediation_&lt;container_id&gt;</code>, where <code>&lt;enrichment_prefix&gt;</code> is user-defined during the Enrichment Datastore configuration and <code>&lt;container_name&gt;</code> corresponds to the original source container.</p>"},{"location":"enrichment/remediation-tables/#illustrative-table","title":"Illustrative Table","text":"<p><code>_{ENRICHMENT_CONTAINER_PREFIX}_REMEDIATION_{CONTAINER_ID}</code></p> <p>This remediation table is an illustrative snapshot of the \"Orders\" container for reference purposes.</p> Name Data Type Description _QUALYTICS_SOURCE_PARTITION STRING The partition from the source data container. ANOMALY_UUID STRING Unique identifier of the anomaly. ORDERKEY NUMBER Unique identifier of the order. CUSTKEY NUMBER The customer key related to the order. ORDERSTATUS CHAR The status of the order (e.g., 'F' for 'finished'). TOTALPRICE FLOAT The total price of the order. ORDERDATE DATE The date when the order was placed. ORDERPRIORITY STRING Priority of the order (e.g., 'urgent'). CLERK STRING The clerk who took the order. SHIPPRIORITY INTEGER The priority given to the order for shipping. COMMENT STRING Comments related to the order. <p>Note</p> <p>In addition to capturing the original container fields, the platform includes two metadata columns designed to assist in the analysis and remediation process.</p> <ul> <li>_QUALYTICS_SOURCE_PARTITION</li> <li>ANOMALY_UUID</li> </ul>"},{"location":"enrichment/remediation-tables/#understanding-remediation-tables-vs-source-record-tables","title":"Understanding Remediation Tables vs. Source Record Tables","text":"<p>When managing data anomalies in containers, it's important to understand the structures of Remediation Tables and Source Record Tables in the Enrichment Datastore.</p>"},{"location":"enrichment/remediation-tables/#remediation-tables_1","title":"Remediation Tables","text":"<p>Purpose: Remediation tables are designed to capture detailed snapshots of the affected containers at the time of anomaly detection. They serve as a primary tool for remediation actions.</p> <p>Creation: These tables are generated based on the remediation strategy selected during the scan operation:</p> <ul> <li>None: No tables are created.</li> <li>Append: Tables are created with new data appended.</li> <li>Overwrite: Tables are created and existing data is overwritten.</li> </ul> <p>Structure: The structure includes all columns from the source container, along with additional columns for metadata and remediation purposes. The naming convention for these tables is <code>&lt;enrichment_prefix&gt;_remediation_&lt;container_id&gt;</code>, where <code>&lt;enrichment_prefix&gt;</code> is defined during the Enrichment Datastore configuration.</p>"},{"location":"enrichment/remediation-tables/#source-record-tables","title":"Source Record Tables","text":"<p>Purpose: The Source Record Table is mainly used within the Qualytics App to display anomalies directly to users by showing the source records.</p> <p>Structure: Unlike remediation tables, the Source Record Table stores each record in a JSON format within a single column named <code>RECORD</code>, along with other metadata columns like <code>SOURCE_CONTAINER</code>, <code>SOURCE_PARTITION</code>, <code>ANOMALY_UUID</code>, and <code>CONTEXT</code>.</p>"},{"location":"enrichment/remediation-tables/#key-differences","title":"Key Differences","text":"<ul> <li> <p>Format: Remediation tables are structured with separate columns for each data field, making them easier to use for consulting and remediation processes. </p> <p>Source Record Tables store data in a JSON format within a single column, which can be less convenient for direct data operations.</p> </li> <li> <p>Usage: Remediation tables are optimal for performing corrective actions and are designed to integrate easily with data workflows. </p> <p>Source Record Tables are best suited for reviewing specific anomalies within the Qualytics App due to their format and presentation.</p> </li> </ul>"},{"location":"enrichment/remediation-tables/#recommendation","title":"Recommendation","text":"<p>For users intending to perform querying or need detailed snapshots for audit purposes, Remediation Tables are recommended. </p> <p>For those who need to quickly review anomalies directly within the Qualytics App, Source Record Tables are more suitable due to their straightforward presentation of data.</p>"},{"location":"enrichment/table-types/","title":"Table Types","text":"<p>The Enrichment Datastore contains several types of tables, each serving a specific purpose in the data enrichment and remediation process. These tables are categorized into:</p> <ul> <li>Enrichment Tables</li> <li>Remediation Tables</li> <li>Metadata Tables</li> </ul>"},{"location":"enrichment/table-types/#enrichment-tables","title":"Enrichment Tables","text":"<p>When anomalies are detected, the platform writes metadata into four primary enrichment tables:</p> <p>Note</p> <p>For more information, please refer to the enrichment tables documentation.</p>"},{"location":"enrichment/table-types/#remediation-tables","title":"Remediation Tables","text":"<p>When anomalies are detected in a container, the platform has the capability to create remediation tables in the Enrichment Datastore.</p> <p>Note</p> <p>For more information, please refer to the remediation tables documentation</p>"},{"location":"enrichment/table-types/#metadata-tables","title":"Metadata Tables","text":"<p>The Qualytics platform enables users to manually export metadata into the enrichment datastore, providing a structured approach to data analysis and management.</p> <p>Note</p> <p>For more information, please refer to the metadata tables documentation.</p>"},{"location":"enrichment-support/supported-enrichment-datastores/","title":"Supported Enrichment Datastores","text":"<p>Qualytics supports enrichment datastore connectors that help enhance data discovery, profiling, and quality checks. Some connectors include enrichment capabilities, while others provide only standard connectivity.</p> <p>In this guide, we will cover:</p> <ul> <li>JDBC Connectors </li> <li>DFS Connectors </li> </ul>"},{"location":"enrichment-support/supported-enrichment-datastores/#jdbc-connectors","title":"JDBC Connectors","text":"<p>The table below shows the list of JDBC connectors and whether they support enrichment or not:</p> No. Connector Enrichment Support 01. Athena \u274c 02. Big Query \u2705 03. Databricks \u2705 04. DB2 \u2705 05. Dremio \u274c 06. Hive \u274c 07. MariaDB \u2705 08. Microsoft SQL Server \u2705 09. MySQL \u2705 10. Oracle \u274c 11. PostgreSQL \u2705 12. Presto \u274c 13. Redshift \u2705 14. Snowflake \u2705 15. Synapse \u2705 16. Teradata \u274c 17. TimescaleDB \u274c 18. Trino \u2705"},{"location":"enrichment-support/supported-enrichment-datastores/#dfs-connectors","title":"DFS Connectors","text":"<p>The table below shows the list of DFS connectors and whether they support enrichment or not:</p> No. Connector Enrichment Support 01. Amazon S3 \u2705 02. Azure Datalake Storage (ABFS) \u2705 03. Google Cloud Storage (GCS) \u2705"},{"location":"explore/activity/","title":"Activity","text":"<p>Activity in Qualytics provides a comprehensive view of all operations, helping users monitor and analyze the performance and workflows across various source datastores. Activities are categorized into Runs and Schedule operations, offering distinct insights into executed and scheduled activities.</p> <p>The Rerun and Resume options depend on the type of operation. Profile and Scan support both because the system can remember where it stopped and continue from there. Catalog, Export, and Materialize only support Rerun, since the system can't pick up from where it left off and must start over. External Scan doesn't support either option, as they don't apply to it.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"explore/activity/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the Explore button in the left side panel of the interface.</p> <p></p> <p>Step 2: Click on \"Activity\" from the Navigation Tab.</p> <p></p> <p>You will be navigated to the Activity tab and here you'll see a list of operations catalog, profile, scan, and external scan across different source datastores.</p> <p></p>"},{"location":"explore/activity/#activity-categories","title":"Activity Categories","text":"<p>Activities are divided into two categories: Runs and Schedule Operations. Runs provide insights into operations that have been performed, while Schedule Operations provide insights into scheduled operations.</p> <p></p>"},{"location":"explore/activity/#runs","title":"Runs","text":"<p>Runs provide a complete record of all executed operations across various source datastores. This section enables users to monitor and review activities such as catalog, profile, scan, and external scan. Each run displays key details like the operation type, status, execution time, duration, and triggering method, offering a clear overview of system performance and data processing workflows.</p> <p></p> No. Field Description 1. Select Source Datastore Select specific source datastores to focus on their operations. 2. Search This feature helps users quickly find specific identifiers. 3. Sort By Sort By option helps users organize the list of performed operations by criteria like Duration and Created Date for quick access. 4. Filter The filter lets users easily refine the list of performed operations by choosing a specific Type Scan, Catalog, Profile, External Scan, etc. along with Status (Success, Failure, Running, and Aborted) or Has Logs to view operations that completed with logs. 5. Activity Heatmap The Activity Heatmap shows daily activity levels, with color intensity indicating operation counts. Hovering over a square reveals details for that day. 6. Operation List Shows a list of operations catalog, profile, scan, and external scan, etc performed across various source datastores."},{"location":"explore/activity/#activity-heatmap","title":"Activity Heatmap","text":"<p>The Activity Heatmap represents activity levels over a period, with each square indicating a day and the color intensity representing the number of operations or activities on that day. It is useful for tracking the number of operations performed on each day within a specific timeframe.</p> <p>Note</p> <p>You can click on any of the squares from the Activity Heatmap to filter operations.  </p> <p></p> <p>By hovering over each square, you can view additional details for that specific day, such as the exact date and the total number of operations executed.</p> <p></p>"},{"location":"explore/activity/#operation-details","title":"Operation Details","text":"<p>Step 1: Click on any successfully performed operation from the list to view its details.</p> <p>For demonstration purposes, we have selected the profile operation.</p> <p></p> <p>Step 2: After clicking, a drop-down will appear, displaying the details of the selected operation.</p> <p></p> <p>Step 3: Users can hover over abbreviated metrics to see the full value for better clarity. For demonstration purposes, we are hovering over the Records Profiled field to display the full value.</p> <p></p> <p>Step 4: Users can view the exact completion timestamp of any operation by hovering over the duration label (e.g., Took less than a minute). The tooltip displays the date and time when the operation was completed.</p> <p></p> <p>Users can also view both profiled and non-profiled File Patterns:</p> <p>Step 5: Click on the Result Button.</p> <p></p> <p>The Profile Results modal displays a list of both profiled and non-profiled containers. You can filter the view to show only non-profiled containers by toggling on the button, which will display the complete list of unprofiled containers.</p> <p></p>"},{"location":"explore/activity/#schedule","title":"Schedule","text":"<p>The Schedule section provides a complete record of all scheduled operations across various source datastores. This section enables users to monitor and review scheduled operations such as catalog, profile, and scan. Each scheduled operation includes key details like operation type, scheduled time, and triggering method, giving users a clear overview of system performance and data workflows.</p> <p></p> No. Field Description 1. Selected Source Datastores Select specific source datastores to focus on their operations. 2. Search This feature helps users quickly find specific identifiers. 3. Sort By Sort By option helps users organize the list of scheduled operations by criteria like Created Date and Operations for quick access. 4. Filter The filter lets users easily refine the list of scheduled operations by choosing a specific operation type: Scan, Catalog, Profile, etc. to view. 5. Operation List Shows the list of scheduled operations such as catalog, profile, scan, etc across various source datastores."},{"location":"explore/activity/#deactivate-schedule-operation","title":"Deactivate Schedule Operation","text":"<p>Users can deactivate a scheduled operation from the Activity tab. This stops the operation from running further until it is activated again.</p> <p>Step 1: Click on the Redirect (\u2197) button to open the datastore.</p> <p></p> <p>Step 2: Click on the Activity tab and select Schedule to view the scheduled operations.</p> <p></p> <p>User can hover over any operation timestamp (e.g., \"1 week ago\") to view the exact Completed at time. Clicking the Redirect link opens the operation details.</p> <p></p> <p>Step 3: Click on the vertical ellipsis (\u22ee) and select Deactivate to stop the scheduled operation.</p> <p></p>"},{"location":"explore/anomalies/","title":"Anomalies","text":"<p>Anomalies tab provides a quick overview of all detected anomalies across your source datastores. In Qualytics, An Anomaly refers to a dataset (record or column) that fails to meet specified data quality checks, indicating a deviation from expected standards or norms. These anomalies are identified when the data does not satisfy the applied validation criteria. You can filter and sort anomalies based on your preferences, making it easy to see which anomalies are active,  acknowledged, or archived. This section is designed to help you quickly identify and address any issues.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"explore/anomalies/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the Explore button on the left side panel of the interface.</p> <p></p> <p>Step 2: Click on the \"Anomalies\" from the Navigation Tab.</p> <p></p> <p>You will be navigated to the Anomalies tab, where you'll see a list of all the detected anomalies across various tables and fields from different source datastores, based on the applied data quality checks.</p> <p></p>"},{"location":"explore/anomalies/#categories-anomalies","title":"Categories Anomalies","text":"<p>Anomalies can be classified into different categories based on their status and actions taken. These categories include Open and Archived anomalies. Managing anomalies effectively helps in maintaining data integrity and ensuring quick response to issues.</p>"},{"location":"explore/anomalies/#open","title":"Open","text":"<p>By selecting Open Anomalies, you can view anomalies that have been detected but remain unacknowledged or unresolved. These anomalies require attention and may need further investigation or corrective action. </p> <p></p> <p>This option helps focus on unaddressed issues while allowing seamless navigation to All, Active, or Acknowledged anomalies as needed.</p> <p>1. Active: By selecting Active Anomalies, you can focus on anomalies that are currently unresolved or require immediate attention. These are the anomalies that are still in play and have not yet been acknowledged, archived, or resolved.</p> <p></p> <p>2. Acknowledge: By selecting Acknowledged Anomalies, you can see all anomalies that have been reviewed and marked as acknowledged. This status indicates that the anomalies have been noted, though they may still require further action.</p> <p></p> <p>3. All: By selecting All Anomalies, you can view the complete list of anomalies, regardless of their status. This option helps you get a comprehensive overview of all issues that have been detected, whether they are currently active, acknowledged, or archived.</p> <p></p>"},{"location":"explore/anomalies/#archived","title":"Archived","text":"<p>By selecting Archived Anomalies, you can view anomalies that have been resolved or moved out of active consideration. Archiving anomalies allows you to keep a record of past issues without cluttering the active list.</p> <p></p> <p>You can also categorize the archived anomalies based on their status as Resolved, Duplicate and Invalid, to review them effectively.</p> <p>1. Resolved: This indicates that the anomaly was a legitimate data quality concern and has been addressed.</p> <p></p> <p>2. Duplicate: This indicates that the anomaly is a duplicate of an existing record and has already been addressed.</p> <p></p> <p>3. Invalid: This indicates that the anomaly is not a legitimate data quality concern and does not require further action.</p> <p></p> <p>4. All: Displays all archived anomalies, including those marked as Resolved, Duplicate, and Invalid, giving a comprehensive view of all past issues.</p> <p></p>"},{"location":"explore/anomalies/#anomaly-details","title":"Anomaly Details","text":"<p>Anomaly Details window provides information about anomalies identified during scan operations. It displays details such as the anomaly ID, status, type, detection time, and where it is in the data, such as the datastore and table.  Additionally, it offers options to explore datasets, share details, and collaborate, making it easier to resolve data issues.</p> <p>Step 1: Click on the anomaly from the list of available (whether Active, Acknowledged or Archived) anomalies to view its details.</p> <p></p> <p>A modal window titled \u201cAnomaly Details\u201d will appear, displaying all the details of the selected anomaly.</p> <p></p> <p>For more details on Anomaly Details, please refer to the Anomaly Insights section in the documentation.</p>"},{"location":"explore/anomalies/#acknowledged-anomalies","title":"Acknowledged Anomalies","text":"<p>By acknowledging anomalies, you indicate that they have been reviewed or recognized. Acknowledging anomalies helps you keep track of issues that have been addressed, even if further action is still required.</p> <p>Warning</p> <p>Once an anomaly is acknowledged, it remains acknowledged and never reverts to the active state. </p> <p>Step 1: Click on the active anomaly from the list of available anomalies that you want to acknowledge.</p> <p></p> <p>Step 2: A modal window will appear displaying the anomaly details. Click on the acknowledge (\ud83d\udc41) icon located in the upper-right corner of the modal window.</p> <p></p> <p>Step 3: After clicking on the Acknowledge icon your anomaly is successfully moved to the acknowledge and a flash message will appear saying \u201cThe Anomaly has been successfully acknowledged\u201d.</p> <p></p>"},{"location":"explore/anomalies/#archive-anomalies","title":"Archive Anomalies","text":"<p>By archiving anomalies, you move them to an inactive state, while still keeping them available for future reference or analysis. Archiving helps keep your active anomaly list clean without permanently deleting the records.</p> <p>Step 1: Click on the anomaly from the list of available (whether Active or Acknowledged) anomalies that you want to archive.</p> <p></p> <p>Step 2: A modal window will appear displaying the anomaly details. Click on the archive (\ud83d\uddd1) icon located in the upper-right corner of the modal window.</p> <p></p> <p>Step 3: A modal window titled \u201cArchive Anomaly\u201d will appear, providing you with the following archive options:</p> <ul> <li> <p>Resolved: Choose this option if the anomaly was a legitimate data quality concern and has been addressed. This helps maintain a record of resolved issues while ensuring that they are no longer active.</p> </li> <li> <p>Duplicate: Choose this option if the anomaly is a duplicate of an existing record and has already been addressed. No further action is required as the issue has been previously resolved.</p> </li> <li> <p>Invalid: Select this option if the anomaly is not a legitimate data quality concern and does not require further action. Archiving anomalies as invalid helps differentiate between real issues and those that can be dismissed, improving overall data quality management.  </p> </li> </ul> <p></p> <p>Step 4: Once you've made your selection, click the Archive button to proceed.</p> <p></p> <p>Step 5: After clicking on the Archive button your anomaly is moved to the archive and a flash message will appear saying \u201cAnomaly has been successfully archived\u201d.</p> <p></p>"},{"location":"explore/anomalies/#restore-archived-anomalies","title":"Restore Archived Anomalies","text":"<p>By restoring archived anomalies, you can bring them back into the acknowledged state for further investigation or review. These anomalies will not return to the active state once they have been acknowledged.</p> <p>Step 1: Click on the anomaly that you want to restore from the list of archived anomalies.</p> <p></p> <p>Step 2: A modal window will appear displaying the anomaly details. Click on the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window, and click on \u201cRestore\u201d from the drop-down menu.</p> <p></p> <p>Step 3: After clicking on the \u201cRestore\u201d button, the selected anomaly is now restored as in acknowledged state.</p> <p></p>"},{"location":"explore/anomalies/#assign-tags","title":"Assign Tags","text":"<p>Assigning tags to an anomaly serves the purpose of labeling and grouping anomalies and driving downstream workflows.</p> <p>Step 1: Click on the Assign tags to this Anomaly or + button.</p> <p></p> <p>Step 2: A dropdown menu will appear with existing tags. Scroll through the list and click on the tag you wish to assign.</p> <p></p>"},{"location":"explore/anomalies/#delete-anomalies","title":"Delete Anomalies","text":"<p>Deleting an anomaly allows you to permanently remove a record that is no longer relevant or was logged in error. This action is done individually, ensuring that your anomaly records remain clean and up to date.</p> <p>Note</p> <p>You can only delete archived anomalies, not active or acknowledged checks. If you want to delete an active or acknowledged anomaly, you must first move it to the archive, and then you can delete it. </p> <p>You can delete individual anomalies using one of two methods:</p>"},{"location":"explore/anomalies/#1-delete-directly","title":"1. Delete Directly","text":"<p>Step 1: Click on Archived from the navigation bar in the Anomalies section to view all archived anomalies.</p> <p></p> <p>Step 2: Locate the anomaly, that you want to delete and click on the Delete icon located on the right side of the anomaly.</p> <p></p> <p>Step 3: A confirmation modal window will appear, click on the Delete button to permanently remove the anomaly from the system.</p> <p></p> <p>Step 4: After clicking on the delete button, your anomaly is successfully deleted and a success flash message will appear saying \u201cAnomaly has been successfully deleted\u201d.</p> <p></p>"},{"location":"explore/anomalies/#2-delete-via-action-menu","title":"2. Delete via Action Menu","text":"<p>Step 1: Click on the archive anomaly from the list of archived anomalies that you want to delete.</p> <p></p> <p>Step 2: A modal window will appear displaying the anomaly details. Click on the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window, and click on \u201cDelete\u201d from the drop-down menu.</p> <p></p> <p>Step 3: A confirmation modal window will appear, click on the Delete button to permanently remove the anomaly from the system.</p> <p></p> <p>Step 4: After clicking on the delete button, your anomaly is successfully deleted and a success flash message will appear saying \u201cAnomaly has been successfully deleted\u201d.</p> <p></p>"},{"location":"explore/anomalies/#filter-and-sort","title":"Filter and Sort","text":"<p>Filter and Sort options allow you to organize your anomaly by various criteria, such as Weight, Anomalous Record, Created Date. You can also apply filters to refine your list of anomaly based on Selected Source Datastores, Selected Tag, Timeframe, Type and Rule .</p>"},{"location":"explore/anomalies/#sort","title":"Sort","text":"<p>You can sort your anomalies by Anomalous Record, Created Date, and Weight to easily organize and prioritize them according to your needs.</p> <p></p> No Sort By Option Description 1 Anomalous Record Sorts anomalies based on the number of anomalous records identified. 2 Created Date Sorts anomalies according to the date they were detected. 3 Weight Sort anomalies by their assigned weight or importance level. <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.</p> <p></p>"},{"location":"explore/anomalies/#filter","title":"Filter","text":"<p>You can filter your anomalies based on values like Source Datastores, Timeframe, Type, Rule, and Tags.</p> <p></p> <p></p> No. Filter Description 1 Selected Source Datastore Select specific source datastores to focus on their anomalies. 2 Select Tags Filter anomalies by specific tags to categorize and prioritize issues effectively. 3 Timeframe Filtering anomalies detected within specific time ranges (e.g., anomalies detected in the last week or year). 4 Type Filter anomalies based on anomaly type (Record or Shape). 5 Rule Filter anomalies based on specific rules applied to the anomaly. By clicking on the caret down button next to the Rule field, the available rule types will be dynamically populated based on the rule types present in the results. The rules displayed are based on the current dataset and provide more granular control over filtering.   Each rule type will show a counter next to it, displaying the total number of occurrences for that rule in the dataset.  For example, the rule type After Date Time is displayed with a total of 14 occurrences."},{"location":"explore/checks/","title":"Checks","text":"<p>Checks tab provides a quick overview of the various checks applied across different tables and fields in multiple source datastores. In Qualytics, checks act as rules applied to data tables and fields to ensure accuracy and maintain data integrity. You can filter and sort the checks based on your preferences, making it easy to see which checks are active, in draft, or archived. This section is designed to simplify the review of applied checks across datasets without focusing on data quality or performance.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"explore/checks/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the Explore button on the left side panel of the interface.</p> <p></p> <p>Step 2: Click on the \"Checks\" from the Navigation Tab.</p> <p></p> <p>You will be navigated to the Checks tabs here and you'll see a list of all the checks that have been applied to various tables and fields across different source datastores.</p>"},{"location":"explore/checks/#categories-check","title":"Categories Check","text":"<p>You can categorize your checks based on their status, such as Active, Draft, Archived (Invalid and Discarded), or All, according to your preference. This categorization offers a clear view of the data quality validation process, helping you manage checks efficiently and maintain data integrity.</p>"},{"location":"explore/checks/#all","title":"All","text":"<p>By selecting All Checks, you can view a comprehensive list of all the checks in the datastores, including both active and draft checks, allowing you to focus on the checks that are currently being managed or are in progress. However, archived checks are not displayed in this.</p> <p></p>"},{"location":"explore/checks/#active","title":"Active","text":"<p>By selecting Active, you can view checks that are currently applied and being enforced on the data. These operational checks are used to validate data quality in real-time, allowing you to monitor all active checks and their performance.</p> <p></p> <p>You can also categorize the active checks based on their importance, favorites, or specific metrics to streamline your data quality monitoring.</p> <p>1. Important: Shows only checks that are marked as important. These checks are prioritized based on their significance, typically assigned a weight of 7 or higher.</p> <p>Note</p> <p>Important checks are prioritized based on a weight of 7 or higher.</p> <p></p> <p>2. Favorite: Displays checks that have been marked as favorites. This allows you to quickly access checks that you use or monitor frequently. </p> <p></p> <p>3. All: Displays a comprehensive view of all active checks, including important, favorite and any checks that do not fall under these specific categories. </p> <p></p>"},{"location":"explore/checks/#draft-checks","title":"Draft Checks","text":"<p>By selecting Draft, you can view checks that have been created but have not yet been applied to the data. These checks are in the drafting stage, allowing for adjustments and reviews before activation. Draft checks provide flexibility to experiment with different validation rules without affecting the actual data.</p> <p></p> <p>You can also categorize the draft checks based on their importance, favorites, or specific metrics to prioritize and organize them effectively during the review and adjustment process.</p> <p>1. Important: Shows only checks that are marked as important. These checks are prioritized based on their significance, typically assigned a weight of 7 or higher.</p> <p></p> <p>2 Favorite: Displays checks that have been marked as favorites. This allows you to quickly access checks that you use or monitor frequently.  </p> <p></p> <p>3. All: Displays a comprehensive view of all draft checks, including important, favorite and any checks that do not fall under these specific categories.</p> <p></p>"},{"location":"explore/checks/#archived-checks","title":"Archived Checks","text":"<p>By selecting Archived, you can view checks that have been marked as discarded or invalid from use but are still stored for future reference or restoration. Although these checks are no longer active, they can be restored if needed.</p> <p></p> <p>You can also categorize the archived checks based on their status as Discarded, Invalid, or view All archived checks to manage and review them effectively.</p> <p>1. Discarded: Shows checks that have been marked as no longer useful or relevant and have been discarded from use.</p> <p></p> <p>2. Invalid: Displays checks that are deemed invalid due to errors or misconfigurations, requiring review or deletion.</p> <p></p> <p>3. All: Provides a view of all archive checks within this category including discarded and invalid checks.</p> <p></p>"},{"location":"explore/checks/#details","title":"Details","text":"<p>Check Details provides important information about each check in the system. It shows when a check was last run, how often it has been used, when it was last updated, who made changes to it, and when it was created. This section helps users understand the status and history of the check, making it easier to manage and track its use over time. </p> <p>Step 1: Locate the check you want to review, then hover over the info icon to view the Check Details.</p> <p></p> <p>A popup will appear with additional details about the check.</p> <p></p>"},{"location":"explore/checks/#last-asserted","title":"Last Asserted","text":"<p>Last Asserted At shows the most recent time the check was run, indicating when the last validation occurred. For example, the check was last asserted on Mar 27, 2025, at 2:16 AM (GMT+5:30).</p> <p></p>"},{"location":"explore/checks/#scans","title":"Scans","text":"<p>Scans show how many times the check has been used in different operations. It helps you track how often the check has been applied. For example, the check was used in 17 operations.</p> <p></p>"},{"location":"explore/checks/#updated-at","title":"Updated At","text":"<p>Updated At shows the most recent time the check was modified or updated. It helps you see when any changes were made to the check\u2019s configuration or settings. For example, the check was last updated on Nov 8, 2024, at 6:37 PM (GMT+5:30).</p> <p></p>"},{"location":"explore/checks/#last-editor","title":"Last Editor","text":"<p>Last Editor indicates who most recently made changes to the check. It helps track who is responsible for the latest updates or modifications. This is useful for accountability and collaboration within teams.</p> <p></p>"},{"location":"explore/checks/#created-at","title":"Created At","text":"<p>Created At shows when the check was first made. It helps you know how long the check has been in use. This is useful for tracking its history. For example, the check was created on Oct 17, 2024, at 11:13 AM (GMT+5:30).</p> <p></p>"},{"location":"explore/checks/#status-management-of-checks","title":"Status Management of Checks","text":""},{"location":"explore/checks/#set-check-as-draft","title":"Set Check as Draft","text":"<p>You can move an active check into a draft state, allowing you to work on the check, make adjustments, and refine the validation rules without affecting live data. This is useful when you need to temporarily deactivate a check for review and updates.</p> <p>Step 1: Click on the active check that you want to move to the draft state.</p> <p></p> <p>Step 2: A modal window will appear displaying the check details. Click on the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window, and select \"Draft\" from the drop-down menu.</p> <p></p> <p>Step 3: After clicking on \"Draft\", the check will be successfully moved to the draft state, and a success flash message will appear stating, \"The checks have been successfully updated.\"</p> <p></p>"},{"location":"explore/checks/#activate-draft-check","title":"Activate Draft Check","text":"<p>You can activate the draft checks after when you have worked on the check, make adjustments, and refine the validation rules. By activating the draft check and making it live, ensures that the defined criteria are enforced on the data. </p> <p>Step 1: Navigate to the Draft check section, and click on the drafted check that you want to activate, whether you have made changes or wish to activate it as is.</p> <p></p> <p>A modal window will appear with the check details. If you want to make any changes to the check details, you can edit them.</p> <p></p> <p>Step 2: Click on the down arrow icon with the Update button. A dropdown menu will appear, click on the Activate button.  </p> <p></p> <p>Step 3: After clicking on the activate button, your check is now successfully moved to the active checks and a success flash message will appear stating \"Check successfully updated\".</p> <p></p>"},{"location":"explore/checks/#set-check-as-archived","title":"Set Check as Archived","text":"<p>You can move an active or draft check into the archive when it is no longer relevant but may still be needed for historical purposes or future use. Archiving helps keep your checks organized without permanently deleting them.</p> <p>Step 1: Click on the check from the list of available (whether Active or Draft) checks that you want to archive.</p> <p></p> <p>Step 2: A modal window will appear displaying the check details. Click on the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window, and click on the \"Archive\" from the drop-down menu. </p> <p></p> <p>Step 3: A modal window titled \u201cArchive Check\u201d will appear, providing you with the following archive options:</p> <ul> <li> <p>Discarded: Select this option if the check is no longer relevant or suitable for the current business rules or data requirements. This helps in archiving checks that are obsolete but still exist for historical reference.</p> </li> <li> <p>Invalid: Choose this option if the check is not valid and should be retired from future inference. This helps the system learn from invalid checks and improves its ability to infer valid checks in the future.</p> </li> </ul> <p></p> <p>Step 4: Once you've made your selection, click the Archive button to proceed.</p> <p></p> <p>Step 5: After clicking on the Archive button your check is moved to the archive and a flash message will appear saying \"The Check has been successfully archived\".</p> <p></p>"},{"location":"explore/checks/#restore-archived-checks","title":"Restore Archived Checks","text":"<p>If a check has been archived, then you can restore it back to an active state or in a draft state. This allows you to reuse your checks that were previously archived without having to recreate them from scratch.</p> <p>Step 1: Click on Archived from the navigation bar in the Checks section to view all archived checks.</p> <p></p> <p>Step 2: Click on the archived check which you want to restore as an active or draft check.</p> <p>For Demonstration purpose, we have selected the \"Metric\" check.</p> <p></p> <p>A modal window will appear with the check details.</p> <p></p> <p>Step 3: If you want to make any changes to the check, you can edit it. Otherwise, click on the Restore button to restore it as an active check.</p> <p></p> <p>To restore the check as a draft, click on the arrow icon next to the Restore button. A dropdown menu will appear\u2014select Restore as Draft from the options.</p> <p></p> <p>After clicking the Restore button, the check will be successfully restored as either an active or draft check, depending on your selection. A success message will appear confirming, \"Check successfully updated.\"</p> <p></p>"},{"location":"explore/checks/#edit-check","title":"Edit Check","text":"<p>You can edit an existing check to modify its properties, such as the rule type, coverage, filter clause, or description. Updating a check ensures that it stays aligned with evolving data requirements and maintains data quality as conditions change.</p> <p>Step 1: Click on the check you want to edit, whether it is an active or draft check.</p> <p></p> <p>A modal window will appear with the check details. </p> <p></p> <p>Step 2: Modify the check details as needed based on your preferences.</p> <p></p> <p>Step 3: Once you have edited the check details, then click on the Validate button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct.</p> <p></p> <p>If the validation is successful, a green message saying \"Validation Successful\" will appear. </p> <p></p> <p>If the validation fails, a red message saying \"Failed Validation\" will appear. This typically occurs when the check logic or parameters do not match the data properly.</p> <p></p> <p>Step 3: Once you have a successful validation, click the \"Update\" button. The system will update the changes you've made to the check, including changes to the fields, filter clause, coverage, description, tags, or metadata.</p> <p></p> <p>After clicking on the Update button, your check is successfully updated and a success flash message will appear stating \"Check successfully updated\".</p> <p></p>"},{"location":"explore/checks/#mark-check-as-favorite","title":"Mark Check as Favorite","text":"<p>Marking a check as a favorite allows you to quickly access and prioritize the checks that are most important to your data validation process. This helps streamline workflows by keeping frequently used or critical checks easily accessible, ensuring you can monitor and manage them efficiently. By marking a check as a favorite, it will appear in the \"Favorite\" category for faster retrieval and management.</p> <p>Step 1: Locate the check which you want to mark as a favorite and click on the bookmark icon located on the right side of the check.</p> <p></p> <p>After Clicking on the bookmark icon your check is successfully marked as a favorite and a success flash message will appear stating \"Check has been favorited\".</p> <p></p> <p>To unmark a check, simply click on the bookmark icon of the marked check.</p> <p></p> <p>This will remove it from your favorites.A success flash message will appear stating \"The Check has been unfavorited\".</p> <p></p>"},{"location":"explore/checks/#clone-check","title":"Clone Check","text":"<p>You can clone both active and draft checks to create a duplicate copy of an existing check. This is useful when you want to create a new check based on the structure of an existing one, allowing you to make adjustments without affecting the original check.</p> <p>Step 1: Click on the check (whether Active or Draft) that you want to clone.</p> <p></p> <p>Step 2: A modal window will appear displaying the check details. Click on the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window, and select \"Clone\" from the drop-down menu.</p> <p></p> <p>Step 3: After clicking the Clone button, a modal window will appear. This window allows you to adjust the cloned check's details.</p> <p></p> <p>1. If you toggle on the \"Associate with a Check Template\" option, the cloned check will be linked to a specific template.</p> <p></p> <p>Choose a Template from the dropdown menu that you want to associate with the cloned check. The check will inherit properties from the selected template.</p> <ul> <li> <p>Locked: The check will automatically sync with any future updates made to the template, but you won't be able to modify the check's properties directly.</p> </li> <li> <p>Unlocked: You can modify the check, but future updates to the template will no longer affect this check.</p> </li> </ul> <p></p> <p>2. If you toggle off the \"Associate with a Check Template\" option, the cloned check will not be linked to any template, which allows you full control to modify the properties independently.</p> <p></p> <p>Select the appropriate Rule type for the check from the dropdown menu.</p> <p></p> <p>Step 4: Once you have selected the template or rule type, fill up the remaining check details as required. </p> <p></p> <p>Step 5: After completing all the check details, click on the \"Validate\" button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct. It ensures that the check will work as expected by running it against the data without committing any changes.</p> <p></p> <p>If the validation is successful, a green message saying \"Validation Successful\" will appear. </p> <p></p> <p>If the validation fails, a red message saying \"Failed Validation\" will appear. This typically occurs when the check logic or parameters do not match the data properly.</p> <p></p> <p>Step 6: Once you have a successful validation, click the \"Save\" button. The system will save any modifications you've made to the check, and create a clone of that check on basis of your changes.  </p> <p></p> <p>After clicking on the \"Save\" button your check is successfully created and a success flash message will appear stating \"Check successfully created\".</p> <p></p>"},{"location":"explore/checks/#create-a-quality-check-template","title":"Create a Quality Check template","text":"<p>You can add checks as a Template, which allows you to create a reusable framework for quality checks. By using templates, you standardize the validation process, enabling the creation of multiple checks with similar rules and criteria across different datastores. This ensures consistency and efficiency in managing data quality checks.</p> <p>Step 1: Locate the check (whether Active or Draft) you want to archive and click on that check.</p> <p></p> <p>Step 2: A modal window will appear displaying the check details. Click on the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window, and select \"Template\" from the drop-down menu.</p> <p></p> <p>After clicking the \"Template\" button, the check will be saved and created as a template in the library, and a success flash message will appear stating, \"The quality check template has been successfully created.\" This allows you to reuse the template for future checks, streamlining the validation process.</p> <p></p>"},{"location":"explore/checks/#filter-and-sort","title":"Filter and Sort","text":"<p>Filter and Sort options allow you to organize your checks by various criteria, such as Weight, Anomalies, Coverage, Created Date, and Rules. You can also apply filters to refine your list of checks based on Selected Source Datastores, Check Type, Asserted State (Passed, Failed, Not Asserted), Tags, Files, and Fields.</p>"},{"location":"explore/checks/#sort","title":"Sort","text":"<p>You can sort your checks by Active Anomalies, Coverage, Created Date, Last Asserted, Rules, and Weight to easily organize and prioritize them according to your needs.</p> <p></p> No Sort By Option Description 1 Active Anomalies Sort checks based on the number of active anomalies. 2 Coverage Sort checks by data coverage percentage. 3 Created Date Sort checks according to the date they were created. 4 Last Asserted Sorts by the last time the check was executed. 5 Rules Sort checks based on specific rules applied to the checks. 6 Weight Sort checks by their assigned weight or importance level. <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.</p> <p></p>"},{"location":"explore/checks/#filter","title":"Filter","text":"<p>You can filter your checks based on values like Source Datastores Check Type, Asserted State, Rule, Tags, File, Field, and Template.</p> <p></p> No Filter Filter Value Description 1 Selected Source Datastores N/A Select specific source datastores to focus on their checks. 2 Select Tags N/A Filter checks by specific tags to categorize and refine results. <p></p> No Filter Filter Value Description 3 Check Type All Displays all types of checks, both inferred and authored. Inferred Shows system-generated checks that automatically validate data based on detected patterns or logic. Authored Displays user-created checks, allowing the user to focus on custom validations tailored to specific requirements. 4 Asserted State All Displays all checks, regardless of their asserted status. This provides a full overview of both passed, failed, and not asserted checks. Passed Shows checks that have been asserted successfully, meaning no active anomalies were found during the validation process. Failed Displays checks that have failed assertion, indicating active anomalies or issues that need attention. Not Asserted Filters out checks that have not yet been asserted, either because they haven\u2019t been processed or validated yet. 5 Rule N/A Select this to filter the checks based on specific rule type for data validation, such as checking non-null values, matching patterns, comparing numerical ranges, or verifying date-time constraints. By clicking on the caret down button next to the Rule field, the available rule types will be dynamically populated based on the rule types present in the results. The rules displayed are based on the current dataset and provide more granular control over filtering. Each rule type will show a counter next to it, displaying the total number of occurrences for that rule in the dataset. For example, the rule type After Date Time is displayed with a total of 46 occurrences. 6 Template N/A This filter allows users to view and apply predefined check templates."},{"location":"explore/insights/","title":"Insights","text":"<p>Insights in Qualytics provides a quick and clear overview of your data's health and performance. It shows key details like Quality Scores, active checks, profiles, scans, and anomalies in a simple and effective way. This makes it easy to monitor and track data quality, respond to issues, and take action quickly. Additionally, users can monitor specific source datastores and check for a particular report date and time frame.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"explore/insights/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the Explore button on the left side panel of the interface.</p> <p></p> <p>You will be navigated to the Insights tab to view a presentation of your data, pulled from the connected source datastore.</p> <p></p>"},{"location":"explore/insights/#filtering-controls","title":"Filtering Controls","text":"<p>Filtering Controls allow you to refine the data displayed on the Insights page. You can customize the data view based on Source Datastores, Tags, Report Date, and Timeframe, ensuring you focus on the specific information that matters to you.</p> <p></p> No Filter Description 1. Select Source Datastores Select specific source datastores to focus on their data. 2. Tags Filter data by specific tags to categorize and refine results. 3. Report Date Set the report date to view data from a particular day. 4. Timeframe Choose a timeframe to view data for a specific  (week, month, quarter, and year)"},{"location":"explore/insights/#understanding-timeframes-and-timeslices","title":"Understanding Timeframes and Timeslices","text":"<p>When analyzing data on the Insights, two key concepts help you uncover trends: timeframes and timeslices. These work together to give you both a broad view and a detailed breakdown of your data.</p>"},{"location":"explore/insights/#timeframes","title":"Timeframes","text":"<p>Timeframe is the total range of time you select to view your data. For example, you can choose to see data:</p> <ul> <li> <p>Weekly: Summarize data for an entire week.</p> </li> <li> <p>Monthly: Group data by months.</p> </li> <li> <p>Quarterly: Cover three months at a time.</p> </li> <li> <p>Yearly: Show data for the entire year.</p> </li> </ul>"},{"location":"explore/insights/#how-metrics-behave-over-a-timeframe","title":"How Metrics Behave Over a Timeframe","text":"<ul> <li>Quality Score and other similar metrics display an average for the selected timeframe.</li> </ul> <p>Example:If you select weekly, the Quality Score shown will be the average score for the entire week.</p> <p></p> <ul> <li>Historical Graphs (like Profiles or Scans) show cumulative totals over time.</li> </ul> <p>Example:If you view a graph for a monthly timeframe, the graph shows how data grows or changes month by month.</p> <p></p>"},{"location":"explore/insights/#timeslices","title":"Timeslices","text":"<p>Timeslice breaks your selected timeframe into smaller parts. It helps you see more detailed trends within the overall timeframe.</p> <p>For example:</p> <ul> <li> <p>A weekly timeframe shows each day of the week.</p> </li> <li> <p>A monthly timeframe breaks into weekly segments.</p> </li> <li> <p>A quarterly timeframe highlights months within that quarter.</p> </li> <li> <p>A yearly timeframe divides into quarters and months.</p> </li> </ul>"},{"location":"explore/insights/#how-timeslices-work","title":"How Timeslices Work","text":"<ul> <li> <p>When you choose a timeframe, the graph automatically breaks it into timeslices.</p> </li> <li> <p>Each bar or point on the graph represents one timeslice.</p> </li> </ul> <p>Example:</p> <ul> <li>If you choose a Weekly timeframe, each bar in the graph will represent one day of the week.</li> </ul> <p></p> <ul> <li>If you choose a Monthly timeframe, each bar will represent one week in that month.</li> </ul> <p></p>"},{"location":"explore/insights/#metrics-within-a-timeslice","title":"Metrics Within a Timeslice","text":"<p>Metrics like Quality Score, Profiles, or Scans are displayed for each timeslice, allowing you to identify trends and patterns over smaller intervals.</p>"},{"location":"explore/insights/#quality-score","title":"Quality Score","text":"<p>Quality Score gives a clear view of your data's overall quality. It shows important measures like Completeness, Conformity, Consistency, Precision, Timeliness, Volumetrics, and Accuracy, each represented by a percentage. This helps you quickly understand the health of your data, making it easier to identify areas that need improvement.  </p> <p></p>"},{"location":"explore/insights/#overview","title":"Overview","text":"<p>Overview provides a quick view of your data. It shows the total amount of data being managed, along with the number of Source Datastores and Containers. This helps you easily track the size and growth of your data.</p> <p></p>"},{"location":"explore/insights/#records-and-fields-data","title":"Records and Fields Data","text":"<p>This section shows important information about the records and fields in the connect source datastores:</p> <ul> <li> <p>Records Profiled: This represents the total number of records that were included in the profiling process.</p> </li> <li> <p>Records Scanned: This refers to the number of records that were checked during a scan operation. The scan performs data quality checks on collections like tables, views, and files.</p> </li> <li> <p>Fields Profiled: This shows how many field profiles were updated as a result of the profiling operation.</p> </li> </ul> <p></p>"},{"location":"explore/insights/#checks","title":"Checks","text":"<p>Checks offer a quick view of active checks, categorizing them based on their results.</p> <p></p> <p>1. Passing Check: Displays the real-time number of passed checks that were successfully completed during the scan or profile operation, indicating that the data met the set quality criteria.</p> <p></p> <p>2. Failing Checks: This shows the real-time number of checks that did not pass during the scan or profile operation, indicating data that did not meet the quality criteria.</p> <p></p> <p>3. Not Asserted Checks: This shows the real-time number of checks that haven't been processed or validated yet, meaning their status is still pending and they have not been confirmed as either passed or failed.</p> <p></p> <p>4. Inferred Checks: This shows the real-time number of system-generated Inferred Checks.These checks are automatically created during a Profile operation using statistical analysis and machine learning methods.</p> <p></p> <p>5. Authored Check: This shows the Authored Checks that are manually created by users within the Qualytics platform or API. These checks can range from simple templates for common validations to complex rules using Spark SQL and User-Defined Functions (UDF) in Scala. </p> <p></p> <p>The count for each category can be viewed by hovering over the relevant check, providing real-time ratios of checks. Users can also click on these checks to navigate directly to the corresponding checks\u2019 dedicated page in the Explore section. </p>"},{"location":"explore/insights/#anomalies","title":"Anomalies","text":"<p>Anomalies section provides a clear overview of identified anomalies in the system. The anomalies are categorized for better clarity and management.</p> <p></p> <p>Anomalies Identified shows the total issues found, divided into active, acknowledged, resolved, duplicate and invalid, helping users quickly manage and fix problems.</p> <p>1. Active Anomalies: Shows the number of unresolved anomalies that require immediate attention. These anomalies are still present and have not been acknowledged, archived, or resolved in the system.</p> <p></p> <p>2. Acknowledged Anomalies:  These are anomalies that have been reviewed and recognized by users but are not yet resolved. Acknowledging anomalies helps keep track of issues that have been addressed, even if further actions are still needed. </p> <p></p> <p>3. Resolved Anomalies: Represent anomalies that were valid data quality issues and have been successfully addressed. These anomalies have been resolved, indicating the data now meets the required quality standards.</p> <p></p> <p>4. Duplicate Anomalies: These anomalies have been identified as duplicates of existing anomalies. This status helps prevent redundant issue tracking and ensures that duplicate records are consolidated into a single entry.</p> <p></p> <p>5. Invalid Anomalies: These anomalies have been reviewed and determined to be false positives or not relevant. Marking an anomaly as invalid removes it from active consideration, preventing unnecessary investigations.</p> <p></p> <p>Info</p> <p>Users can see the checks using the redirect link (the redirect only show the current check statuses).</p> <p>The count for each category can be viewed by hovering over the relevant anomalies, providing real-time ratios of anomalies. Users can also click on these anomalies to navigate directly to the corresponding anomalies\u2019 dedicated page in the Explore section. </p>"},{"location":"explore/insights/#rule-distribution-type","title":"Rule Distribution Type","text":"<p>Rule Type Distribution highlights the top rule types applied to the source datastore, each represented by a different color. The visualization allows users to quickly see which rules are most commonly applied.   </p> <p></p> <p>By clicking the caret down \ud83d\udd3d button, users can choose either the top 5 or top 10 rule types to view in the insights, based on their analysis needs.</p> <p></p>"},{"location":"explore/insights/#profiles","title":"Profiles","text":"<p>Profiles section provides a clear view of data profiling activities over time, showing how often profiling is performed and the amount of data (records) analyzed.</p> <p></p> <p>Profile Runs shows how many times data profiling has been done over a certain period. Each run processes a specific source datastore or table, helping users see how often profiling happens. The graph gives a clear view of the changes in profile runs over time, making it easier to track  profiling activity.</p> <p></p> <p>Click on the caret down \ud83d\udd3d button to choose between viewing Records Profiled or Fields Profiled, depending on your preference.</p> <p></p>"},{"location":"explore/insights/#record-profile","title":"Record Profile","text":"<p>Record Profiled shows the total number of records processed during the profile runs. It provides insight into the amount of data that has been analyzed during those runs. The bars in the graph show the comparison of the number of records profiled over the selected days.</p> <p></p>"},{"location":"explore/insights/#field-profiled","title":"Field Profiled","text":"<p>Field Profiled shows the number of fields processed during the profile runs. It shows how many individual fields within datasets have been analyzed during those runs. The bars in the graph provide a comparison of the fields profiled over the selected days.</p> <p></p>"},{"location":"explore/insights/#scans","title":"Scans","text":"<p>Scans section provides a clear overview of all scanning activities within a selected period. It helps users keep track of how many scans were performed and how many anomalies were detected during those scans. This section makes it easier to understand the scanning process and manage data by offering insight into how often scans occur. </p> <p></p> <p>Scan Runs show how often data scans are performed over a certain period. These scans check the quality of data across tables, views, and files, helping users monitor their data regularly and identify any issues. The process can be customized to scan tables or limit the number of records checked, ensuring that data stays accurate and up to standard.</p> <p></p> <p>Click on the caret down \ud83d\udd3d button to choose between viewing Anomalies Identified or Records Scanned, depending on your preference.</p> <p></p>"},{"location":"explore/insights/#anomalies-identified","title":"Anomalies Identified","text":"<p>Anomalies Identified shows the total number of anomalies detected during the scan runs. The bars in the graph allow users to compare the number of anomalies found across different days, helping them spot trends or irregularities in the data.</p> <p></p>"},{"location":"explore/insights/#records-scanned","title":"Records Scanned","text":"<p>Records Scanned shows the total number of records that were scanned during the scan runs. It gives users insight into how much data has been processed and allows them to compare the scanned records over the selected period.</p> <p></p>"},{"location":"explore/insights/#data-volume","title":"Data Volume","text":"<p>Data Volume allows users to track the size of data stored within all source datastores present in the Qualytics platform over time. This helps in monitoring how the source datastore grows or changes, making it easier to detect irregularities or unexpected increases that could affect system performance. Users can visualize data size trends and manage the source datastore's efficiency, optimizing storage, adjusting resources, and enhancing data processing based on its size and growth.  </p> <p></p>"},{"location":"explore/insights/#export","title":"Export","text":"<p>Export button allows you to quickly download the data from the Insights page. You can export data according to the selected Source Datastores, Tags, Report Date, and Timeframe. This makes it easy to save the data for offline use or share it with others.</p> <p></p> <p>After exporting, the data appears in a structured format, making it easy to save for offline use or to share with others.</p> <p></p>"},{"location":"explore/insights/#refresh","title":"Refresh","text":"<p>Refresh button allows users to quickly update the Insights data. When clicked, it fetches the latest information, ensuring that users always have the most up-to-date insights.</p> <p></p> <p>A label indicates when the page was last refreshed, helping users track data updates. This feature ensures accuracy and keeps the insights current without requiring a full page reload.</p> <p></p>"},{"location":"explore/observability/","title":"Observability","text":"<p>Observability provides a structured way to monitor data behavior, detect anomalies, and identify trends across datastores and tables. It supports consistent tracking through Measures and Metrics, including daily data volumes, data freshness, and specific field-level values measured against predefined thresholds. Automated scans, heatmaps, and visual insights make it easier to spot issues, set thresholds, and adjust monitoring settings to maintain data integrity.</p> <p>Note</p> <p>For more information please refer to the observability overview documentation</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"explore/observability/#manage-observability","title":"Manage Observability","text":"<p>In this section, you can manage the observability settings, including editing checks, thresholds, maximum ages, and marking checks as favorites.</p> <p>Note</p> <p>For more information please refer to the manage observability overview documentation</p>"},{"location":"explore/overview-of-explore/","title":"Explore","text":"<p>Explore page in  Qualytics is where you can easily view and manage all your data. It provides easy access to important features through tabs like Insights, Activity, Profiles, Observability, Checks, and Anomalies. Each tab shows a different part of your data, such as its quality, activities, structure, checks, and issues. You can sort and filter the data by datastore and time frame, making it easier to track performance, spot problems, and take action. The Explore section helps you manage and understand your data all in one place.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"explore/overview-of-explore/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the Explore button on the left side panel of the interface.</p> <p></p> <p>Step 2: After click on the Explore button, you will see the following tabs: Insights, Activity, Profiles, Observability, Checks, and Anomalies.</p> <p></p>"},{"location":"explore/overview-of-explore/#insights","title":"Insights","text":"<p>Insights tab provides a quick and clear overview of your data's health and performance. It shows key details like Quality Scores, active checks, profiles, scans, and anomalies in a simple and effective way. This makes it easy to monitor and track data quality, respond to issues, and take action quickly. Additionally, users can monitor specific source datastores and check for a particular report date and time frame.</p> <p>For more details on Insights, please refer to the Insights documentation.</p>"},{"location":"explore/overview-of-explore/#activity","title":"Activity","text":"<p>Activity tab provides a comprehensive view of all operations, helping users monitor and analyze the performance and workflows across various source datastores. Activity are categorized into Runs and Schedule operations, offering distinct insights into executed and scheduled activities.</p> <p>For more details on Activity, please refer to the Activity documentation.</p>"},{"location":"explore/overview-of-explore/#profiles","title":"Profiles","text":"<p>Profiles tab helps you explore and manage your containers and fields. With features like filtering, sorting, tagging, and detailed profiling, it provides a clear understanding of data quality and structure. This simplifies navigation and enhances data management for quick, informed decisions.</p> <p>For more details on Profiles, please refer to the Profiles documentation.</p>"},{"location":"explore/overview-of-explore/#observability","title":"Observability","text":"<p>Observability tab gives users an easy way to track changes in data volume over time. It introduces two types of checks: Volumetric and Metric. The Volumetric check automatically monitors the number of rows in a table and flags unusual changes, while the Metric check focuses on specific fields, providing more detailed insights from scan operations. Together, these tools help users spot data anomalies quickly and keep their data accurate.</p> <p>For more details on Observability, please refer to the Observability documentation.</p>"},{"location":"explore/overview-of-explore/#checks","title":"Checks","text":"<p>Checks tab provides a quick overview of the various checks applied across different tables and fields in multiple source datastores. In Qualytics, checks act as rules applied to data tables and fields to ensure accuracy and maintain data integrity. You can filter and sort the checks based on your preferences, making it easy to see which checks are active, in draft, or archived. This section is designed to simplify the review of applied checks across datasets without focusing on data quality or performance.</p> <p>For more details on Checks, please refer to the Checks documentation.</p>"},{"location":"explore/overview-of-explore/#anomalies","title":"Anomalies","text":"<p>Anomalies tab provides a quick overview of all detected anomalies across your source datastores. In Qualytics, An Anomaly refers to a dataset (record or column) that fails to meet specified data quality checks, indicating a deviation from expected standards or norms. These anomalies are identified when the data does not satisfy the applied validation criteria. You can filter and sort anomalies based on your preferences, making it easy to see which anomalies are active, acknowledged, or archived. This section is designed to help you quickly identify and address any issues.</p> <p>For more details on Anomalies, please refer to the Anomalies documentation. </p>"},{"location":"explore/profiles/","title":"Profiles","text":"<p>Profiles in Qualytics helps you explore and manage your containers and fields. With features like filtering, sorting, tagging, and detailed profiling, it provides a clear understanding of data quality and structure. This simplifies navigation and enhances data management for quick, informed decisions.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"explore/profiles/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the Explore button on the left side panel of the interface.</p> <p></p> <p>Step 2: Click on the \"Profiles\" from the Navigation Tab.</p> <p></p> <p>You will be navigated to the Profiles section. Here, you will see the data organized into two sections: Containers and Fields, allowing you to explore and analyze the datasets efficiently.</p> <p></p>"},{"location":"explore/profiles/#containers","title":"Containers","text":"<p>By selecting the Containers section, you can explore structured datasets that are organized as either JDBC or DFS containers. JDBC containers represent tables or views within relational databases, while DFS containers include files such as CSV, JSON, or Parquet, typically stored in distributed systems like Hadoop or cloud storage.</p> <p></p>"},{"location":"explore/profiles/#container-details","title":"Container Details","text":"<p>Containers section provides key details about each container, including the last profiled and last scanned dates. Hovering over the info icon for a specific container reveals these details instantly.</p> <p>Step 1: Locate the container you want to review, then hover over the info icon to view the container Details. </p> <p></p> <p>Step 2: A pop-up will appear with additional details about the container, such as the last profiled and last scanned dates.</p> <p></p>"},{"location":"explore/profiles/#explore-tables-and-fields","title":"Explore Tables and Fields","text":"<p>By clicking on a specific container, users can view its associated fields, including detailed profiling information. Additionally, clicking the arrow icon on the right side of a specific container allows users to navigate directly to its corresponding table for a more in-depth exploration.</p>"},{"location":"explore/profiles/#explore-fields","title":"Explore Fields","text":"<p>To explore the data within a container, you can view all its fields. This allows you to gain insights into the structure and quality of the data stored in the container.</p> <p>Step 1: Click on the specific container whose fields you want to preview.</p> <p>For demonstration purposes, we have selected the Netsuite Financials container.</p> <p></p> <p>Step 2: You will be directed to the fields of the selected container, where all the fields of the container will be displayed.</p> <p></p>"},{"location":"explore/profiles/#explore-tables","title":"Explore Tables","text":"<p>To explore the data in more detail, you can view the corresponding table of a selected container. This provides a comprehensive look at the data stored within, allowing for deeper analysis and exploration.</p> <p>Step 1: Click on the arrow icon on the right side of the container you want to preview.</p> <p></p> <p>Step 2: You will be directed to the corresponding table, providing a comprehensive view of the data stored in the container.</p> <p></p>"},{"location":"explore/profiles/#filter-and-sort","title":"Filter and Sort","text":"<p>Filter and Sort options allow you to organize your containers by various criteria, such as Name, Last Profiled, Last Scanned, Quality Score, Records, and Type. You can also apply filters to refine your list of containers based on Type.</p>"},{"location":"explore/profiles/#sort","title":"Sort","text":"<p>You can sort your containers by various criteria, such as Name, Last Profiled, Last Scanned, Quality Score, Records, and Type to easily organize and prioritize them according to your needs.</p> <p></p> No Sort By Description 1. Active Anomalies Sorts containers based on the number of currently active anomalies detected. 2. Active Checks Sorts containers by the number of active validation checks applied. 3. Completeness Sorts containers based on their data completeness percentage. 4. Created Date Sorts containers by the date they were created, showing the newest or oldest fields first. 5. Fields Sorts containers by the number of fields profiled. 6. Last Profiled Sorts by the most recent profiling container. 7. Last Scanned Sorts by the most recent scanned container. 8. Name Sorts containers alphabetically by their names. 9. Quality Score Sorts containers based on their quality score, indicating the reliability of the data in the field. 10. Records Sorts containers by the number of records profiled. 11. Type Sorts containers based on their data type (e.g., string, boolean, etc.). <p></p> <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.</p> <p></p>"},{"location":"explore/profiles/#filter","title":"Filter","text":"<p>You can filter your containers based on values like Type (Table, View, File, Computed Table and Computed File) to easily organize and prioritize them according to your needs.</p> <p></p>"},{"location":"explore/profiles/#mark-as-favorite","title":"Mark as Favorite","text":"<p>Marking a container as a favorite allows you to quickly access and prioritize the containers that are most important to your work, ensuring faster navigation and improved efficiency.</p> <p>Step 1: Locate the container which you want to mark as a favorite and click on the bookmark icon located on the left side of the container.</p> <p></p> <p>After clicking on the bookmark icon your container is successfully marked as a favorite and a success flash message will appear stating \"The Table has been favorited\"</p> <p></p> <p>To unmark, simply click on the bookmark icon of the marked container. This will remove it from your favorites.</p> <p></p>"},{"location":"explore/profiles/#fields","title":"Fields","text":"<p>By selecting the Fields section in the Qualytics platform, you can view all the fields across your data sources, including their quality scores, completeness, and metadata, for streamlined data management.</p>"},{"location":"explore/profiles/#field-details","title":"Field Details","text":"<p>Field Details view in the Qualytics platform provides in-depth insights into a selected field. It displays key information, including the field\u2019s declared type, number of distinct values, minimum and maximum length of observed values, entropy, and unique/distinct ratio. This detailed profiling allows you to understand the field's data structure, quality, and variability, enabling better data governance and decision-making.</p> <p>Step 1: Click on the specific field whose field details you want to preview.</p> <p></p> <p>A modal window will appear, providing detailed information about the selected field, such as its declared type, distinct values, length range, the Last Profile timestamp to indicate when the field was last profiled, and more.</p> <p></p>"},{"location":"explore/profiles/#manage-tags-in-field-details","title":"Manage Tags in Field Details","text":"<p>Tags can now be directly managed in the field profile within the Explore section. Simply access the Field Details panel to create, add, or remove tags, enabling more efficient and organized data management.</p> <p>Step 1: Click on the specific field that you want to manage tags.</p> <p></p> <p>A Field Details modal window will appear. Click on the + button to assign tags to the selected field.</p> <p></p> <p>Step 2: You can also create the new tag by clicking on the \u2795 button.</p> <p></p> <p>A modal window will appear, providing the options to create the tag. Enter the required values to get started.</p> <p></p> <p>For more information on creating tags, refer to the Add Tag section.</p>"},{"location":"explore/profiles/#filter-and-sort_1","title":"Filter and Sort","text":"<p>Filter and Sort options allow you to organize your fields by various criteria, such as Active Anomalies, Active Checks, Completeness, Created Date, Name, Quality Score, and Type. You can also apply filters to refine your list of fields based on Profile and Type. </p>"},{"location":"explore/profiles/#sort_1","title":"Sort","text":"<p>You can sort your containers by various criteria, such as Active Anomalies, Active Checks, Completeness, Created Date, Name, Quality Score, and Type to easily organize and prioritize them according to your needs.</p> <p></p> No Sort By Description 1. Active Anomalies Sorts fields based on the number of currently active anomalies detected. 2. Active Checks Sorts fields by the number of active validation checks applied. 3. Completeness Sorts fields based on their data completeness percentage. 4. Created Date Sorts fields by the date they were created, showing the newest or oldest fields first. 5. Name Sorts fields alphabetically by their names. 6. Quality Score Sorts fields based on their quality score, indicating the reliability of the data in the field. 7. Type Sorts fields based on their data type (e.g., string, boolean, etc.). <p></p> <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.</p> <p></p>"},{"location":"explore/profiles/#filter_1","title":"Filter","text":"<p>You can filter your fields based on Profiles and Type to easily organize and prioritize them according to your needs.</p> No. Filter Description 1. Profile Filters fields based on the Profiles (e.g., accounts, accounts.csv,  etc.). 2. Type Filters fields based on the data type (e.g., string, boolean, date, etc.). <p></p>"},{"location":"flows/flows/","title":"Flows","text":"<p>Flows enable users to create pipelines by chaining actions and configuring how they are triggered. Triggers can be set based on predefined events and filters, offering a flexible and efficient way to automate processes. These actions can be notifications or operations, allowing users to inform various notification channels or execute tasks based on specific operations.</p>"},{"location":"flows/flows/#navigation-to-flows","title":"Navigation to Flows","text":"<p>Step 1: Log in to your Qualytics account and click on Flows on the left side panel of the interface.  </p> <p></p> <p>You will navigate to the Flows interface, where you can add and manage flows. At the top, you will see two tabs:</p> <ul> <li>Definitions: Displays a list of all flows along with details like triggers, actions, tags, and the last triggered time.</li> </ul> <p></p> <ul> <li>Executions: Provides the execution history of flows, including their status and timestamps.</li> </ul> <p></p>"},{"location":"flows/flows/#add-flow","title":"Add Flow","text":"<p>Step 1: Click on the Add Flow button from the top right corner.</p> <p></p> <p>A modal window, Add Flow, will appear, providing options to create a flow. Each flow starts by default with two nodes: Flow and Trigger.</p> <p></p>"},{"location":"flows/flows/#flow","title":"Flow","text":"<p>Step 1: Click on the Flow node.  </p> <p></p> <p>A panel will appear on the right-hand side, allowing you to:</p> No. Field Name Description 1. Name Enter the name for the flow. 2. Description Provide a brief description of the flow (optional) to clarify its purpose or functionality. 3. Deactivated Check the box to deactivate the flow. If selected, the flow won't start even if the trigger conditions are met. <p></p> <p>Step 2: Once the details are filled in, click the Save button to save the flow settings.  </p> <p></p>"},{"location":"flows/flows/#trigger","title":"Trigger","text":"<p>Step 1: After completing the \"Flow\" node setup, users can click on the \"Trigger\" node.</p> <p></p> <p>A panel will appear on the right-hand side, enabling users to define when the flow should start. The panel provides four options for initiating the flow. Users can choose one of the following options:</p> <ul> <li> <p>Operation Completes.</p> </li> <li> <p>Anomalous Table and File Detection.</p> </li> <li> <p>Anomaly Detected.</p> </li> <li> <p>Manual</p> </li> </ul> <p></p>"},{"location":"flows/flows/#operation-completes","title":"Operation Completes","text":"<p>This type of flow is triggered whenever an operation, such as a catalog, profile, or scan, is completed on a source datastore. Upon completion, teams are promptly notified through in-app messages and, if configured, via external notification channels such as email, Slack, Microsoft Teams, and others. For example, the team is notified whenever the catalog operation is completed, helping them proceed with the profile operation on the datastore.</p> <p></p> <p>Filter Conditions</p> <p>Filters can be set to narrow down which operations should trigger the flow execution:</p> <ol> <li> <p>Source Datastore Tags: The flow is triggered only for source datastores that have all the selected tags assigned.</p> </li> <li> <p>Source Datastores: The flow is triggered only for the selected source datastores.</p> </li> <li> <p>Operation Types: The flow is triggered only for operations that match one or more of the selected types.</p> </li> <li> <p>Operation Status: The flow is triggered for operations with a status of either Success or Failure.</p> </li> </ol> <p></p> <p>After defining the conditions, users must click the Save button to finalize the trigger configuration.</p> <p></p>"},{"location":"flows/flows/#anomalous-table-and-file-detected","title":"Anomalous Table and File Detected","text":"<p>This flow is triggered when anomalies are detected within a specific table, file and check rule types. It includes information about the number of anomalies found and the specific scan target within the datastore. This is useful for assessing the overall health of a particular datastore.  </p> <p></p> <p>Filter Conditions</p> <p>Users can optionally set filters to specify which tables or files should trigger the flow execution.</p> <ol> <li> <p>Tables / Files Tags: Only tables or files with all the selected tags assigned will trigger the flow.</p> </li> <li> <p>Source Datastores: The flow is triggered only for the selected source datastores.</p> </li> <li> <p>Check Rule Types: Only anomalies identified by one or more of the selected check rule types will initiate the flow.</p> </li> </ol> <p></p> <p>After defining the conditions, users must click the Save button to finalize the trigger configuration.  </p> <p></p>"},{"location":"flows/flows/#anomaly-detected","title":"Anomaly Detected","text":"<p>This type of flow is triggered when any single anomaly is identified in the data. The flow message typically includes the type of anomaly detected and the datastore where it was found. It provides specific information about the anomaly type, which helps quickly understand the issue's nature.</p> <p></p> <p>Filter Condition</p> <p>Users can define specific conditions to determine when the flow should be initiated.</p> <ol> <li> <p>Anomaly\u2019s Tags: Only anomalies with all selected tags assigned will trigger the flow.</p> </li> <li> <p>Source Datastores: Only triggered when anomalies are detected in the selected datastores.</p> </li> <li> <p>Check Rule Types: Only anomalies identified by one or more of the selected check rule types will initiate the flow.</p> </li> <li> <p>Anomaly Weight (Min): Only anomalies with a weight equal to or greater than the specified value will trigger the flow.</p> </li> </ol> <p></p> <p>Step 2: Once the filter conditions are set, users must click the Save button to finalize the configuration.</p> <p></p>"},{"location":"flows/flows/#manual","title":"Manual","text":"<p>The flow starts only when the user manually triggers it. It doesn\u2019t depend on any automatic conditions or detections, giving the user full control.  </p> <p></p> <p>Once selected, users must click the Save button to confirm the manual trigger configuration.</p> <p></p> <p>Hover over the filter tooltip in trigger nodes to view the applied conditions such as tags, datastores, and operation types. This provides quick visibility into how each trigger is configured.</p> <p></p>"},{"location":"flows/flows/#actions","title":"Actions","text":"<p>Actions define the specific steps the system will execute after a flow is triggered. They allow users to automate tasks, send notifications, or interact with external systems.</p> <p>Step 1: After completing the \"Trigger\" node setup, users can click on the \"Actions\" node.  </p> <p></p> <p>A panel will appear on the right-hand side displaying the list of available actions. These actions define what the system will execute after the flow is triggered. The actions are categorized into three groups:</p> <ul> <li> <p>Operations.</p> </li> <li> <p>Notifications.</p> </li> <li> <p>HTTP.</p> </li> </ul> <p></p> <p>Info</p> <p>Inline summaries are shown within action nodes, displaying key details based on the action type\u2014for example, datastore names for operations, Slack or Teams channels for notifications, and webhook URLs for HTTP actions. This enhancement provides quick clarity during flow configuration.</p>"},{"location":"flows/flows/#operation","title":"Operation","text":"<p>Users can execute specific operations when the trigger activates. They can choose from the following options:</p> <ul> <li> <p>Catalog.</p> </li> <li> <p>Profile.</p> </li> <li> <p>Scan.</p> </li> <li> <p>Export.</p> </li> <li> <p>Materialize.</p> </li> </ul> <p></p> <p>Catalog</p> <p>Step 1: Click on Catalog. </p> <p></p> <p>A panel Catalog Settings will appear on the right-hand side. This window allows you to configure the catalog operation.</p> No. Field Description 1. Source Datastore Select the source datastore to catalog. 2. Prune Checkbox to enable or disable the removal of named collections (tables, views, files, etc.) that no longer exist in the datastore. 3. Recreate Checkbox to enable or disable the recreation of previously deleted named collections in Qualytics for the catalog. 4. Include Checkboxes to select Tables, Views, or both, specifying the resources to include in the catalog. <p></p> <p>Step 2: After configuring the settings, click Save to apply and proceed with the catalog operation.</p> <p></p> <p>Profile</p> <p>Step 1: Click on Profile. </p> <p></p> <p>A panel Profile Settings will appear on the right-hand side. This window allows you to configure the Profile operation.</p> <p></p> No. Field Description 1. Source Datastore Select the source datastore to profile. 2. Select Tables Allows users to select all tables, specific tables, or tables associated with selected tags to profile. 3. Read Settings Configure the starting point for profiling and set a maximum record limit per table for profiling. 4. Inference Settings Set the level of automated checks and decide whether inferred checks should be saved in draft mode. <p></p> <p>Step 2: Click Save to finalize the profile configuration.</p> <p></p> <p>Scan</p> <p>Step 1: Click on Scan.</p> <p></p> <p>A panel Scan Settings will appear on the right-hand side. This window allows you to configure the Scan operation.  </p> <p></p> <p>Source Datastore: Select the datastore to be scanned.</p> <p></p> <p>Select Tables: Choose all tables, specific tables, or tables associated with selected tags to include in the scan.</p> <p></p> <p>Select Check Categories: Select categories of checks to include, such as table properties (Metadata) or value checks (Data Integrity).</p> <p></p> <p>Read Settings: Define the scan strategy: incremental scans updated records; full scans process all records.</p> <p></p> <p>Starting Threshold: Set a starting point for scanning based on an incremental identifier.</p> <p></p> <p>Record Limit: Specify the maximum number of records to scan per table.</p> <p></p> <p>Scan Settings: Choose how to manage duplicate or recurring anomalies by archiving overlaps or reactivating previously archived anomalies with fingerprint tracking.</p> <p></p> <p>Anomaly Rollup Threshold: Set the Rollup Threshold to limit how many anomalies are created per check. When the limit is reached, anomalies will be merged into one for easier management.</p> <p></p> <p>Enrichment Source Record Limit: Define the number of source records to include in the enrichment operation.</p> <p></p> <p>Step 2: Click Save to finalize the scan configuration.</p> <p></p> <p>Export</p> <p>Step 1: Click on Export.</p> <p></p> <p>A panel Export Settings will appear on the right-hand side. This window allows you to configure the Export settings.</p> <p></p> <p>Source Datastore: Select the datastore to export data from.</p> <p></p> <p>Select file patterns to export: All (all file patterns, including future ones), Specific (manually chosen file patterns), or Tag (file patterns based on selected tags).</p> <p></p> <p>Select Metadata: Choose metadata to export anomalies, quality checks, or field profiles. Anomalies detect data issues, quality checks validate data, and field profiles store field metadata.</p> <p></p> <p>Step 2: Click Save to finalize the export configuration.</p> <p></p> <p>Export nodes display the asset type in their titles (e.g., \u201cExport Anomalies\u201d) to help you identify the exported content easily.</p> <p></p> <p>Materialize</p> <p>Step 1: Click on Materialize.</p> <p></p> <p>A panel Materialize Settings will appear on the right-hand side. This window allows you to configure the Materialize settings.</p> <p></p> <p>Source Datastore: Select the datastore to materialize data from.</p> <p></p> <p>Select Tables: Choose which tables (all, specific, or tagged) to extract from your source datastore and export to the enrichment datastore.</p> <p></p> <p>Read Settings: Select the record limit to control how much data is materialized per table.</p> <p></p> <p>Step 2: Click Save to finalize the materialize configuration.</p> <p></p>"},{"location":"flows/flows/#notification","title":"Notification","text":"<p>Users can configure the application to send notifications through various channels. The available notification options include:</p> <ul> <li> <p>In App.</p> </li> <li> <p>Email.</p> </li> <li> <p>Slack.</p> </li> <li> <p>Microsoft Teams.</p> </li> <li> <p>PagerDuty.</p> </li> </ul> <p></p> <p>In App</p> <p>This will send an app notification to all users that use Qualytics. Users can set a custom message using variables and modify the standard text.</p> <p>Step 1: Click on In App.</p> <p></p> <p>A panel In App Settings will appear on the right-hand side, allowing you to configure the notification message.</p> <p></p> <p>Message: Enter your custom message using variables in the Message field, where you can specify the content of the notification that will be sent out.</p> <p></p> <p>Tip</p> <p>You can write your custom notification message by utilizing the autocomplete feature. This feature allows you to easily insert internal variables such as <code>{{ flow_name }}</code>, <code>{{ container_name }}</code>, and <code>{{ datastore_name }}</code>. As you start typing, the autocomplete will suggest and recommend relevant variables in the dropdown.</p> <p>Step 2: After configuring the message, click Save to finalize the settings.</p> <p></p> <p>Email</p> <p>Adding email notifications allows users to receive timely updates or alerts directly in their inbox. By setting up notifications with specific triggers and channels, you can ensure that you are promptly informed about critical events, such as operation completions or detected anomalies. This proactive approach allows you to take immediate action when necessary, helping to address issues quickly and maintain the smooth and efficient operation of your processes.</p> <p>Step 1: Click on Email.</p> <p></p> <p>A panel Email Settings will appear on the right-hand side, allowing you to add email addresses, specify an email subject, and configure the notification message.</p> <p></p> No. Field Description 1. Email Address Enter the email address where the notification should be sent. 2. Email Subject Enter the subject line of the notification email to help recipients identify its purpose. 3. Message Text area to customize the notification message content with dynamic placeholders like <code>{{ flow_name }}</code>, <code>{{ operation_type }}</code>, and <code>{{ operation_result }}</code>. <p></p> <p>Step 2: Click the Test Notification button to send a test email to the provided address. If the email is successfully sent, you will receive a confirmation message indicating Notification successfully sent.</p> <p></p> <p>Step 3: Once all fields are configured, click the Save button to finalize the email notification setup.</p> <p></p> <p>Slack</p> <p>Qualytics integrates with Slack to deliver real-time notifications on scan completions, anomalies, and operational statuses, ensuring teams stay informed and can act quickly. With this integration, users receive instant alerts for system events, monitor scan results, and manage data anomalies directly within Slack. They can view notifications, acknowledge issues, and take necessary actions without switching platforms.</p> <p>Step 1: Click on Slack.</p> <p></p> <p>A Slack Settings panel appears on the right side of the screen.</p> <p></p> No. Field Description 1. Channel Choose the channel where notifications should be sent using the Channel dropdown. For demonstration purposes, the channel #demo is selected. 2. Preview Shows a preview of the Slack notification that will be sent when the flow runs. <p></p> <p>Step 2: Click the Test Notification button to send a sample notification to the selected Slack channel.</p> <p></p> <p>A prompt appears stating Notification successfully sent once the notification is successfully delivered.</p> <p></p> <p>Step 3: Once the notification is successfully sent, check your connected Slack workspace to ensure it is linked to Qualytics. You will see the test notification in the selected Slack channel.</p> <p>Note</p> <p>Each trigger generates a different type of Slack notification message. The content and format of the message vary based on the specific trigger event.</p> <p></p> <p>Step 4: After confirming that the notification was received successfully, return and click the Save button.</p> <p></p>"},{"location":"flows/flows/#examples-of-trigger-messages","title":"Examples of Trigger Messages","text":"<p>Trigger messages in Slack provide real-time notifications for various system events, ensuring timely awareness and action. Each trigger message follows a unique format and conveys different types of information based on the operation performed. Below are examples highlighting distinct scenarios:</p> <p>Scenario 1: Scan Completion Notification</p> <p>When a data cataloging or scan operation completes successfully, a notification is sent to Slack. The message includes details such as the dataset name, operation type (e.g., Catalog Operation), and the result of the operation. </p> <p></p> <p>Scenario 2: Anomalous Table or File Detected</p> <p>When a scan detects a critical data anomaly, Slack sends a detailed notification highlighting the issue. The notification includes the dataset name, flow (such as Quality Monitor), and source datastore. It also provides a summary of the anomaly, specifying the number of records that differ between datasets and the container where the discrepancy was found. Additionally, the message offers an option to view detailed results.</p> <p></p> <p>Scenario 3: Anomaly Detected</p> <p>When a scan detects record anomalies, Slack sends a notification highlighting the affected container, flow, and source datastore. It specifies the number of records that differ between datasets and provides options to view or acknowledge the anomaly.</p> <p></p>"},{"location":"flows/flows/#managing-qualytics-alerts-in-slack","title":"Managing Qualytics Alerts in Slack","text":"<p>Qualytics Slack integration enables real-time monitoring and quick action on data quality issues directly from Slack. This guide outlines the different types of alerts and the actions you can take without leaving Slack.</p> <p>When an Operation Success or failure </p> <p>Step 1: A Slack notification confirms the scan completion with a Success/failure status.</p> <p>For demonstration purposes we are using Success operation.</p> <p></p> <p>Step 2: Click View Operation to be redirected automatically to the result section in Qualytics.</p> <p></p> <p>When an Anomalous File or Table is Detected </p> <p>Step 1: A Slack alert notifies about anomalies in a dataset.</p> <p></p> <p>Step 2: Click View Results to examine the identified discrepancies directly in Qualytics.</p> <p></p> <p>When a Record Anomaly is Detected</p> <p>If a shape or record anomaly is found, you'll receive a Slack notification. You can take the following actions:</p> <p></p> <ul> <li>View Anomaly \u2013  Click on view anomaly to open the details in Qualytics to investigate further.  </li> </ul> <p></p> <ul> <li>Acknowledge \u2013 Click on Acknowledge to mark it as reviewed to avoid duplicate alerts.  </li> </ul> <p> </p> <ul> <li>Horizontal ellipsis(\u22ef)  \u2013 Click on horizontal ellipsis.</li> </ul> <p></p> <p>A dropdown will open with option comment and archive :  </p> <p></p> No. Action Description 1. Comment Add Comment to collaborate with your team. 2. Archive Archive if no further action is needed. <p>Microsoft Teams</p> <p>Step 1: Click on Microsoft Teams.</p> <p></p> <p>A panel Microsoft Teams Settings will appear on the right-hand side, allowing you to add a webhook url and configure the notification message.</p> <p></p> No. Field Description 1. Teams Webhook URL Enter the Teams webhook URL where the notification should be sent. 2. Message Text area to customize the notification message content with dynamic placeholders like <code>{{ flow_name }}</code>, <code>{{ operation_type }}</code>, and <code>{{ operation_result }}</code>. <p></p> <p>Step 2: Click the \"Test Notification\" button to send a test message to the provided \u201cWebhook URL\u201d. If the message is successfully sent, you will receive a confirmation notification indicating \"Notification successfully sent\".</p> <p></p> <p>Step 3: Once all fields are configured, click the Save button to finalize the Microsoft Teams notification setup.</p> <p></p> <p>PagerDuty</p> <p>Integrating PagerDuty with Qualytics ensures that your team gets instant alerts for critical data events and system issues. With this connection, you can automatically receive real-time notifications about anomalies, operation completions and other important events directly in your PagerDuty account. By categorizing alerts based on severity, it ensures the right people are notified at the right time, speeding up decision-making and resolving incidents efficiently. This helps your team respond quickly to issues, reducing downtime and keeping data operations on track.</p> <p>Step 1: Click on PagerDuty.</p> <p></p> <p>A PagerDuty Settings panel will appear on the right-hand side, enabling users to configure and send PagerDuty notifications.</p> <p></p> <p>Integration Key: Enter the Integration Key where you want the notification to be sent.</p> <p></p> <p>Severity: Select the appropriate PagerDuty severity level to categorize incidents based on their urgency and impact. The available severity levels are:</p> <ul> <li> <p>Info: For informational messages that don't require immediate action but provide helpful context.</p> </li> <li> <p>Warning: For potential issues that may need attention but aren't immediately critical.</p> </li> <li> <p>Error: For significant problems that require prompt resolution to prevent disruption.</p> </li> <li> <p>Critical: For urgent issues that demand immediate attention due to their severe impact on system operations.</p> </li> </ul> <p></p> <p>Message: Enter your custom message using variables in the Message field, where you can specify the content of the notification that will be sent out.</p> <p></p> <p>Tip</p> <p>You can write your custom notification message by utilizing the autocomplete feature. This feature allows you to easily insert internal variables such as <code>{{ flow_name }}</code>, <code>{{ operation_type }}</code>, and <code>{{ datastore_name }}</code>. As you start typing, the autocomplete will suggest and recommend relevant variables in the dropdown.</p> <p>Step 2: Click on the Test notification button to check if the integration key is functioning correctly. Once the test notification is sent, you will see a success message, \"Notification successfully sent.\"</p> <p></p> <p>Step 3: Once you have entered all the values, then click on the Save button.</p> <p></p>"},{"location":"flows/flows/#http","title":"HTTP","text":"<p>Users can connect to external apps for notifications using one of these services:</p> <ul> <li> <p>Webhook.</p> </li> <li> <p>HTTP Action.</p> </li> </ul> <p></p> <p>Webhook</p> <p>Qualytics allows you to connect external apps for notifications using webhooks, making it easy to stay updated in real time. When you set up a webhook, it sends an instant alert to the connected app whenever a specific event or condition occurs. This means you can quickly receive notifications about important events as they happen and respond right away. By using webhook notifications, you can keep your system running smoothly, keep everyone informed, and manage your operations more efficiently.</p> <p>Step 1: Click on Webhook.</p> <p></p> <p>A Webhook Settings panel will appear on the right-hand side, enabling users to configure and send webhook notifications.</p> <p></p> No. Field Description 1. Webhook URL Enter the desired \"Webhook URL\" of the target system where you want to receive notifications. 2. Message Text area to customize the notification message content with dynamic placeholders like <code>{{ flow_name }}</code>, <code>{{ operation_type }}</code>, and <code>{{ operation_result }}</code>. <p></p> <p>Step 2: Click on the \"Test HTTP\" button to send a test notification to the webhook URL you provided. If the webhook URL is correct, you will receive a confirmation message saying \"Notification successfully sent.\" This indicates that the webhook is functioning correctly.</p> <p></p> <p>Step 3: Once you have entered all the values, then click on the Save button.</p> <p></p> <p>HTTP Action</p> <p>Integrating HTTP Action notifications allows users to receive timely updates or alerts directly to a specified server endpoint. By setting up HTTP Action notifications with specific trigger conditions, you can ensure that you are instantly informed about critical events, such as operation completions or anomalies detected. This approach enables you to take immediate action when necessary, helping to address issues quickly and maintain the smooth and efficient operation of your processes.</p> <p>Step 1: Click on HTTP Action.</p> <p></p> <p>An HTTP Action Settings panel will appear on the right-hand side, enabling users to configure and send HTTP Action notifications.</p> <p></p> <p>Step 2: Enter the following detail where you want the notification to be sent.</p> <p>1. Action URL: Enter the \u201cAction URL\u201d in this field. It specifies the server endpoint for the HTTP request and defines where data will be sent or retrieved. It must be correctly formatted and accessible, including the protocol (http or https), domain, and path.</p> <p>2. HTTP Verbs: HTTP verbs specify the actions performed on server resources. Common verbs include:</p> <ul> <li>POST: Use POST to send data to the server to create something new. For example, it's used for submitting forms or uploading files. The server processes this data and creates a new resource.  </li> <li>PUT: Updates or creates a resource, replacing it entirely if it already exists. For example, updating a user\u2019s profile information or creating a new record with specific details.  </li> <li>GET: Retrieves data from the server without making any modifications. For example, requesting a webpage or fetching user details from a database.</li> </ul> <p>3. Username: Enter the username needed for authentication.</p> <p>4. Auth Type: This field specifies how to authenticate requests. Choose the method that fits your needs:</p> <ul> <li>Basic: Uses a username and password sent with each request. Example: \u201cAuthorization: Basic \u201d. </li> <li>Bearer: Uses a token included in the request header to access resources. Example: \u201cAuthorization: Bearer &lt; token &gt;\u201d. </li> <li>Digest: Provides a more secure authentication method by using a hashed combination of the username, password, and request details. Example: Authorization: Digest username=\" \", realm=\" \", nonce=\" \", uri=\" \", response=\" \".</li> </ul> <p>5. Secret: Enter the password or token used for authentication. This is paired with the Username and Auth Type to securely access the server. Keep the secret confidential to ensure security.</p> <p>6. Message: Enter your custom message using variables in the Message field, where you can specify the content of the notification that will be sent out.</p> <p></p> <p>Tip</p> <p>You can write your custom notification message by utilizing the autocomplete feature. This feature allows you to easily insert internal variables such as <code>{{ flow_name }}</code>, <code>{{ operation_type }}</code>, and <code>{{ datastore_name }}</code>. As you start typing, the autocomplete will suggest and recommend relevant variables in the dropdown.</p> <p>Step 3: Click the \"Test HTTP\" button to verify the correctness of the Action URL. If the URL is correct, a confirmation message saying \"Notification successfully sent\" will appear, confirming that the HTTP action is set up and functioning properly.</p> <p></p> <p>Step 4: Once you have entered all the values, then click on the Save button.</p> <p></p> <p>Step 3: After completing all the required details in the \"Add Flow\" section, click on the Publish button to finalize the process.</p> <p></p> <p>After clicking the Publish button, a success notification appears confirming that the flow has been successfully added.</p>"},{"location":"flows/flows/#view-created-flows","title":"View Created Flows","text":"<p>Once a flow is added, it will be visible in the Definitions tab, where you can view all the created flows.</p> <p></p>"},{"location":"flows/flows/#clone-a-flow","title":"Clone a Flow","text":"<p>Users can duplicate existing flows to simplify the reuse and modification of flow configurations for similar scenarios.</p> <p>Step 1: Click on the existing flow you want to clone.</p> <p></p> <p>Step 2: A new window will open displaying the flow's detailed configuration. Click the settings icon and select Clone.</p> <p></p> <p>Step 3: After selecting the clone button, click the Publish button to publish it.</p> <p></p> <p>After clicking the Publish button, a success notification appears confirming that the flow has been successfully added.</p>"},{"location":"flows/flows/#sort-flows","title":"Sort Flows","text":"<p>Qualytics allows you to sort your flows by Created Date and Name to easily organize and prioritize them according to your needs.  </p> <p></p> <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.  </p> <p></p>"},{"location":"flows/flows/#execute-manual-flows","title":"Execute Manual Flows","text":"<p>Users can start a manual flow from the vertical ellipsis menu for greater flexibility in executing flows.</p> <p>Step 1: Locate the manual flow in your list of flows.</p> <p></p> <p>Step 2: Click the vertical ellipsis (\u22ee) next to the manual flow you wish to execute, then select \"Execute\" from the dropdown menu to trigger the flow.</p> <p></p> <p>After clicking the Execute button, a success notification appears confirming that the flow has been successfully executed.</p>"},{"location":"flows/flows/#manage-flows","title":"Manage Flows","text":"<p>Manage Flow  allows users to edit, delete, deactivate or activate flows. Users can update configurations, remove outdated flows, or pause triggers to maintain an organized and efficient workflow system.</p>"},{"location":"flows/flows/#edit-flow","title":"Edit Flow","text":"<p>Edit Flow feature lets users update existing flows by modifying configurations or adding actions.</p> <p>Step 1: Click the flow you want to edit.</p> <p></p> <p>Step 2: After clicking the flow, a new window will open displaying the flow's detailed configuration. Click on the boxes you want to edit.</p> <p>For demonstration purposes we have selected the Flow node.</p> <p></p> <p>Step 3: Click the <code>Save</code> button to apply the updates.</p> <p></p> <p>Step 4: After clicking the <code>Save</code> button, click the <code>Publish</code> button located in the top right corner to finalize and publish the changes.</p> <p></p>"},{"location":"flows/flows/#delete-flow","title":"Delete Flow","text":"<p>Delete Flow feature allows you to permanently remove unwanted or outdated flows from the system. This helps in maintaining a clean and organized list of active flows.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the flow that you want to delete, then click on Delete from the dropdown menu.</p> <p></p> <p>After clicking the delete button, a confirmation modal window Delete Flow will appear.</p> <p></p> <p>Step 2: Click on the Delete button to delete the flow.</p> <p></p> <p>After clicking the Delete button, a success notification appears confirming the deletion.</p>"},{"location":"flows/flows/#deactivate-flow","title":"Deactivate Flow","text":"<p>Users can deactivate a flow to pause its triggers by disabling it. This prevents the flow from being executed until it is reactivated.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the flow that you want to deactivate, then click on Deactivate from the dropdown menu.</p> <p></p> <p>After clicking the Deactivate button, a success notification appears confirming the deactivation.</p>"},{"location":"flows/flows/#activate-flow","title":"Activate Flow","text":"<p>Users can reactivate a flow that was previously deactivated. Once reactivated, the flow\u2019s triggers become active again, allowing it to run automatically based on the defined conditions.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the flow that you want to activate, then click on Activate from the dropdown menu.</p> <p></p> <p>After clicking the Activate button, a success notification appears confirming the activation.</p>"},{"location":"flows/flows/#clone-an-action","title":"Clone an Action","text":"<p>Users can duplicate an existing action in just a few clicks. Cloning an action allows you to quickly replicate its configuration without manually setting it up again.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) on the action you want to clone, then select the Clone option from the menu.</p> <p></p> <p>Step 2: After clicking the Clone option, a cloned action will be created.</p> <p></p>"},{"location":"flows/flows/#flows-execution","title":"Flows Execution","text":"<p>Execution tab allows users to view the execution history and current status of a flow. It provides detailed timestamps, status updates, and a comprehensive record of flow executions for efficient tracking and analysis.</p> <p>Click on the Execution tab.</p> <p></p> <p>You will be navigated to the Execution tab, where you can view the complete execution history of all created flows.</p> <p></p>"},{"location":"flows/flows/#see-a-flow-execution","title":"See a Flow Execution","text":"<p>Users can view flow execution in real-time by clicking on the desired flow operation. The page shows detailed operations but does not allow editing.</p> <p>Step 1: Click on the flow operation you want to view.  </p> <p></p> <p>After clicking, the user will navigate to the selected flow operation details. The page displays all operational details in real-time. Note that this page is for viewing only, and no edits can be made here.  </p> <p></p>"},{"location":"flows/flows/#understanding-flow-states","title":"Understanding Flow States","text":"<p>On the bottom-right corner, there is a Legend indicating the possible states of an action, such as:</p> <ul> <li> <p>Success (Green)</p> </li> <li> <p>Failure (Red)</p> </li> <li> <p>Aborted (Orange)</p> </li> <li> <p>Skipped (Yellow)</p> </li> <li> <p>Running (Blue with dotted lines animation)</p> </li> <li> <p>Pending (Gray)</p> </li> </ul> <p></p> <p>If a step is running, you will see a dot-line animation, signaling that the step is in progress. Once completed, the Action box will change its color to reflect the final state.  </p> <p></p>"},{"location":"flows/flows/#accessing-operation-results","title":"Accessing Operation Results","text":"<p>To view detailed results of specific operations:</p> <p>Step 1: Click the Top Right Arrow button within the action operation box.  </p> <p></p> <p>Step 2: You will navigate to the Activity page, where a Result Modal will open, displaying in-depth details of the operation.</p> <p></p>"},{"location":"flows/flows/#delete-flow-execution","title":"Delete Flow Execution","text":"<p>Step 1: Click the Delete icon next to the flow execution you want to remove.</p> <p></p> <p>A confirmation modal window Delete Flow Execution will appear.</p> <p></p> <p>Step 2: Click on the Delete button to delete the flow execution.</p> <p></p> <p>After clicking the Delete button, a success notification appears confirming the deletion.</p>"},{"location":"flows/flows/#filter-and-sort","title":"Filter and Sort","text":"<p>Filter and Sort  in the <code>Executions</code> tab help organize flow execution data. Users can sort by creation date or duration and filter by flow name, status, or trigger type for quick access to specific details.</p>"},{"location":"flows/flows/#sort","title":"Sort","text":"<p>Sort By feature allows users to organize executions by Created Date or Duration, simplifying the process of reviewing flow executions based on their creation or runtime.</p> <p></p>"},{"location":"flows/flows/#filter","title":"Filter","text":"<p>Filter feature allows users to refine flow execution results based on specific criteria. By clicking the filter icon, users can choose from the following options:</p> No. Filter Description 1. Flows Select a specific flow to view its executions. 2. Status Filter executions by their completion status (e.g., success, failure and running). 3. Trigger When Filter executions based on their trigger condition. <p></p>"},{"location":"flows/flows/#operations","title":"Operations","text":"<p>In the Activity tab, users can easily identify flow executions. The <code>Flow</code> column shows the flow name and includes a button to redirect users to the flow's operation. This feature is available in Explore Activities, Datastore Activity, and Container Activity.</p> <p></p>"},{"location":"integrations/alerting/","title":"Alerting Integrations","text":"<p>The Qualytics platform integrates with popular enterprise messaging platforms, such as Slack and Microsoft Teams to enable real-time communication about data quality events. These integrations help ensure that your teams remain informed and can respond quickly when data issues occur.</p> <ul> <li>Receive instant notifications when data quality issues are detected</li> <li>Alert relevant team members about failed quality checks in real-time</li> <li>Share operational status updates and system health notifications</li> <li>Configure custom alerts based on data quality thresholds and conditions</li> <li>Route notifications to specific channels or teams based on data context</li> </ul> <p>These integrations ensure your teams stay informed about data quality events as they happen, enabling rapid response and maintaining continuous data quality awareness across your organization.</p>"},{"location":"integrations/alerting/#available-integration","title":"Available Integration","text":"<p>Qualytics makes it easy to deliver alerts through the communication platforms your teams already rely on. Below are the currently supported integrations:</p>"},{"location":"integrations/alerting/#slack","title":"Slack","text":"<p>Integrate Qualytics with Slack to send real-time alerts directly to your Slack channels. This allows teams to stay on top of data quality events without switching tools.</p> <p>For more details you can refer to the slack integration documentation.</p> <p></p>"},{"location":"integrations/alerting/#microsoft-teams","title":"Microsoft Teams","text":"<p>Connect Microsoft Teams to receive automated alerts about failed checks, system health updates, and threshold-based events right within your team chats.</p> <p>For more details you can refer to the microsoft teams documentation.</p> <p></p>"},{"location":"integrations/analytics/","title":"Analytics Integrations","text":"<p>The Qualytics platform integrates with enterprise analytics and visualization tools, enabling organizations to:</p> <p> </p> <ul> <li>Create custom dashboards showcasing data quality metrics and trends</li> <li>Visualize data quality scores across your entire data ecosystem</li> </ul> <p> </p> <ul> <li>Monitor anomaly patterns and profiling characteristics in real-time</li> <li>Generate detailed reports on data quality at field, schema, and enterprise levels</li> <li>Share data quality insights through familiar business intelligence interfaces</li> <li>Track data quality improvement initiatives with executive-level visibility</li> </ul> <p> </p> <p>These integrations transform Qualytics' rich metadata and quality metrics into actionable insights, helping organizations understand, communicate, and improve their data quality through their preferred analytics platforms.</p>"},{"location":"integrations/compute/","title":"Compute Integrations","text":"<p>The Qualytics platform offers flexible compute deployment options to optimize performance and resource utilization:</p> <ul> <li>Leverage existing Kubernetes infrastructure for seamless deployment</li> <li>Scale compute resources dynamically based on workload demands</li> <li>Deploy the data plane to external Spark environments</li> <li>Maintain data sovereignty and security compliance</li> <li>Take advantage of cloud-native performance optimizations</li> </ul> <p>These compute integration options ensure that Qualytics can adapt to your infrastructure requirements while maximizing performance and cost efficiency.</p>"},{"location":"integrations/compute/#running-qualytics-data-plane-on-kubernetes","title":"Running Qualytics Data Plane on kubernetes","text":"<p>The Qualytics platform's default deployment unifies control and data planes within a single Kubernetes cluster,  simplifying infrastructure management through a declarative approach.  This architecture enables dynamic scaling with cost-optimized spot instances while maintaining seamless  coordination between platform components.</p> <p>Alternatively, we support deploying the Qualytics data plane to any external Spark cluster   (external to the kubernetes cluster running the Qualytics control plane). </p>"},{"location":"integrations/compute/#running-qualytics-data-plane-on-databricks","title":"Running Qualytics Data Plane on Databricks","text":"<p>Deploying the Qualytics data plane within your Databricks account will allow our analytics engine to leverage  Photon acceleration while ensuring that all data transfer &amp; compute occurs inside your Databricks deployment.</p> <p></p>"},{"location":"integrations/compute/#running-qualytics-data-plane-on-gcp-dataproc","title":"Running Qualytics Data Plane on GCP Dataproc","text":"<p>Similarly, the Qualytics data plane can be deployed to Google Cloud's Dataproc to leverage Dataproc serverless and other  Dataproc optimizations in support of the Qualytics analytics engine.   </p>"},{"location":"integrations/overview/","title":"Integrations","text":"<p>The Qualytics platform seamlessly connects with your enterprise technology ecosystem, transforming data quality management from a standalone process into an integral part of your data operations. Our comprehensive integration capabilities ensure that data quality insights and actions flow naturally through your existing tools and workflows.</p>"},{"location":"integrations/overview/#available-integrations","title":"Available Integrations","text":""},{"location":"integrations/overview/#source-datastores","title":"Source Datastores","text":"<p>Connect directly to your data wherever it lives - from traditional databases to modern cloud storage platforms. Qualytics provides unified quality management across your entire data landscape through our Datastore framework.</p>"},{"location":"integrations/overview/#data-catalogs","title":"Data Catalogs","text":"<p>Surface data quality insights directly within your enterprise data catalogs, enhancing data discovery and governance with rich quality metrics and real-time anomaly detection.</p>"},{"location":"integrations/overview/#compute","title":"Compute","text":"<p>Leverage flexible deployment options to optimize performance and resource utilization, whether using our managed Kubernetes infrastructure or your own external compute environment.</p>"},{"location":"integrations/overview/#alerting","title":"Alerting","text":"<p>Receive instant notifications about data quality events through your enterprise messaging platforms, enabling rapid response to quality issues as they emerge.</p>"},{"location":"integrations/overview/#ticketing","title":"Ticketing","text":"<p>Track and manage data quality initiatives within your existing project management tools, seamlessly incorporating quality management into your team's established workflows.</p>"},{"location":"integrations/overview/#workflow","title":"Workflow","text":"<p>Embed data quality checks directly into your data pipelines and transformation processes, ensuring quality gates are enforced at every stage of your data lifecycle.</p>"},{"location":"integrations/overview/#analytics","title":"Analytics","text":"<p>Visualize data quality metrics and trends through your preferred business intelligence tools, providing actionable insights to stakeholders across your organization.</p>"},{"location":"integrations/overview/#single-sign-on","title":"Single Sign-On","text":"<p>Enable secure, frictionless access to Qualytics through your enterprise identity provider, maintaining consistent authentication and access control policies.</p>"},{"location":"integrations/sso/","title":"SSO (Single Sign-On) Integrations","text":"<p>The Qualytics platform provides enterprise-grade authentication integration capabilities, enabling organizations to:</p> <ul> <li>Implement secure, frictionless access across all platform components</li> <li>Leverage existing identity providers and authentication workflows</li> <li>Support both cloud-based and on-premise deployment scenarios</li> <li>Maintain compliance with corporate security policies</li> <li>Enable seamless mobile and web-based access</li> <li>Automate user provisioning and access management</li> </ul> <p>These authentication capabilities ensure that Qualytics seamlessly integrates with your organization's identity and access management infrastructure while maintaining the highest security standards.</p>"},{"location":"integrations/sso/#sso-for-paas-deployments","title":"SSO for PaaS Deployments","text":"<p>Qualytics platform harnesses the power of Auth0's Single Sign-On (SSO) technology to create a frictionless authentication journey for our PaaS users. Once users have successfully logged in to Qualytics, they can conveniently access all linked external applications and services without the need for additional sign-ins. Depending on the application and its compatibility with federated SSO protocols such as SAML, OIDC, or any proprietary authentication methods, Qualytics, with the help of Auth0, establishes a secure connection for user authentication. In essence, SSO allows one central domain to authenticate and then share the session across various other domains. The method of sharing may vary between SSO protocols, but the principle remains constant.</p> <p>Through Auth0's Integration Network (OIN), Qualytics extends SSO access to an extensive range of supported cloud-based applications. These integrations can utilize OpenID Connect (OIDC), SAML, SWA, or proprietary APIs for SSO. Maintenance of SSO protocols and provisioning APIs is reliably managed by Auth0.</p> <p>In addition to this, Qualytics also leverages Auth0's capabilities to provide SSO integrations for on-premises web-based applications. You have the option to integrate these applications via SWA or SAML toolkits. In addition, Auth0 supports user provisioning and deprovisioning with applications that publicly offer their provisioning APIs.</p> <p>Further enhancing our SSO integrations, Qualytics provides seamless access to mobile applications. Whether they are web applications optimized for mobile devices, native iOS apps, or Android apps, users can access web app integrations in the OIN using SSO from any mobile device. These mobile web apps can employ industry-standard OIDC, SAML, or Auth0 SWA technologies. To illustrate, Qualytics, in conjunction with Auth0, can integrate with native applications such as Box Mobile using SAML for registration and OAuth for continuous use.</p> <p>Auth0 supports the following enterprise providers out of the box: -  OAuth2 -   Active Directory/LDAP</p> <ul> <li> <p>ADFS</p> </li> <li> <p>Azure Active Directory Native</p> </li> <li> <p>Google Workspace</p> </li> <li> <p>OpenID Connect</p> </li> <li> <p>Okta</p> </li> <li> <p>PingFederate</p> </li> <li> <p>SAML</p> </li> <li> <p>Azure Active Directory</p> </li> </ul>"},{"location":"integrations/sso/#sso-for-on-premise-deployments","title":"SSO for On-Premise Deployments","text":"<p>In addition to the option of leveraging our robust Auth0 support for federated authentication, customer-managed deployments can choose to directly integrated with their IdP (Identity Provider such as Active Directory, ForgeRock, etc) using OpenID Connect (OIDC). Once configured for direct federated authentication using OIDC, the customer's own user login requirements fully govern the authentication process in support of a fully air-gapped deployment of Qualytics with no egress required for operations.</p>"},{"location":"integrations/ticketing/","title":"Ticketing Integrations","text":"<p>The Qualytics platform seamlessly integrates with enterprise ticketing and project management solutions, enabling teams to:</p> <ul> <li>Track and manage data quality anomalies within their existing workflow tools</li> <li>Automatically create and update tickets when quality issues are detected</li> <li>Sync status changes bidirectionally between Qualytics and ticketing systems</li> <li>Incorporate data quality management into standard project planning processes</li> <li>Maintain a unified view of data quality initiatives alongside other work items</li> </ul> <p>This integration capability ensures that data quality management becomes a natural part of your team's established workflows rather than a separate process to manage.</p>"},{"location":"integrations/workflow/","title":"Workflow Integrations","text":"<p>The Qualytics platform integrates with modern data workflow and orchestration tools, empowering teams to:</p> <p> </p> <ul> <li>Embed data quality checks directly within data pipelines</li> <li>Automate quality verification steps in ETL and transformation processes</li> <li>Trigger remediation workflows when quality issues are detected</li> <li>Schedule and coordinate data quality operations with other pipeline activities</li> <li>Ensure data quality gates are enforced before critical data movements</li> </ul> <p> </p> <p>These integrations enable organizations to make data quality an integral part of their automated data workflows, ensuring quality checks and remediation steps are seamlessly woven into their data engineering processes.</p>"},{"location":"integrations/alerting/msft_teams/","title":"Microsoft Teams","text":"<p>Microsoft Teams integration in Qualytics enables seamless communication by connecting your Microsoft Teams workspace with data quality updates and notifications. It involves configuring Azure resources, providing necessary credentials, and establishing a direct link to your Teams workspace for alerts and communication.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"integrations/alerting/msft_teams/#microsoft-teams-setup-guide","title":"Microsoft Teams Setup Guide","text":"<p>This section provides a comprehensive walkthrough to help you configure the necessary resources and retrieve the required credentials. By following this setup process, you'll have everything you need to complete the integration form.</p> <p>Warning</p> <p>Some steps in this guide may require administrator privileges in your Microsoft Azure environment. If you don't have the necessary permissions, you might need to coordinate with your IT department or someone with administrative access to your Azure tenant.</p>"},{"location":"integrations/alerting/msft_teams/#creating-a-microsoft-entra-app-registration","title":"Creating a Microsoft Entra App Registration","text":"<p>The Microsoft Entra App Registration is used by Qualytics to provision Teams bot resources in your environment.</p> <p>Step 1: Log in to to the Microsoft Entra App Registrations, and select New registration from the main menu to create a new application.</p> <p></p> <p>Step 2: You will be navigated to the App registrations dashboard. Fill in the required details for the app registration:</p> <ul> <li> <p>Name: Enter a name for your app (e.g., Qualytics Bot Manager).</p> </li> <li> <p>Supported account types: Select \"Accounts in this organizational directory only (Single tenant)\".</p> </li> <li> <p>Redirect URI: Leave this field blank, as it is not required for this integration.</p> </li> </ul> <p></p> <p>Step 3: Click Register button to complete the app registration.</p> <p></p> <p>Step 4: After the app is registered, you\u2019ll be redirected to the Overview page, where the Application (client) ID is displayed. Copy this ID since it will be needed later for the Qualytics integration.</p> <p></p>"},{"location":"integrations/alerting/msft_teams/#adding-api-permissions","title":"Adding API Permissions","text":"<p>The Microsoft Entra App needs the \"Application.ReadWrite.OwnedBy\" permission to create and manage bot resources.</p> <p>Step 1: In your app registration, go to the side panel and click Manage, then select API permissions from the dropdown.</p> <p></p> <p>Step 2: Click on Add permission to begin configuring access permissions for the app.</p> <p></p> <p>Step 3: A right side panel titled Request API permissions will appear. Select Microsoft Graph from the list of options.</p> <p></p> <p>Step 4: After selecting Microsoft Graph, choose Application permissions.</p> <p></p> <p>A dropdown appears search for Application.ReadWrite.OwnedBy, check the box under Application permissions, and click Add permissions.</p> <p></p> <p>Step 5: Once the permission is added, you'll return to the API permissions page. Click Grant admin consent for [Your Organization] to approve the selected permissions.</p> <p></p>"},{"location":"integrations/alerting/msft_teams/#creating-a-client-secret","title":"Creating a Client Secret","text":"<p>The Client Secret authorizes Qualytics to programmatically create bot resources.</p> <p>Step 1: In your app registration, go to the side panel and click Manage, then select Certificates &amp; secrets from the dropdown.</p> <p></p> <p>Step 2: Click on + New client secret to generate a new secret key for the application.</p> <p></p> <p>Step 3: After clicking + New client secret, a panel will appear. Enter a description (e.g., Qualytics Integration) and choose an expiration period (up to 24 months). Then click Add.</p> <p></p> <p>Step 4: Once the client secret is created, copy the Value immediately and save it securely. This will be used as the App Client Secret for the Qualytics integration.</p> <p>Warning</p> <p>The client secret value is only displayed once immediately after creation. Make sure to copy and securely store it as you won't be able to retrieve it again.</p>"},{"location":"integrations/alerting/msft_teams/#retrieving-the-azure-subscription-id","title":"Retrieving the Azure Subscription ID","text":"<p>The Subscription ID is required to manage bot resources in your Azure environment.</p> <p>Step 1:  Navigate to Subscriptions in the Azure Portal and select the subscription you want to use for the Teams integration.</p> <p></p> <p>Step 2: Copy the Subscription ID from the Overview section of your selected subscription. This ID is required later to assign roles and permissions for the Microsoft Teams integration.</p> <p></p>"},{"location":"integrations/alerting/msft_teams/#verifying-the-microsoft-bot-service","title":"Verifying the Microsoft Bot Service","text":"<p>You need to verify if the Microsoft Bot Service resource provider is registered in your subscription.</p> <p>Step 1: In your subscription, click Settings from the left-hand menu, then select Resource providers from the dropdown.</p> <p></p> <p>Step 2:Search for Microsoft.BotService in the provider list and check that the Status is Registered.</p> <p></p> <p>Note</p> <p>The step 3 is only required if the resource provider is not already registered. If the Microsoft.BotService provider is already marked as \"Registered\" in your subscription, you can skip this step.</p> <p>Step 3 (maybe optional): Click Register to enable the resource provider if it's not already registered.</p> <p></p>"},{"location":"integrations/alerting/msft_teams/#setting-up-the-resource-group","title":"Setting Up the Resource Group","text":"<p>The Resource Group will hold and manage the bot resources created by Qualytics.</p> <p>Step 1: Navigate to Resource Groups in the Azure Portal and click Create a resource to set up a new resource group if you don\u2019t already have one.</p> <p></p> <p>Step 2: Choose your Subscription, enter a Resource group name (e.g., <code>qualytics-msft-teams-rg</code>), select a Region, and then click Review + create.</p> <p></p> <p>Step 3: After clicking Review + create, you'll see a summary of the details. Once validated, click Create.</p> <p>Note</p> <p>Once created, note the Resource Group name for the Qualytics integration.</p> <p></p>"},{"location":"integrations/alerting/msft_teams/#assigning-the-azure-bot-service-contributor-role","title":"Assigning the Azure Bot Service Contributor Role","text":"<p>The Microsoft Entra App needs the \"Azure Bot Service Contributor\" role to manage bot resources.</p> <p>Step 1: Navigate to your Resource Group and select Access control (IAM) from the left menu and click on  the Add and select Add Role Assignment from the dropdown.</p> <p></p> <p>Step 2: You\u2019ll be navigated to the Add role assignment tab. In the Role section, search and select Azure Bot Service Contributor Role, then click the Next button to continue.</p> <p></p> <p>Step 3: You will be navigated to the Members tab. Under Assign access to, select User, group, or service principal, then click on Select members.</p> <p></p> <p>A Select members panel will appear. Search for the Microsoft Entra app you created earlier (e.g., Qualytics Bot Manager), select it from the list, and click on the Select button.</p> <p></p> <p>Tip</p> <p>Enterprise Applications will only appear in the search results when you start typing the exact name used in your Entra App registration. If you don't see your app immediately, try typing the full name as you entered it when creating the app.</p> <p>Step 4: Click on Review + assign from the navigation bar and confirm the role assignment then click on Review + assign button.</p> <p></p>"},{"location":"integrations/alerting/msft_teams/#getting-your-microsoft-teams-link","title":"Getting Your Microsoft Teams Link","text":"<p>You need to provide the link to your Microsoft Teams workspace.</p> <p>Step 1: Log in to your Microsoft Teams desktop or web application. Navigate to the team where you want to receive Qualytics notifications, then right-click on the team name and select Get link to team.</p> <p></p> <p>Step 2: A modal window titled Get a link to the team will appear. Click the Copy button to copy the team link.</p> <p></p>"},{"location":"integrations/alerting/msft_teams/#integration-summary","title":"Integration Summary","text":"<p>Now that you've gathered all the necessary information and configured the Azure resources, you're ready to integrate Microsoft Teams with Qualytics.</p> <p>In the next section, we'll walk through the steps to access the Qualytics integration interface and enter these credentials to establish the connection between Qualytics and Microsoft Teams.</p>"},{"location":"integrations/alerting/msft_teams/#navigation-to-integration","title":"Navigation to Integration","text":"<p>Step 1: Log in to your Qualytics account and click the \"Settings\" button on the left side panel of the interface.</p> <p></p> <p>Step 2: By default, Connections tab will open. Click on the Integrations tab.</p> <p></p>"},{"location":"integrations/alerting/msft_teams/#connect-microsoft-teams-integration","title":"Connect Microsoft Teams Integration","text":"<p>Connect Microsoft Teams by providing necessary Azure credentials, configuring bot resources, and establishing a direct link to your Teams workspace for secure communication.</p> <p>Step 1: Click on the Connect button next to Microsoft Teams to connect to the Teams Integration.</p> <p></p> <p>A modal window titled \"Add Microsoft Teams Integration\" appears. Fill in the connection properties to connect to Microsoft Teams.</p> <p></p> <p>Step 2: Fill out the required provisioning properties for the Microsoft Teams integration:</p> No. Field Name Description 1. App Client ID The Application (client) ID from the Overview page of your Entra App registration. 2. App Client Secret The secret value you copied after creating a new client secret in your Entra App. 3. Azure Subscription ID The Subscription ID you copied from the Azure Subscriptions page. 4. Azure Resource Group Name The name of the Resource Group you created or selected for bot resources. 5. Microsoft Teams Link The team link you copied from Microsoft Teams using the \"Get link to team\" option. <p></p> <p>Step 3: Click the Provision and Next button to provision the app resources and proceed with publishing the Qualytics app to the Microsoft Teams App Catalog.</p> <p>Note</p> <p>Provisioning the app resources may take around 15 seconds to complete.</p> <p></p> <p>Once the app resources have been successfully provisioned, a confirmation message will appear stating, \"The Teams app resources have been successfully provisioned.\"</p> <p></p> <p>Step 4: Click the Publish button to publish the Qualytics app to your organization's Microsoft Teams App Catalog.</p> <p></p> <p>A microsoft dialog will appear asking you to accept the requested permissions. Click Accept to proceed with the publication.</p> <p>Once the app has been successfully published, a confirmation message will appear stating, \"The Teams app has been successfully published to your organization's App Catalog.\".</p> <p>Warning</p> <p>Microsoft may take up to 24 hours to make the app available in Teams after it's published.</p>"},{"location":"integrations/alerting/msft_teams/#completing-the-teams-integration","title":"Completing the Teams Integration","text":"<p>After publishing the app to your organization's Teams App Catalog, the integration will show a \"Pending\" status in Qualytics until the app is installed in a Teams channel.</p> <p></p>"},{"location":"integrations/alerting/msft_teams/#installing-the-app-in-microsoft-teams","title":"Installing the App in Microsoft Teams","text":"<p>To complete the integration, you need to install the Qualytics app in Microsoft Teams:</p> <p>Step 1: Log in to your Microsoft Teams desktop or web application and click on Apps in the left sidebar.</p> <p></p> <p>Step 2: After click on apps you will navigated to app dashboard. Select Built for your org to see custom apps for your organization and select the \"Qualytics\" app.</p> <p>Note</p> <p>If you don't see the app immediately, it might still be propagating through Microsoft's systems. This can take up to 24 hours.</p> <p></p> <p>Step 3: Click Add to begin the installation process.</p> <p></p> <p>Step 4: After clicking the Add button, a window will appear prompting you to select a team and channel where you want to add the Qualytics app. Once selected, click Go to complete the installation.</p> <p></p> <p>When you add the app to a team and channel, Qualytics will automatically detect the installation. You may need to refresh your browser to see the status update from \"Pending\" to \"Connected\" in the Qualytics Integrations page.</p> <p></p>"},{"location":"integrations/alerting/msft_teams/#manual-verification-optional","title":"Manual Verification (optional)","text":"<p>Important</p> <p>Manual verification serves as a fallback method if Qualytics doesn't automatically detect the app installation after adding it to a channel. If the status remains \"Pending\" after installing the app and refreshing the Qualytics page, use this manual verification process to complete the integration.</p> <p>To manually verify the integration:</p> <p>Step 1: Return to the Qualytics Integrations page and click on the Verify app installation button next to the Microsoft Teams integration.</p> <p></p> <p>When the verification is successful, the integration status will change to \"Connected\", indicating that Qualytics can now send notifications to your Microsoft Teams workspace.</p> <p></p>"},{"location":"integrations/alerting/msft_teams/#manage-microsoft-teams-integration","title":"Manage Microsoft Teams Integration","text":"<p>Microsoft Teams integration enables smooth communication between your platform and Teams channels. Users can easily modify connection settings, update authorization details, or disconnect the integration based on their requirements.</p>"},{"location":"integrations/alerting/msft_teams/#edit-integration","title":"Edit Integration","text":"<p>Edit Integration feature allows users to modify Microsoft Teams connection settings directly from the integration panel. By selecting the Edit option from the menu, users can update configuration details and reauthorize the connection if needed .</p> <p>Step 1: Click on the vertical ellipses(\u22ee) next to the Connected button and select the Edit option .</p> <p></p> <p>Step 2: A modal window Edit Microsoft Teams Integration will appear providing you with options to edit the connection properties.</p> <p></p> <p>Step 3: After editing the connection properties, click on the Update button to apply the changes.</p> <p></p> <p>A confirmation message will appear on the screen displaying \u201cThe Integration has been successfully updated\u201d.</p> <p></p>"},{"location":"integrations/alerting/msft_teams/#disconnect-integration","title":"Disconnect Integration","text":"<p>Disconnecting the Microsoft Teams integration will remove its connection from your platform. This means any existing workflows, notifications, or actions relying on Microsoft Teams may stop working, though they won\u2019t be deleted. Make sure to review any dependent flows before proceeding.</p> <p>Step 1: Click on the vertical ellipses(\u22ee) next to the connected button and select the Disconnect option to disconnect the integration.</p> <p></p> <p>Step 2: A modal window Disconnect Integration will appear allowing you to disconnect the microsoft teams integration.</p> <p></p> <p>Step 3: Click on the Disconnect button to proceed.</p> <p></p> <p>A confirmation message will appear on the screen displaying \u201cThe Integration has been successfully disconnected\u201d.</p> <p></p>"},{"location":"integrations/alerting/slack/","title":"Slack","text":"<p>Slack integration in Qualytics enables seamless communication by connecting your Slack workspace with data quality updates and notifications. It involves generating and applying Slack API tokens, authorizing the integration, and providing options to modify and manage the connection effortlessly.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"integrations/alerting/slack/#navigation-to-integration","title":"Navigation to Integration","text":"<p>Step 1: Log in to your Qualytics account and click the \"Settings\" button on the left side panel of the interface.</p> <p></p> <p>Step 2: By default, Connections tab will open. Click on the Integrations tab .</p> <p></p>"},{"location":"integrations/alerting/slack/#connect-slack-integration","title":"Connect Slack Integration","text":"<p>Connect Slack by generating tokens, configuring connection properties, and authorizing the integration using OAuth for secure communication and seamless app configuration.</p> <p>Step 1 :Click on the Connect button next to Slack to connect to the Slack Integration.</p> <p></p> <p>A modal window titled \"Add Slack Integration\" appears. Fill in the connection properties to connect to Slack.</p> <p></p> <p>Step 2:First, generate the access and refresh tokens through the Slack API by signing in.</p> <p></p> <p>Alternatively, hover over the ? icon and click on the Go to Slack Tokens.</p> <p></p> <p>You will be automatically redirected to the Slack token page, where you can copy the access token for creating and configuring apps and the refresh token for rotating the access token.</p> <p></p> <p>Step 3:Fill out the copied connection properties of slack integration :</p> <p></p> No. Field Name Description 1. Access Token Enter the generated access token. 2. Refresh Token Enter the generated refresh token. <p>Step 4: Click the Create button to apply the access and refresh tokens and proceed with authorizing the Slack integration.</p> <p></p> <p>Once the integration is successfully created, a confirmation message will appear on the screen stating, \"The Integration has been successfully created.\"</p> <p></p> <p>Step 5:Click the Authorize button to complete the Slack integration using OAuth authentication.</p> <p></p> <p>Step 6:After clicking the Authorize button, a window appears requesting permission to access the Slack workspace. Click the Allow button to grant the required permissions.</p> <p></p> <p>A message appears confirming that the integration has been successfully authorized.</p> <p></p>"},{"location":"integrations/alerting/slack/#manage-slack-integration","title":"Manage Slack Integration","text":"<p>Managing Slack integration involves editing or disconnecting the integration to ensure seamless communication and synchronization between platforms. Users can easily modify integration settings, reauthorize the connection, or disconnect the integration if required.</p>"},{"location":"integrations/alerting/slack/#edit-integration","title":"Edit Integration","text":"<p>Editing Slack integration allows modifications to the existing configuration to ensure that the integration functions according to updated requirements. Users can update Slack details, reauthorize the connection, and apply necessary changes seamlessly</p> <p>Step 1:Click on the vertical ellipses(\u22ee) next to the Connected button and select the Edit option.</p> <p></p> <p>Step 2:A modal window Edit Slack Integration will appear providing you with options to edit the connection properties.</p> <p></p> <p>Step 3 :After editing the connection properties of the slack integration, click on the Update button to apply the changes.</p> <p></p> <p>A confirmation message will appear on the screen displaying \u201cThe Integration has been successfully updated\u201d.</p> <p></p> <p>Step 4:Click on the Authorize button to update the authorization details.</p> <p></p> <p>A confirmation message will appear on the screen displaying \u201cThe Integration has been successfully authorized\u201d.</p> <p></p>"},{"location":"integrations/alerting/slack/#disconnect-integration","title":"Disconnect Integration","text":"<p>Disconnecting the Slack integration removes all associated synced assets and disables further data exchange between the platforms. To ensure a smooth disconnection process, follow the steps below to terminate the integration safely and confirm the action.</p> <p>Step 1 : Click on the vertical ellipses(\u22ee) next to the connected button and select the Disconnect option to disconnect the integration.</p> <p></p> <p>Step 2:A modal window Disconnect Integration will appear allowing you to disconnect the slack integration.</p> <p></p> <p>Step 3:Click on the Disconnect button to proceed.</p> <p></p> <p>Note</p> <p>This action will delete all synced assets by this integration. </p> <p>A confirmation message will appear on the screen displaying \u201cThe Integration has been successfully disconnected\u201d.</p> <p></p>"},{"location":"observability/edit-check/","title":"Edit Check","text":"<p>Editing a Check enables users to modify settings such as the unit of measurement, maximum age, description, and metadata. Additionally, they can add tags to streamline organization and retrieval.</p> <p>Step 1: Click the edit icon to modify the check.</p> <p></p> <p>A modal window will appear with the check details.</p> <p></p> <p>Step 2: Modify the check details as needed based on your preferences:</p> No. Field Description 1. Unit Edit the unit of measurement for the freshness check, such as milliseconds (Millis), Minutes, Hours etc. 2. Maximum Age Edit the maximum allowed age (in the specified unit) for data to be considered fresh. 3. Description Edit the Description to better explain what the check does. 4. Tags Edit the Tags to organize and easily find the check later. 5. Additional Metadata(Optional) Edit the Additional Metadata section to add any new custom details for more context. <p></p> <p>Step 3: Once you have edited the check details, then click on the Validate button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct.</p> <p></p> <p>If the validation is successful, a green message saying \"Validation Successful\" will appear.</p> <p></p> <p>Step 4: Once you have a successful validation, click the \"Update\" button. The system will update the changes you've made to the check, including changes to the properties, description, tags, or additional metadata.</p> <p></p> <p>After clicking on the Update button, your check is successfully updated.</p>"},{"location":"observability/edit-maximum-age/","title":"Edit Maximum Age","text":"<p>Maximum Age sets the limit for how long data can remain unchanged before it\u2019s flagged as outdated. This ensures your data stays fresh and reliable for decision-making.</p> <p>Step 1: Click the Edit Maximum Age button on the right side of the graph.</p> <p></p> <p>Step 2: After clicking Edit Maximum Age, the field becomes editable, allowing you to modify the maximum age value.</p> <p></p> <p>Step 3: Once you've updated the maximum age values, click Save to apply the changes.</p> <p></p> <p>After clicking on the Save button, a success flash message will appear.</p>"},{"location":"observability/edit-threshold/","title":"Edit Threshold","text":"<p>Edit thresholds to set specific row count limits for your data checks. By defining minimum and maximum values, you ensure alerts are triggered when data goes beyond the expected range. This helps you monitor unusual changes in data volume. It gives you better control over tracking your data's behavior.</p> <p>Note</p> <p>When editing the threshold, only the min and max values can be modified.</p> <p>Step 1: Click the Edit Thresholds button on the right side of the graph.</p> <p></p> <p>Step 2: After clicking Edit Thresholds, you enter the editing mode where the Min and Max values become editable, allowing you to input new row count limits.</p> <p></p> <p>Step 3: Once you've updated the Min and Max values, click Save to apply the changes and update the thresholds.</p> <p></p> <p>After clicking on the Save button, your threshold is successfully updated.</p>"},{"location":"observability/freshness/","title":"Freshness","text":"<p>This measures the timeliness of data by monitoring when new data was last added or updated. It helps ensure that data remains up-to-date and relevant for decision-making. Users can view timestamp values in a clear date and time format, making it easier to analyze data freshness while maintaining millisecond-level precision in the background. If data updates are delayed or missing, it may indicate pipeline failures, system lag, or unexpected data source changes. Regular freshness checks prevent outdated information from impacting analytics, reporting, or automated workflows.</p> <p></p> No. Field Description 1. Search Bar This feature helps users quickly find specific identifiers or names in the data. 2. Report Date Report Date lets users pick a specific date to view data trends for that day. 3. Timeframe The time frame option lets users choose a period (week, month, quarter, or year.) to view data trends. 4. Sort By Sort By option helps users organize data by criteria like Anomalies, Checks, Created Date, Name, or Last Scanned for quick access. 5. Filters The filter lets users easily refine results by choosing specific tags or tables to view. 6. Favorite Mark this as a favorite for quick access and easy monitoring in the future. 7. Table Displays the name of the selected table being analyzed. 8. Weight Weight shows how important a check is for finding anomalies and sending alerts. 9. Anomaly Detection Represents active anomalies detected in the data. 10. Edit Check Edit the check to modify settings, or add tags for better customization. 11. Freshness (# ID) Each freshness check is assigned a unique identifier, corresponding to the specified time period it monitors (e.g., 1 Day for the customer table). This identifier facilitates precise tracking and management within the system. 12. Group By Users can also Group By specific intervals, such as day, week to observe trends over different periods. 13. Unit The unit used to measure data freshness, shown in milliseconds. 14. Maximum Age Displays the maximum recorded age of data in milliseconds. 15. Last Asserted Shows the latest date when the data was validated or checked. 16. Edit Maximum Age Edit Maximum Age lets users set custom limits for data freshness, allowing control over when alerts are triggered based on the age of the data. 17. Graph Visualization Graph illustrates consistent data patterns over time, with sudden anomalies marked by spikes in red. It reflects changes in data freshness and highlights when the data was last updated."},{"location":"observability/manage-observability/","title":"Overview","text":"<p>In this section, you can manage the observability settings, including editing checks, thresholds, maximum ages, and marking checks as favorites. These features help you fine-tune and optimize your monitoring setup.</p>"},{"location":"observability/manage-observability/#edit-check","title":"Edit Check","text":"<p>Editing a Check enables users to modify settings such as the unit of measurement, maximum age, description, and metadata.</p> <p>Note</p> <p>For more steps refer to the edit checks documentation</p>"},{"location":"observability/manage-observability/#edit-maximum-age","title":"Edit Maximum Age","text":"<p>Maximum Age sets the limit for how long data can remain unchanged before it\u2019s flagged as outdated.</p> <p>Note</p> <p>For more steps refer to the edit maximum age documentation</p>"},{"location":"observability/manage-observability/#edit-threshold","title":"Edit Threshold","text":"<p>Edit thresholds to set specific row count limits for your data checks.</p> <p>Note</p> <p>For more steps refer to the edit threshold documentation</p>"},{"location":"observability/manage-observability/#mark-check-as-favorite","title":"Mark Check as Favorite","text":"<p>Marking a Metric Check as a favorite allows you to easily access important checks quickly.</p> <p>Note</p> <p>For more steps refer to the mark check as favorite documentation</p>"},{"location":"observability/mark-check/","title":"Mark Check as Favorite","text":"<p>Marking a Metric Check as a favorite allows you to easily access important checks quickly. This feature helps you prioritize and manage the checks you frequently use, making data monitoring more efficient.</p> <p>Click on the bookmark icon to mark the Metric Check as a favorite.</p> <p></p> <p>After clicking on the bookmark icon your check is successfully marked as a favorite.</p> <p>To unmark a check, simply click on the bookmark icon of the marked check. This will remove it from your favorites.</p> <p></p>"},{"location":"observability/measures/","title":"Measures","text":"<p>Measures focus on monitoring overall data trends to ensure consistency and reliability. This includes Volumetric Checks, which track data volume to identify trends and detect anomalies, and Freshness Tracking, which measures the last update or addition of data to ensure timeliness. These checks help maintain data integrity by highlighting unexpected changes in volume or delays in data updates. This category includes two key checks:</p>"},{"location":"observability/measures/#volumetric","title":"Volumetric","text":"<p>Volumetric help monitor data volumes over time to keep data accurate and reliable. They automatically count rows in a table and spot any unusual changes, like problems with data loading.</p> <p>For more information please, refer to the volumetric documentation.</p>"},{"location":"observability/measures/#freshness","title":"Freshness","text":"<p>This measures the timeliness of data by monitoring when new data was last added or updated. It helps ensure that data remains up-to-date and relevant for decision-making.</p> <p>For more information please, refer to the freshness documentation.</p>"},{"location":"observability/metric-check/","title":"Metric","text":"<p>Metrics track changes in data over time to ensure accuracy and reliability. They check specific fields against set limits to identify when values, like averages, go beyond expected ranges. With scheduled scans, Metrics automatically log and analyze these data points, making it easy for users to spot any issues. This functionality enhances users' understanding of data patterns, ensuring high quality and dependability. With Metrics, managing and monitoring data becomes straightforward and efficient.</p> <p></p> No Field Description 1 Search The search bar helps users find specific metrics or data by entering an identifier or description. 2 Sort By Sort By allows users to organize data by Weight, Anomalies, or Created Date for easier analysis and prioritization. 3 Filter Filter lets users refine data by Tags or Tables. Use Apply to filter or Clear to reset. 4 Metric(ID) Represents the tracked data metric with a unique ID. 5 Description A brief label or note about the metric, in this case, it's labeled as test. 6 Weight Weight shows how important a check is for finding anomalies and sending alerts. 7 Anomalies Anomalies show unexpected changes or issues in the data that need attention. 8 Favorite Mark this as a favorite for quick access and easy monitoring in the future. 9 Edit Checks Edit the check to modify settings, or add tags for better customization and monitoring. 10 Field This refers to the specific field being measured, here the max_value, which tracks the highest value observed for the metric. 11 Min This indicates the minimum value for the metric, which is set to 1. If not defined, no lower limit is applied. 12 Max This field shows the maximum threshold for the metric, set at 8. Exceeding this may indicate an issue or anomaly. 13 Created Date This field shows when the metric was first set up, in this case, June 18, 2024. 14 Last Asserted Last Asserted field shows the last time the metric was checked, in this case July 25, 2024. 15 Edit Threshold Edit Threshold lets users set custom limits for alerts, helping them control when they\u2019re notified about changes in data. 16 Group By This option lets users group data by periods like Day, Week, or Month. In this example, it's set to Day."},{"location":"observability/metric-check/#comparisons","title":"Comparisons","text":"<p>When you add a metric check, you can choose from three comparison options:</p> <ul> <li>Absolute Change  </li> <li>Absolute Value  </li> <li>Percentage Change</li> </ul> <p>These options help define how the system will evaluate your data during scan operations on the datastore.</p> <p>Once a scan is run, the system analyzes the data based on the selected comparison type. For example, Absolute Change will look for significant differences between scans, Absolute Value checks if the data falls within a predefined range, and Percentage Change identifies shifts in data as a percentage.</p> <p>Based on the chosen comparison type, the system flags any deviations from the defined thresholds. These deviations are then visually represented on a chart, displaying how the metric has fluctuated over time between scans. If the data crosses the upper or lower limits during any scan, the system will highlight this in the chart for further analysis.</p> <p>1. Absolute Change: The Absolute Change comparison checks how much a numeric field's value has changed between scans. If the change exceeds a set limit (Min/Max), it flags this as an anomaly.</p> <p></p> <p>2. Absolute Value: The Absolute Value comparison checks whether a numeric field's value falls within a defined range (between Min and Max) during each scan. If the value goes beyond this range, it identifies it as an anomaly.</p> <p></p> <p>3. Percentage Change: The Percentage Change comparison monitors how much a numeric field's value has shifted in percentage terms. If the change surpasses the set percentage threshold between scans, it triggers an anomaly.</p> <p></p>"},{"location":"observability/metric-check/#minimum-measurements-for-chart-rendering","title":"Minimum Measurements for Chart Rendering","text":"<p>To display metric charts in the UI, a minimum number of measurements must be recorded. If the required number of measurements is not met, the chart remains empty even though some measurements exist.</p> <ul> <li> <p>Absolute Value: Requires at least 2 measurements to render.</p> </li> <li> <p>Absolute Change: Requires at least 3 measurements to render.</p> </li> <li> <p>Percentage Change: Requires at least 3 measurements to render.</p> </li> </ul> <p>These thresholds ensure meaningful visual representation by preventing incomplete or misleading chart data.</p>"},{"location":"observability/observability/","title":"Observability","text":"<p>Observability helps users track changes in data volume and quality over time, ensuring data accuracy and integrity. Within the Source Datastore section, the Observability tab provides visibility into observability metrics across tables or files within a specific datastore. It introduces two main categories: </p> <ul> <li>Measures </li> <li>Metric Checks</li> </ul> <p>Measures include Volumetric Checks, which monitor fluctuations in row counts, and Freshness Tracking, which ensures data is updated on time. </p> <p>Metric Checks focus on specific fields and offer deeper insights derived from scan operations. These tools work together to help detect anomalies early and maintain the reliability of your data assets.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"observability/observability/#why-we-need-observability","title":"Why We Need Observability","text":"<p>Observability is critical in tracking and understanding data behavior, providing insights into how data is moving, evolving, and being processed. By implementing observability, you can monitor key metrics like data volume, freshness, and quality across your systems. It helps you quickly detect anomalies, spot issues early, and ensure data integrity over time.</p>"},{"location":"observability/observability/#how-it-works","title":"How It Works","text":"<p>Observability in Qualytics is designed to give you a continuous, automated view of your data health. Here\u2019s how it works:</p> <ul> <li> <p>Automated Checks: The system automatically runs once every hour at the 30-minute mark to check the container volumes in your data stores.</p> </li> <li> <p>Volume Tracking: Once volume tracking is enabled, no manual intervention is required. The system calculates data volume automatically.</p> </li> <li> <p>Measurement Frequency: The observability job runs 48 times a day to keep data up-to-date, and each measurement is time-stamped according to UTC.</p> </li> <li> <p>Efficient Monitoring: The system skips redundant checks for containers that have already been measured, thus optimizing the process.</p> </li> </ul>"},{"location":"observability/observability/#use-case-understanding-automatic-volume-tracking","title":"Use Case: Understanding Automatic Volume Tracking","text":""},{"location":"observability/observability/#scenario","title":"Scenario","text":"<p>A data operations team needs to monitor table volumes daily to detect unexpected data spikes or drops. They want to understand how Qualytics automatically tracks volume without requiring constant manual intervention.</p>"},{"location":"observability/observability/#common-questions","title":"Common Questions","text":"<p>Q: How is data volume calculated? Do I need to run profiles or scans daily? Ans: Data volume is calculated automatically by the observability job once volume tracking is enabled. No manual profiling or scanning is required.</p>"},{"location":"observability/observability/#how-it-works_1","title":"How It Works:","text":"<ul> <li>The observability job runs 24 times per day (every 30 minutes)</li> <li>On each run, it checks whether a container has already been measured for \"today\" (UTC time)</li> <li>If not measured yet, it records the container's volume</li> <li>If already measured for that day, it skips the measurement</li> <li>The first daily measurement typically occurs around 00:30 UTC (the first run after midnight UTC)</li> </ul>"},{"location":"observability/observability/#setup-requirements","title":"Setup Requirements","text":"<ul> <li>Initial Cataloging: Ensure tables are cataloged in Qualytics</li> <li>Enable Volume Tracking: Turn on volume tracking for the specific container</li> <li>Automatic Monitoring: The observability job handles all subsequent measurements</li> </ul> <p>No additional profiling or scanning operations are needed after the initial setup.</p>"},{"location":"observability/observability/#time-zone-considerations","title":"Time Zone Considerations","text":"<p>The observability system currently operates on UTC time for daily volume calculations. This means:</p> <ul> <li>Daily volume resets occur at midnight UTC</li> <li>Volume measurements begin at 00:30 UTC</li> <li>The UI displays daily totals in UTC time</li> </ul> <p>Note</p> <p>\"Local Time Display\" The UI currently displays observability data in UTC. If you notice date discrepancies (for example, a scan performed on September 17 showing observability data starting September 18), this is due to UTC time zone differences.</p>"},{"location":"observability/observability/#benefits","title":"Benefits","text":"<ul> <li>Zero Maintenance: Automatic checks every 30 minutes without manual intervention</li> <li>Consistent Monitoring: Regular measurements throughout the day ensure comprehensive coverage</li> <li>Early Detection: Quickly identify volume anomalies or data pipeline issues</li> <li>Efficient Processing: Smart skip logic prevents redundant measurements</li> </ul>"},{"location":"observability/observability/#configuration","title":"Configuration","text":"<p>To enable automatic volume tracking:</p> <ol> <li>Navigate to your datastore settings</li> <li>Select the container you want to monitor</li> <li>Enable Volume Tracking</li> <li>The Observability job runs automatically every hour at minute 30.</li> </ol>"},{"location":"observability/observability/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and select the datastore from the left menu that you want to monitor.</p> <p></p> <p>Step 2: Click on the \u201cObservability\u201d from the Navigation tab.</p> <p></p> <p>Observability metrics for tables of the selected source datastore are shown, enabling you to view their detailed insights.</p> <p></p>"},{"location":"observability/observability/#observability-categories","title":"Observability Categories","text":"<p>Observability in data checks is divided into two key categories: Measures and Metric Checks. Measures focus on overall data trends and include Volumetric Checks, which monitor data volume to identify trends and anomalies, and Freshness Tracking, which tracks when data was last added or updated to ensure timeliness. Metric Checks, on the other hand, analyze specific data attributes, providing detailed insights into data quality.</p> <p></p>"},{"location":"observability/observability/#measure","title":"Measure","text":"<p>Measures focus on monitoring overall data trends to ensure consistency and reliability.</p> <p>Note</p> <p>For more information regarding measures please refer to the measure documentation.</p>"},{"location":"observability/observability/#metric","title":"Metric","text":"<p>Metrics track changes in data over time to ensure accuracy and reliability.</p> <p>Note</p> <p>For more information regarding metric please refer to the metric documentation.</p>"},{"location":"observability/volumetric/","title":"Volumetric","text":"<p>Volumetric checks help monitor data volumes over time to keep data accurate and reliable. They automatically count rows in a table and spot any unusual changes, like problems with data loading. This makes it easier to catch issues early and keep everything running smoothly. Volumetric checks also let you track data over different time periods, like daily or weekly. The system sets limits based on past data, and if the row count goes above or below those limits, an anomaly alert is triggered.</p> <p></p> No Field Description 1 Search This feature helps users quickly find specific identifiers or names in the data. 2 Report Date Report Date lets users pick a specific date to view data trends for that day. 3 Time Frame The time frame option lets users choose a period (week, month, quarter, or year) to view data trends. 4 Sort By Sort By option helps users organize data by criteria like Volumetrics Count, Name, or Last Scanned for quick access. 5 Filter The filter lets users easily refine results by choosing specific tags or tables to view. 6 Favorite Mark this as a favorite for quick access and easy monitoring in the future. 7 Table Displays the table for which the volumetric check is being performed (e.g., customer_view, nation). Each table has its own Volumetric Check. 8 Check (# ID) Each check is assigned a unique identifier, followed by the time period it applies to (e.g., 1 Day for the customer table). This ID helps in tracking the specific check in the system. 9 Weight Weight shows how important a check is for finding anomalies and sending alerts. 10 Anomaly Detection The Volumetric Check detects anomalies when row counts exceed set min or max thresholds, triggering an alert for sudden changes. 11 Edit Checks Edit the check to modify settings, or add tags for better customization and monitoring. 12 Group By Users can also Group By specific intervals, such as day, week, or month, to observe trends over different periods. 13 Measurement Period Defines the time period over which the volumetric check is evaluated. It can be customized to 1 day, week, or other timeframes. 14 Comparison These indicate the type of comparison used, indicating the \"Absolute Value\" method. 15 Min Values These indicate the minimum thresholds for the row count of the table being checked (e.g., 150,139 Rows). 16 Max Values These indicate the maximum thresholds for the row count of the table being checked. 17 Last Asserted This shows the date the last check was asserted, which is the last time the system evaluated the Volumetric Check (e.g., Oct 02, 2024). 18 Edit Threshold Edit Threshold lets users set custom limits for alerts, helping them control when they\u2019re notified about changes in data. 19 Graph Visualization The graph provides a visual representation of the row count trends. It shows fluctuations in data volume over the selected period. This visual allows users to quickly identify any irregularities or anomalies."},{"location":"observability/volumetric/#observability-heatmap","title":"Observability Heatmap","text":"<p>The heatmap provides a visual overview of data anomalies by day, using color codes for quick understanding:</p> <p></p> <ul> <li>Blue square: Blue squares represent days with no anomalies, meaning data stayed within the expected range.</li> <li>Orange square: Orange squares indicate days where data exceeded the minimum or maximum threshold range but didn\u2019t qualify as a critical anomaly.</li> <li>Red square: Red squares highlight days with anomalies, signaling significant deviations from expected values that need further investigation.</li> </ul> <p></p> <p>By hovering over each square, you can view additional details for that specific day, including the date, anomaly count, last row count, and last modification time allowing you to easily pinpoint and analyze data issues over time.</p>"},{"location":"operation-automation/automated-setup-using-qualytics-cli/","title":"Automated Setup Using Qualytics CLI","text":"<p>Easily automate scheduled exports with the Qualytics CLI on both Linux and Windows. This setup generates the required scripts and cron/task entries for you, with simple placeholders to customize.</p>"},{"location":"operation-automation/automated-setup-using-qualytics-cli/#for-linux-and-windows-users","title":"For Linux and Windows Users","text":"<p>Use the Qualytics CLI to schedule a task automatically.</p> <pre><code>qualytics schedule export-metadata --crontab \"&lt;cronjob-expression&gt;\" --datastore &lt;datastore-id&gt; --containers &lt;container-ids&gt; --options &lt;metadata-options&gt;\n</code></pre> <p>Replace placeholders as needed.</p>"},{"location":"operation-automation/automated-setup-using-qualytics-cli/#behaviour-on-linux","title":"Behaviour on Linux:","text":"<p>It will create the files inside your <code>home/user/.qualytics</code> folder.</p> <p>The schedule operations commands are going to be located in <code>home/user/.qualytics/schedule-operation.txt</code>.</p> <p>You can see some files with the <code>option</code> you selected with the logs of the cronjob run.</p> <p>It will already create for you a cronjob expression, you can run <code>crontab -l</code> to list all cronjobs.</p>"},{"location":"operation-automation/automated-setup-using-qualytics-cli/#behaviour-on-windows","title":"Behaviour on Windows:","text":"<p>It will create the files inside your <code>home/user/.qualytics</code> folder.</p> <p>The script files will be located in <code>home/user/.qualytics</code> with a pattern <code>task_scheduler_script_&lt;option-you-selected&gt;_&lt;datastore-number&gt;.ps1</code> and it's just a matter for you to follow the step above to create the Task Scheduler.</p>"},{"location":"operation-automation/automated-setup-using-qualytics-cli/#explanation-of-placeholders","title":"Explanation of Placeholders:","text":"<ul> <li> <p><code>&lt;cronjob-expression&gt;</code>: Replace this with your desired cron expression. For example, <code>*/5 * * * *</code> means \"every 5 minutes.\" You can check <code>crontab.guru</code> for more examples.</p> </li> <li> <p><code>&lt;your-instance&gt;</code>: Replace with the actual Qualytics instance URL.</p> </li> <li> <p><code>&lt;operation&gt;</code>: Replace with the specific operation (e.g., \"anomalies\", \"checks\" or \"field-profiles\").</p> </li> <li> <p><code>&lt;datastore-id&gt;</code>: Replace with the ID of the target datastore.</p> </li> <li> <p><code>&lt;container-id-one&gt;</code> and <code>&lt;container-id-two&gt;</code>: Replace with the IDs of the containers. You can add more containers as needed.</p> </li> <li> <p><code>&lt;container-ids&gt;</code>: Comma-separated list of container IDs or array-like format. Example: \"1, 2, 3\" or \"[1,2,3]\".</p> </li> <li> <p><code>&lt;options&gt;</code>: Comma-separated list of operation to export or all for everything. Example: anomalies, checks, field-profiles or all.</p> </li> <li> <p><code>&lt;your-token&gt;</code>: Replace with the access token obtained from Qualytics (<code>Settings</code> -&gt; <code>Security</code> -&gt; <code>API Keys</code>).</p> </li> <li> <p><code>&lt;path-to-show-logs&gt;</code>: Replace with the file path where you want to store the logs.</p> </li> </ul>"},{"location":"operation-automation/linux-machine/","title":"Linux machine","text":"<p>You can automate Qualytics operations on a Linux machine by scheduling them with cron jobs. This guide walks you through setting up a scheduled curl command to trigger exports at defined intervals.</p>"},{"location":"operation-automation/linux-machine/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure that you have the following:</p> <ol> <li>Access to the terminal on your machine.</li> <li>The <code>curl</code> command-line tool installed.</li> <li>The desired Qualytics instance details, including the instance URL and authentication token.</li> </ol>"},{"location":"operation-automation/linux-machine/#steps-to-create-a-scheduled-operation","title":"Steps to Create a Scheduled Operation","text":""},{"location":"operation-automation/linux-machine/#1-open-the-crontab-editor","title":"1. Open the Crontab Editor","text":"<p>Run the following command in your terminal to open the crontab editor:</p> <pre><code>crontab -e\n</code></pre>"},{"location":"operation-automation/linux-machine/#2-add-the-cron-job-entry","title":"2. Add the Cron Job Entry","text":"<p>In the crontab editor, add the following line to execute the curl command at your specified schedule:</p> <pre><code>&lt;cronjob-expression&gt; /usr/bin/curl --request POST --url 'https://&lt;your-instance&gt;.qualytics.io/api/export/&lt;operation&gt;?datastore=&lt;datastore-id&gt;&amp;containers=&lt;container-id-one&gt;&amp;containers=&lt;container-id-two&gt;' --header 'Authorization: Bearer &lt;your-token&gt;' &gt;&gt; &lt;path-to-show-logs&gt; 2&gt;&amp;1\n</code></pre>"},{"location":"operation-automation/linux-machine/#3-example","title":"3. Example:","text":"<p>For example, to run the command every 5 minutes:</p> <pre><code>*/5 * * * * /usr/bin/curl --request POST --url 'https://your-instance.qualytics.io/api/export/anomalies?datastore=123&amp;containers=14&amp;containers=16' --header 'Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...' &gt;&gt; /path/to/show/logs.txt 2&gt;&amp;1\n</code></pre>"},{"location":"operation-automation/linux-machine/#4-verify-or-list-cron-jobs","title":"4. Verify or List Cron Jobs:","text":"<pre><code>crontab -l\n</code></pre> <p>Customize the placeholders based on your specific details and requirements. Save the crontab file to activate the scheduled operation.</p>"},{"location":"operation-automation/overview/","title":"Overview","text":"<p>Users may want to create their own scheduled operations in Qualytics to automate routine tasks such as data exports or running specific operations at defined intervals. Instead of executing these operations manually, they can be scheduled to run automatically on Linux or Windows, or through the Qualytics CLI.</p> <p>Choose the setup guide that matches your environment:</p>"},{"location":"operation-automation/overview/#linux-machine","title":"Linux Machine","text":"<p>This guide explains how to configure scheduled tasks on Linux using cron jobs with curl commands. For more steps, refer to the linux machine documentation.</p>"},{"location":"operation-automation/overview/#windows-machine","title":"Windows Machine","text":"<p>This guide explains how to configure scheduled tasks on Windows using PowerShell scripts and the Windows Task Scheduler. For more steps, refer to the windows machine documentation.</p>"},{"location":"operation-automation/overview/#installation-qualytics-cli","title":"Installation \u2013 Qualytics CLI","text":"<p>This page points you to the Qualytics CLI Overview, where you can find installation and initialization instructions. For more steps, refer to the qualytics CLI documentation.</p>"},{"location":"operation-automation/overview/#automation-setup-using-qualytics-cli","title":"Automation Setup Using Qualytics CLI","text":"<p>This guide explains how to use the Qualytics CLI\u2019s scheduling commands to automate operations, including Linux and Windows behavior. For more steps, refer to the automation setup using qualytics CLI documentation.</p>"},{"location":"operation-automation/windows-machine/","title":"Windows machine","text":"<p>Automate Qualytics operations on Windows using PowerShell and Task Scheduler. This guide shows how to set up and run scheduled export tasks.</p>"},{"location":"operation-automation/windows-machine/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure that you have the following:</p> <ol> <li>Access to PowerShell on your machine.</li> <li>The desired Qualytics instance details, including the instance URL and authentication token.</li> </ol>"},{"location":"operation-automation/windows-machine/#steps-to-create-a-scheduled-operation","title":"Steps to Create a Scheduled Operation","text":""},{"location":"operation-automation/windows-machine/#1-open-your-text-editor-of-your-preference-and-add-the-script-entry","title":"1. Open your text editor of your preference and add the script entry","text":"<p>In the text editor, add the following line to execute the <code>Invoke-RestMethod</code> command:</p> <pre><code>Invoke-RestMethod -Method 'Post' -Uri https://&lt;your-instance&gt;/api/export/anomalies?datastore=&lt;datastore-id&gt;&amp;containers=&lt;container-id-one&gt;&amp;containers=&lt;container-id-two&gt; -Headers @{'Authorization' = 'Bearer &lt;your-token&gt;'; 'Content-Type' = 'application/json'}\n</code></pre>"},{"location":"operation-automation/windows-machine/#2-example","title":"2. Example:","text":"<p>For example, to run the command every 5 minutes:</p> <pre><code>Invoke-RestMethod -Method 'Post' -Uri https://your-instance.qualytics.io/api/export/anomalies?datastore=123&amp;containers=44&amp;containers=22 -Headers @{'Authorization' = 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...'; 'Content-Type' = 'application/json'}\n</code></pre> <p>Customize the placeholders based on your specific details and requirements. Save the script with the desired name with the extension <code>.ps1</code>.</p>"},{"location":"operation-automation/windows-machine/#3-add-the-script-to-the-task-scheduler","title":"3. Add the script to the Task Scheduler:","text":"<ol> <li> <p>Open Task Scheduler:</p> <ul> <li>Press <code>Win + S</code> to open the Windows search bar.</li> <li>Type \"Task Scheduler\" and select it from the search results.</li> </ul> </li> <li> <p>Create a Basic Task:</p> <ul> <li>In the Task Scheduler window, click on <code>Create Basic Task...</code> on the right-hand side.</li> </ul> </li> <li> <p>Provide a Name and Description:</p> <ul> <li>Enter a name and description for your task. Click <code>Next</code> to proceed.</li> </ul> </li> <li> <p>Choose Trigger:</p> <ul> <li>Select when you want the task to start. Options include <code>Daily</code>, <code>Weekly</code>, or <code>At log on</code>.</li> <li>Choose the one that fits your schedule. Click <code>Next</code>.</li> </ul> </li> <li> <p>Set the Start Date and Time:</p> <ul> <li>If you selected a trigger that requires a specific start date and time, set it accordingly. Click <code>Next</code>.</li> </ul> </li> <li> <p>Choose Action:</p> <ul> <li>Select <code>Start a program</code> as the action and click <code>Next</code>.</li> </ul> </li> <li> <p>Specify the Program/Script:</p> <ul> <li>In the <code>Program/script</code> field, provide the path to PowerShell executable (<code>powershell.exe</code>), typically located at <code>C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe</code>. Alternatively, you can just type <code>powershell.exe</code>.</li> <li>In the <code>Add arguments (optional)</code> field, provide the path to your PowerShell script. For example: <code>-File \"C:\\Path\\To\\Your\\GeneratedScript.ps1\"</code>.</li> <li>Click <code>Next</code>.</li> </ul> </li> <li> <p>Review Settings:</p> <ul> <li>Review your task settings. If everything looks correct, click <code>Finish</code>.</li> </ul> </li> <li> <p>Finish:</p> <ul> <li>You should now see your task listed in the Task Scheduler Library.</li> </ul> </li> </ol>"},{"location":"quality-scores/what-are-quality-scores/","title":"Quality Scores","text":"<p>Quality Scores are quantified measures of data quality calculated at the field and container levels, recorded as time-series to enable tracking of changes over time. Scores range from 0-100 with higher values indicating superior quality for the intended purpose. These scores integrate eight distinct dimensions, providing a granular analysis of the attributes that impact the overall data quality. The overall score is a composite reflecting the relative importance and configured weights of these factors:</p> <ul> <li>Completeness: Measures the average percentage of non-null values in a field throughout the measurement period. For example, if a \"phone_number\" field has values present in 90 out of 100 records, its completeness score for the measurement would be 90%.</li> <li>Coverage: Measures the number of quality checks defined for monitoring the field's quality. </li> <li>Conformity: Measures how well the data adheres to specified formats, patterns, and business rules. For example, checking if dates follow the required format (YYYY-MM-DD) or if phone numbers match the expected pattern.See Appendix: Conformity Rule Types for the full Conformity rule type list.</li> <li>Consistency: Measures uniformity in type and scale across all data representations. Verifies that data maintains the same type and representation over time. For example, ensuring that a typed numeric column does not change over time to a string.</li> <li>Precision: Evaluates the resolution of field values against defined quality checks.See Appendix: Precision Rule Types for the full Precision rule type list.</li> <li>Timeliness: Gauges data availability according to schedule.See Appendix: Timeliness Rule Types for the full Timeliness rule type list.</li> <li>Volumetrics: Analyzes consistency in data size and shape over time.See Appendix: Volumetric Rule Types for the full Volumetrics rule type list.</li> <li>Accuracy: Determines the fidelity of field values to their real-world counterparts or expected values. </li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#how-completeness-precision-and-accuracy-differ","title":"How Completeness, Precision, and Accuracy Differ","text":"Dimension Focus Example Question It Answers Completeness Are values present? What % of rows in <code>phone_number</code> are non-null? Precision Are values within the expected level of detail or granularity? Are all <code>age</code> values between 0\u2013120? Do decimals have required 2-digit precision? Accuracy Are values correct compared to real-world truth or integrity checks? Is the relationship between <code>square_footage</code> and <code>price</code> maintained? <p>Important</p> <p>A data asset's quality score is a measure of its fitness for the intended use case. It is not a simple measure of error, but instead a holistic confidence measure that considers the eight fundamental dimensions of data quality as described below.  Quality scores are dynamic and will evolve as your data and business needs change over time.</p>"},{"location":"quality-scores/what-are-quality-scores/#field-level-quality-scoring","title":"Field-Level Quality Scoring","text":"<p>Each field receives individual scores for eight quality dimensions, each evaluated on a 0-100 scale.</p>"},{"location":"quality-scores/what-are-quality-scores/#completeness-dimension","title":"Completeness Dimension","text":"<p>The Completeness score measures the average percentage of non-null values in a field over the measurement period.</p> <p>How Completeness is Calculated</p> <ul> <li>Scale: 0 to 100, representing the average completeness percentage</li> <li>Measurement period: Defined by the configured decay time (default 180 days)</li> <li>Formula: Average of <code>(non-null values / total records) \u00d7 100</code> across all measurements in the period</li> <li>Example: If a \"phone_number\" field averages 90% completeness over the measurement period, its completeness score would be 90</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#coverage-dimension","title":"Coverage Dimension","text":"<p>The Coverage score measures how many distinct quality checks have been applied to a field. It is designed to reward the first few checks heavily, then taper off as more checks are added, following a curve of diminishing returns.</p> <p>How Coverage is Calculated</p> <ul> <li>Scale: The score ranges continuously from 0 to 100</li> <li>Anchor points:<ul> <li>0 checks \u2192 score of 0</li> <li>1 check \u2192 score of approximately 60</li> </ul> </li> <li>Diminishing returns: Each additional check contributes less than the previous one. As the number of checks grows, the score approaches 100 but never exceeds it</li> </ul> <p>Mathematically, the scoring curve follows an exponential growth model: <pre><code>score(n) = 100 \u00d7 (1 - e^(-k \u00d7 n))\n</code></pre> where n is the number of checks and k is tuned so that 1 check = 60.</p> <p>Why This Model?</p> <ul> <li>Strong early reward: The first check dramatically increases confidence in field coverage</li> <li>Fair balance: More checks always improve the score, but the improvement diminishes as coverage becomes robust, preventing runaway inflation</li> </ul> <p>Field vs. Container Coverage</p> <p>At the field level, Coverage reflects the number of distinct quality checks defined for that field. At the container level, Coverage is an aggregate of field-level coverage scores, further adjusted by scan frequency (more frequent scans \u2192 greater confidence).</p>"},{"location":"quality-scores/what-are-quality-scores/#conformity-dimension","title":"Conformity Dimension","text":"<p>The Conformity score measures how well the data adheres to specified formats, patterns, and business rules.</p> <p>How Conformity is Calculated</p> <ul> <li>Scale: 0 to 100 based on the ratio of conforming values</li> <li>Formula: <code>(1 - (rows with anomalous values as specified by conformity checks / min(scanned rows, container rows))) \u00d7 100</code></li> <li>Denominator: Uses the smaller of scanned row count or container row count to prevent score inflation</li> <li>Applicable rule types: Pattern matching, length constraints, type validation, schema expectations, and format-specific validationsSee Appendix: Conformity Rule Types for the full Conformity rule type list.</li> </ul> <p>Examples</p> <ul> <li>Email field where 95% of scanned/total rows match valid email pattern \u2192 Score ~95</li> <li>Date field with consistent YYYY-MM-DD format \u2192 Score ~100</li> <li>Phone field with mixed formats and invalid entries \u2192 Score ~60</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#consistency-dimension","title":"Consistency Dimension","text":"<p>The Consistency score measures how stable a field's values remain over time compared to their expected statistical profile. This highlights fields that are \"drifting\" (changing shape, format, or density).</p> <p>How Consistency is Calculated</p> <ol> <li> <p>Check for type changes</p> <ul> <li>If a field flips between types (e.g., sometimes a number, sometimes a string), score is set to 0</li> </ul> </li> <li> <p>Collect summary statistics per field type:</p> <ul> <li>Numeric fields: median and interquartile range (IQR)</li> <li>String fields: distinct count, min/max length, Shannon entropy</li> <li>Datetime fields: earliest timestamp, distinct timestamp count</li> </ul> </li> <li> <p>Measure stability</p> <ul> <li>Track variation of each statistic across the analysis window</li> <li>Normalize changes for fair comparison across different scales</li> </ul> </li> <li> <p>Apply thresholds and weights</p> <ul> <li>Each change type has an expected tolerance (e.g., \u00b110% for numeric medians)</li> <li>Variations within tolerance incur little/no penalty</li> <li>Larger variations reduce the score proportionally</li> </ul> </li> <li> <p>Combine into final score</p> <ul> <li>100: Field stayed fully consistent</li> <li>60-90: Mild to moderate changes worth monitoring</li> <li>Below 60: Meaningful shift requiring investigation</li> <li>0: Type change detected</li> </ul> </li> </ol> <p>Consistency vs. Accuracy</p> <p>Consistency checks whether a field\u2019s statistical shape and distribution remain stable over time (e.g., numeric medians, string entropy).</p> <p>Accuracy, by contrast, evaluates whether values are correct and aligned to real-world truths or integrity rules.</p> <p>Together, they capture different aspects of trustworthiness.</p> <p>Examples</p> <ul> <li>Numeric \"Price\" field with stable median and IQR \u2192 Score ~100</li> <li>String \"Country\" field where distinct values double unexpectedly \u2192 Score ~75</li> <li>Datetime field with sudden two-year backfill \u2192 Score ~60</li> <li>ID field alternating between numeric and string types \u2192 Score = 0</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#precision-dimension","title":"Precision Dimension","text":"<p>The Precision score evaluates the resolution and granularity of field values against defined quality checks.</p> <p>How Precision is Calculated</p> <ul> <li>Scale: 0 to 100 based on the ratio of values meeting precision requirements</li> <li>Formula: <code>(1 - (rows with anomalous values as specified by precision checks / min(scanned rows, container rows))) \u00d7 100</code></li> <li>Denominator: Uses the smaller of scanned row count or container row count to prevent score inflation</li> <li>Applicable rule types: Range validations, comparisons, mathematical constraints, and temporal boundariesSee Appendix: Precision Rule Types for the full Precision rule type list.</li> </ul> <p>Examples</p> <ul> <li>Decimal field maintaining required 2-digit precision \u2192 Score ~100</li> <li>Timestamp field with appropriate granularity (no future dates) \u2192 Score ~95</li> <li>Age field with values outside valid range (0-120) \u2192 Score ~85</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#accuracy-dimension","title":"Accuracy Dimension","text":"<p>The Accuracy score determines the fidelity of field values to their real-world counterparts or expected values.</p> <p>How Accuracy is Calculated</p> <ul> <li>Scale: 0 to 100 based on the overall anomaly rate across all data integrity (excludes metadata checks like schema, volume, freshness, etc..) check types</li> <li>Formula: <code>(1 - (rows with anomalous values as specified by accuracy checks / min(scanned rows, container rows))) \u00d7 100</code></li> <li>Denominator: Uses the smaller of scanned row count or container row count to prevent score inflation</li> <li>Comprehensive: Considers anomalies from all data integrity rule types</li> <li>Represents: Overall correctness and trustworthiness of the field data</li> </ul> <p>Interpretation</p> <ul> <li>95-100: Highly accurate data suitable for critical decisions</li> <li>80-94: Generally reliable with some known issues</li> <li>60-79: Moderate accuracy requiring validation for important uses</li> <li>Below 60: Significant accuracy concerns requiring remediation</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#timeliness-volumetrics-dimensions","title":"Timeliness &amp; Volumetrics Dimensions","text":"<p>Both the Timeliness and Volumetrics dimensions are measured at the container level as described below. Field-level scores are inherited from their container-level scores.</p>"},{"location":"quality-scores/what-are-quality-scores/#container-level-quality-scoring","title":"Container-Level Quality Scoring","text":"<p>A container (table, view, file, or other structured data asset or any aggregation of data assets such as assets that share a common tag) receives an overall quality score derived from its constituent fields and additional container-specific metrics.</p>"},{"location":"quality-scores/what-are-quality-scores/#how-container-scores-are-calculated","title":"How Container Scores Are Calculated","text":"<p>Your container's total Quality Score starts at a baseline of 70. Each of the eight data quality dimensions then adjusts this baseline:</p> <ul> <li>Dimension aggregation:<ul> <li>Completeness: Weighted average of all field completeness scores</li> <li>Coverage: Weighted average of field coverage scores, adjusted for scan frequency</li> <li>Conformity: Weighted average of field conformity scores, adjusted for schema-level conformity checks</li> <li>Consistency: Weighted average of field consistency scores, adjusted for profiling frequency</li> <li>Precision: Weighted average of field precision scores</li> <li>Accuracy: Weighted average of field accuracy scores</li> <li>Timeliness: Calculated using process described below</li> <li>Volumetrics: Calculated using process described below</li> </ul> </li> <li>Proportional adjustment: Each dimension adjusts the score proportionally to its 0\u2013100 rating</li> <li>Influence capping: Every dimension has maximum positive and negative impact limits</li> <li>Weight controls: Higher weights make dimensions more influential; zero weight removes effect entirely</li> <li>Missing value handling: Documented defaults substitute for unmeasurable dimensions</li> <li>Special case: If only one dimension is weighted, the Quality Score mirrors that dimension's rating</li> <li>Final clipping: Result is always constrained between 0 and 100</li> </ul> <p>Why a 70-Point Baseline?</p> <p>The 70-point baseline represents a neutral confidence starting point.</p> <ul> <li>Dimensions then adjust the baseline downward when issues are found or upward when strong quality signals exist.</li> <li>This calibration ensures that new containers without extensive checks or history begin from a reasonable midpoint rather than 0.</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#timeliness-dimension","title":"Timeliness Dimension","text":"<p>The Timeliness score gauges whether data is available according to its expected schedule.</p> <p>How Timeliness is Calculated</p> <ul> <li>Scale: 0 to 100 based on adherence to freshness requirements</li> <li>Field level: Directly inherited from the container's timeliness score</li> <li>Anomaly counting: Counts distinct anomalies from the relevant check types within the measurement period (cutoff date)</li> <li>Formula (container): Scores start at 100 and decrease based on anomaly count<ul> <li>First anomaly causes a 40-point drop (score becomes 60)</li> <li>Each additional anomaly has diminishing impact</li> <li>Formula: <code>Score = 100 - min(100 \u00d7 (1 - e^(-k \u00d7 anomaly_count)), 100)</code></li> <li>Where k is calibrated so one anomaly = 40% score reduction</li> </ul> </li> <li>Applicable rule types: Time distribution size, freshness constraintsSee Appendix: Timeliness Rule Types for the full Timeliness rule type list.</li> </ul> <p>Score Interpretation</p> <ul> <li>100: No timeliness anomalies detected</li> <li>60: One anomaly detected (40-point penalty)</li> <li>40-60: Multiple anomalies with diminishing penalties</li> <li>0-40: Significant anomaly counts indicating serious issues</li> <li>None/Null: No checks of this type configured (unmeasured)</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#volumetrics-dimension","title":"Volumetrics Dimension","text":"<p>The Volumetrics score analyzes consistency in data size and shape over time.</p> <p>Shared Scoring Formula</p> <p>Timeliness and Volumetrics both use the same exponential penalty formula for anomaly counts. This consistency ensures comparable scoring behavior across dimensions, even though the anomalies being measured differ.</p> <p>How Volumetrics is Calculated</p> <ul> <li>Scale: 0 to 100 based on volumetric stability</li> <li>Field level: Directly inherited from the container's volumetrics score</li> <li>Anomaly counting: Counts distinct anomalies from the relevant check types within the measurement period (cutoff date)</li> <li>Formula (container): Scores start at 100 and decrease based on anomaly count<ul> <li>First anomaly causes a 40-point drop (score becomes 60)</li> <li>Each additional anomaly has diminishing impact</li> <li>Formula: <code>Score = 100 - min(100 \u00d7 (1 - e^(-k \u00d7 anomaly_count)), 100)</code></li> <li>Where k is calibrated so one anomaly = 40% score reduction</li> </ul> </li> <li>Applicable rule types: Row count size, partition size constraintsSee Appendix: Volumetric Rule Types for the full Volumetric rule type list.</li> </ul> <p>Examples</p> <ul> <li>Container with consistent record counts per partition \u2192 Score ~100</li> <li>Container showing unexpected spikes or drops in volume \u2192 Score ~75</li> <li>Container with erratic or missing time distributions \u2192 Score ~50</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#additional-container-level-factors","title":"Additional Container-Level Factors","text":"<p>Beyond the eight dimensions, containers incorporate:</p> <ul> <li>Scanning frequency: More frequent scanning improves confidence and boosts coverage scores</li> <li>Profiling frequency: Regular profiling ensures statistics remain current and boosts consistency scores</li> <li>Field tag weights: Field weights are used when calculated weighted averages for container-level dimensions</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#most-impactful-dimensions","title":"Most Impactful Dimensions","text":"<p>While specific scoring weights can be customized, dimensions that typically most influence quality scores are:</p> <ul> <li>Coverage: Asserting frequent, comprehensive quality checks is critical</li> <li>Accuracy: Large volumes of anomalies severely impact scores</li> <li>Consistency: Erratic or unstable data characteristics reduce confidence</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#how-to-interpret-and-use-quality-scores","title":"How to Interpret and Use Quality Scores","text":"<p>Quality scores are dynamic measures of confidence that reflect the intrinsic quality of your data. It's important to recognize that different types of data will have varying levels of inherent quality. To illustrate this point, let's consider a standard mailing address in the USA. A typical schema representing a mailing address includes fields such as:</p> <ul> <li>Addressee</li> <li>Street</li> <li>Street 2</li> <li>City</li> <li>State</li> <li>Postal Code</li> </ul> <p>The \"State\" field, which is naturally constrained by a limited set of known values, will inherently have a higher level of quality compared to the \"Street 2\" field. \"Street 2\" typically holds free-form text ranging from secondary unit designations to \"care of\" instructions and may often be left blank. In contrast, \"State\" is a required field for any valid mailing address.</p> <p>Consider the level of confidence you would have in making business decisions based on the values held in the \"State\" field versus the \"Street 2\" field. This thought exercise demonstrates how the Qualytics Quality Score (with default configuration) should be interpreted.</p> <p>While there are steps you can take to improve the quality score of the \"Street 2\" field, it would be unrealistic to expect it to meet the same standards as the \"State\" field. Instead, your efforts should focus on the change in measured quality score over time, with the goal of raising scores to an acceptable level of quality that meets your specific business needs.</p> <p>To further explore how to respond to Quality Scores, let's consider the business requirements for capturing \"Street 2\" and its downstream use:</p> <ul> <li> <p>If the primary use case for this address is to support credit card payment processing, where \"Street 2\" is rarely, if ever, considered, there may be no business need to focus on improving the quality of this field over time. In this case, you can reduce the impact of this field on the overall measured quality of the Address by applying a Tag with a negative weight modifier.</p> </li> <li> <p>On the other hand, if the primary use case for this address is to reliably ship a physical product to an intended recipient, ensuring a higher level of quality for the \"Street 2\" field becomes necessary. In this scenario, you may take actions such as defining additional data quality checks for the field, increasing the frequency of profiling and scanning, establishing a completeness goal, and working with upstream systems to enforce it over time.</p> </li> </ul> <p>Important</p> <p>The key to effectively adopting Qualytics's Quality Scores into your data quality management efforts is to understand that it reflects both the intrinsic quality of the data and the steps taken to improve confidence that the data is fit for your specific business needs.</p> <p>Fitness for Purpose in Practice</p> <p>Remember: Quality Scores are not absolute \u201cgrades.\u201d They reflect how well your data is suited for its intended business use, influenced by weighting, tagging, and anomaly detection. Two datasets may have different scores but still both be \"fit for purpose\" depending on use case.</p>"},{"location":"quality-scores/what-are-quality-scores/#customizing-quality-score-weights-and-decay-time","title":"Customizing Quality Score Weights and Decay Time","text":"<p>The default quality score weightings and decay time represent best practice considerations as codified by the data quality experts at Qualytics and our work with enterprises of all shapes, sizes, and sectors. We recommend that both be left in their default state for all customers and use cases.</p> <p>That said, we recognize that customers may desire to alter our default scoring algorithms for a variety of reasons, and we support that optionality by allowing administrators to tailor the impact of each quality dimension on the total score by adjusting their weights. This alters the scoring algorithm to align with customized governance priorities. Additionally, the decay period for considering past data events defaults to 180 days but can be customized to fit your operational needs, ensuring the scores reflect the most relevant data quality insights for your organization.</p> <p>Use Caution When Customizing Weights</p> <p>We strongly recommend retaining default weights unless governance priorities clearly justify changes.</p> <ul> <li>Adjusting weights can significantly alter how anomalies impact overall scores.</li> <li>Misaligned weights may cause misleading signals about data quality.  </li> </ul> <p>Proceed carefully, and document any custom weighting rationale.</p>"},{"location":"quality-scores/what-are-quality-scores/#appendix-rule-types","title":"Appendix: Rule Types","text":"<p>The following lists summarize which rule types contribute to each dimension\u2019s quality score.  </p>"},{"location":"quality-scores/what-are-quality-scores/#conformity-rule-types","title":"Conformity Rule Types","text":"No. Rule Type 1. Matches Pattern 2. Min Length 3. Max Length 4. Data Diff 5. Is Type 6. Entity Resolution 7. Expected Schema 8. Field Count 9. Is Credit Card 10. Is Address 11. Contains Credit Card 12. Contains URL 13. Contains Email 14. Contains Social Security Number"},{"location":"quality-scores/what-are-quality-scores/#precision-rule-types","title":"Precision Rule Types","text":"No. Rule Type 1. After Date Time 2. Before Date Time 3. Between 4. Between Times 5. Equal To 6. Equal To Field 7. Greater Than 8. Greater Than Field 9. Less Than 10. Less Than Field 11. Max Value 12. Min Value 13. Not Future 14. Not Negative 15. Positive 16. Predicted By 17. Sum"},{"location":"quality-scores/what-are-quality-scores/#volumetric-rule-types","title":"Volumetric Rule Types","text":"No. Rule Type 1. Volumetric 2. Min Partition Size 3. Max Partition Size"},{"location":"quality-scores/what-are-quality-scores/#timeliness-rule-types","title":"Timeliness Rule Types","text":"No. Rule Type 1. Freshness 2. Time Distribution Size"},{"location":"settings/catalog-integrations/alation/","title":"Alation","text":"<p>Integrating Alation with Qualytics, allows you to pull metadata from Alation to Qualytics and push Qualytics metadata to Alation. Once integrated, Qualytics can stay updated with key changes in Alation, like metadata updates and anomaly alerts which helps to ensure data quality and consistency. Qualytics updates only active checks, and metadata updates in Qualytics occur if the Event-Driven option is enabled or can be triggered manually using the \"Sync\" button. During sync, Qualytics can replace existing tags in Alation or skip duplicate tags to avoid conflicts. The setup is simple\u2014just provide a refresh token for communication between the systems.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"settings/catalog-integrations/alation/#alation-setup","title":"Alation Setup","text":""},{"location":"settings/catalog-integrations/alation/#create-refresh-token","title":"Create Refresh Token","text":"<p>Before setting up Alation Integration in Qualytics, you have to generate a Refresh token. This allows Qualytics to access Alation's API and keep data in sync between the two platforms.</p> <p>Step 1: Navigate to the \"Profile Settings\".</p> <p></p> <p>Step 2: Select the \"Authentication\" tab.</p> <p></p> <p>Step 3: Click on the \"Create Refresh Token\" button.</p> <p></p> <p>Step 4: Enter a name for the token.</p> <p></p> <p>Step 5: After entering the name for the token, click on \"Create Refresh Token\".</p> <p></p> <p>Step 6: Your \"refresh\" token has been generated successfully. Please Copy and save it securely.</p> <p></p> <p>Step 7: Here you can view the token that is successfully added to the access tokens list.</p> <p></p>"},{"location":"settings/catalog-integrations/alation/#add-alation-integration","title":"Add Alation Integration","text":"<p>Step 1:  Log in to your Qualytics account and click the \"Settings\" button on the left side panel of the interface.</p> <p></p> <p>Step 2: You will be directed to the Settings page, then click on the \"Integration\" tab.</p> <p></p> <p>Step 3: Click on the \"Add Integration\" button.</p> <p></p> <p>Step 4: Complete the configuration form by choosing the Alation integration type.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Provide a name for the integration. 2. Type (Required) Choose the type of integration from the dropdown menu. Currently, 'Atlan' is selected 3. URL (Required) Enter the full address of the Alation instance, for example,  https://instance.alationcloud.com. 4. Refresh Token (Required) Enter the refresh token required to access the Alation API. 5. User ID (Required) Provide the user ID associated with the generated token. 6. Domains Select specific domains to filter assets for synchronization. - Acts as a filtering mechanism to sync specific assets - Uses domain information from the data catalog (e.g. Sales ). Only assets under the selected domains will synchronize. 7. Event Driven If enabled, operations, archiving anomalies, and checks will activate the integration sync. 8. Overwrite Tags If enabled, Alation tags will override Qualytics tags in cases of conflicts (when tags with the same name exist on both platforms). <p>Step 5: Click on the Save button to integrate Alation with Qualytics. </p> <p></p> <p>Step 6: Here you can view the new integration appearing in Qualytics.</p> <p></p>"},{"location":"settings/catalog-integrations/alation/#synchronization","title":"Synchronization","text":"<p>The Alation synchronization supports both push and pull operations. This includes pulling metadata from Alation to Qualytics and pushing Qualytics metadata to Alation. During the syncing process, the integration pulls tags assigned to data assets in Alation and assigns them to Qualytics assets as an external tag.</p> <p>Note</p> <p>Tag synchronization requires manual triggering.</p> <p>Step 1: To sync tags, simply click the \"Sync\" button next to the relevant integration card.</p> <p></p> <p>Step 2: After clicking the Sync button, you will have the following options:</p> <ul> <li>Pull Alation Metadata </li> <li>Push Qualytics Metadata</li> </ul> <p>Specify whether the synchronization will pull metadata, push metadata, or do both.</p> <p></p> <p>Step 3: After selecting the desired options, click on the \"Start\" button.</p> <p></p> <p>Step 4: After clicking the Start button, the synchronization process between Qualytics and Alation begins. This process pulls metadata from Alation and pushes Qualytics metadata, including tags, quality scores, anomaly counts, asset links, and many more.</p> <p></p> <p>Step 5: Once synchronization is complete, the mapped assets from Alation will display an external tag.</p> <p></p>"},{"location":"settings/catalog-integrations/alation/#alerts","title":"Alerts","text":"<p>When Qualytics detects anomalies, alerts are sent to the assets in Alation, showing the number of active anomalies and providing a link to view them.</p> <p></p>"},{"location":"settings/catalog-integrations/alation/#metadata","title":"Metadata","text":"<p>The Quality Score Total, the \"Qualytics 8\" metrics (completeness, coverage, conformity, consistency, precision, timeliness, volume, and accuracy), and counts of checks and anomalies per asset identified by Qualytics are pushed to Alation. This enables users to analyze assets based on data profiling and scanning metrics. A link to the asset in Qualytics is also provided.</p> <p></p>"},{"location":"settings/catalog-integrations/alation/#data-health","title":"Data Health","text":"<p>On the Alation tables page, there's a tab called \u201cData Health\u201d where Qualytics displays insights from data quality checks in a table format, showing the current status based on the number of anomalies per check.</p> <p></p> Column Description Rule The type of data quality check rule Object Name The Table Name Status The check status can be either \"Alert\" if there are active anomalies or \"No Issues\" if no active anomalies exist for the check. Value The current amount of active anomalies Description The data quality check description Last Updated The last synced timestamp"},{"location":"settings/catalog-integrations/atlan/","title":"Atlan","text":"<p>Integrating Atlan with Qualytics allows for easy push and pull of metadata between the two platforms. Specifically, Qualytics \"pushes\" its metadata to the data catalog and \"pulls\" metadata from the data catalog. Once connected, Qualytics automatically updates when key events happen in Atlan, such as metadata changes, anomaly updates, or archiving checks. This helps maintain data quality and consistency. During the sync process, Qualytics can either replace existing tags in Atlan or skip assets that have duplicate tags to avoid conflicts. Setting it up is simple\u2014you just need to provide an API token to allow smooth communication between the systems.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"settings/catalog-integrations/atlan/#atlan-setup","title":"Atlan Setup","text":""},{"location":"settings/catalog-integrations/atlan/#create-an-atlan-persona-and-policy","title":"Create an Atlan persona and policy","text":"<p>Before starting the integration process, it is recommended that you set up an Atlan persona. It allows access to the necessary data and metadata. While you can create this persona simultaneously as your API token, it's easier if you create it first. That way, you can link the persona directly to the token later.</p> <p>Before using Atlan with your data source, authorize the API token with access to the needed data and metadata. You do this by setting up policies within the persona for the Atlan connection that matches your Qualytics data source. Remember, you will need to do this for each data source you want to integrate.</p> <p>Step 1. Navigate to Governance, then select \u201cPersonas\u201d.</p> <p></p> <p>Step 2: Click on \u201c+ New Persona Button\u201d.</p> <p></p> <p>Step 3: Enter a Name and Description for a new persona, then click the \u201cCreate\u201d button.</p> <p></p> <p>Step 4: Here your new Atlan persona has been created.</p> <p></p> <p>Step 5: After creating a new Atlan persona you have to create policies to authorize the personal access token. Click on \"Add Policies\" to create a new policy or to add one if there isn't any available.</p> <p></p> <p>Step 6: Click on \"New Policy\" and select \"Metadata Policy\" from the dropdown menu.</p> <p></p> <p>Step 7: Enter a \"name\", and choose the \"connection\".  </p> <p></p> <p>Step 8: Customize the permissions and assets that Qualytics will access.</p> <p></p> <p>Step 9: Once the policy is created, you\u2019ll see it listed in the Policies section.</p> <p></p>"},{"location":"settings/catalog-integrations/atlan/#create-atlan-personal-access-token","title":"Create Atlan Personal Access Token","text":"<p>After you\u2019ve created the persona, the next step is to create a personal access token.</p> <p>Step 1: Navigate to the API Tokens section in the Admin Center.</p> <p></p> <p>Step 2: Click on \"Generate API Token\" button.</p> <p></p> <p>Step 3: Enter a name and description, and select the persona you created earlier.</p> <p></p> <p>Step 4: Click the \"Save\" button and make sure to store the token in a secure location.</p> <p></p>"},{"location":"settings/catalog-integrations/atlan/#add-atlan-integration","title":"Add Atlan Integration","text":"<p>Integrating Atlan with Qualytics enhances your data management capabilities, allowing seamless synchronization between the two platforms. This guide will walk you through the steps to add the Atlan integration efficiently. By following these steps, you can configure essential settings, provide necessary credentials, and customize synchronization options to meet your organization\u2019s needs.</p> <p>Step 1: Log in to your Qualytics account and click the \"Settings\" button on the left side panel of the interface.  </p> <p></p> <p>Step 2: You will be directed to the Settings page, then click on the \"Integration\" tab.</p> <p></p> <p>Step 3: Click on the Connect button next to Atlan to connect to the Atlan Integration.</p> <p></p> <p>A modal window titled Add Atlan Integration appears.</p> <p></p> <p>Fill in the connection properties to connect to Atlan.</p> REF. FIELDS ACTIONS 1. URL (Required) The complete address for the Atlan instance, for example: https://your-company.atlan.com. 2. Token (Required) Provide the authentication token needed to connect to Atlan. 3. Enable Announcements If enabled, announcements will be automatically posted to Atlan assets whenever anomalies are detected. 4. Domains Select specific domains to filter assets for synchronization. - Acts as a filtering mechanism to sync specific assets - Uses domain information from the data catalog (e.g. Sales ). Only assets under the selected domains will synchronize. 5. Event Driven If enabled, the integration sync will be activated by operations, archiving anomalies, and checks. 6. Overwrite Tags If enabled, Atlan tags will have precedence over Qualytics tags in cases of conflicts (when tags with the same name exist on both platforms). <p></p> <p>Step 4:  Click on the Create button to set up the Atlan integration. </p> <p></p> <p>Step 5: Once the Atlan integration is set up with Qualytics, it will appear in Qualytics as a new integration. </p> <p></p>"},{"location":"settings/catalog-integrations/atlan/#synchronization","title":"Synchronization","text":"<p>The Atlan synchronization supports both push and pull operations. This includes pulling metadata from Atlan to Qualytics and pushing Qualytics metadata to Atlan. During the syncing process, the integration pulls tags assigned to data assets in Atlan and assigns them to Qualytics assets as an external tag.</p> <p>Note</p> <p>Tag synchronization requires manual triggering.</p> <p>Step 1: To sync tags, click the vertical ellipsis next to Atlan and select\u202fSync from the dropdown.</p> <p></p> <p>Step 2: After clicking the \"Sync\" button, you will have the following options:</p> <ul> <li>Pull Atlan Metadata </li> <li>Push Qualytics Metadata</li> </ul> <p>Specify whether the synchronization will pull metadata, push metadata, or do both.</p> <p></p> <p>Step 3: After selecting the desired options, click on the \"Start\" button.</p> <p></p> <p>Step 4: After clicking the Start button, the synchronization process between Qualytics and Atlan begins. This process pulls metadata from Atlan and pushes Qualytics metadata, including tags, quality scores, anomaly counts, asset links, and many more.</p> <p></p> <p>Step 5: Review the logs to verify which assets were successfully mapped from Atlan to Qualytics.</p> <p></p> <p>Step 6: Once synchronization is complete, the mapped assets from \"Atlan\" will display an external tag.   </p> <p></p> <p>Step 7: When Qualytics detects anomalies, alerts are sent to the assets in Atlan, displaying the number of active anomalies and including a link to view the corresponding details</p> <p></p>"},{"location":"settings/catalog-integrations/atlan/#metadata","title":"Metadata","text":"<p>The Quality Score Total, along with the Qualytics 8 metrics (completeness, coverage, conformity, consistency, precision, timeliness, volume, and accuracy), and the count of checks and anomalies per asset identified by Qualytics, are pushed.</p> <p></p>"},{"location":"settings/catalog-integrations/external-tag-propagation/","title":"External Tag Propagation","text":"<p>External tags propagation in Qualytics serve as metadata labels that are automatically synchronized from an integrated data catalog, such as Atlan or Alation. This process helps maintain consistent data tagging across various platforms by using pre-existing tags from the data catalog.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"settings/catalog-integrations/external-tag-propagation/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the Settings button on the left side panel of the interface.</p> <p></p> <p>Step 2: You will be directed to the Settings page, then click on the Integration tab.</p> <p></p> <p>Step 3: Click on the Add Integration button.</p> <p></p> <p>A modal window Add Integration will appear, providing you with the options to add integration.</p> <p></p> REF. FIELDS ACTIONS 1. Name Provide a detailed description of the integration. 2. Type Choose the type of integration from the dropdown menu. Currently, 'Atlan' is selected 3. URL The complete address for the Atlan instance, for example: https://your-company.atlan.com. 4. Token Provide the authentication token needed to connect to Atlan. 5. Event Driven If enabled, the integration sync will be activated by operations, archiving anomalies, and checks. 6. Overwrite Tags If enabled, Atlan tags will have precedence over Qualytics tags in cases of conflicts (when tags with the same name exist on both platforms). <p></p> <p>For demonstration purposes we have selected Atlan integration type.</p> <p>Step 5: Click on the Save button to set up the Atlan integration.</p> <p></p> <p>Step 6: Once the Atlan integration is set up with Qualytics, it will appear in Qualytics as a new integration.</p> <p></p>"},{"location":"settings/catalog-integrations/external-tag-propagation/#synchronization","title":"Synchronization","text":"<p>Synchronization supports both push and pull operations. This includes pulling metadata from one platform to Qualytics and pushing Qualytics metadata to the other platform. During the syncing process, the integration pulls tags assigned to data assets in the source platform and assigns them to Qualytics assets as an external tag.</p> <p>For demonstration purposes we have selected Atlan synchronization.</p> <p>Note</p> <p>Tag synchronization requires manual triggering. </p> <p>Step 1: To sync tags, simply click on the Sync button next to the relevant integration card.</p> <p></p> <p>Step 2: After clicking the Sync button, you will have the following options:</p> <ul> <li>Pull Atlan Metadata </li> <li>Push Qualytics Metadata</li> </ul> <p>Specify whether the synchronization will pull metadata, push metadata, or do both.</p> <p></p> <p>Step 3: After selecting the desired options, click on the Start button.</p> <p></p> <p>Step 4: After clicking the Start button, the synchronization process between Qualytics and Atlan begins. This process pulls metadata from Atlan and pushes Qualytics metadata, including tags, quality scores, anomaly counts, asset links, and many more.</p> <p></p> <p>Step 5: Review the logs to verify which assets were successfully mapped from Atlan to Qualytics.</p> <p></p> <p>Step 6: Once synchronization is complete, the mapped assets from Atlan will display an external tag.</p> <p></p>"},{"location":"settings/catalog-integrations/overview/","title":"Data Catalog Integrations","text":"<p>The Qualytics platform seamlessly integrates with enterprise data catalogs, enabling organizations to: - Surface data quality insights directly within existing catalog tools - Automatically sync metadata between platforms in real-time - Leverage data catalog tags for quality classification - Push quality alerts and anomaly notifications to catalog users - Maintain consistent metadata across platforms - Track data quality metrics within your data governance framework</p> <p>These catalog integrations ensure that data quality insights are readily available to users within their preferred data discovery and governance platforms.</p>"},{"location":"settings/catalog-integrations/overview/#setting-up-catalog-integration","title":"Setting Up Catalog Integration","text":"<p>Navigate to Settings &gt; Integration to configure your data catalog connection:</p> <p></p> <p></p> <p></p>"},{"location":"settings/catalog-integrations/overview/#supported-data-catalogs","title":"Supported Data Catalogs","text":"<p>Currently, Qualytics supports integration with the following data catalog platforms:</p>"},{"location":"settings/catalog-integrations/overview/#atlan","title":"Atlan","text":"<p>The Atlan integration enables bidirectional metadata synchronization, providing: - Automated metadata push from Qualytics to Atlan - Real-time metadata pull from Atlan to Qualytics - Automatic updates based on key events - Flexible tag management options - Simple API-based authentication</p> <p>For detailed configuration steps, see the Atlan documentation.</p>"},{"location":"settings/catalog-integrations/overview/#alation","title":"Alation","text":"<p>The Alation integration supports comprehensive metadata exchange: - Bidirectional metadata synchronization - Real-time quality metric updates - Selective synchronization of active checks - Configurable tag conflict resolution - Token-based secure authentication</p> <p>For detailed configuration steps, see the Alation documentation.</p>"},{"location":"settings/catalog-integrations/overview/#synchronization-options","title":"Synchronization Options","text":"<p>Qualytics provides flexible synchronization methods to match your workflow:</p>"},{"location":"settings/catalog-integrations/overview/#manual-sync","title":"Manual Sync","text":"<p>Trigger complete metadata synchronization on-demand: </p> <p>For detailed steps, see the Synchronization section.</p>"},{"location":"settings/catalog-integrations/overview/#event-driven","title":"Event Driven","text":"<p>Enable automatic synchronization based on platform events: </p> Event Description Run an Operation (Profile Or Scan) Sync all target containers for the operation. Archive an Anomaly (including bulk) Sync the container in which the anomaly was identified. Archive a Check ( including bulk) Sync the container to which the check belongs."},{"location":"settings/connections/manage-connections/","title":"Connections","text":"<p>The Connections Management section allows you to manage global configurations for various connections to different data sources. This provides you with a centralized interface for managing all the data connections, ensuring efficient data integration and enrichment processes. You can easily navigate and manage your connections by utilizing the search, sort, edit, and delete features.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"settings/connections/manage-connections/#navigation-to-connection","title":"Navigation to Connection","text":"<p>Step 1: Log in to your Qualytics account and click the Settings button on the left sidebar of the interface. </p> <p></p> <p>Step 2: By default, you will be navigated to the Connections section.</p> <p></p>"},{"location":"settings/connections/manage-connections/#manage-connection","title":"Manage Connection","text":"<p>You can effectively manage your connections by editing, deleting, and adding datastores to maintain accuracy and efficiency.</p> <p>Warning</p> <p>Before deleting a connection, ensure that all associated datastores and enrichment datastores have been removed.</p>"},{"location":"settings/connections/manage-connections/#edit-connection","title":"Edit Connection","text":"<p>You can edit connections to update details such as name, account, role, warehouse, and authentication to improve performance. This keeps connection settings up-to-date and suited to your data needs.</p> <p>Note</p> <p>You can only edit the connection name and connection details, but you are not able to edit the connector itself.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the connection that you want to edit, then click on Edit from the dropdown menu.</p> <p></p> <p>Step 2: Edit the connection details as needed.</p> <p>Note</p> <p>Connection details vary from connection to connection, which means that each connection may have its unique configuration settings.</p> <p></p> <p>Step 3: Once you have updated the values, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>Step 4: After the connection is verified, click on the Save button to save the changes.</p> <p></p>"},{"location":"settings/connections/manage-connections/#delete-connection","title":"Delete Connection","text":"<p>This allows you to remove outdated or unnecessary connections to maintain a clean and efficient network configuration.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the connection that you want to delete, then click on Delete from the dropdown menu.</p> <p></p> <p>Step 2: A modal window Delete Connection will appear.</p> <p>Warning</p> <p>Source Datastores and Enrichment Datastores that are associated must be removed before deleting the connection.</p> <p></p> <p>Step 3: Enter the Name of the Connection in the given field (confirmation check) and then click on the I\u2019M SURE, DELETE THIS CONNECTION button to delete the connection.</p> <p></p>"},{"location":"settings/connections/manage-connections/#add-datastore","title":"Add Datastore","text":"<p>You can add new or existing datastores and enrichment datastores directly from the connection, making it easy to manage and access your data while ensuring all sources are connected and available.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the connection where you want to add a datastore, then click on Add Datastore from the dropdown menu.</p> <p></p> <p>A modal window labeled Add Datastore will appear, giving you options to connect a datastore. For more information on adding a datastore, please refer to the Configuring Source Datastores section.</p> <p>Once you have successfully added a datastore to the connection, a success message will appear.</p>"},{"location":"settings/connections/manage-connections/#view-connection","title":"View Connection","text":"<p>Once you have added a new datastore and enrichment datastore, you can view them in the connections list.</p> <p></p>"},{"location":"settings/connections/manage-connections/#sort-connection","title":"Sort Connection","text":"<p>You can sort your connections by Name and Created Date to easily find and manage them.</p> <p></p>"},{"location":"settings/connections/manage-connections/#filter-connection","title":"Filter Connection","text":"<p>You can filter connections by selecting specific data source types from the dropdown menu, making it easier to locate and manage the desired connections.</p> <p></p>"},{"location":"settings/security/directory-sync/","title":"Directory Sync","text":"<p>Directory Sync, also known as User and Group Provisioning, automates the synchronization of users and groups between your identity provider (IDP) and the Qualytics platform. This ensures that your user data is consistent across all systems, improving security and reducing the need for manual updates.</p>"},{"location":"settings/security/directory-sync/#directory-sync-overview","title":"Directory Sync Overview","text":"<p>Directory Sync automates the management of users and groups by synchronizing information between an identity provider (IDP) and your application. This ensures that access permissions, user attributes, and group memberships are consistently managed across platforms, eliminating the need for manual updates.</p>"},{"location":"settings/security/directory-sync/#how-directory-sync-works-with-scim","title":"How Directory Sync Works with SCIM","text":"<p>SCIM is an open standard protocol designed to simplify the exchange of user identity information. When integrated with Directory Sync, SCIM automates the creation, updating, and de-provisioning of users and groups. SCIM communicates securely between the IDP and your platform\u2019s API using OAuth tokens to ensure only authorized actions are performed.</p>"},{"location":"settings/security/directory-sync/#general-setup-requirements","title":"General Setup Requirements","text":"<p>To set up Directory Sync, the following are required:</p> <ul> <li>Administrative access to both the identity provider and Qualytics platform  </li> <li>A SCIM-enabled identity provider or custom integration  </li> <li>The OAuth client set up in your IDP  </li> <li>SCIM URL and OAuth Bearer Token generated from the Qualytics platform</li> </ul>"},{"location":"settings/security/directory-sync/#getting-started","title":"Getting Started","text":""},{"location":"settings/security/directory-sync/#prerequisites-for-setting-up-directory-sync","title":"Prerequisites for Setting Up Directory Sync","text":"<p>Before setting up Directory Sync, ensure you have the following:</p> <ul> <li>A SCIM-supported identity provider  </li> <li>Administrative privileges for both your IDP and Qualytics  </li> <li>A SCIM URL and OAuth Bearer Token, which will be generated from your Qualytics instance</li> </ul>"},{"location":"settings/security/directory-sync/#quick-start-guide","title":"Quick Start Guide","text":"<ol> <li>Set up an OAuth client in your IDP.  </li> <li>Configure the SCIM endpoints with the SCIM URL and OAuth Bearer Token.  </li> <li>Assign users and groups to provision in the IDP.  </li> <li>Monitor the synchronization to ensure proper operation.</li> </ol>"},{"location":"settings/security/directory-sync/#what-is-scim","title":"What is SCIM?","text":"<p>SCIM is a standardized protocol used to automate the exchange of user identity information between IDPs and service providers. Its goal is to simplify the process of user provisioning and management.</p> <p>SCIM improves efficiency by automating user lifecycle management (creation, updating, and de-provisioning) and ensures that data remains consistent across platforms. It also enhances security by minimizing manual errors and ensuring proper access control.</p> <p>SCIM includes endpoints that are configured within your IDP and your platform. It uses OAuth tokens for secure communication between the IDP and the Qualytics API, ensuring that only authorized users can manage identity data.</p>"},{"location":"settings/security/directory-sync/#benefits-of-using-scim-for-user-and-group-provisioning","title":"Benefits of Using SCIM for User and Group Provisioning","text":"<p>By leveraging SCIM (System for Cross-domain Identity Management), Directory Sync simplifies user management with:</p> <ul> <li>Automated user provisioning and de-provisioning  </li> <li>Reduced manual intervention, improving efficiency and security  </li> <li>Real-time updates of user data, ensuring accuracy and compliance</li> <li>Support for scaling user management across organizations of any size</li> </ul> <p>Supported Providers</p> <p>Our API supports SCIM 2.0 (System for Cross-domain Identity Management) as defined in RFC 7643 and RFC 7644. It is designed to ensure seamless integration with any SCIM-compliant identity management system, supporting standardized user provisioning, de-provisioning, and lifecycle management. Additionally, we have verified support with the following providers:</p> <ul> <li>Microsoft Entra (Azure Active Directory) </li> <li>Okta </li> <li>OneLogin </li> <li>JumpCloud</li> </ul> <p>Unsupported Providers</p> <p>We do not support Google Workspace, as it does not offer SCIM support. Organizations using Google Workspace must use alternate methods for user provisioning.</p>"},{"location":"settings/security/directory-sync/#providers","title":"Providers","text":""},{"location":"settings/security/directory-sync/#1-microsoft-entra","title":"1. Microsoft Entra","text":""},{"location":"settings/security/directory-sync/#creating-an-app-registration","title":"Creating an App Registration","text":"<p>Step 1: Log in to the Microsoft Azure Portal, and select \u201cMicrosoft Entra ID\u201d from the main menu.</p> <p></p> <p>Step 2: Click on \u201cEnterprise Applications\u201d from the left navigation menu.</p> <p></p> <p>Step 3: If your application is already created, choose it from the list and move to the section Configuring SCIM Endpoints. If you haven't created your application yet, click on the New Application button.</p> <p></p> <p>Step 4: Click on the \u201cCreate your own application\u201d button to create your application.</p> <p></p> <p>Step 5: Give your application a name (e.g., \"Qualytics OAuth Client\" or \"Qualytics SCIM Client\").</p> <p></p> <p>Step 6: After entering the name for your application, click the Create button to finalize the creation of your app.</p> <p></p>"},{"location":"settings/security/directory-sync/#configuring-scim-endpoints","title":"Configuring SCIM Endpoints","text":"<p>Step 1: Click on Provisioning from the left-hand menu.</p> <p></p> <p>Step 2: A new window will appear, click on the Get Started button.</p> <p></p> <p>Step 3: In the Provisioning Mode dropdown, select \u201cAutomatic\u201d and enter the following details in the Admin Credentials section:</p> <ol> <li> <p>Provisioning Mode: Select Automatic.</p> </li> <li> <p>Tenant URL: <code>https://your-domain.qualytics.io/api/scim/v2</code> </p> </li> <li> <p>Secret Token: Generate this token from the Qualytics UI when logged in as an admin user. For more information on how to generate tokens in Qualytics, refer to the documentation on Tokens.</p> </li> </ol> <p></p> <p>Step 4: Click on the Test Connection button to test the connection to see if the credentials are correct.</p> <p></p> <p>Step 5: Expand the Mappings section and enable your app to enable group and user attribute mappings. The default mapping should work.</p> <p></p> <p>Step 6: Expand the Settings section and make the following changes:</p> <ol> <li>Select Sync only assigned users and groups from the Scope dropdown.  </li> <li>Confirm the Provisioning Status is set to On.</li> </ol> <p></p> <p>Step 7: Click on the Save to save the credentials. Now you've successfully configured the Microsoft Entra ID SCIM API integration.</p> <p></p>"},{"location":"settings/security/directory-sync/#assigning-users-and-groups-for-provisioning","title":"Assigning Users and Groups for Provisioning","text":"<p>Step 1: Click on the Users and groups from the left navigation menu and then click Add user/group.</p> <p></p> <p>Step 2: Click on the None Selected under the Users and Groups.</p> <p></p> <p>Step 3: From the right side of the screen, select the users and groups you want to assign to the app.</p> <p></p> <p>Step 4: Once you selected the group and users for your app, click the \u201cSelect\u201d button.</p> <p></p> <p>Step 5: Click on the Assign button to assign the users and groups to the application.</p> <p>Warning</p> <p>When you assign a group to an application, only users directly in the group will have access. The assignment does not cascade to nested groups.</p> <p></p>"},{"location":"settings/security/directory-sync/#2-okta","title":"2. Okta","text":""},{"location":"settings/security/directory-sync/#setting-up-the-oauth-client-in-okta","title":"Setting up the OAuth Client in Okta","text":"<p>Step 1: Log in to your Okta account using your administrator credentials. From the left-hand navigation menu, click Applications, then select Browse App Catalog.</p> <p></p> <p>Step 2: In the search bar, type SCIM 2.0 Test App (OAuth Bearer Token), and select the app called SCIM 2.0 Test App (OAuth Bearer Token) from the search results.</p> <p></p> <p>Step 3: On the app\u2019s details page, click Add Integration.</p> <p></p> <p>Step 4: Enter a name for your application (e.g., \"Qualytics SCIM Client\").</p> <p></p> <p>Step 5: Click on the Next button.</p> <p></p>"},{"location":"settings/security/directory-sync/#configuring-scim-endpoints_1","title":"Configuring SCIM Endpoints","text":"<p>Step 1: In the newly created app, go to the Provisioning tab and click Configure API Integration.</p> <p></p> <p>Step 2: Check the box labeled Enable API Integration, and enter the following details:</p> <ul> <li> <p>SCIM 2.0 Base URL: <code>https://your-domain.qualytics.io/api/scim/v2</code> </p> </li> <li> <p>OAuth Bearer Token: Generate this token from the Qualytics UI when logged in as an admin user. For more information on how to generate tokens in Qualytics, refer to the documentation on Tokens.</p> </li> </ul> <p></p> <p>Step 3: Click Test API Credentials to verify the connection. Once the credentials are validated, click Save.</p> <p></p> <p>Step 4: A new settings page will appear. Under the To App section, enable the following settings:</p> <ul> <li>Create Users </li> <li>Update User Attributes </li> <li>Deactivate Users</li> </ul> <p>After enabling these settings, your Okta SCIM API integration is successfully configured.</p> <p></p>"},{"location":"settings/security/directory-sync/#assigning-users-for-provisioning","title":"Assigning users for provisioning","text":"<p>Step 1: Click the Assignments tab and select Assign to People from the dropdown Assign.</p> <p></p> <p>Step 2: Select the users you want to assign to the app and click the Assign button.</p> <p></p> <p>Step 3: After you click the Assign button, you'll see a new popup window with various fields. Confirm the field values and click the Save and Go Back buttons.</p> <p></p>"},{"location":"settings/security/directory-sync/#assigning-groups-for-provisioning","title":"Assigning groups for provisioning","text":"<p>Step 1: Navigate to the tab Push Groups and select Find group by name from the dropdown Push Groups.</p> <p></p> <p>Step 2: Search for the group you want to assign to the app.</p> <p></p> <p>Step 3: After assigning the group name, then click on the Save button.</p> <p></p>"},{"location":"settings/security/directory-sync/#3-onelogin","title":"3. OneLogin","text":""},{"location":"settings/security/directory-sync/#setting-up-the-oauth-client-in-onelogin","title":"Setting up the OAuth Client in OneLogin","text":"<p>Step 1: Log in to your OneLogin account using your administrator credentials. From the top navigation menu, click Applications, then select Add App.</p> <p></p> <p>Step 2: In the search bar, type SCIM and select the app called SCIM Provisioner with SAML (SCIM V2 Enterprise) from the list of apps.</p> <p></p> <p>Step 3: Enter a name for your app, then click Save. You have successfully created the SCIM app in OneLogin.</p> <p></p>"},{"location":"settings/security/directory-sync/#configuring-scim-endpoints_2","title":"Configuring SCIM Endpoints","text":"<p>Step 1: In your created application, navigate to the Configuration tab on the left and enter the following information:</p> <ul> <li> <p>API Status: Enable the API status for the integration to work properly. </p> </li> <li> <p>SCIM Base URL: <code>https://your-domain.qualytics.io/api/scim/v2</code> </p> </li> <li> <p>SCIM Bearer Token: Generate this token from the Qualytics UI when logged in as an admin user. For more information on how to generate tokens in Qualytics, refer to the documentation on Tokens.</p> </li> </ul> <p></p> <p>Step 2: Click on the Save button to store the credentials.</p> <p></p> <p>Step 3: Navigate to the Provisioning tab, and check the box labeled Enable Provisioning.</p> <p></p> <p>Step 4: Click on Save to apply the changes.</p> <p></p> <p>Step 5: Navigate to the Parameters tab and select the row for Groups.</p> <p></p> <p>Step 5: A popup window will appear, check the box Include in User Provisioning, then click the Save button.</p> <p></p>"},{"location":"settings/security/directory-sync/#assigning-users-for-provisioning_1","title":"Assigning Users for Provisioning","text":"<p>Step 1: To assign users to your app, go to Users from the top navigation menu, and select the user you want to assign to the app.</p> <p>From the User page, click the Applications tab on the left, and click the + (plus) sign.</p> <p></p> <p>Step 3: A popup window will show a list of apps. Select the app you created earlier and click Continue.</p> <p></p> <p>Step 4: A new modal window will appear, click on the Save to confirm the assignment.</p> <p></p> <p>Step 5: If you see the status Pending in the table, click that text. A modal window will appear, where you can click Approve to confirm the assignment.</p> <p></p>"},{"location":"settings/security/directory-sync/#assigning-groups-for-provisioning_1","title":"Assigning Groups for Provisioning","text":"<p>Step 1: To push groups to your app, go to the top navigation menu, click Users, select Roles from the dropdown, and click New Role to create the role.</p> <p></p> <p>Step 2: Enter a name for the role, select the app you created earlier</p> <p></p> <p>Step 3: Click on the \u201cSave\u201d button.</p> <p></p> <p>Step 4: Click the Users tab for the role and search for the user you want to assign to the role.</p> <p></p> <p>Step 5: Click the Add To Role button to assign the user, then click Save to confirm the assignment.</p> <p></p> <p>Step 6: A modal window will appear, click on the \u201cSave\u201d button to confirm the assignment.</p> <p></p> <p>Step 7: Go back to your app and click the Rule tab on the left and click the Add Rule button.</p> <p>Give the rule a name. Under the Actions, select the Set Groups in your-app-name from the dropdown, then select each role with values that match your-app-name.</p> <p></p> <p>Step 8: Click on the Save button.</p> <p></p> <p>Step 9: Click on the Users tab on the left, you may see Pending under the provisions state. Click on it to approve the assignment.</p> <p></p> <p>Step 10: A modal window will appear, click on the Approve to finalize the assignment.</p> <p></p>"},{"location":"settings/security/directory-sync/#4-jumpcloud","title":"4. JumpCloud","text":""},{"location":"settings/security/directory-sync/#configuring-scim-endpoints_3","title":"Configuring SCIM Endpoints","text":"<p>JumpCloud supports SCIM provisioning within an existing SAML application. Follow these steps to configure SCIM provisioning:</p> <p>Step 1: Log in to JumpCloud and either choose an existing SAML application or create a new one. From the left navigation menu, click SSO and select your Custom SAML App.</p> <p></p> <p>Step 2: Click on the tab Identity Management within your SAML application.</p> <p>Under the SCIM Version, choose SCIM 2.0 and enter the following information:</p> <ol> <li> <p>Base URL: <code>https://your-domain.qualytics.io/api/scim/v2</code></p> </li> <li> <p>Token Key: Generate this token from the Qualytics UI when logged in as an admin user. For more information on how to generate tokens in Qualytics, refer to the documentation on Tokens.  </p> </li> <li> <p>Test User Email</p> </li> </ol> <p></p> <p>Step 4: Click Test Connection to ensure the credentials are correct, then click Activate to enable SCIM provisioning.</p> <p></p> <p>Step 5: Click Save to store your settings. Once saved, SCIM provisioning is successfully configured for your JumpCloud SAML application.</p> <p></p>"},{"location":"settings/security/directory-sync/#assigning-users-for-provisioning_2","title":"Assigning Users for Provisioning","text":"<p>Step 1: Click the tab User Groups within your SAML application. You can see all the available groups, select the groups you want to sync, and click Save.</p> <p></p> <p>If no existing groups are available, click User Groups from the left navigation menu and click on the plus (+) icon to create a new group.</p> <p></p> <p>Step 2: Select the Users tab and choose the users you want to assign to the group.</p> <p></p> <p>Step 3: Select the Applications tab and choose the app you want to assign the group to.</p> <p></p>"},{"location":"settings/security/overview/","title":"Security","text":"<p>You can easily manage user and team access by assigning roles and permissions within the system. This includes setting up specific access levels and roles for different users and teams. By doing so, you ensure that data and resources are accessed securely and appropriately, with only authorized individuals and groups having the necessary permissions to view or modify them. This helps maintain the integrity and security of your system.</p> <p>Note</p> <p>Only users with the Admin role have the authority to manage global platform settings, such as user permissions and team access controls.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"settings/security/overview/#navigation-to-security","title":"Navigation to Security","text":"<p>Step 1: Log in to your Qualytics account and click the Settings button on the left side panel of the interface. </p> <p></p> <p>Step 2: By default, you will be navigated to the Tags section. Click on the Security tab.</p> <p></p>"},{"location":"settings/security/overview/#user-roles","title":"User Roles","text":"<p>In Qualytics, every user is assigned a role: <code>Admin</code>,<code>Manager</code> or <code>Member</code>. Admins have the ability to edit any User selected from the user listing and change that User's role.  </p> <ul> <li>Admin: Admin users have full access to the system and can manage datastores, teams, and users. This means they can access everything in the application, as well as manage user accounts and team permissions.</li> </ul> Category Functionality Description Source Datastore Delete Permanently remove a source datastore from the system. Enrichment Datastore Delete Permanently remove an enrichment datastore from the system. Global Settings Settings Manage global system configurations and preferences. Security Manage user access and team permissions within the system. User (Manage) Add, modify, or delete user accounts, assign roles, and control access levels. Team (Manage) Manage teams by adding or removing members, and setting team-specific permissions. Health Monitor the system\u2019s health status and performance metrics. Restart Analytical Engine Restart the analytics engine to refresh data processing or resolve issues. <ul> <li>Manager: Manager role has limited administrative access over global assets but remains subject to team permissions when interacting with datastores. Managers cannot manage user roles or teams. They can list all datastores (but cannot view their content without explicit team permission) and create datastores for teams where they have Editor permission. Additionally, Managers on a team with Editor permission can manage datastore teams. They can manage global assets such as Tags, Templates, and Notifications but do not have the ability to manage user accounts or team permissions like Admins.</li> </ul> Category Functionality Description Source Datastore Create Managers can create new source datastores for data integration. List Managers can view all source datastores that are listed in the system. Add Enrichment Add enrichment processes to source datastores to enhance data quality. Teams Manage Managers on a team with Editor permission can manage datastore teams. Enrichment Datastore Create Managers can create enrichment datastores and assign them to teams with an \"Editor\" role. List Managers can view all enrichment datastores available in the system. Global Settings DataStore (Source &amp;Enrichment) Create Managers can create new source and enrichment datastores and assign them to teams with an \"Editor\" role. List Managers can view all datastores (source and enrichment) listed in the system. Library View View the checks, or assets available in the library. Manage Manage library content, such as adding, modifying, or removing checks. Tags View View tags assigned to datastores, records, or other elements in the system. Manage Manage the tags themselves \u2014 create, update (name, color, description), or delete tags. Assigning tags to assets such as datastores, fields, checks, and anomalies depends on the user\u2019s role and team permissions.Note: For Flows, only Admin and Manager roles can assign tags. Notifications Rules View View existing notification rules and actions configured for alerts. Manage Configure and manage notification rules for different actions or triggers. Settings Connections Create Create new connections for integrating external systems or databases. Update Update existing connections to modify their settings or credentials. Delete Remove existing connections that are no longer needed. Security View Users View the list of users in the system and their access details. View Teams View the teams and their roles/permissions within the system. Integration Add Add new integrations to the system for external systems or data sources. Sync Sync external data with the system to ensure the most up-to-date information. Health View View the health status of the system to monitor performance and stability. API only (ATM) Transaction History View the history of transactions made via the API for auditing and tracking. <ul> <li>Member: Members are normal users with access explicitly granted to them, usually inherited from the teams they are assigned to.</li> </ul> Category Functionality Description Library View Access and browse available checks. Tags View View tags associated with datastores, records, or other system elements. Actions View View existing notification rules and action configurations. Settings Connection Read Access connection details without modification permissions. Tokens Generate Token Create new tokens for secure access or integrations. Revoke Disable existing tokens to restrict access. Restore Reactivate previously revoked tokens. Delete Permanently remove tokens. View Access and review all token details."},{"location":"settings/security/overview/#manage-users","title":"Manage Users","text":"<p>You can easily manage users by assigning roles, teams, and deactivating users who are not active. This ensures that access control is streamlined, security is maintained, and only active users have access to resources.</p> <p>The Security section, visible only to Admins, allows for granting and revoking permissions for Member users.</p> <p>Access controls in Qualytics are assigned at the datastore level. A non-administrator user (Member) can have one of three levels of access to any datastore connected to Qualytics:</p> <ul> <li> <p>Editor: Editor role has the most advanced permissions, enabling users to manage datastore functions comprehensively. Editors can control enrichment, scoring, computed fields, operations, and more. However, they cannot add teams outside their access; only administrators can perform this task.</p> </li> <li> <p>Author: Author role focuses on managing checks and their associated metadata. This role is essential for tasks like activating, validating, and editing checks but has limited access to datastore functionalities.</p> </li> <li> <p>Drafter: Drafter role is designed for users who need to create and prepare checks without performing or finalizing them. This role focuses on adding and organizing content for future use.</p> </li> <li> <p>Viewer: Viewer role provides read-only access to anomalies and allows users to add comments or create notes. This role is ideal for those who need to monitor activities without making changes.</p> </li> <li> <p>Reporter: Reporter role has extensive access to all app report information, including dashboards, overviews, and anomalies. Reporters can view various data contexts and generate analytical insights.</p> </li> </ul> <p>Note</p> <p>Permissions are assigned to Teams rather than directly to users. Users inherit the permissions of the teams to which they are assigned.</p> <p>All users are part of the default Public team, which provides access to all Public Datastores. Admins can create and manage additional teams, assigning both users and datastores to them. When a datastore is assigned to a team, the team is granted either Read or Write access, and all team members inherit this permission.</p>"},{"location":"settings/security/overview/#view-users","title":"View Users","text":"<p>Whenever new users are added to the system, they will appear in the Users list. Click the Users tab to view the list of users.</p> <p></p>"},{"location":"settings/security/overview/#edit-users","title":"Edit Users","text":"<p>You can edit user details to update their role, and team assignments, ensuring their access and team information are current and accurate.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the user name that you want to edit, then click on Edit from the dropdown menu.</p> <p></p> <p>Step 2: Edit the user details as needed, including:</p> <ol> <li>Updating their role</li> <li>Assigning them additional teams</li> </ol> <p>Note</p> <p>All users are inside the Public team by default and that can't be changed. If users have no default access to any datastore, then no datastores should be assigned to the Public team.</p> <p></p> <p>Step 3: Once you have made the necessary changes, then click on the Save button.</p> <p></p> <p>After clicking the Save button, your changes will be updated, and a success message will appear.</p>"},{"location":"settings/security/overview/#deactivate-users","title":"Deactivate Users","text":"<p>You can deactivate users to revoke their access to the system while retaining their account information for future reactivation if needed.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the user name that you want to deactivate, then click on Deactivate from the dropdown menu.</p> <p></p> <p>Step 2: A modal window Deactivate User will appear.</p> <p></p> <p>Step 3: Enter deactivate in the given field (confirmation check) and then click on the I\u2019M SURE, DEACTIVATE THIS USER button to deactivate the user.</p> <p></p>"},{"location":"settings/security/overview/#sort-users","title":"Sort Users","text":"<p>You can sort users by various criteria, such as Created date, Name, Role, and Teams, to easily manage and organize user information.</p> <p></p>"},{"location":"settings/security/overview/#filter-users","title":"Filter Users","text":"<p>You can filter the users by their roles, deactivated and team, to quickly find and manage particular groups of users.</p> <p></p>"},{"location":"settings/security/overview/#manage-teams","title":"Manage Teams","text":"<p>You can manage teams by editing their permissions, adding or removing users, and adjusting access to source and enrichment datastores. If a team is no longer needed, you can delete it from the system. This ensures that team configurations are always up-to-date and relevant, enhancing overall data management and security.</p>"},{"location":"settings/security/overview/#view-team","title":"View Team","text":"<p>Whenever new teams are added to the system, they will appear in the Teams list. Click the Teams tab to view the list of teams.</p> <p></p>"},{"location":"settings/security/overview/#edit-team","title":"Edit Team","text":"<p>You can edit a team to update its permissions, name, manage users within the team, and adjust access to source and enrichment datastores, ensuring the team's configuration is current and effective.</p> <p>Note</p> <p>The name and users of a public team cannot be edited.</p> <p>Step 1:  Click on the vertical ellipsis (\u22ee) next to the team name that you want to edit, then click on Edit from the dropdown menu.</p> <p></p> <p>Step 2:  Edit the team details as needed, including updating their permissions, users, source, and enrichment datastores.</p> <p></p> <p>Step 3: Once you have made the necessary changes, then click on the Save button.</p> <p></p> <p>After clicking on the Save button, your team is successfully updated, and a success message will appear.</p>"},{"location":"settings/security/overview/#delete-team","title":"Delete Team","text":"<p>You can delete a team from the system when it is no longer needed, removing its access and permissions to streamline management and maintain security.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the team name that you want to delete, then click on Edit from the dropdown menu.</p> <p></p> <p>A modal window Delete Team will appear.</p> <p></p> <p>Step 2: Click on the Delete button to delete the team from the system.</p> <p></p>"},{"location":"settings/security/overview/#sort-team","title":"Sort Team","text":"<p>You can sort teams by various criteria, such as name or creation date, to easily organize and manage team information.</p> <p></p> <ul> <li>Team Permissions</li> <li>Directory Sync</li> </ul>"},{"location":"settings/security/team-permissions/","title":"Team Permissions","text":"<p>Admins are not subject to Team permissions and can therefore access all data assets. By contrast, users assigned the Member and Manager roles are subject to Team permissions which control the data assets they can interact with.</p> <p>Team permissions are granted at the Datastore level and extend to all data assets under that Datastore (Tables/View/Files, Fields, Quality Checks, Anomalies, etc...)</p>"},{"location":"settings/security/team-permissions/#permission-matrix","title":"Permission Matrix","text":"<p>Legend:</p> <ul> <li><code>\u2705</code> The given Team permission grants the ability to perform the Action on associated Datastores</li> <li><code>\u274c</code> The given Team permission does not grant the ability to perform the Action on associated Datastores</li> </ul> Action Reporter Viewer Drafter Author Editor Delete Source Datastore \u274c \u274c \u274c \u274c \u274c View Source Datastore \u2705 \u2705 \u2705 \u2705 \u2705 Edit Datastore Settings \u274c \u274c \u274c \u274c \u2705 Preview Source Datastore \u274c \u2705 \u2705 \u2705 \u2705 Create/Delete Computed Asset \u274c \u274c \u274c \u274c \u2705 View Activity \u2705 \u2705 \u2705 \u2705 \u2705 Run &amp; Manage Operations \u274c \u274c \u274c \u274c \u2705 Schedule Operations \u274c \u274c \u274c \u274c \u2705 View Profiles \u2705 \u2705 \u2705 \u2705 \u2705 Delete Profiles \u274c \u274c \u274c \u274c \u2705 View Checks \u2705 \u2705 \u2705 \u2705 \u2705 Create Checks \u274c \u274c \u2705 \u2705 \u2705 Save Check to draft \u274c \u274c \u2705 \u2705 \u2705 Restore Check to draft \u274c \u274c \u2705 \u2705 \u2705 Activate / Validate Check \u274c \u274c \u274c \u2705 \u2705 Edit Check Metadata \u274c \u274c \u274c \u2705 \u2705 View Anomalies \u2705 \u2705 \u2705 \u2705 \u2705 View Anomaly Source Records \u274c \u2705 \u2705 \u2705 \u2705 Change Anomaly Status \u274c \u274c \u274c \u2705 \u2705 Add Comment to Anomaly \u274c \u2705 \u2705 \u2705 \u2705 Delete Enrichment Datastore \u274c \u274c \u274c \u274c \u274c View Enrichment Datastore \u274c \u2705 \u2705 \u2705 \u2705 Preview Enrichment Datastore \u274c \u2705 \u2705 \u2705 \u2705 View Tags \u2705 \u2705 \u2705 \u2705 \u2705 Manage Tags (create / update / delete) \u274c \u274c \u2705 \u2705 \u2705 Assign Tags to Assets (Datastore, Container, Field, Check, Anomaly) \u274c \u2705 \u2705 \u2705 \u2705 Assign Tags in Flows \u274c \u274c \u2705 \u2705 \u2705"},{"location":"settings/security/team-permissions/#add-team","title":"Add Team","text":"<p>You can create a new team for efficient and secure data management. Teams make it easier to control who has access to what, help people work together better, keep things secure with consistent rules, and simplify managing and expanding user groups. You can assign permissions to the team, such as Editor, Author, Drafter, Viewer and Reporter access, by selecting the datastore and enrichment datastore to which you want them to have access. This makes data management easier.</p> <p>Step 1: Click on the Add Team button located in the top right corner.</p> <p></p> <p>Step 2: A modal window will appear, providing the options for creating the team. Enter the required values to get started.</p> REF. FIELD ACTION EXAMPLE 1. Name Enter the name of the team Data Insights Team 2. Description Provide a brief description of the team. Analyzes data to provide actionable insights, supporting data-driven decisions <p></p>"},{"location":"settings/security/team-permissions/#permissions","title":"Permissions","text":"<p>Permissions decide what users can see, create, or manage based on their role. Each role is designed for specific tasks, giving users access to the tools and information they need without going beyond their limits. From Editors who manage advanced settings to Viewers with read-only access, these roles make it easy to use the system while keeping everything secure.</p> <p></p>"},{"location":"settings/security/team-permissions/#editor","title":"Editor","text":"<p>Editor role allows users to manage datastore functions comprehensively. They can handle tasks such as controlling enrichment, scoring, computed fields, and operations.</p> <p></p> Feature Operation Can View/Can Run Can Manage Datastores Add Datastore \u274c \u2705 Edit Settings \u274c \u2705 Enrichment Add Enrichment \u274c \u2705 Edit Enrichment \u274c \u2705 Scoring Edit Scoring \u274c \u2705 Computed Field Add Computed \u274c \u2705 Operation Run Operation \u2705 \u2705 Manage Operation \u274c \u2705 Manage Scheduled Operation \u274c \u2705 Profiles Add Computed \u274c \u2705 Delete Computed \u274c \u2705 Field Context Edit Field Context \u274c \u2705 Delete Field Context \u274c \u2705"},{"location":"settings/security/team-permissions/#author","title":"Author","text":"<p>Author role focuses on managing checks within the system. Users can activate, validate, change the status of checks, and edit their metadata. It is specifically designed for handling these functions efficiently.</p> <p></p> Feature Functionality Can View/ Can Run Can Edit Source Datastore Checks \u274c \u2705 Activate Checks \u274c \u2705 Validate Checks \u274c \u2705 Change Status of Checks \u274c \u2705 Edit Metadata \u274c \u2705 Anomalies \u274c \u2705 Anomalies Change Status of Anomalies \u274c \u2705"},{"location":"settings/security/team-permissions/#drafter","title":"Drafter","text":"<p>Drafter role is designed specifically for adding and saving data within the system. Users can create new, make edits to existing ones, and save their progress as drafts. It is focused on these basic functions without access to advanced features or management tasks.</p> <p></p> Feature Functionality Can View Can Edit Source Datastore Open Datastore \u2705 \u274c Add Checks \u274c \u2705 Profiles Add Check \u274c \u2705 Checks Create as Draft \u274c \u2705 Field Context Add Check \u274c \u2705"},{"location":"settings/security/team-permissions/#viewer","title":"Viewer","text":"<p>Viewer role is focused on viewing anomalies within the system and creating notes as needed. It offers read-only access while allowing users to add comments to document their observations.</p> <p></p> Features Functionality Can View Can Edit Source Datastore Anomalies \u2705 \u274c Add Comment \u2705 \u274c Preview (Container) \u2705 \u274c Enrichment Datastore View \u2705 \u274c Preview \u2705 \u274c Explore Anomalies \u2705 \u274c Source Records \u2705 \u274c"},{"location":"settings/security/team-permissions/#reporter","title":"Reporter","text":"<p>Reporter role provides access to all report-related information, including dashboards, overviews, checks, anomalies, fields, containers, and datastores. It is intended for users who need to view and analyze data to generate reports.</p> <p></p> Feature Operation Can View Can Edit Source Datastore List \u2705 \u274c View \u2705 \u274c Overview \u2705 \u274c Activity \u2705 \u274c Profiles \u2705 \u274c Observability \u2705 \u274c Checks \u2705 \u274c Anomalies \u2705 \u274c Fields (Containers) \u2705 \u274c Enrichment Datastores List \u2705 \u274c Explore Insights \u2705 \u274c Activity \u2705 \u274c Profiles \u2705 \u274c Observability \u2705 \u274c Checks \u2705 \u274c Anomalies \u2705 \u274c <p></p> REF. FIELD ACTION EXAMPLE 4. Users Add users to the team John, Michael 5. Source Datastores Grant access to specific source datastores (single or multiple) for the team Athena 6. Enrichment Datastores Add and grant access to additional enrichment datastores (single or multiple) for the team Bank Enrichment <p>Step 3: Click on the Save button to save your team.</p> <p></p> <p>After clicking on the Save button, your team is created, and a success message will appear saying.</p>"},{"location":"settings/status/overview-of-status/","title":"Status","text":"<p>System status provides a real-time overview of your system's resources, essential for monitoring performance and diagnosing potential issues. It provides key indicators and status updates to help you maintain system health and quickly address potential issues.</p>"},{"location":"settings/status/overview-of-status/#navigation-to-status","title":"Navigation to Status","text":"<p>Step 1: Log in to your Qualytics account and click the Settings button on the left side panel of the interface. </p> <p></p> <p>Step 2: You will be directed to the Settings page; then click on the Status section.</p> <p></p>"},{"location":"settings/status/overview-of-status/#summary-section","title":"Summary Section","text":"<p>The Summary section displays the current platform version, along with the database status and RabbitMQ state.</p> REF. FIELD ACTION EXAMPLE 1 Current Platform Version Shows the current version of your platform's core software. 20240808-3019c60 2 Cloud Platform Indicates which cloud provider the platform is hosted on. Amazon Web Services (AWS) 3 Deployment Size Indicates the size of the deployment that the client has contracted. Medium 4 Database Verifies your database connection. An \"OK\" status means it\u2019s connected. Status:OK 5 RabbitMQ Confirms RabbitMQ (a message broker software) is running correctly with an \"OK\" state. State:OK <p></p>"},{"location":"settings/status/overview-of-status/#status-indicator","title":"Status Indicator","text":"<p>The status indicator reflects the overall system resources health.For example, in the image below, a green checkmark indicates that our system resources are healthy.</p> <p>Note</p> <p>Status indicators are simple: a green checkmark indicates \"Healthy,\" and a red exclamation mark means \"Critical.\"</p> <p></p>"},{"location":"settings/status/overview-of-status/#analytics-engine","title":"Analytics Engine","text":"<p>The Analytics Engine section provides advanced information about the analytics engine's configuration and current state for technical users and developers.</p> REF FIELD ACTION EXAMPLE 1 Build Date This shows the date and time when the Analytics Engine was built. Aug 8 2024,7:39 AM (GMT+5:30) 2 Implementation Version The version of the analytics engine implementation being used. 2.0.0 3 Max Executors Maximum number of executors allocated for processing tasks. 10 4 Max Memory Per Executor This shows the maximum amount of memory allocated to each executor. 25000 MB 5 Spark Version The version of Apache Spark that the Analytics Engine uses for processing. 3.5.1 6 Core Per Executor This shows the number of CPU cores assigned to each executor. 3 7 Max Dataframe Size The maximum size of dataframes that can be processed. 50000 MB 8 Thread Pool State Indicates the current state of the thread pool used for executing tasks. [Running, parallelism \\= 3, size \\= 0, active \\= 0, running \\= 0, steals \\= 0, tasks \\= 0, submissions \\= 0] supporting 0 running operation with 0 queued requests <p></p>"},{"location":"settings/status/overview-of-status/#private-routes","title":"Private Routes","text":"<p>Users can now utilize private routes to view their IP addresses along with relevant system messages in the Analytics Engine, ensuring greater transparency and visibility into network activity.</p> <p></p>"},{"location":"settings/status/overview-of-status/#manage-status-summary","title":"Manage Status Summary","text":"<p>You can perform essential tasks such as copying the status summary, refreshing it, and restarting the analytics engine. These functionalities help maintain an up-to-date overview of system performance and ensure accurate analytics.</p>"},{"location":"settings/status/overview-of-status/#copy-status-summary","title":"Copy Status Summary","text":"<p>The Copy Status Summary feature lets you duplicate all data from the Health Section for easy sharing or saving.</p> <p>Step 1: Click the vertical ellipsis from the right side of the summary section and choose Copy Status Summary from the drop-down menu.</p> <p></p> <p>Step 2: After clicking on Copy Status Summary,  a success message saying Copied.</p> <p></p>"},{"location":"settings/status/overview-of-status/#refresh-status-summary","title":"Refresh Status Summary","text":"<p>The Refresh Status Summary option updates the Health Section with the latest data. This ensures that you see the most current performance metrics and system status.</p> <p>Step 1: Click the vertical ellipsis from the right side of the summary section and choose Refresh Status Summary to update the latest data.  </p> <p></p>"},{"location":"settings/status/overview-of-status/#restart-analytics-engine","title":"Restart Analytics Engine","text":"<p>The Restart Analytics Engine option restarts the analytics processing system. This helps resolve issues and ensures that analytics data is accurately processed.</p> <p>Step 1: Click the vertical ellipsis from the right side of the summary section and choose Restart Analytics Engine from the drop-down menu. </p> <p></p> <p>Step 2: A modal window will pop up. Click the Restart button in this window to restart the analytics engine. Restarting the engine helps resolve any issues and ensures that your analytics data is up-to-date and accurately processed.</p> <p></p> <p>Step 3: After clicking on Restart button a success message saying Successfully triggered Analytics Engine restart.</p> <p></p>"},{"location":"settings/tokens/overview-of-tokens/","title":"Tokens","text":"<p>A token is a secure way to access the Qualytics API instead of using a password. Each user gets a unique Personal API Token (PAT) for authentication. These tokens are created only once, so you need to copy and store them safely because you'll use them to log in and interact with the platform in the future.</p> <p>Let\u2019s get Started \ud83d\ude80</p>"},{"location":"settings/tokens/overview-of-tokens/#navigation-to-tokens","title":"Navigation to Tokens","text":"<p>Step 1: Log in to your Qualytics account and click the Settings button on the left side panel of the interface. </p> <p></p> <p>Step 2: By default, you will be navigated to the Tags section. Click on the Tokens tab.</p> <p></p>"},{"location":"settings/tokens/overview-of-tokens/#generate-token","title":"Generate Token","text":"<p>Generating a token provides a secure method for authenticating and interacting with your platform, ensuring that only authorized users and applications can access your resources. Personal Access Tokens (PATs) are particularly useful for automated tools and scripts, allowing them to perform tasks without needing manual intervention. By using PATs, you can leverage our Qualytics CLI to streamline data management and operations, making your workflows more efficient and secure.</p> <p>Step 1: Click on the Generate Token button located in the top right corner.</p> <p></p> <p>A modal window will appear providing the options for generating the token.</p> <p></p> <p>Step 2: Enter the following values:</p> <ol> <li>Name: Enter the name for the Token ( e.g., DataAccessToken) </li> <li>Expiration: Set the expiration period for the token (e.g., 30 days)</li> </ol> <p></p> <p>Step 3: Once you have entered the values, then click on the Generate button.</p> <p></p> <p>Step 4: After clicking on the Generate button, your token is successfully generated.</p> <p>Warning</p> <p>Make sure to copy your secret key as you won't be able to see it again. Keep your secret keys confidential and avoid sharing them with anyone. Use a password manager or an encrypted vault to store your secret keys.</p> <p></p>"},{"location":"settings/tokens/overview-of-tokens/#token-usage-status","title":"Token Usage Status","text":"<p>Each personal API token displays a usage status to indicate whether it has been used for interaction with the Qualytics API:</p> <p>Last Used: Its show the token has been successfully used recently and is actively in use.</p> <p></p> <p>Not Used: The token has been generated but has not been used for any API requests since creation.</p> <p></p>"},{"location":"settings/tokens/overview-of-tokens/#revoke-token","title":"Revoke Token","text":"<p>You can revoke your token to prevent unauthorized access or actions, especially if the token has been compromised, is no longer needed, or to enhance security by limiting the duration of access.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the user token, that you want to revoke, then click on Revoke from the dropdown menu.</p> <p></p> <p>Step 2: After clicking the Revoke button, your user token will be successfully revoked. A success message will display saying User token successfully revoked. Following revocation, the token's status color will change from green to orange.</p> <p></p>"},{"location":"settings/tokens/overview-of-tokens/#restore-token","title":"Restore Token","text":"<p>You can restore a token to reactivate its access, allowing authorized use again. This is useful if the token was mistakenly revoked or if access needs to be temporarily re-enabled without generating a new token.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the revoked tokens, that you want to restore, then click on the Restore button from the dropdown menu.</p> <p></p> <p>Step 2: After clicking on the \u201cRestore\u201d button, your secret token will be restored and a confirmation message will display saying User token successfully restored</p> <p></p>"},{"location":"settings/tokens/overview-of-tokens/#delete-token","title":"Delete Token","text":"<p>You can delete a token to permanently remove its access, ensuring it cannot be used again. This is important for maintaining security when a token is no longer needed, has been compromised, or to clean up unused tokens in your system.</p> <p>Note</p> <p>You can only delete revoked tokens, not active tokens. If you want to delete an active token, you must first revoke it before you can delete it.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the revoked tokens, that you want to delete, then click on the Delete button from the dropdown menu.</p> <p></p> <p>After clicking the delete button, a confirmation modal window Delete Token will appear.</p> <p></p> <p>Step 2: Click on the Delete button to delete the token.</p> <p></p> <p>After clicking on the Delete button, your token will be deleted and a confirmation message will display saying User token successfully deleted.</p> <p></p>"},{"location":"source-datastore/assign-tags/","title":"Assign Tag","text":"<p>Assigning tags to your Datastore helps you to identify and categorize your datastore easily. Tags serve as labels that categorize and identify various data sets, enhancing efficiency and organization. By highlighting checks and anomalies, tags make it easier to monitor data quality. They also allow you to list file patterns and assign quality scores, enabling quick identification and resolution of issues.</p> <p>In this documentation, we will explore the steps to assign a tag to the datastore.</p> <p>Step 1: Login in to your Qualytics account and select the datastore from the left menu on which you want to assign a tag.</p> <p></p> <p>Step 2: Click on Assign Tag to this Datastore located at the bottom-left corner of the interface.</p> <p></p> <p>Step 3: A drop-up menu will appear, providing you with a list of tags. Assign an appropriate tag to your datastore to simplify sorting, accessing, and managing data.</p> <p></p> <p>You can also create a new tag by clicking on the call to action (\u2795) button.</p> <p></p> <p>A modal window will appear, providing the options to create the tag. Enter the required values to get started.</p> <p></p> <p>For more information on creating tags, refer to the Add Tag section.</p> <p>Step 4: Once you have assigned a tag, the tag will be instantly labeled on your source Datastore, and all related records will be updated.</p> <p>For demonstration, we have assigned the High tag for the Snowflake source datastore Covid-19 Data, so it will automatically be applied to all related tables and checks within the datastore.</p> <p></p>"},{"location":"source-datastore/catalog/","title":"Catalog Operation","text":"<p>A Catalog Operation imports named data collections like tables, views, and files into a Source Datastore. It identifies incremental fields for incremental scans, and offers options to recreate or delete these containers, streamlining data management and enhancing data discovery.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"source-datastore/catalog/#key-components","title":"Key Components","text":""},{"location":"source-datastore/catalog/#incremental-identifier","title":"Incremental Identifier","text":"<p>An incremental identifier is essential for supporting incremental scan operations, as it allows the system to detect changes since the last operation.</p>"},{"location":"source-datastore/catalog/#partition-identifier","title":"Partition Identifier","text":"<p>For large data containers or partitions, a partition identifier is necessary to process data efficiently. In DFS datastores, the default fields for both incremental and partition identifiers are set to the last-modified timestamp. If a partition identifier is missing, the system uses repeatable ordering candidates (order-by fields) to process containers, although this method is less efficient for handling large datasets with many rows.</p> <p>Info</p> <p>Attribute Overrides: After the profile operation, the qualytics engine might automatically update the containers to have partition fields and incremental fields. Those \"attributes\" can be manually overridden.</p> <p>Note</p> <p>Advanced users should be able to override these auto-detected selections and overridden options should persist through subsequent Catalog Operations.</p>"},{"location":"source-datastore/catalog/#initialization-operation-options","title":"Initialization &amp; Operation Options","text":""},{"location":"source-datastore/catalog/#automatic-catalog-operation","title":"Automatic Catalog Operation","text":"<p>While adding the datastore, tick the Initiate Cataloging checkbox to automatically perform a catalog operation on the configured source datastore.</p> <p></p> <p>With the automatic cataloging option turned on, you will be redirected to the datastore details page once the datastore (whether JDBC or DFS) is successfully added. You will observe the cataloging operation running automatically with the following default options:</p> <ul> <li> <p>Prune: Disabled \u274c  </p> </li> <li> <p>Recreate: Disabled \u274c  </p> </li> <li> <p>Include: Tables and views \u2714\ufe0f</p> </li> </ul> <p></p>"},{"location":"source-datastore/catalog/#manual-catalog-operation","title":"Manual Catalog Operation","text":"<p>If automatic cataloging is disabled while adding the datastore, users can initiate the catalog operation manually by selecting preferred options. Manual catalog operation offers users the flexibility to set up custom catalog configurations like syncing only tables or views.</p> <p>Step 1: Select a source datastore from the side menu on which you would like to perform the catalog operation.</p> <p></p> <p>Step 2: Clicking on your preferred datastore will navigate you to the datastore details page. Within the overview tab (default view), click on the Run button under Catalog to initiate the catalog operation.</p> <p></p> <p>A modal window will display Operation Triggered and you will be notified once the catalog operation is completed.  </p> <p>Note</p> <p>You will receive a notification when the catalog operation is completed.</p> <p></p> <p>Step 3: Close the Success modal window and you will observe in the UI that the Catalog operation has been completed and it has gathered the data structures, file patterns, and corresponding metadata from your configured datastore.  </p> <p></p> <p>Users might encounter a error if the schema of the datastore is empty or if the specified user for logging does not have the necessary permissions to read the objects. This ensures that proper access controls are in place and that the data structure is correctly defined.</p> <p></p>"},{"location":"source-datastore/catalog/#custom-catalog-configuration","title":"Custom Catalog Configuration","text":"<p>The catalog operation can be custom-configured with the following options:  </p> <ul> <li> <p>Prune: Remove any existing named collections that no longer appear in the datastore</p> </li> <li> <p>Recreate: Restore any previously removed named collection that does currently appear in the database</p> </li> <li> <p>Include: Include Tables and Views</p> </li> </ul> <p>Step 1: Click on the Run button from the datastore details page (top-right corner) and select Catalog from the dropdown list.</p> <p></p> <p>Step 2: When configuring the catalog operation settings, you have two options to tune:</p> <ul> <li> <p>Prune: This option allows the removal of any named collections (tables, views, files, etc.) that no longer exist in the datastore. This ensures that outdated or obsolete collections are not included in future operations, keeping the datastore clean and relevant.</p> </li> <li> <p>Recreate: This option enables the recreation of any named collections that have been previously deleted in Qualytics. It is useful for restoring collections that may have been removed accidentally or need to be brought back for analysis.</p> </li> </ul> <p></p> <p>Step 3: The user can choose whether to include only tables, only views, or both in the catalog operation. This flexibility allows for more targeted metadata analysis based on the specific needs of the data management task.</p> <p></p>"},{"location":"source-datastore/catalog/#run-instantly","title":"Run Instantly","text":"<p>Click on the \u201cRun Now\u201d button to perform the catalog operation immediately.</p> <p></p> <p>After clicking Run Now, a confirmation message appears stating \"Operation Triggered\".</p> <p></p>"},{"location":"source-datastore/catalog/#schedule","title":"Schedule","text":"<p>Step 1: Click on the \u201cSchedule\u201d button to configure the available schedule options in the catalog operation.</p> <p></p> <p>Step 2: Set the scheduling preferences for the catalog operation.</p> <p>1. Hourly: This option allows you to schedule the catalog operation to run every hour at a specified minute. You can define the frequency in hours and the exact minute within the hour the cataloging should start. Example: If set to \"Every 1 hour(s) on minute 0,\" the catalog operation will run every hour at the top of the hour (e.g., 1:00, 2:00, 3:00).</p> <p></p> <p>2. Daily: This option schedules the catalog operation to run once every day at a specific time. You specify the number of days between scans and the exact time of day in UTC. Example: If set to \"Every 1 day(s) at 00:00 UTC,\" the scan will run every day at midnight UTC.</p> <p></p> <p>3. Weekly: This option schedules the catalog operation to run on specific days of the week at a set time. You select the days of the week and the exact time of day in UTC for the catalog operation to run. Example: If configured to run on \"Sunday\" and \"Friday\" at 00:00 UTC, the scan will execute at midnight UTC on these days.</p> <p></p> <p>4. Monthly: This option schedules the catalog operation to run once a month on a specific day at a set time. You specify the day of the month and the time of day in UTC. If set to \"On the 1st day of every 1 month(s), at 00:00 UTC,\" the catalog operation will run on the first day of each month at midnight UTC.</p> <p></p> <p>5. Advanced: The advanced section for scheduling operations allows users to set up more complex and custom scheduling using Cron expressions. This option is particularly useful for defining specific times and intervals for catalog operations with precision.</p> <p>Cron expressions are a powerful and flexible way to schedule tasks. They use a syntax that specifies the exact timing of the task based on five fields:</p> <ul> <li>Minute (0 - 59)  </li> <li>Hour (0 - 23)  </li> <li>Day of the month (1 - 31)  </li> <li>Month (1 - 12)  </li> <li>Day of the week (0 - 6) (Sunday to Saturday)</li> </ul> <p>Each field can be defined using specific values, ranges, or special characters to create the desired schedule.</p> <p>Example: For instance, the Cron expression <code>0 0 * * *</code> schedules the catalog operation to run at midnight (00:00) every day. Here\u2019s a breakdown of this expression:</p> <ul> <li>0 (Minute) - The task will run at the 0th minute.  </li> <li>0 (Hour) - The task will run at the 0th hour (midnight).  </li> <li>*(Day of the month) - The task will run every day of the month.  </li> <li>*(Month) - The task will run every month.  </li> <li>*(Day of the week) - The task will run every day of the week.</li> </ul> <p>Users can define other specific schedules by adjusting the Cron expression. For example:</p> <ul> <li>0 12 * * 1-5 - Runs at 12:00 PM from Monday to Friday.  </li> <li>30 14 1 * * - Runs at 2:30 PM on the first day of every month.  </li> <li>0 22 * * 6 - Runs at 10:00 PM every Saturday.</li> </ul> <p>To define a custom schedule, enter the appropriate Cron expression in the \"Custom Cron Schedule (UTC)\" field before specifying the schedule name. This will allow for precise control over the timing of the catalog operation, ensuring it runs exactly when needed according to your specific requirements.</p> <p></p> <p>Step 3: Define the \u201cSchedule Name\u201d to identify the scheduled operation at run time.</p> <p></p> <p>Step 4: Click on the \u201cSchedule\u201d button to activate your catalog operation schedule.</p> <p></p> <p>After clicking Schedule, a confirmation message appears stating \"Operation Scheduled\".</p> <p></p> <p>Once the catalog operation is triggered, your view will be automatically switched to the Activity tab, allowing you to explore post-operation details on your ongoing/completed catalog operation.</p> <p></p>"},{"location":"source-datastore/catalog/#operations-insights","title":"Operations Insights","text":"<p>When the catalog operation is completed, you will receive a notification and can navigate to the Activity tab for the datastore on which you triggered the Catalog Operation and learn about the operation results.</p>"},{"location":"source-datastore/catalog/#top-panel","title":"Top Panel","text":"<p>1. Runs (Default View): Provides insights into the operations that have been performed.</p> <p>2. Search: Search any operation (including catalog) by entering the operation ID</p> <p>3. Sort by: Organize the list of operations based on the Created Date or the Duration.</p> <p>4. Filter: Narrow down the list of operations based on:</p> <ul> <li> <p>Operation Type</p> </li> <li> <p>Operation Status</p> </li> <li> <p>Table</p> </li> </ul> <p></p>"},{"location":"source-datastore/catalog/#activity-heatmap","title":"Activity Heatmap","text":"<p>The activity heatmap shown in the snippet below represents activity levels over a period, with each square indicating a day and the color intensity representing the number of operations or activities on that day. It is useful for tracking the number of operations performed on each day within a specific timeframe.  </p> <p>Tip</p> <p>You can click on any of the squares from the Activity Heatmap to filter operations</p> <p></p>"},{"location":"source-datastore/catalog/#operation-detail","title":"Operation Detail","text":""},{"location":"source-datastore/catalog/#running","title":"Running","text":"<p>This status indicates that the catalog operation is still running at the moment and is yet to be completed. A catalog operation having a running status reflects the following details and actions:</p> Parameter Interpretation Operation ID Unique identifier Operation Type Type of operation performed (catalog, profile, or scan) Timestamp Timestamp when the operation was started Progress Bar The progress of the operation Triggered By The author who triggered the operation Schedule Whether the operation was scheduled or not Prune Indicates whether Prune was enabled or disabled in the operation Recreate Indicates whether Recreate was enabled or disabled in the operation Table Indicates whether the Table was included in the operation or not Views Indicates whether the Views was included in the operation or not Abort Click on the Abort button to stop the catalog operation <p></p>"},{"location":"source-datastore/catalog/#aborted","title":"Aborted","text":"<p>This status indicates that the catalog operation was manually stopped before it could be completed. A catalog operation having an aborted status reflects the following details and actions:</p> Parameter Interpretation Operation ID Unique identifier Operation Type Type of operation performed (catalog, profile, or scan) Timestamp Timestamp when the operation was started Progress Bar The progress of the  operation Triggered By The author who triggered the operation Schedule Whether the operation was scheduled or not Prune Indicates whether Prune was enabled or disabled in the operation Recreate Indicates whether Recreate was enabled or disabled in the operation Table Indicates whether the Table was included in the operation or not Views Indicates whether the Views was included in the operation or not Resume Click on the Resume button to continue a previously aborted catalog operation from where it left off Rerun Click on the Rerun button to initiate the catalog operation from the beginning, ignoring any previous attempts Delete Click on the Delete button to remove the record of the catalog operation from the list <p></p>"},{"location":"source-datastore/catalog/#warning","title":"Warning","text":"<p>This status signals that the catalog operation encountered some issues and displays the logs that facilitate improved tracking of blockers and issue resolution. A catalog operation having a warning status reflects the following details and actions:</p> Parameter Interpretation Operation ID Unique identifier Operation Type Type of operation performed (catalog, profile, or scan) Timestamp Timestamp when the operation was started Progress Bar The progress of the operation Triggered By The author who triggered the operation Schedule Whether the operation was scheduled or not Prune Indicates whether Prune was enabled or disabled in the operation Recreate Indicates whether Recreate was enabled or disabled in the operation Table Indicates whether the Table was included in the operation or not Views Indicates whether the Views was included in the operation or not Rerun Click on the Rerun button to initiate the catalog operation from the beginning, ignoring any previous attempts Delete Click on the Delete button to remove the record of the catalog operation from the list Logs Logs include error messages, warnings, and other pertinent information generated during the execution of the Catalog Operation <p></p>"},{"location":"source-datastore/catalog/#success","title":"Success","text":"<p>This status confirms that the catalog operation was completed successfully without any issues. A catalog operation having a success status reflects the following details and actions:</p> Parameter Interpretation Operation ID Unique identifier Operation Type Type of operation performed (catalog, profile, or scan) Timestamp Timestamp when the operation was started Progress Bar The progress of the operation Triggered By The author who triggered the operation Schedule Whether the operation was scheduled or not Prune Indicates whether Prune was enabled or disabled in the operation Recreate Indicates whether Recreate was enabled or disabled in the operation Table Indicates whether the Table was included in the operation or not Views Indicates whether the Views was included in the operation or not Rerun Click on the Rerun button to initiate the catalog operation from the beginning, ignoring any previous attempts Delete Click on the Delete button to remove the record of the catalog operation from the list <p></p>"},{"location":"source-datastore/catalog/#post-operation-details","title":"Post-Operation Details","text":""},{"location":"source-datastore/catalog/#for-jdbc-source-datastores","title":"For JDBC Source Datastores","text":"<p>After the catalog operation is completed on a JDBC source datastore, users can view the following information:</p> <p>Container Names: These are the names of the data collections (e.g., tables, views) identified during the catalog operation.</p> <p></p> <p>Fields for Each Container: Each container will display its fields or columns, which were detected during the catalog operation.</p> <p></p> <p>Incremental Identifiers and Partition Fields: These settings are automatically configured based on the catalog operation. Incremental identifiers help in recognizing changes since the last scan, and partition fields aid in efficient data processing.</p> <p>Tree view &gt; Container node &gt; Gear icon &gt; Settings option</p> <p></p>"},{"location":"source-datastore/catalog/#for-dfs-source-datastores","title":"For DFS Source Datastores","text":"<p>After the catalog operation is completed on a DFS source datastore, users can view the following information:</p> <ul> <li> <p>Container Names: Similar to JDBC, these are the data collections identified during the catalog operation.</p> </li> <li> <p>Fields for Each Container: Each container will list its fields or metadata detected during the catalog operation.</p> </li> <li> <p>Directory Tree Traversal: The catalog operation traverses the directory tree, treating each file with a supported extension as a single-partition container. It reveals metadata such as the relative path, filename, and extension.</p> </li> <li> <p>Incremental Identifier and Partition Field: By default, both the incremental identifier and partition field are set to the last-modified timestamp. This ensures efficient incremental scans and data partitioning.</p> </li> <li> <p>\"Globbed\" Containers: Files in the same folder with the same extensions and similar naming formats are grouped into a single container, where each file is treated as a partition. This helps in managing and querying large datasets effectively.</p> </li> </ul>"},{"location":"source-datastore/catalog/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides API payload examples for initiating and checking the running status of a catalog operation. Replace the placeholder values with data specific to your setup.</p>"},{"location":"source-datastore/catalog/#running-a-catalog-operation","title":"Running a Catalog operation","text":"<p>To run a catalog operation, use the API payload example below and replace the placeholder values with your specific values:</p> <p>Endpoint (Post):  <code>/api/operations/run (post)</code></p> <pre><code>{\n  \"type\": \"catalog\",\n  \"datastore_id\": \"datastore-id\",\n  \"prune\": false,\n  \"recreate\": false,\n  \"include\": [\n    \"table\",\n    \"view\"\n  ]\n}\n</code></pre>"},{"location":"source-datastore/catalog/#retrieving-catalog-operation-status","title":"Retrieving Catalog Operation Status","text":"<p>To retrieve the catalog operation status, use the API payload example below and replace the placeholder values with your specific values:</p> <p>Endpoint (Get):  <code>/api/operations/{id} (get)</code></p> <pre><code>{\n  \"items\": [\n    {\n      \"id\": 12345,\n      \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n      \"type\": \"catalog\",\n      \"start_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n      \"end_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n      \"result\": \"success\",\n      \"message\": null,\n      \"triggered_by\": \"user@example.com\",\n      \"datastore\": {\n        \"id\": 54321,\n        \"name\": \"Datastore-Sample\",\n        \"store_type\": \"jdbc\",\n        \"type\": \"db_type\",\n        \"enrich_only\": false,\n        \"enrich_container_prefix\": \"_data_prefix\",\n        \"favorite\": false\n      },\n      \"schedule\": null,\n      \"include\": [\n        \"table\",\n        \"view\"\n      ],\n      \"prune\": false,\n      \"recreate\": false\n    }\n  ],\n  \"total\": 1,\n  \"page\": 1,\n  \"size\": 50,\n  \"pages\": 1\n}\n</code></pre>"},{"location":"source-datastore/delete-datastore/","title":"Delete Datastore","text":"<p>Step 1: Click on the Delete option in the settings icon.</p> <p></p> <p>Step 2: A modal window titled Delete Datastore will appear.</p> <p></p> <p>Step 3: Enter the name of the datastore in the given field (confirmation check) and then click on the I\u2019M SURE, DELETE THIS DATASTORE button to delete the datastore.</p> <p></p> <p>After clicking the I\u2019M SURE, DELETE THIS DATASTORE button, a success notification appears confirming the deletion.</p>"},{"location":"source-datastore/edit-datastore/","title":"Edit Datastore","text":"<p>Step 1: Click on the Edit option.</p> <p></p> <p>Step 2: After selecting the Edit option, a modal window will appear, displaying the connection details. This window allows you to modify any specific connection details.</p> <p></p> <p>Step 3: After editing the connection details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided connection details are verified, a success message will be displayed indicating that the connection has been verified.</p> <p>Step 4: Click on the Save button.</p> <p></p> <p>After clicking on the Save button, a success notification appears on the screen showing the action was completed successfully.</p>"},{"location":"source-datastore/external-scan/","title":"External Scan Operation","text":"<p>An external scan is ideal for ad hoc scenarios, where you may receive a file intended to be replicated to a source datastore. Before loading, you can perform an external scan to ensure the file aligns with existing data standards. The schema of the file must match the target table or file pattern that has already been profiled within Qualytics, allowing you to reuse the quality checks to identify any issues before data integration.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"source-datastore/external-scan/#navigation-to-external-scan-operation","title":"Navigation to External Scan Operation","text":"<p>Step 1: Select a source datastore from the side menu to perform the external scan operation.</p> <p></p> <p>Step 2: After selecting your preferred source datastore, you will be taken to the details page. From there, click on \"Tables\" and select the table you want to perform the external scan operation on.</p> <p>Note</p> <p>This example is based on a JDBC table, but the same steps apply to a DFS as well. For DFS source datastores, you will need to click on \"File Patterns\" and select a File Pattern to run the external scan.</p> <p></p> <p>For demonstration purposes, we have selected the \u201cCUSTOMER\u201d table.</p> <p></p>"},{"location":"source-datastore/external-scan/#external-scan-configuration","title":"External Scan Configuration","text":"<p>Step 1: Click on the \u201cRun\u201d button and select the \u201cExternal Scan\u201d option. </p> <p></p> <p>Step 2: After selecting the \"External Scan\" option, a modal window will appear with an input for uploading your external file. After uploading the file, click the \u201cRun\u201d button to start the operation.</p> <p></p> <p>Note</p> <p>An External Scan operation supports the following file formats: CSV, XLSX, and XLS.</p> <p>Step 3: After clicking the \"Run\" button, the external scan operation will begin, and you will receive a confirmation message if the operation is successfully triggered.</p> <p></p>"},{"location":"source-datastore/external-scan/#supported-file-formats","title":"Supported File Formats","text":"<p>External scan operation accepts CSV, XLSX, and XLS files. CSV is a simple text format, while XLSX and XLS are Excel formats that support more complex data structures. This versatility enables seamless integration of data from various sources.</p> <p>An External Scan Operation can be configured with the following file formats:</p> File Extension .csv .xls .xlsx File Format Comma-separated values Microsoft Excel 97-2003 Workbook Microsoft Excel 2007+ Workbook Header Row Required for optimal reading. It should contain column names. Recommended, but not strictly required. Recommended, but not strictly required. Empty Cells Represented as empty strings. Allowed. Allowed. Data Types Typically inferred by Spark. May require explicit specification for complex types. May require explicit specification for complex types. Nested Data Not directly supported. Consider flattening or using alternative file formats. Not directly supported. Consider flattening or using alternative file formats. Not directly supported. Consider flattening or using alternative file formats. Additional Considerations - Ensure consistent delimiter usage (usually commas). - Avoid special characters or line breaks within fields. - Enclose text fields containing commas or delimiters in double quotes. - Use a plain XLS format without macros or formatting. - Consider converting to CSV for simpler handling. - Use a plain XLSX format without macros or formatting. - Consider converting to CSV for simpler handling."},{"location":"source-datastore/external-scan/#scenario","title":"Scenario","text":"<p>A company maintains a large sales database containing information about various transactions, customers, and products. They have received a new sales data file that will be integrated into the existing database. Before loading the data, the organization wants to ensure there are no issues with the file.</p> <p>An External Scan is initiated to perform checks on the incoming file, validating that it aligns with the quality standards of the sales table.</p>"},{"location":"source-datastore/external-scan/#specific-checks","title":"Specific Checks:","text":"Check Description <code>Expected Schema</code> Verify that all columns have the same data type as the selected profile structure. <code>Exists in</code> Verify that all transactions have valid customer and product references. <code>Between Times</code> Ensure that transaction dates fall within an expected range. <code>Satisfies Expression</code> Validate that the calculated revenue aligns with the unit price and quantity sold. The formula is: <code>R = Quantity \u00d7 Unit Price</code>"},{"location":"source-datastore/external-scan/#potential-anomalies","title":"Potential Anomalies:","text":"<p>This overview highlights common issues such as data type mismatches, missing references, out-of-range dates, and inconsistent revenue calculations. Each anomaly affects data integrity and requires corrective action.</p> Anomaly Description Data type issue The external resource does not follow the data type schema. Missing References Transactions without valid customer or product references. Out-of-Range Dates Transactions with dates outside the expected range. Inconsistent Revenue Mismatch between calculated revenue and unit price times quantity."},{"location":"source-datastore/external-scan/#benefits-of-external-scan","title":"Benefits of External Scan:","text":"Benefit Description Quality Assurance Identify and rectify data inconsistencies before downstream processes. Data Integrity Ensure that all records adhere to defined schema and constraints. Anomaly Detection Uncover potential issues that might impact business analytics and reporting."},{"location":"source-datastore/external-scan/#csv-table-sales-data","title":"CSV Table (Sales Data):","text":"<p>This dataset includes transaction records with details such as Transaction_ID, Customer_ID, Product_ID, Transaction_Date, Quantity, and Unit_Price. It provides essential information for tracking and analyzing sales activities.</p> Transaction_ID Customer_ID Product_ID Transaction_Date Quantity Unit_Price 1 101 201 2023-01-15 5 20.00 2 102 202 2023-02-20 3 15.50 3 103 201 2023-03-10 2 25.00 4 104 203 2023-04-05 1 30.00 ... ... ... ... ... ... Flowchart <pre><code>graph TB\nsubgraph Init\n  A[Start] --&gt; B[Load Sales Data]\nend\n\nsubgraph Checks\n  B --&gt; C1[Expected schema]\n  B --&gt; C2[Exists in]\n  B --&gt; C3[Between times]\n  B --&gt; C4[Satisfies expression]\n\n  C1 --&gt;|Invalid| E1[Expected schema anomaly]\n  C2 --&gt;|Invalid| E2[Exists in anomaly]\n  C3 --&gt;|Invalid| E3[Between times anomaly]\n  C4 --&gt;|Invalid| E4[Satisfies expression anomaly]\nend\n\nsubgraph End\n\n  E1 --&gt; J[Finish]\n  E2 --&gt; J[Finish]\n  E3 --&gt; J[Finish]\n  E4 --&gt; J[Finish]\nend\n</code></pre>"},{"location":"source-datastore/external-scan/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"source-datastore/external-scan/#running-an-external-scan-operation","title":"Running an External Scan operation","text":"<p>This section provides a sample payload for running an external scan operation. Replace the placeholder values with actual data relevant to your setup.</p>"},{"location":"source-datastore/external-scan/#endpoint-post","title":"Endpoint (Post)","text":"<p><code>/api/containers/{container-id}/scan</code> (post)</p> Running an external scan operation of a datastore <pre><code>{\n    \"name\":\"file_name.csv\",\n    \"records\": [{\\\"COLUMN_1\\\":\\\"VALUE 1\\\",\\\"COLUMN_2\\\":\\\"VALUE 1\\\"},{\\\"COLUMN_1\\\":\\\"VALUE_2\\\",\\\"COLUMN_2\\\":\\\"VALUE 2\\\"}]\n}\n</code></pre>"},{"location":"source-datastore/external-scan/#retrieving-an-external-scan-operation-status","title":"Retrieving an External Scan Operation Status","text":""},{"location":"source-datastore/external-scan/#endpoint-get","title":"Endpoint (Get)","text":"<p><code>/api/operations/{id}</code> (get)</p> Example result response <pre><code>{\n    \"items\": [\n        {\n            \"id\": 12345,\n            \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n            \"type\": \"external_scan\",\n            \"start_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n            \"end_time\": null,\n            \"result\": \"running\",\n            \"message\": null,\n            \"triggered_by\": \"user@example.com\",\n            \"datastore\": {\n                \"id\": 101,\n                \"name\": \"Datastore-Sample\",\n                \"store_type\": \"jdbc\",\n                \"type\": \"db_type\",\n                \"enrich_only\": false,\n                \"enrich_container_prefix\": \"data_prefix\",\n                \"favorite\": false\n            },\n            \"schedule\": null,\n            \"incremental\": false,\n            \"remediation\": \"none\",\n            \"max_records_analyzed_per_partition\": -1,\n            \"greater_than_time\": null,\n            \"greater_than_batch\": null,\n            \"high_count_rollup_threshold\": 10,\n            \"enrichment_source_record_limit\": 10,\n            \"status\": {\n                \"total_containers\": 1,\n                \"containers_analyzed\": 0,\n                \"partitions_scanned\": 0,\n                \"records_processed\": 0,\n                \"anomalies_identified\": 0\n            },\n            \"containers\": [\n                    {\n                    \"id\": 234,\n                    \"name\": \"Container1\",\n                    \"container_type\": \"table\",\n                    \"table_type\": \"table\"\n                    }\n                ],\n                \"container_scans\": [\n                    {\n                    \"id\": 456,\n                    \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                    \"container\": {\n                        \"id\": 234,\n                        \"name\": \"Container1\",\n                        \"container_type\": \"table\",\n                        \"table_type\": \"table\"\n                    },\n                    \"start_time\": null,\n                    \"end_time\": null,\n                    \"records_processed\": 0,\n                    \"anomaly_count\": 0,\n                    \"result\": \"running\",\n                    \"message\": null\n                    }\n                ],\n                \"tags\": []\n        }\n    ],\n    \"total\": 1,\n    \"page\": 1,\n    \"size\": 50,\n    \"pages\": 1\n}\n</code></pre>"},{"location":"source-datastore/link-enrichment/","title":"Link Enrichment","text":"<p>Step 1: Click on the Enrichment from the dropdown list.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear, providing you with two options to link an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list. 4. Anomaly Rollup Threshold Sets the maximum number of anomalies per check before they are merged into one anomaly. Value must be between 1 and 1,000. 5. Source Record Limit Sets the maximum number of records written to the enrichment datastore for each detected anomaly. The value must be between 1 and 1,000,000,000. 6. Remediation Strategy The Remediation Strategy defines how anomalous source tables are replicated in the enrichment datastore. You can choose None (no replication), Append (append new data), or Overwrite (replace existing data)."},{"location":"source-datastore/link-enrichment/#option-i-link-new-enrichment","title":"Option I: Link New Enrichment","text":"<p>If the toggle for Add new connection is turned on, then this will prompt you to link a new enrichment datastore from scratch without using existing connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for adding new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. 5. Anomaly Rollup Threshold Sets the maximum number of anomalies per check before they are merged into one anomaly. Value must be between 1 and 1,000. 6. Source Record Limit Sets the maximum number of records written to the enrichment datastore for each detected anomaly. The value must be between 1 and 1,000,000,000. 7. Remediation Strategy The Remediation Strategy defines how anomalous source tables are replicated in the enrichment datastore. You can choose None (no replication), Append (append new data), or Overwrite (replace existing data). <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p>Note</p> <p>Connection details can vary from datastore to datastore. For illustration, we have demonstrated linking BigQuery as a new enrichment datastore.</p> <p></p> <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p> <p>Step 4: Click on the Save button.</p> <p></p> <p>After clicking on the Save button, a success notification appears on the screen showing the action was completed successfully.</p>"},{"location":"source-datastore/link-enrichment/#option-ii-link-existing-connection","title":"Option II: Link Existing Connection","text":"<p>If the Use an existing enrichment datastore option is selected from the dropdown menu, you will be prompted to link the enrichment datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. 3. Anomaly Rollup Threshold Sets the maximum number of anomalies per check before they are merged into one anomaly. Value must be between 1 and 1,000. 4. Source Record Limit Sets the maximum number of records written to the enrichment datastore for each detected anomaly. The value must be between 1 and 1,000,000,000. 5. Remediation Strategy The Remediation Strategy defines how anomalous source tables are replicated in the enrichment datastore. You can choose None (no replication), Append (append new data), or Overwrite (replace existing data). <p>Step 3: View and check the connection details of the enrichment datastore and click on the Save button.</p> <p></p> <p>After clicking on the Save button, a success notification appears on the screen showing the action was completed successfully.</p>"},{"location":"source-datastore/link-enrichment/#endpoint-patch","title":"Endpoint (Patch)","text":"<p><code>/api/datastores/{datastore-id}/enrichment/{enrichment-id}</code> (patch)</p>"},{"location":"source-datastore/profile/","title":"Profile Operation","text":"<p>The Profile Operation is a comprehensive analysis conducted on every record within all available containers in a datastore. This process is aimed at understanding and improving data quality by generating metadata for each field within the collections of data (like tables or files).</p> <p>By gathering detailed statistical data and interacting with the Qualytics Inference Engine, the operation not only identifies and evaluates data quality but also suggests and refines checks to ensure ongoing data integrity. Executing profile operations periodically helps maintain up-to-date and accurate data quality checks based on the latest data.</p> <p>This guide explains how to configure the profile operation with available functionalities such as tables, tags, and schedule options.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"source-datastore/profile/#how-profiling-works","title":"How Profiling Works","text":""},{"location":"source-datastore/profile/#fields-identification","title":"Fields Identification","text":"<p>The initial step involves recognizing and identifying all the fields within each data container. This step is crucial as it lays the foundation for subsequent analysis and profiling.</p>"},{"location":"source-datastore/profile/#statistical-data-gathering","title":"Statistical Data Gathering","text":"<p>After identifying the fields, the Profile Operation collects statistical data for each field based on its declared or inferred data type. This data includes essential metrics such as minimum and maximum values, mean, standard deviation, and other relevant statistics. These metrics provide valuable insights into the characteristics and distribution of the data, helping to understand its quality and consistency.</p>"},{"location":"source-datastore/profile/#metadata-generation","title":"Metadata Generation","text":"<p>The gathered statistical data is then submitted to the Qualytics Inference Engine. The engine utilizes this data to generate metadata that forms the basis for creating appropriate data quality checks. This metadata is essential for setting up robust quality control mechanisms within the data management system.</p>"},{"location":"source-datastore/profile/#data-quality-checks","title":"Data Quality Checks","text":"<p>The inferred data quality checks are rigorously tested against the actual source data. This testing phase is critical to fine-tuning the checks to the desired sensitivity levels, ensuring they are neither too strict (causing false positives) nor too lenient (missing errors). By calibrating these checks accurately, the system can maintain high data integrity and reliability.</p>"},{"location":"source-datastore/profile/#navigation-to-profile-operation","title":"Navigation to Profile Operation","text":"<p>Step 1: Select a source datastore from the side menu on which you would like to perform the profile operation.</p> <p></p> <p>Step 2: Clicking on your preferred datastore will navigate you to the datastore details page. Within the overview tab (default view), click on the Run button under Profile to initiate the profile operation.</p> <p></p>"},{"location":"source-datastore/profile/#configuration","title":"Configuration","text":"<p>Step 1: Click on the Run button to initiate the profile operation.</p> <p></p> <p>Note</p> <p>You can run Profile Operation anytime to update the inferred data quality checks, automatically based on new data in the datastore. It is recommended to schedule the profile operations periodically to update inferred rules. More details are discussed in the Schedule section below.</p> <p>Step 2: Select tables (in your JDBC datastore) or file patterns (in your DFS datastore) and tags you would like to be profiled.</p> <p>1. All Tables/File Patterns</p> <p>This option includes all tables or files currently available in the datastore for profiling. Selecting this will profile every table within the source datastore without the need for further selection.</p> <p></p> <p>2. Specific</p> <p>This option allows users to manually select individual tables or files for profiling. It provides the flexibility to focus on particular tables of interest, which can be useful if the user is only interested in a subset of the available data.</p> <p></p> <p>3. Tag</p> <p>This option automatically profiles tables associated with selected tags. Tags are used to categorize tables, and by selecting a specific tag, all tables associated with that tag will be profiled. This option helps in managing and profiling grouped data efficiently.</p> <p></p> <p>Step 3: After making the relevant selections, click on the Next button to configure the Operation Settings.</p> <p></p> <p>Step 4: Configure the following two Read Settings:</p> <ul> <li>Starting Threshold</li> <li>Record Limit</li> </ul>"},{"location":"source-datastore/profile/#starting-threshold","title":"Starting Threshold","text":"<p>This setting allows users to specify a minimum incremental identifier value to set a starting point for the profile operation. It helps in filtering data from a specific point in time or a particular batch value.  </p> <ul> <li> <p>Greater Than Time: Users can select a timestamp in UTC to start profiling data from a specific time onwards. This is useful for focusing on recent data or data changes since a particular time.  </p> </li> <li> <p>Greater Than Batch: Users can enter a batch value to start profiling from a specific batch. This option is helpful for scenarios where data is processed in batches, allowing the user to profile data from a specific batch number onwards.</p> </li> </ul> <p>Note</p> <p>The starting threshold i.e. Greater Than Time and Greater Than Batch are applicable only to the tables or files with an incremental timestamp strategy.</p> <p></p>"},{"location":"source-datastore/profile/#record-limit","title":"Record Limit","text":"<p>Define the number of records to be profiled per table: This feature allows users to manually enter a custom record limit value using a text field in the profile operation. This setting helps in controlling the scope of the profiling operation, particularly for large datasets, by capping the number of records to analyze.</p> <p></p> <p>You can also use a drop-down menu to quickly select from commonly used limits such as 1M, 10M, 100M, and All.</p> <p></p> <p>Note</p> <p>The number of records must be between 1 and 1,000,000,000.</p> <p>Step 5: After making the relevant selections, click on the Next button to configure the Inference Settings. </p> <p></p> <p>Step 6: Configure the following two Inference Settings: </p> <ul> <li>Inference Threshold</li> <li>Inference State</li> </ul>"},{"location":"source-datastore/profile/#inference-threshold","title":"Inference Threshold","text":"<p>The Inference Threshold allows you to customize the data quality checks that are automatically created and updated when your data is analyzed. This means you can adjust the data quality checks based on how complex the data rules are, giving you more control over how your data is checked and monitored.</p> <p>Default Configuration</p> <p>By default, the Inference Threshold is set to 2, which provides a comprehensive range of checks designed to ensure data integrity across different scenarios. Users have the flexibility to adjust this threshold based on their specific needs, allowing for either basic or advanced checks as required.</p> <p></p>"},{"location":"source-datastore/profile/#levels-of-check-inference","title":"Levels of Check Inference","text":"<p>The Inference Threshold ranges from 0 to 5, with each level including progressively more complex and comprehensive checks. Below is an explanation of each level:</p> <p>Note</p> <p>Each level includes all the checks from the previous levels and adds new checks specific to that level. For example, at Level 1, there are five basic checks. At Level 2, you get those five checks plus additional ones for Level 2. By the time you reach Level 5, it covers all the checks from Levels 1 to 4 and adds its own new checks for complete review. </p> <p>Level 0: No Inference</p> <p>At this level, no checks are automatically inferred. This is suitable when users want complete control over which checks are applied, or if no checks are needed. Ideal for scenarios where profiling should not infer any constraints, and all checks will be manually defined.</p> <p></p> <p>Level 1: Basic Data Integrity and Simple Value Threshold Checks</p> <p>This level includes fundamental rules for basic data integrity and simple validations. It ensures that basic constraints like completeness, non-negative numbers, and valid date ranges are applied. Included Checks:</p> <ul> <li> <p>Completeness Checks: Ensure data fields are complete if previously complete.</p> </li> <li> <p>Categorical Range Checks: Validate if values fall within a predefined set of categories. </p> </li> <li> <p>Non-Negative Numbers: Ensure numeric values are non-negative.  </p> </li> <li> <p>Non-Future Date/Time: Ensure datetime values are not set in the future.</p> </li> </ul> <p>Use Case: Suitable for datasets where basic integrity checks are sufficient.</p> <p>The following table shows the inferred checks that the Analytics Engine can generate based on the user's data. At Level 1, five checks are created.</p> Inferred Checks Reference Not Null (record) See more. Any Not Null (record) See more. Expected Values (record) See more. Not Negative See more. Not Future See more. <p></p> <p>Level 2: Value Range and Pattern Checks</p> <p>Builds upon Level 1 by adding more specific checks related to value ranges and patterns. This level is more detailed and begins to enforce rules related to the nature of the data itself. Included Checks:</p> <ul> <li> <p>Date Range Checks: Ensure dates fall within a specified range. </p> </li> <li> <p>Numeric Range Checks: Validate that numeric values are within acceptable ranges.  </p> </li> <li> <p>String Pattern Checks: Ensure strings match specific patterns (e.g., email formats).  </p> </li> <li> <p>Approximate Uniqueness: Validate uniqueness of values if they are approximately unique.</p> </li> </ul> <p>Use Case: Ideal for datasets where patterns and ranges are important for ensuring data quality.</p> <p>The following table shows the inferred checks that the Analytics Engine can generate based on the user's data. At Level 2, four checks are created.</p> Checks Reference Between Times See more. Between See more. Matches Pattern See more. Unique See more. <p></p> <p>Level 3: Time Series and Comparative Relationship Checks</p> <p>This level includes all checks from Level 2 and adds sophisticated checks for time series and comparative relationships between datasets. Included Checks:</p> <ul> <li> <p>Date Granularity Checks: Ensure the granularity of date values is consistent (e.g., day, month, year).</p> </li> <li> <p>Consistent Relationships: Validate that relationships between overlapping datasets are consistent.</p> </li> </ul> <p>Use Case: Suitable for scenarios where data quality depends on time-series data or when comparing data across different datasets.</p> <p>The following table shows the inferred checks that the Analytics Engine can generate based on the user's data. At Level 3, eight checks are created.</p> Inferred checks Reference Time Distribution Size See more. After Date Time See more. Before Date Time See more. Greater Than See more. Greater Than Field See more. Less Than See more. Less Than Field See more. Equal To Field See more. <p></p> <p>Level 4: Linear Regression and Cross-Datastore Relationship Checks</p> <p>This level includes all checks from Level 3 and adds even more advanced checks, including linear regression analysis and validation of relationships across different data stores. Included Checks:</p> <ul> <li> <p>Linear Regression Checks: Validate data using regression models to identify trends and outliers.</p> </li> <li> <p>Cross-Datastore Relationships: Ensure that data relationships are maintained across different data sources.</p> </li> </ul> <p>Use Case: Best for complex datasets where advanced analytical checks are necessary.</p> <p>The following table shows the inferred checks that the Analytics Engine can generate based on the user's data. At Level 4, four checks are created.</p> Inferred Checks Reference Data Diff See more. Exists In See more. Not Exists In See more. Predicted By See more. Is Replica Of (is sunsetting) See more. <p></p> <p>Level 5: Shape Checks</p> <p>The most comprehensive level includes all previous checks plus checks that validate the shape of certain distribution patterns that can be identified in your data. Included Checks:</p> <ul> <li>Shape Checks: Checks that define an expectation for some percentage of your data less than 100%.  The property \u201ccoverage\u201d holds the percentage of your data for which the expressed check should be true.</li> </ul> <p>Use Case: Ideal for scenarios where each incremental set of scanned data should exhibit the same distributions of values as the training set.  For example, a transactions table is configured for a weekly incremental scan after each week\u2019s data is loaded. A shape check could define that 80% of all transactions are expected to be performed using \u201ccash\u201d or \u201ccredit\u201d.</p> <p>This table shows the inferred checks that the Analytics Engine can generate based on the user's data. At Level 5, three checks are created.</p> Inferred Checks Reference Expected Values (Shape) See more. Matches Pattern (Shape) See more. Not Null (Shape) See more. <p></p> <p>Warning</p> <p>If the checks inferred during a profile operation do not detect any anomalies, and the check inference level decreases in the next profile operation, the checks that did not generate anomalies will be archived or discarded. However, if the checks detect any anomalies, they will be retained to continue monitoring the data and addressing potential issues.</p>"},{"location":"source-datastore/profile/#inference-state","title":"Inference State","text":"<p>Check the box labeled \"Infer As Draft\" to ensure that all inferred checks will be generated in a draft state. This allows for greater flexibility as you can review and refine these checks before they are finalized.</p> <p></p>"},{"location":"source-datastore/profile/#run-instantly","title":"Run Instantly","text":"<p>Click on the Run Now button, and perform the profile operation immediately.</p> <p></p>"},{"location":"source-datastore/profile/#schedule","title":"Schedule","text":"<p>Step 1: Click on the Schedule button to configure the available schedule options in the profile operation.</p> <p></p> <p>Step 2: Set the scheduling preferences for the profile operation.</p> <p>1. Hourly: This option allows you to schedule the profile operation to run every hour at a specified minute. You can define the frequency in hours and the exact minute within the hour the profiling should start. Example: If set to \"Every 1 hour(s) on minute 0,\" the profile operation will run every hour at the top of the hour (e.g., 1:00, 2:00, 3:00).</p> <p></p> <p>2. Daily: This option schedules the profile operation to run once every day at a specific time. You specify the number of days between scans and the exact time of day in UTC. Example: If set to \"Every 1 day(s) at 00:00 UTC,\" the scan will run every day at midnight UTC.</p> <p></p> <p>3. Weekly: This option schedules the profile operation to run on specific days of the week at a set time. You select the days of the week and the exact time of day in UTC for the profile operation to run. Example: If configured to run on \"Sunday\" and \"Friday\" at 00:00 UTC, the scan will execute at midnight UTC on these days.</p> <p></p> <p>4. Monthly: This option schedules the profile operation to run once a month on a specific day at a set time. You specify the day of the month and the time of day in UTC. If set to \"On the 1st day of every 1 month(s), at 00:00 UTC,\" the profile operation will run on the first day of each month at midnight UTC.</p> <p></p> <p>5. Advanced: The advanced section for scheduling operations allows users to set up more complex and custom scheduling using Cron expressions. This option is particularly useful for defining specific times and intervals for profile operations with precision.</p> <p>Cron expressions are a powerful and flexible way to schedule tasks. They use a syntax that specifies the exact timing of the task based on five fields:</p> <ul> <li>Minute (0 - 59)</li> <li>Hour (0 - 23)</li> <li>Day of the month (1 - 31)</li> <li>Month (1 - 12)</li> <li>Day of the week (0 - 6) (Sunday to Saturday)</li> </ul> <p>Each field can be defined using specific values, ranges, or special characters to create the desired schedule.</p> <p>Example: For instance, the Cron expression <code>0 0 * * *</code> schedules the profile operation to run at midnight (00:00) every day. Here\u2019s a breakdown of this expression:</p> <ul> <li>0 (Minute) - The task will run at the 0th minute.</li> <li>0 (Hour) - The task will run at the 0th hour (midnight).</li> <li>*(Day of the month) - The task will run every day of the month.</li> <li>*(Month) - The task will run every month.</li> <li>*(Day of the week) - The task will run every day of the week.  </li> </ul> <p>Users can define other specific schedules by adjusting the Cron expression. For example:</p> <ul> <li>0 12 * * 1-5 - Runs at 12:00 PM from Monday to Friday.</li> <li>30 14 1 * * - Runs at 2:30 PM on the first day of every month.</li> <li>0 22 * * 6 - Runs at 10:00 PM every Saturday.</li> </ul> <p>To define a custom schedule, enter the appropriate Cron expression in the Custom Cron Schedule (UTC) field before specifying the schedule name. This will allow for precise control over the timing of the profile operation, ensuring it runs exactly when needed according to your specific requirements.</p> <p></p> <p>Step 3: Define the Schedule Name to identify the scheduled operation at the running time.</p> <p></p> <p>Step 4: Click on the Schedule button to activate your profile operation schedule.</p> <p></p> <p>Note</p> <p>You will receive a notification when the profile operation is completed.</p>"},{"location":"source-datastore/profile/#operation-insights","title":"Operation Insights","text":"<p>When the profile operation is completed, you will receive the notification and can navigate to the Activity tab for the datastore on which you triggered the Profile Operation and learn about the operation results.</p>"},{"location":"source-datastore/profile/#top-panel","title":"Top Panel","text":"<ol> <li> <p>Runs (Default View): Provides insights into the operations that have been performed</p> </li> <li> <p>Schedule: Provides insights into the scheduled operations.</p> </li> <li> <p>Search: Search any operation (including profile) by entering the operation ID</p> </li> <li> <p>Sort by: Organize the list of operations based on the Created Date or the Duration.</p> </li> <li> <p>Filter: Narrow down the list of operations based on:</p> </li> <li> <p>Operation Type</p> </li> <li>Operation Status</li> <li>Table</li> </ol> <p></p>"},{"location":"source-datastore/profile/#activity-heatmap","title":"Activity Heatmap","text":"<p>The activity heatmap shown in the snippet below represents activity levels over a period, with each square indicating a day and the color intensity representing the number of operations or activities on that day. It is useful in tracking the number of operations performed on each day within a specific timeframe.</p> <p>Tip</p> <p>You can click on any of the squares from the Activity Heatmap to filter operations</p> <p></p>"},{"location":"source-datastore/profile/#operation-detail","title":"Operation Detail","text":""},{"location":"source-datastore/profile/#running","title":"Running","text":"<p>This status indicates that the profile operation is still running at the moment and is yet to be completed. A profile operation having a running status reflects the following details and actions:</p> No. Parameter Interpretation 1. Operation ID &amp; Operation Type Unique identifier and type of operation performed (catalog, profile, or scan). 2. Timestamp Timestamp when the operation was started. 3. Progress Bar The progress of the operation. 4. Triggered By The author who triggered the operation. 5. Schedule Whether the operation was scheduled or not. 6. Inference Threshold Indicates how much control you have over automatic data quality checks, adjustable based on complexity. 7. Checks Synchronized Indicates the count of Checks Synchronized in the operation. 8. Infer as Draft Indicates whether Infer as Draft was enabled or disabled in the operation. 9. Read Record Limit Defines the maximum number of records to be scanned per table after initial filtering. 10. Results Provides immediate insights into the profile operation conducted. 11. Abort The \"Abort\" button enables you to stop the ongoing profile operation. 12. Summary The \"Summary\" section provides a real-time overview of the profile operation's progress. It includes key metrics such as: <ul><li> Tables Requested: The total number of tables that were requested for profiling. Click on the adjacent magnifying glass icon to view the tables requested.</li><li> Tables Profiled: The number of tables that have been profiled so far. Click on the adjacent magnifying glass icon to view the tables profiled. </li><li> Records Profiled:  This represents the total number of records that were included in the profiling process. </li><li> Field Profiles Updates:  This number shows how many field profiles were updated as a result of the profiling operation. </li><li> Inferred Checks Synchronized:  This indicates the number of inferred checks that were synchronized based on the profile operation. </li> <li> Added: Shows the count of newly added inferred checks. </li><li> Updated: Indicates the count of checks that were updated in the operation.</li> <p></p>"},{"location":"source-datastore/profile/#aborted","title":"Aborted","text":"<p>This status indicates that the profile operation was manually stopped before it could be completed. A profile operation having an aborted status reflects the following details and actions:</p> No. Parameter Interpretation 1. Operation ID &amp; Operation Type Unique identifier and type of operation performed (catalog, profile, or scan). 2. Timestamp Timestamp when the operation was started. 3. Progress Bar The progress of the operation. 4. Aborted By The author who Aborted the operation. 5. Schedule Whether the operation was scheduled or not. 6. Inference Threshold Indicates how much control you have over automatic data quality checks, adjustable based on complexity. 7. Checks Synchronized Indicates the count of Checks Synchronized in the operation. 8. Infer as Draft Indicates whether Infer as Draft was enabled or disabled in the operation. 9. Read Record Limit Defines the maximum number of records to be scanned per table after initial filtering. 10. Results Provides immediate insights into the profile operation conducted. 11. Resume Provides an option to continue the profile operation from where it left off. 12. Rerun Allows you to start a new profile operation using the same settings as the aborted scan. 13. Delete Removes the record of the aborted profile operation from the system, permanently deleting results. 14. Summary The \"Summary\" section provides a real-time overview of the profile operation's progress. It includes key metrics such as: <ul><li> Tables Requested: The total number of tables that were requested for profiling. Click on the adjacent magnifying glass icon to view the tables requested.</li><li> Tables Profiled: The number of tables that were profiled before the operation was aborted. Click on the adjacent magnifying glass icon to view the tables profiled. </li><li> Records Profiled: This represents the total number of records that were included before the profiling process was aborted. </li><li> Field Profiles Updates: This number shows how many field profiles were updated as a result of the profiling operation. </li><li> Inferred Checks Synchronized: This indicates the number of inferred checks that were synchronized based on the profile operation. </li> <li> Added: Shows the count of newly added inferred checks. </li><li> Updated: Indicates the count of checks that were updated in the operation.</li> <p></p>"},{"location":"source-datastore/profile/#warning","title":"Warning","text":"<p>This status signals that the profile operation encountered some issues and displays the logs that facilitate improved tracking of the blockers and issue resolution. A profile operation having a completed with warning status reflects the following details and actions:</p> No. Parameter Interpretation 1. Operation ID &amp; Operation Type Unique identifier and type of operation performed (catalog, profile, or scan). 2. Timestamp Timestamp when the operation was started. 3. Progress Bar The progress of the operation. 4. Triggered By The author who triggered the operation. 5. Schedule Whether the operation was scheduled or not. 6. Inference Threshold Indicates how much control you have over automatic data quality checks, adjustable based on complexity. 7. Checks Synchronized Indicates the count of Checks Synchronized in the operation. 8. Infer as Draft Indicates whether Infer as Draft was enabled or disabled in the operation. 9. Read Record Limit Defines the maximum number of records to be scanned per table after initial filtering. 10. Results Provides immediate insights into the profile operation conducted. 11. Rerun Allows you to start a new profile operation using the same settings as the warning scan. 12. Delete Removes the record of the profile operation, permanently deleting all results. 13. Summary The \"Summary\" section provides a real-time overview of the profile operation's progress. It includes key metrics such as: <ul><li> Tables Requested: The total number of tables that were requested for profiling. Click on the adjacent magnifying glass icon to view the tables requested.</li><li> Tables Profiled: The number of tables that were profiled before the operation completed. Click on the adjacent magnifying glass icon to view the tables profiled. </li><li> Records Profiled: This represents the total number of records that were included before the profiling process was completed. </li><li> Field Profiles Updates: This number shows how many field profiles were updated as a result of the profiling operation.</li><li> Inferred Checks Synchronized: This indicates the number of inferred checks that were synchronized based on the profile operation. </li></ul> 14. Logs Logs include error messages, warnings, and other pertinent information that occurred during the execution of the Profile Operation. <p></p>"},{"location":"source-datastore/profile/#success","title":"Success","text":"<p>This status confirms that the profile operation was completed successfully without any issues. A profile operation having a success status reflects the following details and actions:</p> No. Parameter Interpretation 1. Operation ID &amp; Operation Type Unique identifier and type of operation performed (catalog, profile, or scan). 2. Timestamp Timestamp when the operation was started. 3. Progress Bar The progress of the operation. 4. Triggered By The author who triggered the operation. 5. Schedule Whether the operation was scheduled or not. 6. Inference Threshold Indicates how much control you have over automatic data quality checks, allowing adjustments based on data complexity. 7. Checks Synchronized Indicates the count of Checks Synchronized in the operation. 8. Infer as Draft Indicates whether Infer as Draft was enabled or disabled in the operation. 9. Read Record Limit Defines the maximum number of records to be scanned per table after initial filtering. 10. Results Provides immediate insights into the profile operation conducted. 11. Rerun Allows you to start a new profile operation using the same settings as the warning scan, useful for restarting after errors. 12. Delete Removes the record of the profile operation from the system, permanently deleting all results; this action cannot be undone. 13. Summary The \"Summary\" section provides a real-time overview of the profile operation's progress. It includes key metrics such as: <ul><li> Tables Requested: The total number of tables that were requested for profiling. Click on the adjacent magnifying glass icon to view the tables requested. </li><li> Tables Profiled: The number of tables that were profiled before the operation was aborted. Click on the adjacent magnifying glass icon to view the tables profiled. </li><li> Records Profiled: This represents the total number of records that were included before the profiling process was aborted. </li><li> Field Profiles Updates: This number shows how many field profiles were updated as a result of the profiling operation. </li><li> Inferred Checks Synchronized: This indicates the number of inferred checks that were synchronized based on the profile operation. </li><li> Added: Shows the count of newly added inferred checks. </li><li> Updated: Indicates the count of checks that were updated in the operation.</li> <p></p>"},{"location":"source-datastore/profile/#full-view-of-metrics-in-operation-summary","title":"Full View of Metrics in Operation Summary","text":"<p>Users can now hover over abbreviated metrics to see the full value for better clarity. For demonstration purposes, we are hovering over the Records Profiled field to display the full value.</p> <p></p>"},{"location":"source-datastore/profile/#post-operation-details","title":"Post Operation Details","text":"<p>Step 1: Click on any of the successful Profile Operations from the list and hit the Results button.</p> <p></p> <p>Step 2: The Profile Results modal displays a list of both profiled and non-profiled containers. You can filter the view to show only non-profiled containers by toggling on button, which will display the complete list of unprofiled containers.</p> <p></p> <p>The Profile Results modal also provides two analysis options for you:</p> <ul> <li>Details for a Specific Container (Container's Profile)</li> <li>Details for a Specific Field of a Container (Field Profile)</li> </ul> <p>Unwrap any of the containers from the Profile Results modal and click on the arrow icon.</p> <p></p>"},{"location":"source-datastore/profile/#details-for-a-specific-container-containers-profile","title":"Details for a Specific Container (Container's Profile)","text":"<p>Based on your selection of container from the profile operation results, you will be automatically redirected to the container details on the source datastore details page.</p> <p></p> <p>The following details (metrics) will be visible for analyzing the specific container you selected:</p> <ol> <li> <p>Quality Score (79): This represents an overall quality assessment of the field, likely on a scale of 0 to 100. A score of 79 suggests that the data quality is relatively good but may need further improvement.</p> </li> <li> <p>Sampling (100%): Indicates that 100% of the data in this field was sampled for analysis. This means the entire dataset for this field was reviewed.</p> </li> <li> <p>Completeness (100%): Suggests that all entries in this field are complete, with no missing or null values, signifying data integrity.</p> </li> <li> <p>Active Checks (2): This shows that 2 data quality checks are actively running on this field. These checks likely monitor aspects such as format, uniqueness, or consistency.</p> </li> <li> <p>Active Anomalies (0): Indicates that there are no active anomalies or issues detected in the field, meaning no irregularities have been found during the checks.</p> </li> </ol> <p></p>"},{"location":"source-datastore/profile/#details-for-a-specific-field-of-a-container-field-profile","title":"Details for a Specific Field of a Container (Field Profile)","text":"<p>Unwrap the container to view the underlying fields. The following details (metrics) will be visible for analyzing a specific field of the container:</p> No Profile Description 1 Declared Type Indicates whether the type is declared by the source or inferred. 2 Distinct Values Count of distinct values observed in the dataset. 3 Min Length Shortest length of the observed string values or lowest value for numerics. 4 Max Length Greatest length of the observed string values or highest value for numerics. 5 Mean Mathematical average of the observed numeric values. 6 Median The median of the observed numeric values. 7 Standard Deviation Measure of the amount of variation in observed numeric values. 8 Kurtosis Measure of the 'tailedness' of the distribution of observed numeric values. 9 Skewness Measure of the asymmetry of the distribution of observed numeric values. 10 Q1 The first quartile; the central point between the minimum and the median. 11 Q3 The third quartile; the central point between the median and the maximum. 12 Sum Total sum of all observed numeric values. <p></p>"},{"location":"source-datastore/profile/#histogram","title":"Histogram","text":"<p>Shows how the values in the field are spread out. Each bar represents how many values fall within a certain range, making it easy to spot trends and outliers.</p> <p></p>"},{"location":"source-datastore/profile/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides payload examples for initiating and checking the running status of a profile operation. Replace the placeholder values with data specific to your setup.</p>"},{"location":"source-datastore/profile/#running-a-profile-operation","title":"Running a Profile Operation","text":"<p>To run a profile operation, use the API payload example below and replace the placeholder values with your specific values:</p> <p>Endpoint (Post):  <code>/api/operations/run (post)</code></p>"},{"location":"source-datastore/profile/#option-i-running-a-profile-operation-of-all-containers","title":"Option I: Running a profile operation of all containers","text":"<ul> <li> <p>container_names: [ ]: This setting indicates that profiling will encompass all containers.</p> </li> <li> <p>max_records_analyzed_per_partition: null: This setting implies that all records within all containers will be profiled.</p> </li> <li> <p>infer_threshold: 5: This setting indicates that the engine will automatically infer quality checks of level 5 for you.</p> </li> </ul> <pre><code>{  \n    \"type\":\"profile\",  \n    \"datastore_id\": datastore-id,  \n    \"container_names\":[],  \n    \"max_records_analyzed_per_partition\":null,  \n    \"inference_threshold\":5  \n}\n</code></pre>"},{"location":"source-datastore/profile/#option-ii-running-a-profile-operation-of-specific-containers","title":"Option II: Running a profile operation of specific containers","text":"<ul> <li> <p>container_names: [\"table_name_1\", \"table_name_2\"]: This setting indicates that profiling will only cover the tables named table_name_1 and table_name_2.</p> </li> <li> <p>max_records_analyzed_per_partition: 1000000: This setting means that up to 1 million rows per container will be profiled.</p> </li> <li> <p>infer_threshold: 0: This setting indicates that the engine will not automatically infer quality checks for you.</p> </li> </ul> <pre><code>{  \n    \"type\":\"profile\",  \n    \"datastore_id\":datastore-id,  \n    \"container_names\":[  \n        \"table_name_1\",  \n        \"table_name_2\"  \n    ],  \n    \"max_records_analyzed_per_partition\":1000000,  \n    \"inference_threshold\":0  \n}\n</code></pre>"},{"location":"source-datastore/profile/#scheduling-a-profile-operation","title":"Scheduling a Profile Operation","text":"<p>Below is a sample payload for scheduling a profile operation. Please substitute the placeholder values with the appropriate data relevant to your setup.</p> <p>Endpoint (Post): <code>/api/operations/schedule (post)</code></p> <p>INFO: This payload is to run a scheduled profile operation every day at 00:00</p>"},{"location":"source-datastore/profile/#scheduling-profile-operation-of-all-containers","title":"Scheduling profile operation of all containers","text":"<pre><code>{  \n    \"type\":\"profile\",  \n    \"name\":\"My scheduled Profile operation\",  \n    \"datastore_id\":\"datastore-id\",  \n    \"container_names\":[],  \n    \"max_records_analyzed_per_partition\":null,  \n    \"infer_constraints\":5,  \n    \"crontab\":\"00 00 /1  *\"  \n}\n</code></pre>"},{"location":"source-datastore/profile/#retrieving-profile-operation-status","title":"Retrieving Profile Operation Status","text":"<p>To retrieve the profile operation status, use the API payload example below and replace the placeholder values with your specific values:</p> <p>Endpoint (Get): <code>/api/operations/{id} (get)</code></p> <pre><code>{\n    \"items\": [\n        {\n            \"id\": 12345,\n            \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n            \"type\": \"profile\",\n            \"start_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n            \"end_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n            \"result\": \"success\",\n            \"message\": null,\n            \"triggered_by\": \"user@example.com\",\n            \"datastore\": {\n                \"id\": 101,\n                \"name\": \"Sample-Store\",\n                \"store_type\": \"jdbc\",\n                \"type\": \"db_type\",\n                \"enrich_only\": false,\n                \"enrich_container_prefix\": \"data_prefix\",\n                \"favorite\": false\n            },\n            \"schedule\": null,\n            \"inference_threshold\": 5,\n            \"max_records_analyzed_per_partition\": -1,\n            \"max_count_testing_sample\": 100000,\n            \"histogram_max_distinct_values\": 100,\n            \"greater_than_time\": null,\n            \"greater_than_batch\": null,\n            \"percent_testing_threshold\": 0.4,\n            \"high_correlation_threshold\": 0.5,\n            \"status\": {\n                \"total_containers\": 2,\n                \"containers_analyzed\": 2,\n                \"partitions_analyzed\": 2,\n                \"records_processed\": 1126,\n                \"fields_profiled\": 9,\n                \"checks_synchronized\": 26\n            },\n            \"containers\": [\n                {\n                    \"id\": 123,\n                    \"name\": \"Container1\",\n                    \"container_type\": \"table\",\n                    \"table_type\": \"table\"\n                },\n                {\n                    \"id\": 456,\n                    \"name\": \"Container2\",\n                    \"container_type\": \"table\",\n                    \"table_type\": \"table\"\n                }\n            ],\n            \"container_profiles\": [\n                {\n                    \"id\": 789,\n                    \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                    \"parent_profile_id\": null,\n                    \"container\": {\n                        \"id\": 456,\n                        \"name\": \"Container2\",\n                        \"container_type\": \"table\",\n                        \"table_type\": \"table\"\n                    },\n                    \"records_count\": 550,\n                    \"records_processed\": 550,\n                    \"checks_synchronized\": 11,\n                    \"field_profiles_count\": 4,\n                    \"result\": \"success\",\n                    \"message\": null\n                },\n                {\n                    \"id\": 790,\n                    \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                    \"parent_profile_id\": null,\n                    \"container\": {\n                        \"id\": 123,\n                        \"name\": \"Container1\",\n                        \"container_type\": \"table\",\n                        \"table_type\": \"table\"\n                    },\n                    \"records_count\": 576,\n                    \"records_processed\": 576,\n                    \"checks_synchronized\": 15,\n                    \"field_profiles_count\": 5,\n                    \"result\": \"success\",\n                    \"message\": null\n                }\n            ],\n            \"tags\": []\n        }\n    ],\n    \"total\": 1,\n    \"page\": 1,\n    \"size\": 50,\n    \"pages\": 1\n}\n</code></pre>"},{"location":"source-datastore/quality-score-settings/","title":"Quality Score Settings","text":"<p>Quality Scores are quantified measures of data quality calculated at the field and container levels recorded as time series to enable tracking of changes over time. Scores range from 0-100, with higher values indicating superior quality. These scores integrate eight distinct factors, providing a granular analysis of the attributes that impact the overall data quality.</p> <p>Each field receives a total quality score based on eight key factors, each evaluated on a 0-100 scale. The overall score is a composite reflecting the relative importance and configured weights of these factors:</p> <ul> <li>Completeness: Measures the average completeness of a field across all profiles.  </li> <li>Coverage: Assesses the adequacy of data quality checks for the field.  </li> <li>Conformity: Checks alignment with standards defined by quality checks.  </li> <li>Consistency: Ensures uniformity in type and scale across all data representations.  </li> <li>Precision: Evaluates the resolution of field values against defined quality checks.  </li> <li>Timeliness: Gauges data availability according to schedule, inheriting the container's timeliness.  </li> <li>Volumetrics: Analyzes consistency in data size and shape over time, inheriting the container's volumetrics.  </li> <li>Accuracy: Determines the fidelity of field values to their real-world counterparts.</li> </ul> <p>The Quality Score Settings allow users to tailor the impact of each quality factor on the total score by adjusting their weights, allowing the scoring system to align with your organization\u2019s data governance priorities.</p> <p>Step 1: Click on the Score option in the settings icon.</p> <p></p> <p>Step 2: A modal window \"Quality Score Settings\" will appear.</p> <p></p> <p>Step 3: The Decay Period slider sets the time frame over which the system evaluates historical data to determine the quality score. The decay period for considering past data events defaults to 180 days, but can be customized to fit your operational needs, ensuring the scores reflect the most relevant data quality insights.</p> <p></p> <p>Step 4: Adjust the Factor Weights using the sliding bar. The factor weights determine the importance of different data quality aspects.</p> <p></p> <p>Step 5: Click on the Save button to save the quality score settings.</p> <p></p> <p>After clicking the Save button, a success notification appears on the screen showing the action was completed successfully.</p>"},{"location":"source-datastore/right-click-options/","title":"Right Click Options","text":"<p>Once you add a source datastore, whether a JDBC or DFS, Qualytics provides right-click options on the following:  </p> <ul> <li>Added source datastore</li> <li>Tables or files within the source datastore  </li> <li>Fields within the tables</li> <li>Checks within the source datastore  </li> <li>Anomalies within the source datastore  </li> </ul> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"source-datastore/right-click-options/#right-click-source-datastore","title":"Right Click Source Datastore","text":"<p>Log in to your Qualytics account and right-click on the source datastore whether a JDBC or DFS. A dropdown list of options will appear:</p> <ul> <li> <p>Open in New Tab.</p> </li> <li> <p>Copy Link.</p> </li> <li> <p>Copy ID.</p> </li> <li> <p>Copy Name.</p> </li> </ul> <p></p> No Field Description 1 Open in New Tab Opens the selected source datastore in a new browser tab, where you can view its quality score, sampling, completeness, active checks, active anomalies, etc. 2 Copy Link Copy the unique URL of the selected source datastore to your clipboard. 3 Copy ID Copy the unique ID of the selected source datastore. 4 Copy Name Copy the name of the selected source datastore to your clipboard. <p>Alternatively, you can access these right-click options by performing the direct right-click operation on a source datastore from the list.</p> <p></p>"},{"location":"source-datastore/right-click-options/#right-click-tables-files","title":"Right Click Tables &amp; Files","text":"<p>Right-click on the specific table or file underlying a connected source datastore.</p> <p>A dropdown list of options will appear:</p> <ul> <li> <p>Open in New Tab.</p> </li> <li> <p>Copy Link.</p> </li> <li> <p>Copy ID.</p> </li> <li> <p>Copy Name.</p> </li> </ul> <p></p> No Field Description 1 Open in New Tab Open the selected table from the datastore in a new browser tab, where you can view its quality score, sampling, completeness, active checks, active anomalies, etc. 2 Copy Link Copy the unique URL of the selected table to your clipboard. 3 Copy ID Copy the unique identifier (ID) of the selected table. 4 Copy Name Copy the name of the selected table to your clipboard. <p>Alternatively, you can access these right-click options by opening the dedicated page of the source datastore, navigating to its Tables or files section, and performing the right-click operation on any table or file from the list.</p> <p></p>"},{"location":"source-datastore/right-click-options/#right-click-fields","title":"Right Click Fields","text":"<p>Right-click on the specific field underlying within a table or file.</p> <p>A dropdown list of options will appear:</p> <ul> <li> <p>Open in New Tab.</p> </li> <li> <p>Copy Link.</p> </li> <li> <p>Copy ID.</p> </li> <li> <p>Copy Name.</p> </li> </ul> <p></p> No Field Description 1 Open in New Tab Open the selected field in a new browser tab, where you can view its quality score, sampling, completeness, active checks, active anomalies, etc. 2 Copy Link Copy the unique URL of the selected field to your clipboard. 3 Copy ID Copy the unique identifier (ID) of the selected field. 4 Copy Name Copy the name of the selected field to your clipboard. <p>Alternatively, you can access these right-click options by opening the dedicated page of the table, navigating to its Fields section, and performing the right-click operation on any field from the list.</p> <p></p>"},{"location":"source-datastore/right-click-options/#right-click-checks","title":"Right Click Checks","text":"<p>Right-click on the specific check from All, Active, Draft, and Archived within a connected source datastore.</p> <p>A dropdown list of options will appear:</p> <ul> <li> <p>Open in New Tab.</p> </li> <li> <p>Copy Link.</p> </li> <li> <p>Copy ID.</p> </li> <li> <p>Copy Name.</p> </li> </ul> <p></p> No Field Description 1 Open in New Tab Open the selected check in a new browser tab, where you can view its quality score, sampling, completeness, active checks, active anomalies, etc. 2 Copy Link Copy the unique URL of the selected check to your clipboard. 3 Copy ID Copy the unique identifier (ID) of the selected check. 4 Copy Name Copy the name of the selected check to your clipboard. <p>Alternatively, you can access these right-click options by navigating to the Checks from the Explore section.</p> <p></p>"},{"location":"source-datastore/right-click-options/#right-click-anomalies","title":"Right Click Anomalies","text":"<p>Right-click on the specific anomaly from All, Active, Acknowledged, and Archived within a connected source datastore.</p> <p>A dropdown list of options will appear:</p> <ul> <li> <p>Open in New Tab.</p> </li> <li> <p>Copy Link.</p> </li> <li> <p>Copy ID.</p> </li> <li> <p>Copy Name.</p> </li> </ul> <p></p> No Field Description 1 Open in New Tab Open the selected anomaly in a new browser tab, where you can view its quality score, sampling, completeness, active checks, active anomalies, etc. 2 Copy Link Copy the unique URL of the selected anomaly to your clipboard. 3 Copy ID Copy the unique identifier (ID) of the selected anomaly. 4 Copy Name Copy the name of the selected anomaly to your clipboard. <p>Alternatively, you can access these right-click options by navigating to the Anomalies from the Explore section.</p> <p></p>"},{"location":"source-datastore/scan/","title":"Scan Operation","text":"<p>The Scan Operation in Qualytics is performed on a datastore to enforce data quality checks for various data collections, such as tables, views, and files. It supports centralized configuration through the Datastore Enrichment Settings, where options like the Remediation Strategy, Source Record Limit, and Anomaly Rollup Threshold are defined. While these defaults are applied automatically during a scan, users retain the flexibility to adjust the Source Record Limit and Anomaly Rollup Threshold directly within the scan form. This operation has several key functions:</p> <ul> <li> <p>Record Anomalies: Identifies a single record (row) as anomalous and provides specific details regarding why it is considered anomalous. The simplest form of a record anomaly is a row that lacks an expected value for a field.  </p> </li> <li> <p>Shape Anomalies: Identifies structural issues within a dataset at the column or schema level. It highlights broader patterns or distributions that deviate from expected norms. If a dataset is expected to have certain fields and one or more fields are missing or contain inconsistent patterns, this would be flagged as a shape anomaly.  </p> </li> <li> <p>Anomaly Data Recording: All identified anomalies, along with related analytical data, are recorded in the associated Enrichment Datastore for further examination.</p> </li> </ul> <p>Additionally, the Scan Operation offers flexible options, including the ability to:</p> <ul> <li>Perform checks on incremental loads versus full loads.</li> <li>Limit the number of records scanned.</li> <li>Run scans on a selected list of tables or files.</li> <li>Schedule scans for future execution.</li> </ul> <p>Let's get started! \ud83d\ude80</p>"},{"location":"source-datastore/scan/#navigation-to-scan-operation","title":"Navigation to Scan Operation","text":"<p>Step 1: Select a source datastore from the side menu on which you would like to perform the scan operation.</p> <p></p> <p>Step 2: Clicking on your preferred datastore will navigate you to the datastore details page. Within the overview tab (default view), click on the Run button under Scan to initiate the scan operation.</p> <p></p> <p>Note</p> <p>Scanning operation can be commenced once the catalog operation and profile operation are completed.</p>"},{"location":"source-datastore/scan/#configuration","title":"Configuration","text":"<p>Step 1: Click on the Run button to initiate the scan operation.</p> <p></p> <p>Step 2: Select tables (in your JDBC datastore) or file patterns (in your DFS datastore) and tags you would like to be scanned.</p> <p>Note</p> <p>Scan operation also supports .txt.gz and .csv.gz files in DFS datastores.</p> <p>1. All Tables/File Patterns</p> <p>This option includes all tables or file patterns currently available for scanning in the datastore. It means that every table or file pattern recognized in your datastore will be subjected to the defined data quality checks. Use this when you want to perform a comprehensive scan covering all the available data without any exclusions.</p> <p></p> <p>2. Specific Tables/File Patterns</p> <p>This option allows you to manually select the individual table(s) or file pattern(s) in your datastore to scan. Upon selecting this option, all the tables or file patterns associated with your datastore will be automatically populated, allowing you to select the datasets you want to scan.</p> <p>You can also search the tables/file patterns you want to scan directly using the search bar. Use this option when you need to target particular datasets or when you want to exclude certain files from the scan for focused analysis or testing purposes.</p> <p></p> <p>3. Tag</p> <p>This option enables you to automatically scan file patterns associated with the selected tags. Tags can be predefined or created to categorize and manage file patterns effectively.</p> <p></p> <p>Step 3: Click on the Next button to Configure Select Check Categories.</p> <p></p> <p>Step 4: Configure Select Check Categories Setting</p> <p>Users can choose one or more check categories when initiating a scan. This allows for flexible selection based on the desired scope of the operation:</p> <ul> <li> <p>Metadata: Include checks that define the expected properties of the table, such as volume. It belongs to the Volumetric rule type.</p> </li> <li> <p>Data Integrity: Include checks that specify the expected values for the data stored in the table. It belongs to all rule types except volumetric.</p> </li> </ul> <p></p> <p>Step 5: Click on the Next button to Configure the Read Settings.</p> <p></p> <p>Step 6: Configure Read Settings, Starting Threshold (Optional), and the Record Limit.</p> <ol> <li> <p>Select the Read Strategy for your scan operation.</p> </li> <li> <p>Incremental: This strategy is used to scan only the new or updated records since the last scan operation.  On the initial run, a full scan is conducted unless a specific starting threshold is set. For subsequent scans, only the records that have changed since the last scan are processed. If tables or views do not have a defined incremental key, a full scan will be performed. Ideal for regular scans where only changes need to be tracked, saving time and computational resources.</p> </li> </ol> <p>Note</p> <p>Incremental scans fully support Apache Iceberg table, significantly expanding the range of asset types eligible for incremental scanning operations.</p> <ul> <li>Full: This strategy performs a comprehensive scan of all records within the specified data collections, regardless of any previous changes or scans. Every scan operation will include all records, ensuring a complete check each time. Suitable for periodic comprehensive checks or when incremental scanning is not feasible due to the nature of the data.</li> </ul> <p></p> <p>Warning</p> <p>If any selected tables do not have an incremental identifier, a full scan will be performed for those tables.</p> <p>Info</p> <p>When running an Incremental Scan for the first time, Qualytics automatically performs a full scan, saving the incremental field for subsequent runs.</p> <ul> <li> <p>This ensures that the system establishes a baseline and captures all relevant data.</p> </li> <li> <p>Once the initial full scan is completed, the system intelligently uses the saved incremental field to execute    future Incremental Scans efficiently, focusing only on the new or updated data since the last scan.</p> </li> <li> <p>This approach optimizes the scanning process while maintaining data quality and consistency.</p> </li> </ul> <ol> <li> <p>Define the Starting Threshold (Optional) i.e., specify a minimum incremental identifier value to set a starting point for the scan.</p> </li> <li> <p>Greater Than Time: This option applies only to tables with an incremental timestamp strategy. Users can specify a timestamp to scan records that were modified after this time.</p> </li> <li> <p>Greater Than Batch: This option applies to tables with an incremental batch strategy. Users can set a batch value, ensuring that only records with a batch identifier greater than the specified value are scanned.</p> </li> </ol> <p></p> <ol> <li>Define the Record Limit - the maximum number of records to be scanned per table after any initial filtering. This is a crucial feature for managing large datasets.</li> </ol> <p></p> <p>You can manually enter a custom value in the text field or quickly select from a dropdown menu with commonly used limits such as 1M, 10M, 100M, and All.</p> <p></p> <p>Note</p> <p>The number of records must be between 1 and 1,000,000,000.</p> <p>Step 7: Click on the Next button to Configure the Scan Settings.</p> <p></p> <p>Step 8: Configure the Scan Settings.</p> <p>1. Anomaly Options: Manage duplicate anomalies efficiently by archiving duplicates or reactivating recurring ones. These settings help streamline anomaly tracking and maintain data accuracy.</p> <ul> <li> <p>Archive Duplicate Anomalies: Automatically archive duplicate anomalies from previous scans that overlap with the current scan to enhance data management efficiency.</p> </li> <li> <p>Reactivate Recurring Anomalies: Enabling Reactivate Recurring Anomalies marks new anomalies as duplicates of archived ones, reactivates the original anomaly, and creates a Fingerprint column in the Enrichment Datastore.</p> </li> </ul> <p></p> <p>2. Anomaly Rollup Threshold: Set the maximum number of anomalies generated per check before they are merged into a single rolled-up anomaly. This helps manage anomaly volume and simplifies review.</p> <p></p> <p>3. Source Record Limit: Sets a maximum limit on the number of records written to the enrichment datastore for each detected anomaly. This helps manage storage and processing requirements effectively.</p> <p></p>"},{"location":"source-datastore/scan/#run-instantly","title":"Run Instantly","text":"<p>Click on the Run Now button to perform the scan operation immediately.</p> <p></p>"},{"location":"source-datastore/scan/#schedule","title":"Schedule","text":"<p>Step 1: Click on the Schedule button to configure the available schedule options for your scan operation.</p> <p></p> <p>Step 2: Set the scheduling preferences for the scan operation.</p> <p>1. Hourly: This option allows you to schedule the scan to run every hour at a specified minute. You can define the frequency in hours and the exact minute within the hour the scan should start. Example: If set to Every 1 hour(s) on minute 0, the scan will run every hour at the top of the hour (e.g., 1:00, 2:00, 3:00).</p> <p></p> <p>2. Daily: This option schedules the scan to run once every day at a specific time. You specify the number of days between scans and the exact time of day in UTC. Example: If set to Every 1 day(s) at 00:00 UTC, the scan will run every day at midnight UTC.</p> <p></p> <p>3. Weekly: This option schedules the scan to run on specific days of the week at a set time. You select the days of the week and the exact time of day in UTC for the scan to run. Example: If configured to run on \"Sunday\" and \"Friday\" at 00:00 UTC, the scan will execute at midnight UTC on these days.</p> <p></p> <p>4. Monthly: This option schedules the scan to run once a month on a specific day at a set time. You specify the day of the month and the time of day in UTC. If set to \"On the 1st day of every 1 month(s), at 00:00 UTC,\" the scan will run on the first day of each month at midnight UTC.</p> <p></p> <p>5. Advanced: The advanced section for scheduling operations allows users to set up more complex and custom scheduling using Cron expressions. This option is particularly useful for defining specific times and intervals for scan operations with precision.</p> <p>Cron expressions are a powerful and flexible way to schedule tasks. They use a syntax that specifies the exact timing of the task based on five fields:</p> <ul> <li>Minute (0 - 59)</li> <li>Hour (0 - 23)</li> <li>Day of the month (1 - 31)</li> <li>Month (1 - 12)  </li> <li>Day of the week (0 - 6) (Sunday to Saturday)</li> </ul> <p>Each field can be defined using specific values, ranges, or special characters to create the desired schedule.</p> <p>Example: The Cron expression <code>0 0 * * *</code> schedules the scan operation to run at midnight (00:00) every day. Here\u2019s a breakdown of this expression:</p> <ul> <li>0 (Minute) - The task will run at the 0th minute.</li> <li>0 (Hour) - The task will run at the 0th hour (midnight).</li> <li>*(Day of the month) - The task will run every day of the month.</li> <li>*(Month) - The task will run every month.</li> <li>*(Day of the week) - The task will run every day of the week.</li> </ul> <p>Users can define other specific schedules by adjusting the Cron expression. For example:</p> <ul> <li>0 12 * * 1-5 - Runs at 12:00 PM from Monday to Friday.</li> <li>30 14 1 * * - Runs at 2:30 PM on the first day of every month.</li> <li>0 22 * * 6 - Runs at 10:00 PM every Saturday.</li> </ul> <p>To define a custom schedule, enter the appropriate Cron expression in the \"Custom Cron Schedule (UTC)\" field before specifying the schedule name. This will allow for precise control over the timing of the scan operation, ensuring it runs exactly when needed according to your specific requirements.</p> <p></p> <p>Step 3: Define the Schedule Name to identify the scheduled operation at the running time.</p> <p></p> <p>Step 4: Click on the Schedule button to schedule your scan operation.</p> <p></p> <p>Note</p> <p>You will receive a notification when the scan operation is completed.</p>"},{"location":"source-datastore/scan/#advanced-options","title":"Advanced Options","text":"<p>The advanced use cases described below require options that are not yet exposed in our user interface but possible through interaction with Qualytics API.</p>"},{"location":"source-datastore/scan/#runtime-variable-assignment","title":"Runtime Variable Assignment","text":"<p>It is possible to reference a variable in a check definition (declared in double curly braces) and then assign that variable a value when a Scan operation is initiated. Variables are supported within any Spark SQL expression and are most commonly used in a check filter.</p> <p>If a Scan is meant to assert a check with a variable, a value for that variable must be supplied as part of the Scan operation's <code>check_variables</code> property.</p> <p>For example, a check might include a filter.- <code>transaction_date == {{ checked_date }}</code> which will be asserted against any records where transaction_date is equal to the value supplied when the Scan operation is initiated. In this case that value would be assigned by passing the following payload when calling <code>/api/operations/run</code></p> <pre><code>{\n    \"type\": \"scan\",\n    \"datastore_id\": 42,\n    \"container_names\": [\"my_container\"],\n    \"incremental\": true,\n    \"remediation\": \"none\",\n    \"max_records_analyzed_per_partition\": 0,\n    \"check_variables\": {\n        \"checked_date\": \"2023-10-15\"\n    },\n    \"high_count_rollup_threshold\": 10\n}\n</code></pre>"},{"location":"source-datastore/scan/#operations-insights","title":"Operations Insights","text":"<p>When the scan operation is completed, you will receive the notification and can navigate to the Activity tab for the datastore on which you triggered the Scan Operation and learn about the scan results.</p>"},{"location":"source-datastore/scan/#top-panel","title":"Top Panel","text":"<p>1. Runs (Default View): Provides insights into the operations that have been performed.</p> <p>2. Schedule: Provides insights into the scheduled operations.</p> <p>3. Search: Search for any operation (including scan) by entering the operation ID.</p> <p>4. Sort by: Organize the list of operations based on the Created Date or the Duration.</p> <p>5. Filter: Narrow down the list of operations based on:</p> <ul> <li>Operation Type</li> <li>Operation Status</li> <li>Table</li> </ul> <p></p>"},{"location":"source-datastore/scan/#activity-heatmap","title":"Activity Heatmap","text":"<p>The activity heatmap shown in the snippet below represents activity levels over a period, with each square indicating a day and the color intensity representing the number of operations or activities on that day. It is useful in tracking the number of operations performed on each day within a specific timeframe.</p> <p>Tip</p> <p>You can click on any of the squares from the Activity Heatmap to filter operations.</p> <p></p>"},{"location":"source-datastore/scan/#operation-detail","title":"Operation Detail","text":""},{"location":"source-datastore/scan/#running","title":"Running","text":"<p>This status indicates that the scan operation is still running at the moment and is yet to be completed. A scan operation having a running status reflects the following details and actions:</p> <p></p> No. Parameter Interpretation 1 Operation ID and Type Unique identifier and type of operation performed (catalog, profile, or scan). 2 Timestamp Timestamp when the operation was started. 3 Progress Bar The progress of the operation. 4 Triggered By The author who triggered the operation. 5 Schedule Indicates whether the operation was scheduled or not. 6 Incremental Field Indicates whether Incremental was enabled or disabled in the operation. 7 Remediation Indicates whether Remediation was enabled or disabled in the operation. 8 Anomalies Identified Provides a count of the number of anomalies detected during the running operation. 9 Read Record Limit Defines the maximum number of records to be scanned per table after initial filtering. 10 Check Categories Indicates which categories should be included in the scan (e.g., Metadata, Data Integrity). 11 Archive Duplicate Anomalies Indicates whether Archive Duplicate Anomalies was enabled or disabled in the operation. 12 Reactivate Recurring Anomalies Indicates whether previously detected anomalies that reappear in subsequent scans will be reactivated. 13 Source Record Limit Indicates the limit on records stored in the enrichment datastore for each detected anomaly. 14 Anomaly Rollup Threshold Number of anomalies grouped together for rollup reporting. 15 Results View the details of the ongoing scan operation. This includes information on which tables are currently being scanned, the anomalies identified so far (if any), and other related data collected during the active scan. 16 Abort The Abort button enables you to stop the ongoing scan operation. 17 Summary The summary section provides an overview of the scan operation in progress. It includes:  <ul><li> Tables Requested: The total number of tables that were scheduled for scanning. Click on the adjacent magnifying glass icon to view the tables requested. </li><li> Tables Scanned: The number of tables that have been scanned so far. Click on the adjacent magnifying glass icon to view the tables scanned. </li><li> Partitions Scanned: The number of partitions scanned during the ongoing operation.</li><li> Records Scanned: The total number of records processed up to this point. </li><li> Anomalies Identified: The total number of detected anomalies, with a breakdown of open and archived ones. </li></ul>"},{"location":"source-datastore/scan/#aborted","title":"Aborted","text":"<p>This status indicates that the scan operation was manually stopped before it could be completed. A scan operation having an aborted status reflects the following details and actions:</p> <p></p> No. Parameter Interpretation 1 Operation ID and Type Unique identifier and type of operation performed (catalog, profile, or scan). 2 Timestamp Timestamp when the operation was started 3 Progress Bar The progress of the operation 4 Aborted By The author who triggered the operation 5 Schedule Whether the operation was scheduled or not 6 Incremental Field Indicates whether Incremental was enabled or disabled in the operation 7 Remediation Indicates whether Remediation was enabled or disabled in the operation 8 Anomalies Identified Provides a count on the number of anomalies detected before the operation was aborted 9 Read Record Limit Defines the maximum number of records to be scanned per table after initial filtering 10 Check Categories Indicates which categories should be included in the scan (Metadata, Data Integrity) 11 Archive Duplicate Anomalies Indicates whether Archive Duplicate Anomalies was enabled or disabled in the operation 12 Reactivate Recurring Anomalies Indicates whether previously detected anomalies that reappear in subsequent scans will be reactivated. 13 Source Record Limit Indicates the limit on records stored in the enrichment datastore for each detected anomaly 14 Anomaly Rollup Threshold Number of anomalies grouped together for rollup reporting. 15 Results View the details of the scan operation that was aborted, including tables scanned and anomalies identified 16 Resume Provides an option to continue the scan operation from where it left off 17 Rerun The \"Rerun\" button allows you to start a new scan operation using the same settings as the aborted scan 18 Delete Removes the record of the aborted scan operation from the system, permanently deleting scan results and anomalies 19 Summary The summary section provides an overview of the scan operation up to the point it was aborted. It includes:  <ul><li> Tables Requested: The total number of tables that were scheduled for scanning. Click on the adjacent magnifying glass icon to view the tables requested. </li><li> Tables Scanned: The number of tables that have been scanned so far. Click on the adjacent magnifying glass icon to view the tables scanned. </li><li> Partitions Scanned: The number of partitions scanned before the operation was aborted. </li><li> Records Scanned: The total number of records processed before the scan was stopped. </li><li> Anomalies Identified: The total number of detected anomalies, with a breakdown of open and archived ones. </li></ul>"},{"location":"source-datastore/scan/#warning","title":"Warning","text":"<p>This status signals that the scan operation encountered some issues and displays the logs that facilitate improved tracking of the blockers and issue resolution. A scan operation having a completed with warning status reflects the following details and actions:</p> <p></p> No. Parameter Interpretation 1 Operation ID and Type Unique identifier and type of operation performed (catalog, profile, or scan). 2 Timestamp Timestamp when the operation was started 3 Progress Bar The progress of the operation 4 Triggered By The author who triggered the operation 5 Schedule Whether the operation was scheduled or not 6 Incremental Field Indicates whether Incremental was enabled or disabled in the operation 7 Remediation Indicates whether Remediation was enabled or disabled in the operation 8 Anomalies Identified Provides a count on the number of anomalies detected before the operation was warned. 9 Read Record Limit Defines the maximum number of records to be scanned per table after initial filtering 10 Check Categories Indicates which categories should be included in the scan (Metadata, Data Integrity) 11 Archive Duplicate Anomalies Indicates whether Archive Duplicate Anomalies was enabled or disabled in the operation 12 Source Record Limit Indicates the limit on records stored in the enrichment datastore for each detected anomaly 13 Anomaly Rollup Threshold Number of anomalies grouped together for rollup reporting. 14 Result View the details of the scan operation that was completed with warning, including tables scanned and anomalies identified 15 Rerun The \"Rerun\" button allows you to start a new scan operation using the same settings as the warning scan 16 Delete Removes the record of the warning operation from the system, permanently deleting scan results and anomalies 17 Summary The summary section provides an overview of the scan operation, highlighting any warnings encountered. It includes: <ul><li> Tables Requested: The total number of tables that were scheduled for scanning. Click on the adjacent magnifying glass icon to view the tables requested. </li><li> Tables Scanned: The number of tables that have been scanned so far. Click on the adjacent magnifying glass icon to view the tables scanned. </li><li> Partitions Scanned: The number of partitions scanned during the operation, including any partitions that triggered warnings. </li><li> Records Scanned: The total number of records processed during the scan, along with any records that raised warnings. </li><li> Anomalies Identified: The total number of detected anomalies, with a breakdown of open and archived ones. </li></ul> 18 Logs Logs include error messages, warnings, and other pertinent information that occurred during the execution of the Scan Operation."},{"location":"source-datastore/scan/#success","title":"Success","text":"<p>The summary section provides an overview of the scan operation upon successful completion. It includes:</p> <p></p> No. Parameter Interpretation 1 Operation ID and Type Unique identifier and type of operation performed (catalog, profile, or scan). 2 Timestamp Timestamp when the operation was started 3 Progress Bar The progress of the operation 4 Triggered By The author who triggered the operation 5 Schedule Whether the operation was scheduled or not 6 Incremental Field Indicates whether Incremental was enabled or disabled in the operation 7 Remediation Indicates whether Remediation was enabled or disabled in the operation 8 Anomalies Identified Provides a count of the number of anomalies detected during the successful completion of the operation. 9 Read Record Limit Defines the maximum number of records to be scanned per table after initial filtering 10 Check Categories Indicates which categories should be included in the scan (Metadata, Data Integrity) 11 Archive Duplicate Anomalies Indicates whether Archive Duplicate Anomalies was enabled or disabled in the operation 12 Source Record Limit Indicates the limit on records stored in the enrichment datastore for each detected anomaly 13 Anomaly Rollup Threshold Number of anomalies grouped together for rollup reporting. 14 Results View the details of the completed scan operation. This includes information on which tables were scanned, the anomalies identified (if any), and other relevant data collected throughout the successful completion of the scan. 15 Rerun The \"Rerun\" button allows you to start a new scan operation using the same settings as the success scan 16 Delete Removes the record of the aborted scan operation from the system, permanently deleting scan results and anomalies 17 Summary The summary section provides an overview of the scan operation upon successful completion. It includes: <ul><li> Tables Requested: The total number of tables that were scheduled for scanning. Click on the adjacent magnifying glass icon to view the tables requested. </li><li> Tables Scanned: The number of tables that have been scanned successfully. Click on the adjacent magnifying glass icon to view the tables scanned. </li><li> Partitions Scanned: The number of partitions scanned. </li><li> Records Scanned: The total number of records processed.</li><li> Anomalies Identified: The total number of detected anomalies, with a breakdown of open and archived ones. </li></ul>"},{"location":"source-datastore/scan/#full-view-of-metrics-in-operation-summary","title":"Full View of Metrics in Operation Summary","text":"<p>Users can now hover over abbreviated metrics to see the full value for better clarity. For demonstration purposes, we are hovering over the Records Scanned field to display the full value.</p> <p></p>"},{"location":"source-datastore/scan/#post-operation-details","title":"Post Operation Details","text":"<p>Step 1: Click on any of the successful Scan Operations from the list and hit the Results button.</p> <p></p> <p>Step 2: The Scan Results modal demonstrates the highlighted anomalies (if any) identified in your datastore with the following properties:</p> <p></p> Ref. Scan Properties Description 1. Table/File The table or file where the anomaly is found. 2. Field The field(s) where the anomaly is present. 3. Location Fully qualified location of the anomaly. 4. Rule Inferred and authored checks that failed assertions. 5. Description Human-readable, auto-generated description of the anomaly. 6. Status The status of the anomaly: Active, Acknowledged, Resolved, or Invalid. 7. Type The type of anomaly (e.g., Record or Shape) 8. Date time The date and time when the anomaly was found. <p>Step 3: By clicking the dropdown button next to the All button, you can filter anomalies based on their status.</p> <p></p>"},{"location":"source-datastore/scan/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides payload examples for running, scheduling, and checking the status of scan operations. Replace the placeholder values with data specific to your setup.</p>"},{"location":"source-datastore/scan/#running-a-scan-operation","title":"Running a Scan operation","text":"<p>To run a scan operation, use the API payload example below and replace the placeholder values with your specific values.</p>"},{"location":"source-datastore/scan/#endpoint-post","title":"Endpoint (Post):","text":"<p><code>/api/operations/run (post)</code></p> Option I: Running a scan operation of all containersOption II: Running a scan operation of specific containers <ul> <li>container_names: <code>[]</code> means that it will scan all containers.</li> <li>max_records_analyzed_per_partition: <code>null</code> means that it will scan all records of all containers.</li> <li>Remediation: <code>append</code> replicates source containers using an append-first strategy.</li> </ul> <pre><code>{\n    \"type\":\"scan\",\n    \"name\":null,\n    \"datastore_id\": datastore-id,\n    \"container_names\":[],\n    \"remediation\":\"append\",\n    \"incremental\":false,\n    \"max_records_analyzed_per_partition\":null,\n    \"enrichment_source_record_limit\":10\n}\n</code></pre> <ul> <li>container_names: <code>[\"table_name_1\", \"table_name_2\"]</code> means that it will scan only the tables table_name_1 and table_name_2.</li> <li>max_records_analyzed_per_partition: <code>1000000</code> means that it will scan a maximum of 1 million records per partition.  </li> <li>Remediation: <code>overwrite</code> replicates source containers using an overwrite strategy.</li> </ul> <pre><code>{\n    \"type\":\"scan\",\n    \"name\":null,\n    \"datastore_id\":datastore-id,\n    \"container_names\":[\n      \"table_name_1\",\n      \"table_name_2\"\n    ],\n    \"max_records_analyzed_per_partition\":1000000,\n    \"enrichment_source_record_limit\":10\n}\n</code></pre>"},{"location":"source-datastore/scan/#scheduling-scan-operation-of-all-containers","title":"Scheduling scan operation of all containers","text":"<p>To schedule a scan operation, use the API payload example below and replace the placeholder values with your specific values.</p>"},{"location":"source-datastore/scan/#endpoint-post_1","title":"Endpoint (Post):","text":"<p><code>/api/operations/schedule (post)</code></p> <p>This payload is to run a scheduled scan operation every day at 00:00</p> Scheduling scan operation of all containers <pre><code>{\n    \"type\":\"scan\",\n    \"name\":\"My scheduled Scan operation\",\n    \"datastore_id\":\"datastore-id\",\n    \"container_names\":[],\n    \"remediation\": \"overwrite\"\n    \"incremental\": false,\n    \"max_records_analyzed_per_partition\":null,\n    \"enrichment_source_record_limit\":10,\n    \"crontab\":\"00 00 */2 * *\"\n}\n</code></pre>"},{"location":"source-datastore/scan/#retrieving-scan-operation-information","title":"Retrieving Scan Operation Information","text":""},{"location":"source-datastore/scan/#endpoint-get","title":"Endpoint (Get)","text":"<p><code>/api/operations/{id} (get)</code></p> Example result response <pre><code>{\n    \"items\": [\n        {\n            \"id\": 12345,\n            \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n            \"type\": \"scan\",\n            \"start_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n            \"end_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n            \"result\": \"success\",\n            \"message\": null,\n            \"triggered_by\": \"user@example.com\",\n            \"datastore\": {\n                \"id\": 101,\n                \"name\": \"Datastore-Sample\",\n                \"store_type\": \"jdbc\",\n                \"type\": \"db_type\",\n                \"enrich_only\": false,\n                \"enrich_container_prefix\": \"data_prefix\",\n                \"favorite\": false\n            },\n            \"schedule\": null,\n            \"incremental\": false,\n            \"remediation\": \"none\",\n            \"max_records_analyzed_per_partition\": -1,\n            \"greater_than_time\": null,\n            \"greater_than_batch\": null,\n            \"high_count_rollup_threshold\": 10,\n            \"enrichment_source_record_limit\": 10,\n            \"status\": {\n                \"total_containers\": 2,\n                \"containers_analyzed\": 2,\n                \"partitions_scanned\": 2,\n                \"records_processed\": 28,\n                \"anomalies_identified\": 2\n            },\n            \"containers\": [\n                {\n                \"id\": 234,\n                \"name\": \"Container1\",\n                \"container_type\": \"table\",\n                \"table_type\": \"table\"\n                },\n                {\n                \"id\": 235,\n                \"name\": \"Container2\",\n                \"container_type\": \"table\",\n                \"table_type\": \"table\"\n                }\n            ],\n            \"container_scans\": [\n                {\n                \"id\": 456,\n                \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                \"container\": {\n                    \"id\": 235,\n                    \"name\": \"Container2\",\n                    \"container_type\": \"table\",\n                    \"table_type\": \"table\"\n                },\n                \"start_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                \"end_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                \"records_processed\": 8,\n                \"anomaly_count\": 1,\n                \"result\": \"success\",\n                \"message\": null\n                },\n                {\n                \"id\": 457,\n                \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                \"container\": {\n                    \"id\": 234,\n                    \"name\": \"Container1\",\n                    \"container_type\": \"table\",\n                    \"table_type\": \"table\"\n                },\n                \"start_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                \"end_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                \"records_processed\": 20,\n                \"anomaly_count\": 1,\n                \"result\": \"success\",\n                \"message\": null\n                }\n            ],\n            \"tags\": []\n        }\n    ],\n    \"total\": 1,\n    \"page\": 1,\n    \"size\": 50,\n    \"pages\": 1\n}\n</code></pre>"},{"location":"source-datastore/settings-overview/","title":"Settings Overview","text":"<p>Qualytics allows you to manage your datastore efficiently by editing source datastore information, linking an enrichment datastore for enhanced insights, establishing new connections to expand data sources, choosing connectors to integrate diverse data, adjusting the quality score to ensure data accuracy, and deleting the store. This ensures flexibility and control over your data management processes within the platform.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"source-datastore/settings-overview/#navigation","title":"Navigation","text":"<p>Step 1: Select a source datastore from the side menu for which you would like to manage the settings.</p> <p></p> <p>Step 2: Click on the Settings icon from the top right window. A drop-down menu will appear with the following options:</p> <ol> <li>Edit  </li> <li>Enrichment  </li> <li>Score  </li> <li>Delete</li> </ol> <p></p>"},{"location":"source-datastore/settings-overview/#edit-datastore","title":"Edit Datastore","text":"<p>Use the Edit Datastore option to make changes to your datastore\u2019s connection setup whenever updates are needed.</p> <p>For more information on how to edit the datastore, please refer to the Edit Datastore documentation.</p>"},{"location":"source-datastore/settings-overview/#link-enrichment","title":"Link Enrichment","text":"<p>An enrichment datastore is a database used to enhance your existing data by adding additional, relevant information. This helps you to provide more comprehensive insight into data and improve data accuracy.</p> <p>You have the option to link an enrichment datastore to your existing source datastore. However, some datastores cannot be linked as enrichment datastores. For example, Oracle, Athena, and Timescale cannot be used for this purpose.</p> <p>For more information on how to link an enrichment datastore, please refer to the Link Enrichment documentation.</p>"},{"location":"source-datastore/settings-overview/#unlink-enrichment","title":"Unlink Enrichment","text":"<p>You can remove the connection between the source datastore and the enrichment datastore using the Unlink Enrichment Datastore option. This action stops the enrichment process and ensures that no further data is enhanced using the unlinked datastore.</p> <p>For more information on how to unlink an enrichment datastore, please refer to the Unlink Enrichment documentation. </p>"},{"location":"source-datastore/settings-overview/#quality-score-settings","title":"Quality Score Settings","text":"<p>Use the Quality Score Settings option to adjust factor weights and decay period, aligning data quality scoring with your organization\u2019s needs.</p> <p>For more information on the quality score, please refer to the Quality Score documentation.</p>"},{"location":"source-datastore/settings-overview/#delete-datastore","title":"Delete Datastore","text":"<p>Use the Delete Datastore option to remove outdated or unused datastores along with their related configurations, helping keep your workspace organized.</p> <p>For more information on how to delete a datastore, please refer to the Delete Datastore documentation.</p>"},{"location":"source-datastore/unlink-enrichment/","title":"Unlink Enrichment Datastore","text":"<p>Step 1: Click on the Enrichment from the drop-down list.</p> <p></p> <p>A modal window titled Enrichment Datastore Settings will appear, displaying configuration options for the linked enrichment datastore.</p> <p></p> <p>Step 2: Click the Unlink Enrichment Datastore option (represented by unlink icon) located on the right side of the Details section to remove the linked enrichment datastore.</p> <p></p> <p>A modal window titled Unlink Enrichment Datastore will appear.</p> <p></p> <p>Step 3: Click the Unlink button to remove the enrichment datastore connection.</p> <p></p> <p>After clicking the Unlink button, a success message confirms that the datastore has been updated successfully.</p>"},{"location":"tags/add-tag/","title":"Add Tag","text":"<p>Step 1: Click on the Add Tag button from the top right corner.</p> <p></p> <p>Step 2: A modal window will appear, providing the options to create the tag. Enter the required values to get started. </p> REF. FIELD ACTION EXAMPLE 1. Preview This shows how the tag will appear to users. Preview 2. Name Assign a name to your tag. Sensitive 3. Color A color picker feature is provided, allowing you to select a color using its hex code. #E74C3C 4. Description Explain the nature of your tag. Maintain data that is highly confidential and requires strict access controls. 5. Category Choose an existing category or create a new one to group related tags for easier organization. Demo2 6. Weight Modifier Adjust the tag's weight for prioritization, where a higher value represents greater significance. The range is between -10 and 10. \ud83d\udca1 Tip: Increasing the value boosts the weight of a recipient and its children by the same amount, while negative values decrease it. 10 <p></p> <p>Step 3: Click on the Save button to save your tag.</p> <p></p> <p>After clicking the Save button, the tag will be added to the system and a success message will appear.</p>"},{"location":"tags/add-tag/#view-created-tags","title":"View Created Tags","text":"<p>Once you have created a tag, you can view it in the tags list.</p> <p></p>"},{"location":"tags/applying-a-tag/","title":"Applying a Tag","text":"<p>Once a Tag is created, it's ready to be associated with a <code>Datastore</code>, <code>Profile</code>, <code>Check</code>, <code>Notification</code> and ultimately an <code>Anomaly</code>.</p>"},{"location":"tags/applying-a-tag/#tag-inheritance","title":"Tag Inheritance","text":"<ul> <li> <p>When a <code>Tag</code> is applied to a data asset, all the descendants of that data asset also receive the <code>Tag</code>.</p> <ul> <li>For example, if a <code>Tag</code> named Critical is applied to a Datastore then all the Tables, Fields, and Checks under that Datastore also receive the <code>Tag</code>.</li> </ul> <p>Note</p> <p>Anomalies will inherit the tags if a scan has been run.</p> </li> <li> <p>Likewise, if the Critical <code>Tag</code> is subsequently removed from one of the Tables in that Datastore, then all the Fields and Checks belonging to that Table will have the Critical <code>Tag</code> removed as well.</p> </li> <li> <p>When a new data asset is created, it inherits the <code>Tags</code> from the owning data asset. For example, if a user creates a new Computed Table, it inherits all the <code>Tags</code> that are applied to the Datastore in which it is created.</p> </li> </ul>"},{"location":"tags/applying-a-tag/#tagging-anomalies","title":"Tagging Anomalies","text":"<ul> <li> <p>Anomalies also inherit <code>Tags</code> at the time they are created. They inherit all the <code>Tags</code> of all the associated failed checks.</p> </li> <li> <p>Thus Anomalies do not inherit subsequent tag changes from those checks. They only inherit tags one time - at creation time.</p> </li> <li> <p><code>Tags</code> can be directly applied to or removed from Anomalies at any time after creation.</p> </li> </ul>"},{"location":"tags/delete-tag/","title":"Delete Tags","text":"<p>This allows you to remove outdated or unnecessary tags to maintain a clean and efficient tag system.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the tag that you want to delete, then click on Delete from the dropdown menu.</p> <p>Warning</p> <p>Deleting a tag is permanent and cannot be undone.</p> <p></p> <p>Step 2: After clicking the Delete button, your tag will be removed from the system, and a success message will appear.</p>"},{"location":"tags/edit-tag/","title":"Edit Tags","text":"<p>This allows you to keep your tags updated with current information and relevance.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the tag that you want to edit, then click on Edit from the dropdown menu.</p> <p></p> <p>Step 2: Edit the tag's name, color, description, category and weight as needed.</p> <p></p> <p>Step 3: Click the Save button to apply your changes.</p> <p></p> <p>Step 4: After clicking the Save button, a success message will appear.</p>"},{"location":"tags/external-tag/","title":"External Tags","text":"<p>External Tags in Qualytics make it easy to keep your data catalog and Qualytics in sync \u2014 so you never have to create the same tags twice.</p> <p>They automatically bring in tags (like Customer Data, Finance, PII, etc.) from platforms such as Atlan or Alation, and apply them to the same data assets inside Qualytics.</p>"},{"location":"tags/external-tag/#what-are-external-tags","title":"What Are External Tags?","text":"<p>External Tags are tags that come from an external data catalog, not from Qualytics itself. They\u2019re read-only, meaning you can view and use them in Qualytics, but can\u2019t edit or delete them there.</p> <p>Instead, they stay connected to your catalog \u2014 whenever your catalog updates, Qualytics updates too.</p>"},{"location":"tags/external-tag/#example","title":"Example","text":"<p>Let\u2019s say your team uses Atlan to tag assets with labels like:</p> <ul> <li>Finance Data \u2013 for tables related to financial reports  </li> <li>Customer Data \u2013 for customer information  </li> <li>PII \u2013 for sensitive or personally identifiable data  </li> </ul> <p>Once Atlan is integrated, Qualytics will automatically import these same tags and show them as External Tags. You\u2019ll see them in the tag list, filters, and on relevant assets \u2014 always in sync with your catalog.</p>"},{"location":"tags/external-tag/#why-external-tags-matter","title":"Why External Tags Matter","text":"<p>Without External Tags, teams often have to create the same tags separately in Qualytics \u2014 leading to confusion and duplicate work. With External Tags, you get:</p> <ul> <li>Consistency \u2014 the same tags appear across Atlan, Alation, and Qualytics  </li> <li>Automatic sync \u2014 no need to manually recreate or update tags  </li> <li>Unified visibility \u2014 see data quality insights for each tag  </li> <li>Governance alignment \u2014 your existing tagging standards stay intact  </li> </ul> <p>In short, External Tags help Qualytics \u201cspeak the same language\u201d as your data catalog.</p>"},{"location":"tags/external-tag/#how-it-works-simple-view","title":"How It Works (Simple View)","text":"Step What Happens 1 You connect Atlan, Alation, or another supported catalog to Qualytics under Settings \u2192 Integrations. 2 Qualytics uses a secure API connection to read your catalog\u2019s tags. 3 Those tags appear inside Qualytics automatically as External Tags. 4 Whenever a tag or asset updates in your catalog, Qualytics syncs those changes. 5 You can filter, sort, or view these tags \u2014 but editing happens in the catalog itself."},{"location":"tags/external-tag/#real-life-example","title":"Real-Life Example","text":"<p>Imagine your company runs both a data catalog (Atlan) and a data quality platform (Qualytics).</p> <ul> <li> <p>In Atlan, your governance team adds a tag called \u201cSensitive Data\u201d to all customer-related tables. </p> </li> <li> <p>When synced, Qualytics automatically imports this tag and marks it as External.  </p> </li> <li> <p>Now, your data quality team can filter all anomalies or checks by \u201cSensitive Data\u201d \u2014 without ever tagging them manually. </p> </li> <li> <p>If the governance team later renames the tag to \u201cConfidential Data\u201d, Qualytics updates automatically.</p> </li> </ul> <p>This creates a single, reliable view of your data health \u2014 using the same tags everywhere.</p>"},{"location":"tags/external-tag/#use-cases","title":"Use Cases","text":"<p>Here are a few practical examples of how teams use External Tags:</p>"},{"location":"tags/external-tag/#governance-teams","title":"Governance Teams","text":"<p>Use External Tags to track which datasets are PII, Customer Data, or Financial Records \u2014 without managing tags in multiple places.</p>"},{"location":"tags/external-tag/#data-engineers","title":"Data Engineers","text":"<p>Quickly filter quality checks in Qualytics by tags synced from your catalog \u2014 for example, view all Finance datasets with active anomalies.</p>"},{"location":"tags/external-tag/#compliance-risk-teams","title":"Compliance &amp; Risk Teams","text":"<p>Easily identify sensitive assets tagged as GDPR or Confidential and monitor their data quality.</p>"},{"location":"tags/external-tag/#business-analysts","title":"Business Analysts","text":"<p>Filter dashboards and reports by External Tags like Sales, Marketing, or Customer Behavior to analyze data quality in context.</p>"},{"location":"tags/external-tag/#example-in-action","title":"Example in Action","text":"<p>Let\u2019s take a real example:</p> <ol> <li>In Atlan, a governance user tags three tables as Finance.  </li> <li>Qualytics syncs with Atlan \u2192 those three tables now show Finance (External) tags in Qualytics.  </li> <li>When viewing anomalies in Qualytics, you can filter by Finance to check if those datasets have open data quality issues.  </li> <li>The moment the tag changes in Atlan, Qualytics updates it automatically \u2014 no manual work needed.</li> </ol>"},{"location":"tags/external-tag/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>External Tags come from platforms like Atlan or Alation.  </li> <li>They are read-only in Qualytics but automatically stay up-to-date.  </li> <li>You can filter, view, and analyze your data quality using these tags.  </li> <li>All updates happen through integration \u2014 keeping catalog and Qualytics perfectly aligned.  </li> </ul>"},{"location":"tags/filter-and-sort/","title":"Filter and Sort","text":"<p>Qualytics allows you to sort and filter your tags so that you can easily organize and find the most relevant tags according to your criteria, improving data management and workflow efficiency.</p>"},{"location":"tags/filter-and-sort/#sort","title":"Sort","text":"<p>You can sort your tags by Category, Color, Created Date, Name, and Weight to easily organize and prioritize them according to your needs.</p> <p></p> No. Sort Option Description 1 Category Sort tags based on their assigned category. 2 Color Sort tags according to their color label. 3 Created Date Sort tags by the date they were created. 4 Name Sort tags alphabetically by name. 5 Weight Sort tags based on their assigned weight value. <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.</p> <p></p>"},{"location":"tags/filter-and-sort/#filter","title":"Filter","text":"<p>You can filter your tags by type and category, which allows you to categorize and manage them more effectively. </p> <p>Info</p> <p>Users can search with typos or partial matches in filter inputs for easier, more flexible results.</p> <p></p> <p></p> No. Filter Option Description 1 Type Filter tags based on their origin.  External: Imported automatically from integrated catalog systems like Atlan or Alation via API. These tags cannot be manually created or edited and ensure consistent tagging across connected platforms.  Global: Created and managed directly within Qualytics. Used internally to organize datasets and unaffected by external integrations unless the Overwrite Tags option is enabled. 2 Category Filter tags based on predefined groups or categories, making it easier to locate and manage related tags efficiently."},{"location":"tags/overview/","title":"Tags","text":"<p>Tags allow users to categorize and organize data assets effectively and provide the ability to assign weights for prioritization. They drive notifications and downstream workflows, enabling users to stay informed and take appropriate actions. Tags can be configured and associated with specific properties, allowing for targeted actions and efficient management of entities across multiple datastores. </p> <p>Tags can be applied to Datastores, Profiles, Fields, Checks, and Anomalies, streamlining data management and improving workflow efficiency. Overall, tags enhance organization, prioritization, and decision-making.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"tags/overview/#what-are-tags-and-why-they-matter","title":"What Are Tags and Why They Matter","text":"<p>A Tag is a reusable label that you can assign to Datastores, Profiles, Fields, Checks, and Anomalies. Tags bring consistency, context, and automation to your data workflows.</p>"},{"location":"tags/overview/#why-we-use-tags","title":"Why We Use Tags","text":"<p>Without tags, managing hundreds of data assets quickly becomes difficult. Tags help you:</p> <ul> <li>Categorize assets logically (e.g., <code>Finance</code>, <code>PII</code>, <code>Deprecated</code>).</li> <li>Identify priorities by applying weight values.</li> <li>Filter views and dashboards for faster navigation.</li> <li>Automate responses in Flows (e.g., alert when a \u201cCritical\u201d check fails).</li> <li>Enforce governance by grouping data by sensitivity or ownership.</li> </ul> <p>In short, Tags help you find what matters faster and act automatically based on context.</p>"},{"location":"tags/overview/#how-tags-work","title":"How Tags Work","text":"<p>Tags can be applied across the full data hierarchy in Qualytics:</p> <ul> <li>Datastore level: Applies to the entire data source and cascades to all related assets.  </li> <li>Container (Table/View): Inherits from the parent datastore and passes tags to its fields and checks.  </li> <li>Field: Reflects any inherited or directly applied tags.  </li> <li>Check: Inherits from the container or datastore; defines context for anomalies.  </li> <li>Anomaly: Inherits tags from the failed check when it\u2019s created.</li> </ul>"},{"location":"tags/overview/#tag-inheritance","title":"Tag Inheritance","text":"<p>Inheritance ensures consistency:</p> <ul> <li>If a tag named Critical is applied to a Datastore, it automatically applies to all its Containers, Fields, and Checks.  </li> <li>When a Check fails, the resulting Anomaly inherits the same Critical tag.  </li> <li>If you remove the Critical tag from the parent datastore, all child assets lose that tag.  </li> <li>However, existing Anomalies keep the tag they inherited when they were created (no retroactive removal).</li> </ul> <p>Note</p> <p>Tag inheritance occurs only downward (from parent to child). Anomalies inherit tags at creation time only \u2014 subsequent tag updates do not propagate automatically.</p>"},{"location":"tags/overview/#real-life-example","title":"Real-Life Example","text":"<p>Imagine your organization manages multiple Datastores \u2014 Customer Data, Transactions, and Logs.</p> <p>Here\u2019s how Tags make this easier:</p> <ul> <li>You apply a PII tag to all fields containing personal data (e.g., email, phone).  </li> <li>You apply a Finance tag to your Transactions datastore, which cascades to all related fields and checks.  </li> <li>You assign a Critical (Weight: 10) tag to checks that monitor payment processing errors.</li> </ul> <p>Now your team can:</p> <ul> <li>Filter anomalies by tag (e.g., view only \u201cCritical\u201d issues).  </li> <li>Trigger Flows for specific tags (e.g., auto-alert the Finance team).  </li> <li>Generate reports grouped by classification (e.g., all PII fields).  </li> </ul> <p>Tags turn scattered data into a structured, actionable map of your ecosystem.</p>"},{"location":"tags/overview/#understanding-weight-modifier","title":"Understanding Weight Modifier","text":"<p>Each tag includes a Weight Modifier \u2014 a numeric value between \u201310 and +10 that represents its relative importance.  </p> Range Purpose Example \u201310 to \u20131 Low priority Deprecated or test data 0 Neutral Informational or general tags +1 to +10 High priority Critical, PII, or Production data"},{"location":"tags/overview/#how-weight-affects-the-system","title":"How Weight Affects the System","text":"<ul> <li>In Dashboards: Higher-weight tags appear first in sorted lists and visualizations.  </li> <li>In Checks: High-weight tags help prioritize anomaly reviews and notifications.  </li> <li>In Flows: Tags can be used to trigger automated actions for higher-priority data.</li> </ul> <p>Note</p> <p>Weight values affect prioritization and filtering, not computation or scoring.</p>"},{"location":"tags/overview/#scope-user-level-or-system-level","title":"Scope: User-Level or System-Level?","text":"<p>Tags in Qualytics are system-wide, not user-specific. Once created, a tag becomes available for all users who have permission to view or apply it.</p>"},{"location":"tags/overview/#types-of-tags","title":"Types of Tags","text":"<ul> <li>Global Tags: Created manually inside Qualytics. Editable by permitted roles and visible to all teams.  </li> <li>External Tags: Imported automatically from integrated catalog systems like Atlan or Alation.   These cannot be edited or deleted in Qualytics and remain read-only.</li> </ul>"},{"location":"tags/overview/#use-cases","title":"Use Cases","text":"Scenario Example Benefit Data Classification Tag all personal data fields with <code>PII</code>. Simplifies privacy compliance checks. Operational Priority Tag high-risk checks as <code>Critical (Weight: 10)</code>. Drives targeted alerts and prioritization. Lifecycle Management Tag outdated datasets as <code>Deprecated</code>. Makes cleanup easier and safer. Automation Configure Flows to run only for <code>Finance</code> tags. Enables targeted workflows."},{"location":"tags/overview/#permissions-and-security","title":"Permissions and Security","text":"<p>Tag permissions are determined by Team Roles in the Qualytics security model.</p>"},{"location":"tags/overview/#permission-matrix-for-tags","title":"Permission Matrix for Tags","text":"<p>Legend: \u2705 \u2192 The role can perform the action \u274c \u2192 The role cannot perform the action  </p> Action Reporter Viewer Drafter Author Editor Create Tag \u274c \u274c \u274c \u274c \u2705 Edit / Delete Tag \u274c \u274c \u274c \u274c \u2705 Apply Existing Tag \u2705 \u2705 \u2705 \u2705 \u2705 View Tag \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"tags/overview/#tags-in-flows","title":"Tags in Flows","text":"<p>Tags can be used in Flow configurations to trigger or filter actions.</p>"},{"location":"tags/overview/#example-use-cases","title":"Example Use Cases","text":"<ul> <li>Run a Flow only for Checks tagged \u201cCritical.\u201d </li> <li>Send Slack alerts for Anomalies tagged \u201cPII.\u201d </li> <li>Trigger Data Export for Datastores tagged \u201cFinance.\u201d</li> </ul> <p>Tags act as metadata filters that determine which entities are included or excluded in automated workflows.</p>"},{"location":"tags/overview/#navigation-to-tags","title":"Navigation to Tags","text":"<p>Step 1: Log in to your Qualytics account and click on the Tags on the left side panel of the interface. </p> <p></p> <p>You will be navigated to the Tags section, where you can view all the tags available in the system.</p> <p></p>"},{"location":"tags/overview/#add-tag","title":"Add Tag","text":"<p>Note</p> <p>For more steps please refer to the add tag documentation.</p>"},{"location":"tags/overview/#applying-a-tag","title":"Applying a Tag","text":"<p>Note</p> <p>For more steps please refer to the applying a tag documentation.</p>"},{"location":"tags/overview/#external-tag","title":"External Tag","text":"<p>Note</p> <p>For more information refer to the external tag</p>"},{"location":"tags/overview/#filter-and-sort","title":"Filter and Sort","text":"<p>Note</p> <p>For more steps please refer to the filter and sort documentation.</p>"},{"location":"tags/overview/#edit-tags","title":"Edit Tags","text":"<p>Note</p> <p>For more steps please refer to the edit tag documentation.</p>"},{"location":"tags/overview/#delete-tags","title":"Delete Tags","text":"<p>Note</p> <p>For more steps please refer to the delete tag documentation.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/","title":"Our Helm chart","text":"<p>Welcome to the Installation Guide for setting up Helm for your Qualytics Single-Tenant Instance. </p> <p>Qualytics is a closed source container-native platform for assessing, monitoring, and ameliorating data quality for the Enterprise. </p> <p>Learn more about our product and capabilities here.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#what-is-qualytics","title":"What is Qualytics?","text":"<p>Important Note for Deployment Type</p> <p>Before proceeding with the installation of Helm for Qualytics Single-Tenant Instance, please note the following:</p> <ul> <li> <p>This installation guide is specifically designed for customer-managed deployments where you manage your own infrastructure.</p> </li> <li> <p>If you are a Qualytics Software as a Service (SaaS) customer, you do not need to perform this installation. The Helm setup is managed by Qualytics for SaaS deployments.</p> </li> </ul> <p>If you are unsure about your deployment type or have any questions, please reach out to your Qualytics account manager for clarification.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#what-is-in-this-chart","title":"What is in this chart?","text":"<p>This chart will deploy a single-tenant instance of the qualytics platform to a CNCF compliant kubernetes control plane.</p> <p> </p> Customer-Managed Deployment Architecture"},{"location":"upgrades/qualytics-single-tenant-instance/#prerequisites","title":"Prerequisites","text":"<p>Before deploying Qualytics, ensure you have:</p> <ul> <li>A Kubernetes cluster (recommended version 1.30+)</li> <li><code>kubectl</code> configured to access your cluster</li> <li><code>helm</code> CLI installed (recommended version 3.12+)</li> <li>Docker registry credentials from your Qualytics account manager</li> <li>Auth0 configuration details from your Qualytics account manager</li> </ul>"},{"location":"upgrades/qualytics-single-tenant-instance/#how-should-i-use-this-chart","title":"How should I use this chart?","text":"<p>Please work with your account manager at Qualytics to secure the right values for your licensed deployment. If you don't yet have an account manager, please write us here to say hello!</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#1-create-a-cncf-compliant-cluster","title":"1. Create a CNCF compliant cluster","text":"<p>Qualytics fully supports kubernetes clusters hosted in AWS, GCP, and Azure as well as any CNCF-compliant control plane.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#node-requirements","title":"Node Requirements","text":"<p>Node(s) with the following labels must be made available:</p> <ul> <li><code>appNodes=true</code></li> <li><code>driverNodes=true</code></li> <li><code>executorNodes=true</code></li> </ul> <p>Nodes with the <code>driverNodes=true</code> and <code>executorNodes=true</code> labels will be used for Spark jobs, while nodes with the <code>appNodes=true</code> label will be used for all other needs. </p> <p>Users have the flexibility to merge the <code>driverNodes=true</code> and <code>executorNodes=true</code> labels into a single label, <code>sparkNodes=true</code>, within the same node group, as long as the provided node group can supply sufficient resources to handle both Spark driver and executors. </p> <p>Alternatively, users may choose not to use node selectors at all, allowing the entire cluster to be used without targeting specific node groups. However, it is highly recommended to set up autoscaling for Apache Spark operations by providing separate node groups with the <code>driverNodes=true</code> and <code>executorNodes=true</code> labels to ensure optimal performance and scalability.</p> Application Nodes Spark Driver Nodes Spark Executor Nodes Label appNodes=true driverNodes=true executorNodes=true Scaling Autoscaling (1 node on-demand) Autoscaling (1 node on-demand) Autoscaling (1 - 12 nodes spot) EKS t3.2xlarge (8 vCPUs, 32 GB) r5.2xlarge (8 vCPUs, 64 GB) r5d.2xlarge (8 vCPUs, 64 GB) GKE n2-standard-8 (8 vCPUs, 32 GB) n2-highmem-8 (8 vCPUs, 64 GB) n2-highmem-8 (8 vCPUs, 64 GB) AKS Standard_D8_v5 (8 vCPUs, 32 GB) Standard_E8s_v5 (8 vCPUs, 64 GB) Standard_E8s_v5 (8 vCPUs, 64 GB)"},{"location":"upgrades/qualytics-single-tenant-instance/#docker-registry-secrets","title":"Docker Registry Secrets","text":"<p>Execute the command below using the credentials supplied by your account manager as a replacement for \"&lt;token&gt;\". The secret created will provide access to Qualytics private registry on dockerhub and the required images that are available there.</p> <pre><code>kubectl create namespace qualytics\nkubectl create secret docker-registry regcred -n qualytics --docker-username=qualyticsai --docker-password=&lt;token&gt;\n</code></pre> <p>Important</p> <p>The above configuration will connect your cluster directly to our private dockerhub repositories for pulling our images. </p> <p>If you are unable to directly connect your cluster to our image repository for technical or compliance reasons, then you can instead import our images into your preferred registry using these same credentials (<code>docker login -u qualyticsai -p &lt;token&gt;</code>). </p> <p>You'll need to update the image URLs in the values.yaml file in the next step to point to your repository instead of ours.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#2-create-your-configuration-file","title":"2. Create your configuration file","text":"<p>For a quick start, copy the simplified template configuration:</p> <pre><code>cp template.values.yaml values.yaml\n</code></pre> <p>The <code>template.values.yaml</code> file contains essential configurations with sensible defaults. You'll need to update these required settings:</p> <ol> <li> <p>DNS Record (provided by Qualytics or managed by customer):</p> <pre><code>global:\n  dnsRecord: \"your-company.qualytics.io\"  # or your custom domain\n</code></pre> </li> <li> <p>Auth0 Settings (provided by your Qualytics account manager):</p> <pre><code>secrets:\n  auth0:\n    auth0_audience: your-api-audience\n    auth0_organization: org_your-org-id\n    auth0_spa_client_id: your-spa-client-id\n</code></pre> </li> <li> <p>Security Secrets (generate secure random values):</p> <pre><code>secrets:\n  auth:\n    jwt_signing_secret: your-secure-jwt-secret\n  postgres:\n    secrets_passphrase: your-secure-passphrase\n  rabbitmq:\n    rabbitmq_password: your-secure-password\n</code></pre> </li> </ol> <p>Optional configurations:</p> <ul> <li>Enable <code>nginx</code> if you need an ingress controller</li> <li>Enable <code>certmanager</code> for automatic SSL certificates</li> <li>Configure <code>controlplane.smtp</code> settings for email notifications</li> <li>Node selectors are now enabled by default for dedicated node groups</li> </ul> <p>For advanced configuration, refer to the full <code>charts/qualytics/values.yaml</code> file which contains all available options.</p> <p>Info</p> <p>Contact your Qualytics account manager for assistance.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#3-deploy-qualytics-to-your-cluster","title":"3. Deploy Qualytics to your cluster","text":"<p>Add the Qualytics Helm repository and deploy the platform:</p> <pre><code># Add the Qualytics Helm repository\nhelm repo add qualytics https://qualytics.github.io/qualytics-helm-public\nhelm repo update\n\n# Deploy Qualytics\nhelm upgrade --install qualytics qualytics/qualytics \\\n  --namespace qualytics \\\n  --create-namespace \\\n  -f values.yaml \\\n  --timeout=20m\n</code></pre> <p>Monitor the deployment:</p> <pre><code># Check deployment status\nkubectl get pods -n qualytics\n</code></pre> <p>Get the ingress IP address:</p> <pre><code># If using nginx ingress\nkubectl get svc -n qualytics qualytics-nginx-controller\n\n# Or check ingress resources\nkubectl get ingress -n qualytics\n</code></pre> <p>Note</p> <p>Note this IP address as it's needed for the next step!</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#4-configure-dns-for-your-deployment","title":"4. Configure DNS for your deployment","text":"<p>You have two options for DNS configuration:</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#option-a-qualytics-managed-dns-recommended","title":"Option A: Qualytics-managed DNS (Recommended)","text":"<p>Send your account manager the IP address from step 3. Qualytics will assign a DNS record under <code>*.qualytics.io</code> (e.g., <code>https://acme.qualytics.io</code>) and handle SSL certificate management.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#option-b-custom-domain","title":"Option B: Custom Domain","text":"<p>If using your own domain:</p> <ol> <li>Create an A record pointing your domain to the ingress IP address</li> <li>Ensure your <code>global.dnsRecord</code> in values.yaml matches your custom domain</li> <li>Configure SSL certificates (enable <code>certmanager</code> or provide your own)</li> <li>Update any firewall rules to allow traffic to your domain</li> </ol> <p>Info</p> <p>Contact your account manager for assistance with either option.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#can-i-run-a-fully-air-gapped-deployment","title":"Can I run a fully \"air-gapped\" deployment?","text":"<p>Yes. The only egress requirement for a standard self-hosted Qualytics deployment is to https://auth.qualytics.io which provides Auth0-powered federated authentication. This is recommended for ease of installation and support, but not a strict requirement. If you require a fully private deployment with no access to the public internet, you can instead configure an OpenID Connect (OIDC) integration with your enterprise identity provider (IdP). Simply contact your Qualytics account manager for more details.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#upgrade-qualytics-helm-chart","title":"Upgrade Qualytics Helm chart","text":"<p>Do you have the Qualytics Helm chart repository locally?</p> <p>Make sure you have the Qualytics Helm chart repository in your local Helm repositories. Run the following command to add them: <pre><code>helm repo add qualytics https://qualytics.github.io/qualytics-helm-public\n</code></pre></p>"},{"location":"upgrades/qualytics-single-tenant-instance/#update-qualytics-helm-chart","title":"Update Qualytics Helm Chart:","text":"<pre><code>helm repo update\n</code></pre> <p>Target Helm chart version?</p> <p>The target Helm chart version must be higher than the current Helm chart version.</p> <p>To see all available Helm chart versions of the specific product run this command:</p> <pre><code>helm search repo qualytics\n</code></pre>"},{"location":"upgrades/qualytics-single-tenant-instance/#upgrade-qualytics-helm-chart_1","title":"Upgrade Qualytics Helm Chart:","text":"<pre><code>helm upgrade --install qualytics qualytics/qualytics --namespace qualytics --create-namespace -f values.yaml --timeout=20m\n</code></pre>"},{"location":"upgrades/qualytics-single-tenant-instance/#monitor-update-progress","title":"Monitor Update Progress:","text":"<p>Monitor the progress of the update by running the following command:</p> <pre><code>kubectl get pods --namespace qualytics --watch\n</code></pre> <p>Watch the status of the pods in real-time. Ensure that the pods are successfully updated without any issues.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#verify-update","title":"Verify Update","text":"<p>Once the update is complete, verify the deployment by checking the pods' status:</p> <pre><code>kubectl get pods --namespace qualytics\n</code></pre> <p>Ensure that all pods are running, indicating a successful update.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#troubleshooting","title":"Troubleshooting","text":""},{"location":"upgrades/qualytics-single-tenant-instance/#common-issues","title":"Common Issues","text":"<p>Pods stuck in Pending state:</p> <ul> <li>Check node resources: <code>kubectl describe nodes</code></li> <li>Verify node selectors match your cluster labels</li> <li>Ensure storage classes are available</li> </ul> <p>Image pull errors:</p> <ul> <li>Verify Docker registry secret: <code>kubectl get secret regcred -n qualytics -o yaml</code></li> <li>Check if images are accessible from your cluster</li> </ul> <p>Ingress not working:</p> <ul> <li>Ensure an ingress controller is installed and running</li> <li>Check ingress resources: <code>kubectl describe ingress -n qualytics</code></li> </ul>"},{"location":"upgrades/qualytics-single-tenant-instance/#useful-commands","title":"Useful Commands","text":"<pre><code># Check all resources\nkubectl get all -n qualytics\n\n# Restart a deployment\nkubectl rollout restart deployment/qualytics-api -n qualytics\nkubectl rollout restart deployment/qualytics-cmd -n qualytics\n\n# View detailed pod information\nkubectl describe pod &lt;pod-name&gt; -n qualytics\n\n# Get spark application logs\nkubectl logs -f pod qualytics-spark-driver -n qualytics\n</code></pre>"},{"location":"weight/weighting/","title":"Weight Mechanism","text":"<p>Weight Mechanism for checks is designed to evaluate and prioritize checks based on three key factors: Rule Type Weighting, Anomaly Weighting, and Tag Weighting.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"weight/weighting/#1-rule-type-weighting","title":"1. Rule Type Weighting","text":"<p>Each quality check rule type has a specific weight based on its importance. The rule types are divided into three categories:</p>"},{"location":"weight/weighting/#high-importance-weight-3","title":"High Importance (Weight: 3)","text":"<p>These rules are assigned the highest weight of 3 to reflect their crucial role in maintaining data quality.</p> No. Rule Type Weight 1 Entity Resolution 3 2 Expected Schema 3 3 Matches Pattern 3 4 Predicted By 3 5 Satisfies Expression 3 6 Contains Social Security Number 3 7 Time Distribution Size 3 8 User Defined Function 3 9 Is Replica Of (is sunsetting) 3 10 Metric 3 11 Aggregation Comparison 3 12 Is Address 3 14 Data Diff 3"},{"location":"weight/weighting/#medium-importance-weight-2","title":"Medium Importance (Weight: 2)","text":"<p>These rules are assigned the medium weight of 2 to reflect their role in maintaining data quality.</p> No. Rule Type Weight 1 Any Not Null 2 2 Between 2 3 Between Times 2 4 Contains Credit Card 2 5 Contains Email 2 6 Equal To 2 7 Equal To Field 2 8 Exists In 2 9 Not Exists In 2 10 Expected Values 2 11 Greater Than Field 2 12 Less Than Field 2 13 Not Future 2 14 Required Values 2 15 Unique 2 16 Contains URL 2 17 Min Partition Size 2 18 Is Credit Card 2 19 Volumetric 2"},{"location":"weight/weighting/#low-importance-weight-1","title":"Low Importance (Weight: 1)","text":"<p>These rules are assigned the lowest weight of 1 to reflect their role in maintaining data quality.</p> No. Rule Type Weight 1 After Date Time 1 2 Before DateTime 1 3 Distinct Count 1 4 Field Count 1 5 Is Type 1 6 Max Length 1 7 Max Value 1 8 Min Length 1 9 Min Value 1 10 Not Exists In 1 11 Not Negative 1 12 Not Null 1 13 Positive 1 14 Sum 1"},{"location":"weight/weighting/#2-anomaly-weighting","title":"2. Anomaly Weighting","text":"<p>Anomalies can impact the importance of a check by adjusting its weight. The adjustment is based on whether the check has anomalies and whether it is authored or inferred:</p> <ol> <li> <p>Authored Check with Anomalies: - The check's weight increases by 12 points.</p> </li> <li> <p>Authored Check without Anomalies: - The check's weight increases by 9 points.</p> </li> <li> <p>Inferred Check with Anomalies: - The check's weight increases by 6 points.</p> </li> <li> <p>Inferred Check without Anomalies: - The check's weight remains 0 points.</p> </li> </ol>"},{"location":"weight/weighting/#3-tag-weighting","title":"3. Tag Weighting","text":"<p>Tags can further modify the weight of a check. When tags with weight modifiers are applied, their weights are added to the check\u2019s total weight.</p> <ul> <li>Tag with Weight Modifier: Each tag that has a specific weight modifier will contribute to the overall weight of the check. For example, if Tag B has a weight of 2, it will add 2 points to the total weight of the check.</li> </ul>"},{"location":"weight/weighting/#example-of-weight-calculation","title":"Example of Weight Calculation","text":"<p>Let's break down an example calculation for a check of type Authored, using the isCreditCard rule (Medium Importance), with no anomalies, and Tag B applied:</p>"},{"location":"weight/weighting/#step-by-step-calculation","title":"Step-by-Step Calculation","text":"<ul> <li>Step 1: Rule Type Weight \u2013 The isCreditCard rule has a weight of 2 (Medium Importance).</li> <li>Step 2: Anomaly Weight \u2013 An Authored Check without anomalies adds 9 points.</li> <li>Step 3: Tag Weight \u2013 Tag B adds 2 points.</li> </ul> <p>Total Weight = 2 (rule type) + 9 (no anomalies) + 2 (Tag B) = 13 points</p>"},{"location":"weight/weighting/#additional-notes","title":"Additional Notes","text":"<p>If the table itself has a Tag A with a weight of 10, the check will inherit that tag. In this case, the total weight will include both tag weights.</p> <p>Total Weight = 2 (rule type) + 9 (no anomalies) + 2 (Tag B) + 10 (Tag A) = 23 points</p>"},{"location":"weight/weighting/#quick-calculation-formula","title":"Quick Calculation Formula","text":"<p>To make the calculation easier, here are the quick formulas for different types of checks:</p> <ul> <li> <p>For Authored Checks with Anomalies: <code>[Rule Type Weight] + 12 (Anomaly) + Check\u2019s Tag Weight + Table\u2019s Tag Weight</code></p> </li> <li> <p>For Authored Checks without Anomalies: <code>[Rule Type Weight] + 9 (No Anomaly) + Check\u2019s Tag Weight + Table\u2019s Tag Weight</code></p> </li> <li> <p>For Inferred Checks with Anomalies: <code>[Rule Type Weight] + 6 (Anomaly) + Check\u2019s Tag Weight + Table\u2019s Tag Weight</code></p> </li> <li> <p>For Inferred Checks without Anomalies: <code>[Rule Type Weight] + 0 (No Anomaly) + Check\u2019s Tag Weight + Table\u2019s Tag Weight</code></p> </li> </ul>"},{"location":"weight/weighting/#example-calculation-extended","title":"Example Calculation (Extended)","text":"<p>Let's extend the example with the inclusion of both Tag A and Tag B:</p> <ul> <li> <p>For Authored Checks with Anomalies: <code>[Rule Type Weight] + 12 + 10 (Tag A) + 2 (Tag B)</code></p> </li> <li> <p>For Authored Checks without Anomalies: <code>[Rule Type Weight] + 9 + 10 (Tag A) + 2 (Tag B)</code></p> </li> <li> <p>For Inferred Checks with Anomalies: <code>[Rule Type Weight] + 6 + 10 (Tag A) + 2 (Tag B)</code></p> </li> <li> <p>For Inferred Checks without Anomalies: <code>[Rule Type Weight] + 0 + 10 (Tag A) + 2 (Tag B)</code></p> </li> </ul>"}]}