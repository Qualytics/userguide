{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to Qualytics","text":"<p>Qualytics is your Active Data Quality Platform that empowers teams to manage data quality at scale through advanced automation. By analyzing the shapes and patterns in your historical data, Qualytics infers contextual data quality rules that actively monitor new data, including incremental loads, to identify anomalies. When issues arise, Qualytics provides your team with everything needed to take corrective actions using your existing data tools and preferred monitoring solutions.</p>"},{"location":"#managing-data-quality","title":"Managing Data Quality","text":"<p>Qualytics helps your data teams proactively address data issues by automating the discovery and maintenance of essential data quality measures.</p> <p>Here's how it works:</p> <ol> <li> <p>Analyzing Historical Data: Qualytics examines your existing data to understand its patterns and characteristics, creating a comprehensive set of rules that define good data quality.</p> </li> <li> <p>Finding Anomalies: These automatically inferred rules, along with any custom rules you create, work together to identify abnormalities in both historical and new data, even as it arrives incrementally.</p> </li> <li> <p>Taking Corrective Actions: When Qualytics detects an anomaly, it springs into action. Using tags, it can:</p> <ul> <li>Send notifications through your preferred platforms (Teams, Slack, PagerDuty)</li> <li>Trigger workflows in your tools (Airflow, Fivetran, Airbyte)</li> <li>Provide detailed anomaly information to your chosen datastore</li> <li>Suggest optimal solutions through its intuitive interface and API</li> </ul> </li> <li> <p>Continuous Monitoring and Improvement: Qualytics maintains constant vigilance over your data quality, automatically adapting quality checks to reflect changes in your data and business needs. This ongoing process strengthens data quality and builds confidence in your organization's data assets.</p> </li> </ol>"},{"location":"#key-features","title":"Key Features","text":"<p>Qualytics delivers powerful capabilities designed to transform your data quality management:</p> <ol> <li> <p>Automated Data Profiling: Qualytics creates comprehensive profiles of your data assets automatically, providing deep insights that form the foundation of robust data quality management.</p> </li> <li> <p>Rule Inference: Say goodbye to the challenge of manually crafting and maintaining data quality rules at scale. Qualytics automatically infers appropriate rules based on your data profiles, saving time while ensuring precise anomaly detection.</p> </li> <li> <p>Anomaly Detection: Detect data irregularities both at rest and in flight throughout your data ecosystem. Qualytics excels at highlighting outliers and anomalies, helping you maintain high data quality standards.</p> </li> <li> <p>Anomaly Remediation: When issues emerge, Qualytics seamlessly integrates with your preferred tools to enable swift corrective actions through automated workflows.</p> </li> <li> <p>Freshness Monitoring: Keep your data current with built-in monitoring of data freshness Service Level Agreements (SLAs). Define and track timeliness requirements to ensure your data meets critical business needs.</p> </li> <li> <p>Insights Dashboard: Access a clear, intuitive executive dashboard that provides a holistic view of your data health and quality. Visualize key metrics, track progress, and derive actionable insights to drive data-driven strategies.</p> </li> </ol>"},{"location":"#seamless-integration-and-deployment","title":"Seamless Integration and Deployment","text":"<p>Qualytics adapts to your infrastructure with flexible integration options:</p> <ul> <li> <p>Deployment Options: Choose the deployment model that works best for you - on-premise, single-tenant cloud, or SaaS. Qualytics meets you where your data lives.</p> </li> <li> <p>Support for Modern &amp; Legacy Data Stacks: Whether you use modern solutions like Snowflake and Amazon S3 or legacy systems like Oracle and MSSQL, Qualytics seamlessly integrates with your entire data stack to maintain quality across all sources.</p> </li> </ul>"},{"location":"#demo","title":"Demo","text":"<p>Here is a short video demonstrating the platform with a quick walkthrough:</p>"},{"location":"#embarking-on-your-journey","title":"Embarking on Your Journey","text":"<p>This user guide will walk you through Qualytics' key capabilities with clear, step-by-step instructions. Whether you're new to the platform or looking to deepen your expertise, we're here to help you optimize your data quality management journey.</p> <p>Let's begin empowering your organization with accurate, reliable, and trustworthy data using Qualytics!</p>"},{"location":"404/","title":"404 - Page Not Found","text":"<p>Oops! The page you're looking for doesn't exist.</p>"},{"location":"404/#what-happened","title":"What happened?","text":"<p>The page you requested could not be found. This might happen because:</p> <ul> <li>The URL was typed incorrectly</li> <li>The page has been moved or deleted</li> <li>The link you followed is broken</li> </ul>"},{"location":"404/#what-can-you-do","title":"What can you do?","text":"<ul> <li>Go back to the homepage</li> <li>Use the search function to find what you're looking for</li> <li>Check the navigation menu for available sections</li> <li>Contact support if you think this is an error</li> </ul> <p>Need help? Contact our support team</p>"},{"location":"changelog-2023/","title":"2023","text":""},{"location":"changelog-2023/#release-notes","title":"Release Notes","text":""},{"location":"changelog-2023/#2023.12.20","title":"2023.12.20","text":""},{"location":"changelog-2023/#general-fixes","title":"General Fixes","text":"<ul> <li> <p>Resolved Datastore Creation Issue with Databricks:</p> <ul> <li>Fixed an issue encountered when creating source datastores using Databricks with catalog names other than the default <code>hive_metastore</code>. This fix ensures a smoother and more flexible datastore creation process in Databricks environments.</li> </ul> </li> <li> <p>Conflict Resolution for 'anomaly_uuid' Field in Source Container:</p> <ul> <li>Corrected a problem where source containers with a field named <code>anomaly_uuid</code> were unable to run scan operations. This fix eliminates the conflict with internal system columns, allowing for uninterrupted operation of these containers.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2023/#2023.12.14","title":"2023.12.14","text":""},{"location":"changelog-2023/#feature-enhancements","title":"Feature Enhancements","text":"<ul> <li> <p>Auto-Detection of Partitioned Files:</p> <ul> <li>Improved file handling to automatically detect partitioned files like <code>*.delta</code> without the need for an explicit extension. This update resolves the issue of previously unrecognized delta tables.</li> </ul> </li> <li> <p>Anomaly Weight Threshold for Notifications:</p> <ul> <li>Enhanced the notification system to support a minimum anomaly weight threshold for the trigger type \"An anomaly is detected\". Notifications will now be triggered only for anomalies that meet or exceed the defined weight threshold.</li> </ul> </li> <li> <p>Team Assignment in Datastore Forms:</p> <ul> <li>Updated the Datastore Forms to enable users to manage teams. This enhancement provides Admins with the flexibility to assign or adjust teams right at the point of datastore setup, moving away from the default assignment to the Public team.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_1","title":"General Fixes","text":"<ul> <li> <p>Corrected Health Page Duplication:</p> <ul> <li>Addressed an issue on the Health Page where \"Max Executors\" information was being displayed twice. This duplication has been removed for clearer and more accurate reporting.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2023/#2023.12.12","title":"2023.12.12","text":""},{"location":"changelog-2023/#feature-enhancements_1","title":"Feature Enhancements","text":"<ul> <li>Incremental Catalog Results Posting:<ul> <li>Enhanced the catalog operation to post results incrementally for each container catalogued. Previously, results were only available after the entire operation was completed. With this enhancement, results from successfully catalogued containers are now preserved and posted incrementally, ensuring containers identified are not lost even if the operation does not complete successfully.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_2","title":"General Fixes","text":"<ul> <li> <p>Aggregation Comparison Rule Filter:</p> <ul> <li>Resolved an issue where filters were not being applied to the Aggregation Comparison Check, affecting both the reference and target filters.</li> </ul> </li> <li> <p>Case Sensitivity File Extension Support</p> <ul> <li>Addressed a limitation in handling file extensions, ensuring that uppercase formats like .TXT and .CSV are now correctly recognized and processed. This update enhances the system's ability to handle files consistently, irrespective of extension case.</li> </ul> </li> <li> <p>SLA Violation Notification Adjustment:</p> <ul> <li>Modified the SLA violation notifications to trigger only once per violation, preventing a flood of repetitive alerts and improving the overall user experience.</li> </ul> </li> <li> <p>Source record not Available for Max Length Rule</p> <ul> <li>Addressed a bug where the Max Length Rule was not producing source records in cases involving null values. The rule has been updated to correctly handle null values, ensuring accurate anomaly marking and data enrichment.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2023/#2023.12.8","title":"2023.12.8","text":""},{"location":"changelog-2023/#breaking-changes","title":"Breaking Changes","text":"<ul> <li> <p>Renaming of Enrichment Datastore Tables</p> <p>Due to lack of consistency and to avoid conflicts between different categories of Enrichments tables, changes were performed to the table name patterns:</p> <ul> <li>The Enrichment table previously named <code>&lt;enrichment_prefix&gt;_anomalies</code> has been renamed to <code>&lt;enrichment_prefix&gt;_failed_checks</code> due to its content and granularity.</li> <li>The terms <code>remediation</code> and <code>export</code> were added to distinguish Enrichment Remediation and Export tables from others, resulting in:<ul> <li><code>&lt;enrichment_prefix&gt;_remediation_&lt;container_name&gt;</code> for Remediation tables.</li> <li><code>&lt;enrichment_prefix&gt;_export_&lt;asset&gt;</code> for Export tables.</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog-2023/#feature-enhancements_2","title":"Feature Enhancements","text":"<ul> <li>Refactor Notifications Panel:<ul> <li>Introduced a new side panel for Notifications, categorizing alerts by type (Operations, Anomalies, SLA) for improved organization.</li> <li>Added notification tags, receivers, and an action menu enabling users to mute or edit notifications directly from the panel</li> <li>Enhanced UI for better readability and interaction, providing an overall improved user experience.</li> </ul> </li> <li>Add Enrichment Export Anomalies available asset: <ul> <li>Anomalies are now supported as a type of asset for export to an enrichment datastore, enhancing data export capabilities.</li> </ul> </li> <li>Add files count metric to profile operation summary <ul> <li>Displayed file count (number of partitions) in addition to existing file patterns count metric in profile operations for DFS datastores.</li> </ul> </li> <li>Improve Globing Logic:<ul> <li>Optimized support for multiple subgroups when globing files from DFS datastores during profile operations, enhancing efficiency.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_3","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.12.5","title":"2023.12.5","text":""},{"location":"changelog-2023/#feature-enhancements_3","title":"Feature Enhancements","text":"<ul> <li>Navigation Improvements in Explore Profiles Page:<ul> <li>Upgraded the Explore Profiles Page by adding direct link icons for more precise navigation. Users can now use these links on container and field cards/lists for a direct redirection to detailed views.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_4","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.12.1","title":"2023.12.1","text":""},{"location":"changelog-2023/#feature-enhancements_4","title":"Feature Enhancements","text":"<ul> <li> <p>List View Layout Support:</p> <ul> <li>Introduced list view layouts for Datastores, Profiles, Checks, and Anomalies, providing users with an alternative way to display and navigate through their data.</li> </ul> </li> <li> <p>Bulk Acknowledgement Performance:</p> <ul> <li>Improved the performance of bulk acknowledging in-app notifications, streamlining the user experience and enhancing the application's responsiveness.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_5","title":"General Fixes","text":"<ul> <li> <p>Checks and Anomalies Dialog Navigation:</p> <ul> <li>Resolved an issue with arrow key navigation in Checks and Anomalies dialogs where unintended slider movement occurred when using keyboard navigation. This fix ensures that arrow keys will only trigger slider navigation when the dialog is the main focus.</li> </ul> </li> <li> <p>Profiled Container Count Inconsistency</p> <ul> <li>Ensured that containers that fail to load data during profiling are not mistakenly counted as successfully profiled, improving the accuracy of the profiling process.</li> </ul> </li> <li> <p>Histogram Field Selection Update:</p> <ul> <li>Fixed a bug where histograms were not updating correctly when navigating to a new field. Histograms now properly reflect the data of the newly selected field.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2023/#2023.11.28","title":"2023.11.28","text":""},{"location":"changelog-2023/#feature-enhancements_5","title":"Feature Enhancements","text":"<ul> <li> <p>Operations with Tag Selectors:</p> <ul> <li>Users can now configure operations (including schedules) with multiple tags, enabling dynamic profile evaluation based on tags at the operation's trigger time.</li> </ul> </li> <li> <p>Asserted State Filter for Checks:</p> <ul> <li>Introduced a new check list filter, allowing users to filter checks by those that have passed or identified active anomalies.</li> </ul> </li> <li> <p>Bulk Delete for Profiles:</p> <ul> <li>Enhanced the system to allow bulk deletion of multiple profiles, streamlining the management process where previously only individual deletions were possible.</li> </ul> </li> <li> <p>Resizable Columns in Source Records Table:</p> <ul> <li>Columns in the anomaly dialog source records can now be manually resized, improving visibility and preventing content truncation.</li> </ul> </li> <li> <p>Automated Partition Field Setting for BigQuery:</p> <ul> <li>For BigQuery tables constrained by a required partition filter, the profile partition field setting is now automatically populated during the Catalog operation.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_6","title":"General Fixes","text":"<ul> <li> <p>Sharable Link Authentication Flow:</p> <ul> <li>Fixed an issue where direct links did not work if the user was not signed in. Now, users are redirected to the intended page post-authentication.</li> </ul> </li> <li> <p>Clarified Violation Messages for 'isUnique' Check:</p> <ul> <li>Updated the violation message for the 'isUnique' check to describe the anomaly, reducing misinterpretation clearly.</li> </ul> </li> <li> <p>Access Restriction and Loading Fix for Health Page:</p> <ul> <li>Corrected the health page visibility so only admin users can view it, and improved loading behavior for Qualytics services.</li> </ul> </li> <li> <p>Availability of Requested Tables During Operations:</p> <ul> <li>The dialog displaying requested tables/files is now accessible immediately after an operation starts, enhancing transparency for both Profile and Scan operations.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2023/#2023.11.14","title":"2023.11.14","text":""},{"location":"changelog-2023/#feature-enhancements_6","title":"Feature Enhancements","text":"<ul> <li>Qualytics App Color Palette and Design Update:<ul> <li>Implemented a comprehensive design update across the Qualytics App, introducing a new color palette for a refreshed and modern look. This update includes a significant change to the anomalies color, transitioning from red to orange for a more distinct visual cue. Additionally, the font-family has been updated to enhance readability and provide a more cohesive aesthetic experience across the application.</li> </ul> </li> <li>System Health Readout:<ul> <li>A new <code>Health</code> tab has been added to the Admin menu, offering a comprehensive view of each deployment's operational status. This feature encompasses critical details such as the status of app services, current app version, and analytics engine information, enabling better control over system health.</li> </ul> </li> <li>Enhanced Check with Metadata Input:<ul> <li>The Check form now includes a new input field for custom metadata. This enhancement allows users to add key-value pairs for tailored metadata, significantly increasing the flexibility and customization of the Check definition.</li> </ul> </li> <li>Responsiveness Improvement in Cards Layout:<ul> <li>The Cards layout has been refined to improve responsiveness and compactness. This adjustment addresses previous UI inconsistencies and ensures a consistent visual experience across different devices, enhancing overall usability and aesthetic appeal.</li> </ul> </li> <li>Source Record Enrichment for 'isUnique' Checks:<ul> <li>The <code>isUnique</code> check has been enhanced to support source record enrichment. This significant update allows users to view specific records that fail to meet the 'isUnique' condition. This feature adds a layer of transparency and detail to data validation processes, enabling users to easily identify and address data uniqueness issues.</li> </ul> </li> <li>New Enrichment Data:<ul> <li>Scan operations now record operation metadata in a new enrichment table with the suffix <code>scan_operations</code> including an entry for each table/file scanned with the number of records processed and anomalies identified as well as start/stop time and other relevant details. </li> </ul> </li> <li>Insights Enhancement with Check Pass/Fail Metrics:<ul> <li>Insights now features the checks section with new metrics indicating the total number of checks passed and failed. This enhancement also offers a visual representation through a chart, detailing the passed and failed checks over a specified reporting period.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_7","title":"General Fixes","text":"<ul> <li><code>isAddress</code> now supports defining multiple checks against the same field with different required label permutations</li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.11.8","title":"2023.11.8","text":""},{"location":"changelog-2023/#feature-enhancements_7","title":"Feature Enhancements","text":"<ul> <li> <p>Is Address Check:</p> <ul> <li>Introduced a new check for address conformity that ensures the presence of required components such as road, city, and state, enhancing data quality controls for address fields. This check leverages machine learning to support multilingual street address parsing/normalization trained on over 1.2 billion records of data from over 230 countries, in 100+ languages. It achieves 99.45% full-parse accuracy on held-out addresses (i.e. addresses from the training set that were purposefully removed so we could evaluate the parser on addresses it hasn\u2019t seen before).</li> </ul> </li> <li> <p>Revamped Heatmap Flow in Activity Tab:</p> <ul> <li>Improved the user interaction with the heatmap by filtering the operation list upon selecting a date. A new feature has been added to operation details allowing users to view comprehensive information about the profiles scanned, with the ability to drill down to partitions and anomalies.</li> </ul> </li> <li> <p>Link to Schedule in Operation List:</p> <ul> <li>Enhanced the operation list with a new \"Schedule\" column, providing direct links to the schedules triggering the operations, thus improving traceability and scheduling visibility.</li> </ul> </li> <li> <p>Insights Tag Filtering Improvement:</p> <ul> <li>Enhanced the tag filtering capability on the Insights page to now include table/file-level analysis. This ensures a more granular and accurate reflection of data when using tags to filter insights.</li> </ul> </li> <li> <p>Support for Incremental Scanning of Partitioned Files:</p> <ul> <li>Optimized the incremental scanning process by tracking changes at the record level rather than the last modified timestamp of the folder. This enhancement prevents the unnecessary scanning of all records and focuses on newly added data.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_8","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.11.2","title":"2023.11.2","text":""},{"location":"changelog-2023/#feature-enhancements_8","title":"Feature Enhancements","text":"<ul> <li> <p>Auto Selection of All Fields in Check Form:</p> <ul> <li>Improved the user experience in the Check Form by introducing a \"select all\" option for fields. Users can now auto-select all fields when applying rules that expects a multi select input, streamlining the process especially for profiles with a large number of fields.</li> </ul> </li> <li> <p>Enhanced Profile Operations with User-Defined Starting Points for Profiling:</p> <ul> <li>Users can now specify a value for the incremental identifier, to determine the comprehensive set that will be analyzed.</li> <li>Two new options have been added:<ul> <li>Greater Than Time: Targets profiles with incremental timestamp strategies, allowing the inclusion of rows where the incremental field's value surpasses a specified time threshold.</li> <li>Greater Than Batch: Tailored for profiles employing an incremental batch strategy, focusing the analysis on rows where the incremental field\u2019s value is beyond a certain numeric threshold.</li> </ul> </li> </ul> </li> <li> <p>Configurable Enrichment Source Record Limit in Scan Operations:</p> <ul> <li>Users can now configure the <code>enrichment_source_record_limit</code> to dictate the number of anomalous records retained for analysis, adapting to various use case necessities beyond the default sample limit of 10 per anomaly. This improvement allows for a more tailored and comprehensive analysis based on user requirements.</li> </ul> </li> <li> <p>Introduction of Passed Status in Check Card:</p> <ul> <li>A new indicative icon has been added to the Check Card to assure users of a \"passed\" status based on the last scan. This icon will be displayed only when there are no active anomalies.</li> </ul> </li> <li> <p>Inclusion of Last Asserted Time in Check Card:</p> <ul> <li>Enhanced the Check Card by including the last asserted time, offering users more detailed and up-to-date information regarding the checks.</li> </ul> </li> <li> <p>Enhanced Anomaly Search with UUID Support:</p> <ul> <li>Improved the anomaly search functionality by enabling users to search anomalies using the UUID of the anomaly, making the search process more flexible and comprehensive.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_9","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.10.27","title":"2023.10.27","text":""},{"location":"changelog-2023/#feature-enhancements_9","title":"Feature Enhancements","text":"<ul> <li> <p>Check Creation through Field Details Page:</p> <ul> <li>Users can now initiate check creation directly from the Field Details page, streamlining the check creation process and improving usability.</li> </ul> </li> <li> <p>Tree View Enhancements:</p> <ul> <li>Introduced a favorite group feature where favorite datastores are displayed in a specific section, making them quicker and easier to access.</li> <li>Added search functionalities at both Profile and Field levels to improve the navigation experience.</li> <li>Nodes now follow the default sorting of pages, creating consistency across various views.</li> <li>Enhanced the descriptions in tree view nodes for non-catalogued datastores and non-profiled profiles, providing a clearer explanation for the absence of sub-items.</li> </ul> </li> <li> <p>Bulk Actions for Freshness &amp; SLAs:</p> <ul> <li>Users can now perform bulk actions in Freshness &amp; SLAs, enabling or disabling freshness tracking and setting or unsetting SLAs for profiles efficiently.</li> </ul> </li> <li> <p>Archived Check Details Visualization:</p> <ul> <li>Enhanced the anomaly modal to allow users to view the details of archived checks in a read-only mode, improving the visibility and accessibility of archived checks\u2019 information.</li> </ul> </li> <li> <p>User Pictures as Avatars:</p> <ul> <li>User pictures have been incorporated across the application as avatars, enhancing the visual representation in user listings, teams, and anomaly comments.</li> </ul> </li> <li> <p>Slide Navigation in Card Dialogs:</p> <ul> <li>Introduced a slide navigation feature in the Anomalies and Checks dialogs, enhancing user navigation. Users can now effortlessly navigate between items using navigational arrows, eliminating the need to close the dialog to view next or previous items.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_10","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.10.23","title":"2023.10.23","text":""},{"location":"changelog-2023/#feature-enhancements_10","title":"Feature Enhancements","text":"<ul> <li> <p>Enhanced Data Asset Navigation:</p> <ul> <li>Tree View Implementation: Easily navigate through your data assets with our new organized tree view structure</li> <li>Context-Specific Actions: Access settings and actions that matter most depending on your current level of interaction.</li> <li>Simplified User Experience: This update is designed to streamline and simplify your data asset navigation and management.</li> </ul> </li> <li> <p>Aggregation Comparison Check:</p> <ul> <li>New Rule Added: Ensure valid comparisons by checking the legitimacy of operators between two aggregation expressions. </li> <li>Improved Monitoring: Conduct in-depth comparisons, such as verifying if total row counts match across different source assets.</li> </ul> </li> <li> <p>Efficient Synchronization for Schema Changes:</p> <ul> <li>Seamless Integration: Our system now adeptly synchronizes schema changes in source datastores with Qualytics profiles. </li> <li>Avoid Potential Errors: We reduced the risk of creating checks with fields that have been removed or altered in the source datastore.</li> </ul> </li> <li> <p>Clarity in Quality Check Editors:</p> <ul> <li>Distinct Update Sources: Easily identify if an update was made manually by a user or automatically through the API.</li> </ul> </li> <li> <p>Dynamic Quality Score Updates:</p> <ul> <li>Live Anomaly Status Integration: Quality Scores now reflect real-time changes based on anomaly status updates.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_11","title":"General Fixes","text":"<ul> <li>Various bug fixes and system improvements for a smoother experience.</li> </ul>"},{"location":"changelog-2023/#2023.10.13","title":"2023.10.13","text":""},{"location":"changelog-2023/#feature-enhancements_11","title":"Feature Enhancements","text":"<ul> <li> <p>Export Metadata Enhancements:</p> <ul> <li>Added a \"weight\" property to the quality check asset</li> </ul> </li> <li> <p>New AWS Athena Connector:</p> <ul> <li>Introduced support for a new connector, AWS Athena, expanding the options and flexibility for users managing data connections.</li> </ul> </li> <li> <p>Operations List:</p> <ul> <li>Introduced a multi-select filter to the operation list, enabling users to efficiently view operations based on their status such as running, success, failure, and warning, thereby streamlining navigation and issue tracking.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_12","title":"General Fixes","text":"<ul> <li>Logging Adjustments:<ul> <li>Enhanced logging for catalog operations, ensuring that logs are visible and accessible even for catalogs with a warning status, facilitating improved tracking and resolution of issues.</li> </ul> </li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.10.9","title":"2023.10.9","text":""},{"location":"changelog-2023/#feature-enhancements_12","title":"Feature Enhancements","text":"<ul> <li> <p>Check Categorization:</p> <ul> <li>Introduced new check categories on the checks page to streamline UX and prioritize viewing:<ol> <li>Important: Designed around a check's weight value, this category will by default comprise authored checks and inferred checks with active anomalies.</li> <li>Favorite: Featuring all user-favorited checks</li> <li>Metrics: Incorporating all metric checks</li> <li>All: Displaying all checks, whether inferred, authored, or anomalous</li> </ol> </li> <li>The default view is set to \"Important\" (if available) to highlight critical checks and avoid overwhelming users</li> </ul> </li> <li> <p>Anomalies Page Update:</p> <ul> <li>Revamped the Anomalies page with a simplified status filter, adopting a design in alignment with the checks page:<ul> <li>Quick Status Filter: Facilitates an effortless switch between anomaly statuses.</li> <li>The \"Active\" tab is presented as the default, providing immediate visibility into ongoing anomalies.</li> </ul> </li> </ul> </li> <li> <p>Notification Testing:</p> <ul> <li>Enhanced the Notification Form with a \"Test Notification\" button, enabling users to validate notification settings before saving</li> </ul> </li> <li> <p>Metadata Export to Enrichment Stores:</p> <ul> <li>Enabled users to export metadata from their datastore directly into enrichment datastores, with initial options for quality checks and field profiles.</li> <li>Users can specify which profiles to include in the export operation, ensuring relevant data transfer.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_13","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.10.4","title":"2023.10.4","text":""},{"location":"changelog-2023/#feature-enhancements_13","title":"Feature Enhancements","text":"<ul> <li> <p>Anomalies Details User Experience:</p> <ul> <li>Implemented a \"skeleton loading\" feature in the Anomaly Details dialog, enhancing user feedback during data loading.</li> </ul> </li> <li> <p>Enhanced Check Dialog:</p> <ul> <li>Added \"Last Updated\" date to the Check Dialog to provide users with additional insights regarding check modifications.</li> </ul> </li> <li> <p>API Engine Control:</p> <ul> <li>Exposed a new endpoint allowing users to gracefully restart the analytics engine through the API.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_14","title":"General Fixes","text":"<ul> <li>Timezone Handling on MacOS:<ul> <li>Resolved an issue affecting timezone retrieval due to MacOS privacy updates, ensuring accurate timezone handling.</li> </ul> </li> <li>Notifications and Alerts:<ul> <li>Pager Duty Integration: Resolved issues preventing message sending and improved UI for easier configuration.</li> <li>HTTP Action Notification: Fixed Anomaly meta-data serialization issues affecting successful delivery in some circumstances.</li> </ul> </li> <li>Scan Duration Accuracy:<ul> <li>Adjusted scan duration calculations to accurately represent the actual processing time, excluding time between a failed scan and a successful retry.</li> </ul> </li> <li>Spark Partitioning:<ul> <li>Certain datastores may fail to properly coerce types into Spark-compatible partition column values if that column itself contains anomalous values. When this occurs, an attempt will be made to load the data without a partition column and a warning will be generated for the user.</li> </ul> </li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.9.29","title":"2023.9.29","text":""},{"location":"changelog-2023/#feature-enhancements_14","title":"Feature Enhancements","text":"<ul> <li> <p>Operations &amp; Schedules UI Update:</p> <ul> <li>Redesigned the UI for the operations and schedules lists for a more intuitive UX and to provide additional information.<ul> <li>Introduced pagination, filtering, and sorting for the schedules list.</li> <li>Added a \"Next Trigger\" column to the schedules list to inform users of upcoming schedule triggers.</li> </ul> </li> <li>Improved Profile List Modal:<ul> <li>Enhanced the profile list modal accessible from operations and schedules.</li> <li>Users can now search by both ID and profile name.</li> </ul> </li> </ul> </li> <li> <p>Check Navigation Enhancements:</p> <ul> <li>Enhanced navigation between Standard and Metric Cards by introducing direct links that allow users to access metric charts seamlessly from check forms.</li> <li>The checks page navigation state is now reflected in the URL, enhancing UX and enabling precise redirect capabilities.</li> </ul> </li> <li> <p>Computed Table Enhancements:</p> <ul> <li>Upon the creation or update of a computed table, a minimalistic profile operation is now automatically triggered. This basic profile limits sampling to 1,000 and does not infer quality checks.</li> <li>This enhancement streamlines the process when working with computed tables. Users can now directly create checks after computed table creation without manually initiating a profile operation, as the system auto-fetches required field data types.</li> </ul> </li> <li> <p>Analytics Engine Enhancements:</p> <ul> <li>This release replaces our previous consistency model with a more robust one relying upon AMQP brokered durable messaging. The change dramatically improves Qualytics' internal fault tolerance with accompanying performance enhancements for common operations.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_15","title":"General Fixes","text":"<ul> <li>Insights Filter Consistency:<ul> <li>Fixed an inconsistency issue with the datastore filter that was affecting a couple of charts in Insights</li> </ul> </li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.9.21","title":"2023.9.21","text":""},{"location":"changelog-2023/#feature-enhancements_15","title":"Feature Enhancements","text":"<ul> <li> <p>Anomalies Modal Redesign:</p> <ul> <li>Streamlined the presentation of Failed Checks by removing the Anomalous Fields grouping. The new layout focuses on a list of Failed Checks, each tagged with the associated field(s) name, if applicable. This eliminates redundancy and simplifies the UI, making it easier to compare failed checks directly against the highlighted anomalous fields in the Source Record.</li> <li>Added the ability to filter Failed Checks by anomalous fields.</li> <li>Introduced direct links to datastores and profiles for enhanced navigation.</li> <li>Updated the tag input component for better UX.</li> <li>Removed the 'Hide Anomalous' option and replaced it with an 'Only Anomalous' option for more focused analysis.</li> <li>Included a feature to display the number of failed checks a field has across the modal.</li> <li>Implemented a menu allowing users to copy Violation messages easily.</li> </ul> </li> <li> <p>Bulk Operation for Profiles:</p> <ul> <li>Extended the profile selection functionality to allow initiating bulk operations like profiling and scanning directly from the selection interface.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_16","title":"General Fixes","text":"<ul> <li>DFS Incremental Scans:<ul> <li>Addressed an issue that caused incremental scans to fail when no new files were detected on globs. Scans will now proceed without failure or warning in such cases.</li> </ul> </li> <li>Improve performance of the Containers endpoint</li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.9.16","title":"2023.9.16","text":""},{"location":"changelog-2023/#feature-enhancements_16","title":"Feature Enhancements","text":"<ul> <li> <p>Insights Timeframe and Grouping:</p> <ul> <li>Trend tooltips have been refined to change responsively based on the selected timeframe and grouping, ensuring that users receive the most relevant information at a glance.</li> </ul> </li> <li> <p>Enhanced PDF export for Insights:</p> <ul> <li>Incorporated the selected timeframe and grouping settings into the exported PDF, ensuring that users experience consistent detail and clarity both within the application and in the exported document.</li> <li>Added a \"generated at\" timestamp to the PDF exports, providing traceability and context to when the data was captured, further enhancing the comprehensiveness of exported insights.</li> </ul> </li> <li> <p>Source Record Display Improvements:</p> <ul> <li>The internal columns' background color has been calibrated to offer a seamless appearance in both light and dark themes.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_17","title":"General Fixes","text":"<ul> <li> <p>Time Series Chart Rendering:</p> <ul> <li>Addressed an issue where the time series chart would not display data points despite having valid measurements. The core of the problem was pinpointed to how the system handled <code>0</code> values, especially when set as min and/or max thresholds.</li> <li>Resolved inconsistencies in how undefined min/max thresholds were displayed across different comparison types. While we previously had a UI indicator displaying for some comparison types, this was missing for \"Absolute Change\" and \"Absolute Value\".</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2023/#2023.9.14","title":"2023.9.14","text":""},{"location":"changelog-2023/#feature-enhancements_17","title":"Feature Enhancements","text":"<ul> <li> <p>Insights Improvements:</p> <ul> <li>Performance has been significantly optimized for smoother interactions.</li> <li>Introduced timeframe filters, allowing users to view insights data by week, month, quarter, or year.</li> <li>Introduced grouping capabilities, enabling users to segment visualizations within a timeframe, such as by days or weeks.</li> </ul> </li> <li> <p>Metric Checks Enhancements:</p> <ul> <li>Introduced a new Metric Checks tab in both the datastore and explore perspectives.</li> <li>Added a Time Series Chart within the Metric Checks tab:<ul> <li>Displays check measurements over time.</li> <li>Allows on-the-fly adjustments of min/max threshold values.</li> <li>Showcases enhanced check metadata including tags, active anomaly counts, and check weights.</li> </ul> </li> </ul> </li> <li> <p>Check Form Adjustments:</p> <ul> <li>Disabled the <code>Comparison Type</code> input for asserted checks</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_18","title":"General Fixes","text":"<ul> <li>Configuring Metric Checks through the Check Form:<ul> <li>Resolved a bug where users were unable to clear optional inputs such as \"min\" or \"max\".</li> </ul> </li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.9.8","title":"2023.9.8","text":""},{"location":"changelog-2023/#feature-enhancements_18","title":"Feature Enhancements","text":"<ul> <li>Presto &amp; Trino Connectors:<ul> <li>We've enhanced our suite of JDBC connectors by introducing dedicated support for both Presto and Trino. Whether you're utilizing the well-established Presto or the emerging Trino, our platform ensures seamless compatibility to suit your data infrastructure needs.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_19","title":"General Fixes","text":"<ul> <li>Incremental Scan: <ul> <li>Resolved an issue where the scan operation would fail during the \"Exists In Check\" if there were no records to be processed.</li> </ul> </li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.9.7","title":"2023.9.7","text":""},{"location":"changelog-2023/#feature-enhancements_19","title":"Feature Enhancements","text":"<ul> <li> <p>Concurrent Operations:</p> <ul> <li>Introduced the ability to run multiple operations of the same type concurrently within a single datastore, even if one is yet to finish. This brings more flexibility and efficiency in executing operations</li> </ul> </li> <li> <p>Autocomplete Widget:</p> <ul> <li>A hint for a shortcut has been added, allowing users to manually trigger the autocomplete widget and enhancing usability</li> </ul> </li> <li> <p>Source Record Display Enhancements:</p> <ul> <li>Added a new 'Hide Anomalous' option, providing users with the choice to hide anomalous records for clearer viewing</li> <li>Transitioned from hover-based tooltips to click-activated ones for better UX</li> <li>For a consistent data presentation, internal columns will now always be displayed first</li> </ul> </li> <li> <p>Check Form Improvements:</p> <ul> <li>Users now receive feedback directly within the form upon successful validation, replacing the previous toast notification method</li> <li>Additionally, for 504 validation timeouts, a more detailed and context-specific message is provided</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_20","title":"General Fixes","text":"<ul> <li>Addressed issues for 'Is Replica Of' failed checks in source record handling</li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.8.31","title":"2023.8.31","text":""},{"location":"changelog-2023/#general-fixes_21","title":"General Fixes","text":"<ul> <li>Fixed an issue where the Source Record remediation was incorrectly displayed for all fields</li> <li>Adjusted the display of field Quality Scores and Suggestion Scores within the Source Record</li> <li>Fixed a bug in the Check Form where the field input wouldn\u2019t display when cloning a check that hasn\u2019t been part of a scan yet</li> <li>Resolved an issue where failed checks for shape anomalies were not receiving violation messages</li> </ul>"},{"location":"changelog-2023/#2023.8.30","title":"2023.8.30","text":""},{"location":"changelog-2023/#feature-enhancements_20","title":"Feature Enhancements","text":"<ul> <li> <p>Anomaly Dialog Updates:</p> <ul> <li>Optimized Source Data Columns Presentation: To facilitate faster identification of issues, anomalous fields are now presented first. This enhancement will prove particularly useful for data sources with a large number of columns.</li> <li>Enhanced Sorting Capabilities: Users can now sort the source record data by name, weight, and quality score, providing more flexible navigation and ease of use.</li> <li>Field Information at a Glance: A new menu box has been introduced to deliver quick insights about individual fields. Users can now view weight, quality score, and suggested remediation for each field directly from this menu box.</li> </ul> </li> <li> <p>Syntax Highlighting Autocomplete Widget:</p> <ul> <li>Improved UX: The widget has been enhanced to better identify and display hint types, including distinctions between tables, keywords, views, and columns. This enhancement enriches the autocomplete experience.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_22","title":"General Fixes","text":"<ul> <li>Check Dialog Accessibility:<ul> <li>Addressed an issue where the check dialog was not opening as expected when accessed through a direct link from the profile page.</li> </ul> </li> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.8.23","title":"2023.8.23","text":""},{"location":"changelog-2023/#feature-enhancements_21","title":"Feature Enhancements","text":"<ul> <li> <p>Profiles Page:</p> <ul> <li>Introduced two new sorting methods to provide users with more intuitive ways to explore their profiles: Sort by last profiled and Sort by last scanned.</li> <li>Updated the default sorting behavior. Profiles will now be ordered by name right from the start, rather than by their creation date.</li> </ul> </li> <li> <p>Add New isNotReplicaOf Check:</p> <ul> <li>With this rule, users can assert that certain datasets are distinct and don't contain matching data, enhancing the precision and reliability of data comparisons and assertions.</li> </ul> </li> <li> <p>Introduce new Metric Check</p> <ul> <li>We've added a new Metric check tailored specifically for handling timeseries data. This new check is set to replace the previous Absolute and Relative Change Checks.</li> <li>To offer a more comprehensive and customizable checking mechanism, the Metric check comes with a comparison input:<ul> <li>Percentage Change: Asserts that the field hasn't deviated by more than a certain percentage (inclusive) since the last scan.</li> <li>Absolute Change: Ensures the field hasn't shifted by more than a predetermined fixed amount (inclusive) from the previous scan.</li> <li>Absolute Value: During each scan, this option records the field value and asserts that it remains within a specified range (inclusive).</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_23","title":"General Fixes","text":"<ul> <li> <p>Schema Validation:</p> <ul> <li>We've resolved an issue where the system was permitting the persistence of empty values under certain conditions for datastores and checks. This fix aims to prevent unintentional data inconsistencies, ensuring data integrity.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2023/#2023.8.18","title":"2023.8.18","text":""},{"location":"changelog-2023/#feature-enhancements_22","title":"Feature Enhancements","text":"<ul> <li> <p>Auditing:</p> <ul> <li>Introduced significant enhancements to the auditing capabilities of the platform, designed to provide better insights and control over changes. The new auditing features empower users to keep track of change sets across all entities, offering transparency and accountability like never before. A new activity endpoint has been introduced, providing a log of user interactions across the application.</li> </ul> </li> <li> <p>Search Enhancements:</p> <ul> <li>Profiles and Anomalies lists can now be searched by both identifiers and descriptions using the same search input.</li> </ul> </li> <li> <p>Catalog Operation Flow Update:</p> <ul> <li>Made a minor update to the datastore creation and catalog flow to enhance user flexibility and experience. Instead of automatically running a catalog operation post datastore creation, users now have a clearer, intuitive manual process. This change offers users the flexibility to set custom catalog configurations, like syncing only tables or views.</li> </ul> </li> <li> <p>Operation Flow Error Handling:</p> <ul> <li>Enhanced user experience during failures in the Operation Flow. Along with the failure message, a \"Try Again\" link has been added. Clicking this link will revert to the configuration state, allowing users to make necessary edits without restarting the entire operation process.</li> </ul> </li> <li> <p>Sorting Enhancements:</p> <ul> <li>Introduced new sorting options: \"Completeness\" and \"Quality Score\". These options are now available on the profiles &amp; fields pages.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_24","title":"General Fixes","text":"<ul> <li> <p>Datastore Connection Edit:</p> <ul> <li>Improved the Datastore connection edit experience, especially for platforms like BigQuery. Resolved an issue where file inputs were previously obligatory for minor edits. For instance, renaming a BigQuery Datastore no longer requires a file input, addressing this past inconvenience.</li> </ul> </li> <li> <p>Pagination issues:</p> <ul> <li>Resolved an issue with paginated endpoints returning 500 instead of 422 on requests with invalid parameters.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#2023.8.11","title":"2023.8.11","text":""},{"location":"changelog-2023/#feature-enhancements_23","title":"Feature Enhancements","text":"<ul> <li>Insights Export: Added a new feature that allows users to export Insights directly to PDF, making it easier to share and review data insights.</li> <li>Check Form UX: <ul> <li>Fields in the Check Form can now be updated if the check hasn't been used in a Scan operation, offering more flexibility to users.</li> <li>Enhanced visual cues in the form with boxed information to clarify the limitations certain properties have, depending on the state of the form.</li> <li>A new icon has been introduced to represent the number of scan operations that have utilized the check, providing users with a clearer overview.</li> </ul> </li> <li>SLA Form UX: <ul> <li>Revamped Date Time handling for enhanced time zone coverage, allowing for user-specified date time configurations based on their preferred time zone.</li> </ul> </li> <li>Filter and Sorting:<ul> <li>Added Datastore Type filter and sorting for source datastores</li> <li>Added Profile Completeness sorting and type filtering and sorting</li> <li>Added Check search by identifier or description</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_25","title":"General Fixes","text":"<ul> <li>SparkSQL Expressions: Added support to field names with special characters to SparkSQL expressions using backticks</li> <li>Pagination Adjustment: The pagination limit has been fine-tuned to support a maximum of 100 items per page, improving readability and navigation.</li> </ul>"},{"location":"changelog-2023/#2023.8.3","title":"2023.8.3","text":""},{"location":"changelog-2023/#maintenance-release","title":"Maintenance Release","text":"<ul> <li>Updated enrichment sidebar details design.</li> <li>Tweaked SQL input dialog sizing.</li> <li>Fixed filter components width bug.</li> <li>Retain the start time of operation on restart.</li> <li>Fixed exclude fields to throw exceptions on errors.</li> <li>Improved performance when using DFS to load reference data.</li> </ul>"},{"location":"changelog-2023/#2023.7.31","title":"2023.7.31","text":""},{"location":"changelog-2023/#maintenance-release_1","title":"Maintenance Release","text":"<ul> <li>Changed UX verbiage and iconography for Anomaly status updates.</li> <li>Fixed intermittent notification template failure.</li> <li>Fixed UI handling of certain rule types where unused properties were required.</li> <li>Improved error messages when containers are no longer accessible.</li> <li>Fixed Hadoop authentication conflicts with ABFS.</li> <li>Fixed an issue where a Profile operation run on an empty container threw a runtime exception.</li> </ul>"},{"location":"changelog-2023/#2023.7.29","title":"2023.7.29","text":""},{"location":"changelog-2023/#feature-enhancements_24","title":"Feature Enhancements","text":"<ul> <li>Added a NotExistsIn Check Type: Introducing a new rule type that asserts that values assigned to this field do not exist as values in another field.</li> <li>Check Authoring UI enhancements: Improved user interface with larger edit surfaces and parenthesis highlighting for better usability.</li> <li>Container Details UI enhancement: Improved presentation of container information in sidebars for easier accessibility and understanding.</li> <li>Added Check Authoring Validation: Users can now perform a dry run of the proposed check against representative data to ensure accuracy and effectiveness.</li> <li>Change in default linkage between Checks and Anomalies: Filters now default to \"Active\" status, providing more refined results and support for specific use cases.</li> </ul>"},{"location":"changelog-2023/#2023.7.25","title":"2023.7.25","text":""},{"location":"changelog-2023/#feature-enhancements_25","title":"Feature Enhancements","text":"<ul> <li>Satisfies Expression Enhancement: The Satisfies Expression feature has been upgraded to automatically bind fields referenced in the user-defined expressions, streamlining integration and improving usability.</li> </ul>"},{"location":"changelog-2023/#added-support","title":"Added Support","text":"<ul> <li>Extended Support for ExistsIn Checks: The ExistsIn checks now offer support for computed tables, empowering users to perform comprehensive data validation on computed data.</li> </ul>"},{"location":"changelog-2023/#general-fixes_26","title":"General Fixes","text":"<ul> <li> <p>Enhanced Check Referencing: Checks can now efficiently reference the full dataframe by using the alias \"qualytics_self,\" simplifying referencing and providing better context within checks.</p> </li> <li> <p>Improved Shape Anomaly Descriptions: Shape anomaly descriptions now include totals alongside percentages, providing more comprehensive insights into data irregularities.</p> </li> <li> <p>Fix for Computed Table Record Calculation: A fix has been implemented to ensure accurate calculation of the total number of records in computed tables, improving data accuracy and reporting.</p> </li> <li> <p>Enhanced Sampling Source Records Anomaly Detection: For shape anomalies, sampling source records now explicitly exclude replacement, leading to more precise anomaly detection and preserving data integrity during analysis.</p> </li> </ul>"},{"location":"changelog-2023/#2023.7.23","title":"2023.7.23","text":""},{"location":"changelog-2023/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fix for total record counts when profiling large tables</li> </ul>"},{"location":"changelog-2023/#2023.7.21","title":"2023.7.21","text":""},{"location":"changelog-2023/#feature-enhancements_26","title":"Feature Enhancements","text":"<ul> <li>Notification Form: Enhanced the user interface and experience by transforming the Channel and Tag inputs into a more friendly format.</li> <li>Checks &amp; Anomalies: Updated the default Sort By criterion to be based on \"Weight\", enabling a more effective overview of checks and anomalies.</li> <li>Profile Details (Side Panel): Introduced a tooltip to display the actual value of the records metric, providing clearer and instant information.</li> <li>Freshness Page: Added a new navigation button that directly leads to the Profile Details page, making navigation more seamless.</li> <li>Profile Details: Introduced a settings option for the user to perform actions identical to those from the Profile Card, such as changing profile settings and configuring Checks and SLAs.</li> <li>SparkSQL Inputs: Implemented a new autocomplete feature to enhance user experience. Writing SQL queries is now more comfortable and less error-prone.</li> </ul>"},{"location":"changelog-2023/#2023.7.19","title":"2023.7.19","text":""},{"location":"changelog-2023/#general-fixes_27","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2023/#2023.7.14","title":"2023.7.14","text":""},{"location":"changelog-2023/#feature-enhancements_27","title":"Feature Enhancements","text":"<ul> <li>API enhancements <ul> <li>Improved performance of our json validation through the adoption of Pydantic 2.0</li> <li>Upgraded our API specification to OpenAPI 3.1.0 compatible, this uses JSON Schema 2020-12.</li> </ul> </li> <li>Upgraded to Spark 3.4<ul> <li>Significant performance enhancements for long-running tasks and shuffles</li> </ul> </li> <li>Added support for Kerberos authentication for Hive datastores</li> <li>Enhanced processing for large dataframes with JDBC sources<ul> <li>Handle arbitrarily large tables and views by chunking into sequentially processed dataframes</li> </ul> </li> <li>Improvements for Insights view when limited data is available</li> <li>Various user experience enhancements</li> </ul>"},{"location":"changelog-2023/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>Date Picker fix for Authored Checks</li> <li>Allow tags with special characters to be edited</li> </ul>"},{"location":"changelog-2023/#2023.7.3","title":"2023.7.3","text":""},{"location":"changelog-2023/#feature-enhancements_28","title":"Feature Enhancements","text":"<ul> <li>Insights Made Default View on Data Explorer<ul> <li>Gain valuable data insights more efficiently with the revamped Insights feature, now set as the default view on the Data Explorer.</li> </ul> </li> <li>Reworked Freshness with Sorting and Grouping<ul> <li>Easily analyze and track data freshness based on specific requirements thanks to the improved Freshness feature, now equipped with sorting and grouping functionalities.</li> </ul> </li> <li>Enhanced Tables/Files Cards Design:<ul> <li>Experience improved data analysis with the updated design of tables/files cards, including added average completeness information and reorganized identifiers.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#added-support_1","title":"Added Support","text":"<ul> <li> <p>Support for Recording Sample Shape Anomalies to Remediation Tables</p> <ul> <li>Address potential data shape issues more effectively as the platform now supports recording a sample of shape anomalies to remediation tables.</li> </ul> </li> <li> <p>New Metrics and Redirect to Anomalies for Profile/Scan Results</p> <ul> <li>Access additional metrics for profile/scan results and easily redirect to anomalies generated by a scan from Activity tab for efficient identification and resolution of data issues.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_28","title":"General Fixes","text":"<ul> <li>Reduced Margin Between Form Input Fields:<ul> <li>Enjoy a more compact and streamlined design with a reduced margin between form input fields for an improved user experience.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>Fixed Pagination Reset Issue During Check Updates<ul> <li>Pagination will no longer reset when checks are updated, providing a smoother user experience, with reset now occurring only during filtering.</li> </ul> </li> <li>Resolved Vertical Misalignment of Check and Anomaly Icons<ul> <li>The issue causing vertical misalignment between Check and Anomaly icons on the Field Profile page has been fixed, resulting in a visually pleasing and intuitive user interface.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#2023.6.24","title":"2023.6.24","text":""},{"location":"changelog-2023/#feature-enhancements_29","title":"Feature Enhancements","text":"<ul> <li>Refactored Partition Reads on JDBC <ul> <li>Refactored partitioned reads on JDBC to improve performance, resulting in faster and more efficient data retrieval.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li> <p>Fixed Inputs on Change Checks</p> <ul> <li>Refined inputs on change checks to differentiate between Absolute and Relative measurements, ensuring precise detection and handling of data modifications based on numeric values (Absolute) and percentage (Relative) variations.</li> </ul> </li> <li> <p>Resolved Enum Type Ordering Bug for Paginated Views</p> <ul> <li>Fixed bug causing inconsistent and incorrect sorting of enum values across all paginated views, ensuring consistent and accurate sorting of enum types.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_29","title":"General Fixes","text":"<ul> <li>Added Success Effect<ul> <li>Added effect when a datastore is configured successfully, enhancing the user experience by providing visual confirmation of a successful configuration process.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#2023.6.20","title":"2023.6.20","text":""},{"location":"changelog-2023/#feature-enhancements_30","title":"Feature Enhancements","text":"<ul> <li> <p>Reworked Tags View</p> <ul> <li>Improved the usability and visual appeal of the tags view. Added new properties like description and weight modifier to provide more detailed information and assign relative importance to tags. The weight value directly correlates with the level of importance, where a higher weight indicates higher significance.</li> </ul> </li> <li> <p>Inherited Tags Support</p> <ul> <li>Implemented support for inherited tags in taggable entities. Now tags can be inherited from parent entities, streamlining the tagging process and ensuring consistency across related items. Inherited Tags will be applied to anomalies AFTER a Scan operation.</li> </ul> </li> <li> <p>Added Total Data Under Management to Insights</p> <ul> <li>Introduced a new metric under Insights that displays the total data under management. This provides users with valuable insights into the overall data volume being managed within the system.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#added-support_2","title":"Added Support","text":"<ul> <li> <p>Bulk Update Support</p> <ul> <li>Introduced bulk update functionality for tables, files, and fields. Users can now efficiently Tag multiple items simultaneously, saving time and reducing repetitive tasks.</li> </ul> </li> <li> <p>Smart Partitioning of BigQuery</p> <ul> <li>Enabled smart partitioning in BigQuery using cluster keys. Optimized data organization within BigQuery for improved query performance and cost savings.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>Fixed Scheduling Operation Issues<ul> <li>Addressed a bug causing scheduling operations to fail with invalid days in crontabs. Users can now rely on accurate scheduling for time-based tasks without encountering errors.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_30","title":"General Fixes","text":"<ul> <li> <p>Improved Backend Performance</p> <ul> <li>Implemented various internal fixes to optimize backend performance. This results in faster response times, smoother operations, and an overall better user experience.</li> </ul> </li> <li> <p>Enhanced Tag Input:</p> <ul> <li>Improved tag input functionality in the Check form dialog. Users can now input tags more efficiently with enhanced suggestions and auto-complete features, streamlining the tagging process.</li> </ul> </li> <li> <p>Enhanced File Input Component</p> <ul> <li>Upgraded the file input component in the Datastore form dialog, providing a more intuitive and user-friendly interface for uploading files. Simplifies attaching files to data entries and improves overall usability.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#2023.6.12","title":"2023.6.12","text":""},{"location":"changelog-2023/#feature-enhancements_31","title":"Feature Enhancements","text":"<ul> <li>Explore is the new centralized view of Activities, Containers (Profiles, Tables, Computed Tables), Checks, Anomalies and Insights across ALL Datastores. This new view allows for filtering by Datastores &amp; Tags, which will persist the filters across all of the submenu tabs. The goal is to help with Critical Data Elements and filter out irrelevant information.</li> <li>Enhanced Navigation Features<ul> <li>The navigation tabs have been refined for increased user-friendliness.</li> <li>Enhanced the Profile View and added a toggle between card and list views.</li> <li><code>Datastores</code> and <code>Enrichment Datastores</code> have been unified, with a tabular view introduced to distinguish between your Source Datastores and Enrichment Datastores.</li> <li><code>Explore</code> has been added to the main navigation, and <code>Insights</code> has been conveniently relocated into the Explore submenu.</li> <li>Renamed <code>Tables/Files</code> to <code>Profiles</code> in the Datastore details page.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#added-support_3","title":"Added Support","text":"<ul> <li> <p>We're thrilled to introduce two new checks, the <code>Absolute Change Limit</code> and the <code>Relative Change Limit</code>, tailored to augment data change monitoring. These checks enable users to set thresholds on their numeric data fields and monitor fluctuations from one scan to the next. If the changes breach the predefined limits, an anomaly is generated. </p> <ul> <li> </li> <li>The <code>Absolute Change Limit</code> check is designed to monitor changes in a field's value by a fixed amount. If the field's value changes by more than the specified limit since the last applicable scan, an anomaly is generated.</li> <li>The <code>Relative Change Limit</code> check works similarly but tracks changes in terms of percentages. If the change in a field's value exceeds the defined percentage limit since the last applicable scan, an anomaly is generated.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_31","title":"General Fixes","text":"<ul> <li>General UI fixes with new navigational tabs</li> <li>Resolved an issue when creating a computed table</li> <li>Incorporated functionality to execute delete operations and their related results.</li> <li>Renamed \"Rerun\" button to \"Retry\" in the operation list</li> </ul>"},{"location":"changelog-2023/#2023.6.2","title":"2023.6.2","text":""},{"location":"changelog-2023/#general-fixes_32","title":"General Fixes","text":"<ul> <li> <p>Added GCS connector with Keyfile support:</p> <ul> <li>The GCS connector now supports Keyfile authentication, allowing users to securely connect to Google Cloud Storage.</li> </ul> </li> <li> <p>Improved BigQuery connector by removing unnecessary inputs:</p> <ul> <li>Enhancements have been made to the BigQuery connector by streamlining the inputs, eliminating any unnecessary fields or options.</li> <li>This results in a more user-friendly and efficient experience.</li> </ul> </li> <li> <p>Renamed satisfiesEquation to satisfiesExpression:</p> <ul> <li>The function \"satisfiesEquation\" has been renamed to \"satisfiesExpression\" to better reflect its functionality.</li> <li>This change makes it easier for users to understand and use the function.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#added-support_4","title":"Added Support","text":"<ul> <li> <p>Added Check Description to Notification rule messages:</p> <ul> <li>Notification rule messages now include the Check Description.</li> <li>This allows users to add additional context and information about the specific rule triggering the notification and passing that information to downstream workflows.</li> </ul> </li> <li> <p>Added API support for tuning operations with a high correlation threshold for profiles and high count rollup threshold for anomalies in scan:</p> <ul> <li>The API now supports tuning operations by allowing users to set a higher correlation threshold for profiles.</li> <li>It also enables users to set a higher count rollup threshold for anomalies in scan.</li> <li>This customization capability helps users fine-tune the behavior of the system according to their specific needs and preferences.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#2023.5.26","title":"2023.5.26","text":""},{"location":"changelog-2023/#usability","title":"Usability","text":"<ul> <li>Improved the navigation in the Activity tab\u2019s side panel for easier and more intuitive browsing including exposing the ability to comment directly into an anomaly</li> <li>Added a redirect to the Activity tab when an operation is initiated for a smoother workflow.</li> </ul>"},{"location":"changelog-2023/#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>Resolved an issue where the date and time were not displaying correctly for the highest value in profiles.</li> <li>Fixed a problem with scheduled operations when the configured timing was corrupted.</li> <li>Addressed an issue where filtered checks were causing unexpected errors outside of the intended dataset.</li> </ul>"},{"location":"changelog-2023/#2023.5.23","title":"2023.5.23","text":""},{"location":"changelog-2023/#feature-enhancements_32","title":"Feature Enhancements","text":"<ul> <li>Scheduled operation editing<ul> <li>Added the ability for users to edit a scheduled operation. This allows users to make changes to the schedule of an operation.</li> </ul> </li> <li>Catalog includes filters<ul> <li>Added catalog include filters to only process tables, views, or both in JDBC datastores. This allows users to control which object types are processed in the datastore.</li> </ul> </li> <li>isReplicaOf check filters<ul> <li>Added filter support to the isReplicaOf check. This allows users to control which tables are checked for replication.</li> </ul> </li> <li>Side panel updates<ul> <li>Updated side panel design and added an enrichment redirect option.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#added-support_5","title":"Added Support","text":"<ul> <li>IBM DB2 datastore<ul> <li>Added support for the IBM DB2 datastore. This allows users to connect to and process data from IBM DB2 databases.</li> </ul> </li> <li>API support for tagging fields<ul> <li>Added API support for tagging fields. This allows users to tag fields in the datastore with custom metadata.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>Freshness attempting to measure views<ul> <li>Fixed an issue with freshness attempting to measure views.</li> </ul> </li> <li>Enrichment to Redshift and string data types<ul> <li>Fixed an issue with enrichment to Redshift and string data types. This issue caused enrichment to fail for tables that contained string data types</li> </ul> </li> </ul>"},{"location":"changelog-2023/#2023.5.10","title":"2023.5.10","text":""},{"location":"changelog-2023/#feature-enhancements_33","title":"Feature Enhancements","text":"<ul> <li> <p>Container Settings</p> <ul> <li>Introducing the ability to Group fields for improved insights and profiling precision.</li> <li>Added functionality to Exclude fields from the container, allowing associated checks to be ignored during operations, leading to reduced processing time and power consumption.</li> <li>We now support identifiers on commuted tables during profiling operations.</li> </ul> </li> <li> <p>Checks</p> <ul> <li>Improved usability by enabling quick cloning of checks within the same datastore.<ul> <li>Users can now easily create a new check with minor edits to tables, fields, descriptions, and tags based on an existing check.</li> </ul> </li> <li>Introducing the ability to write Check Descriptions to the Enrichment store, enabling better organization and management of check-related data downstream. <ul> <li>Note: Updating the Enrichment store data requires a new Scan operation.</li> </ul> </li> <li>Enhanced anomaly management by providing a convenient way to filter and view all anomalies generated by a specific check.<ul> <li>Users can now access the Anomaly warning sign icon within the Check dialog, providing quick access to two options: View Anomalies and Archive Anomalies.</li> </ul> </li> </ul> </li> <li>Usability<ul> <li>Introducing the ability to generate an API token from within the user interface.<ul> <li>This can be done through the Settings &gt; Security section, providing a convenient way to manage API authentication.</li> </ul> </li> <li>Added the ability to search tables/files and apply filters to running operations.<ul> <li>This feature eliminates the need to rely solely on pagination, making it easier to select specific tables/files for operations.</li> </ul> </li> <li>Included API and SparkSQL links in the documentation for easy access to additional resources and reference materials.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#added-support_6","title":"Added Support","text":"<ul> <li>Hive datastore support has been added, allowing seamless integration with Hive data sources.</li> <li>Timescale datastore support has been added, enabling efficient handling of time-series data.</li> <li>Added support for HTTP(S) and SOCKS5 proxies, allowing users to configure proxy settings for data operations.</li> <li>Default encryption for rabbitMQ has been implemented, enhancing security for data transmission.</li> </ul>"},{"location":"changelog-2023/#bug-fixes_7","title":"Bug Fixes","text":"<ul> <li>Resolved a bug related to updating tag names, ensuring that tag name changes are properly applied.</li> <li>Fixed an overflow bug in freshness measurements for data size, resulting in accurate measurements and improved reliability.</li> </ul>"},{"location":"changelog-2023/#general-fixes_33","title":"General Fixes","text":"<ul> <li>Updated default weighting for shape anomalies, enhancing the accuracy of anomaly detection and analysis.</li> <li>Increased datastore connection timeouts, improving stability and resilience when connecting to data sources.</li> <li>Implemented general bug fixes and made various improvements to enhance overall performance and user experience.</li> </ul>"},{"location":"changelog-2023/#2023.4.19","title":"2023.4.19","text":"<p>We're pleased to announce the latest update that includes enhancements to UI for an overall better experience:</p>"},{"location":"changelog-2023/#feature-enhancements_34","title":"Feature Enhancements","text":"<ul> <li>Added Volumetric measurements to Freshness Dashboard:<ul> <li>Gain valuable insights into your data's scale and storage requirements with our new volumetric measurements. SortBy Row Count or Data Size to make informed decisions about your data resources.</li> </ul> </li> <li>Added <code>isReplicaOf</code> check:<ul> <li>The new <code>isReplicaOf</code> check allows you to easily compare data between two different tables or fields, helping you identify and resolve data inconsistencies across your datastores.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#added-support_7","title":"Added Support","text":"<ul> <li>Redesigned Checks and Anomalies listing:<ul> <li>Enjoy a cleaner, more organized layout with more information that makes navigating and managing checks and anomalies even easier.</li> <li></li> </ul> </li> <li>Redesigned Anomaly Details view:<ul> <li>The updated anomaly view provides a more thoughtful and organized layout.</li> <li></li> </ul> </li> <li>Improved Filter components:<ul> <li>With a streamlined layout and organized categories, filtering your data is now more intuitive. Dropdown options are now to the right to allow view of the Clear and Apply buttons</li> </ul> </li> <li>Updated Importance score to Weight &amp; added SortBy support:<ul> <li>Manage checks and anomalies more effectively with our updated \u2018Weight' feature (formerly \u2018Importance Score') and the new SortBy support function, allowing you to quickly identify high-priority issues.</li> </ul> </li> </ul>"},{"location":"changelog-2023/#general-fixes_34","title":"General Fixes","text":"<ul> <li>General Fixes and Performance Improvements</li> </ul>"},{"location":"changelog-2023/#2023.4.7","title":"2023.4.7","text":""},{"location":"changelog-2023/#feature-enhancements_35","title":"Feature Enhancements","text":"<ul> <li>We've just deployed an MVP version of the Freshness Dashboard! This feature lets you create, manage, and monitor all of the SLAs for each of your datastores and their child files/tables/containers, all in one place. It's like having a birds-eye view of how your datastores are doing in relation to their freshness.<ul> <li>To access the Freshness Dashboard, just locate and click on the clock icon in the top navigation between Insights and Anomalies. By default, you'll see a rollup of all the datastores in a list view with their child files/tables/containers collapsed. Simply click on a datastore row to expand the list.</li> </ul> </li> <li>We've also made some improvements to the UI, including more sorting and filtering options in Datastores, Files/Tables, Checks, and Anomalies. Plus, we've added the ability to search the description field in checks, making it easier to find what you're looking for.</li> <li>Last but not least, we've added a cool new feature to checks - the ability to archive ALL anomalies generated by a check. Simply click on the anomaly warning icon at the top of the check details box to bring up the archive anomalies dialog box.</li> </ul>"},{"location":"changelog-2024/","title":"2024","text":""},{"location":"changelog-2024/#release-notes","title":"Release Notes","text":""},{"location":"changelog-2024/#2024.12.23","title":"2024.12.23","text":""},{"location":"changelog-2024/#feature-enhancements","title":"Feature Enhancements","text":"<ul> <li> <p>User and Teams Permissions</p> <ul> <li>We are excited to introduce an enhancement to User and Team Permissions.<ul> <li>Users can now have <code>Admin</code>, <code>Manager</code>, or <code>Member</code> roles.<ul> <li>The Manager role provides a subset of Admin permissions for global assets or settings but does not include the \"Admin exemption to team roles.\"</li> </ul> </li> <li>Teams can have specific permissions: <code>Editor</code>, <code>Author</code>, <code>Drafter</code>, <code>Viewer</code>, and <code>Reporter</code>.<ul> <li>Each permission type includes restricted capabilities tailored to its role.</li> </ul> </li> <li>Admins can now create special tokens that grant access exclusively to SCIM endpoints. These tokens allow customers to enable SCIM integrations with minimal access, ensuring the holder cannot access other endpoints or log in to the platform.</li> </ul> </li> </ul> </li> <li> <p>Improve Visibility of Datastore Teams</p> <ul> <li>Users can now view respective teams in the tree view footer. Depending on privileges, they can manage this field.</li> <li>Teams are also visible in the table and field context for improved collaboration and data transparency.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes","title":"General Fixes","text":"<ul> <li> <p>Completeness Rounding</p> <ul> <li>Added two more decimal places to the Completeness metric in the Overview tab.<ul> <li>Previously, percentages were being rounded up incorrectly.</li> </ul> </li> </ul> </li> <li> <p>\"Is Replica Of\" Check Validation</p> <ul> <li>Fixed a bug that occurred when users attempted to validate this check using the same container.</li> </ul> </li> <li> <p>Global Search</p> <ul> <li>Fixed the label to better distinguish between Enrichment Datastores and Source Datastores.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2024/#2024.12.11","title":"2024.12.11","text":""},{"location":"changelog-2024/#feature-enhancements_1","title":"Feature Enhancements","text":"<ul> <li>Add <code>Max Parallelization</code> Field on Datastore Connection<ul> <li>Users can now configure the maximum parallelization level for certain datastores, providing greater control over operation performance.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_1","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2024/#2024.11.29","title":"2024.11.29","text":""},{"location":"changelog-2024/#feature-enhancements_2","title":"Feature Enhancements","text":"<ul> <li>Activity List<ul> <li>Removed the <code>Warning</code> status for a cleaner and more concise status display.</li> <li>Added an alert icon to indicate if an operation completed with warnings, improving visibility into operation outcomes.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_2","title":"General Fixes","text":"<ul> <li>Better handling of Oracle Date and Numeric columns during Catalog operations for improved partition field selection.</li> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2024/#2024.11.21","title":"2024.11.21","text":""},{"location":"changelog-2024/#feature-enhancements_3","title":"Feature Enhancements","text":"<ul> <li>Improved Operations Container Dialogs<ul> <li>Added container status details based on profile and scan results, providing better visibility of container-level operations.</li> <li>Introduced a loading tracker component for containers, enhancing feedback during operation processing.</li> <li>Made the entire modal reactive to operation updates, enabling real-time tracking of operation progress within the modal.</li> <li>Removed \"containers requested\" and \"containers analyzed\" dialogs for a cleaner interface.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_3","title":"General Fixes","text":"<ul> <li> <p>Resolved an issue where the table name was not rendering correctly in notifications when using the <code>{{ customer_name }}</code> variable.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2024/#2024.11.12","title":"2024.11.12","text":""},{"location":"changelog-2024/#feature-enhancements_4","title":"Feature Enhancements","text":"<ul> <li> <p>Enhance Data Catalog Integration</p> <ul> <li>Introduced a new domain input field that allows users to select specific domains, enabling more granular control over assets synchronization.</li> </ul> </li> <li> <p>Scan Results Enhancements</p> <ul> <li>Added partition label to the scan results modal for improved partition identification.</li> <li>Removed unnecessary metadata partitions created solely for volumetric checks, reducing clutter in scan results.</li> </ul> </li> <li> <p>Activity Tab</p> <ul> <li>Display of Unprocessed Containers in the Operation List<ul> <li>Unprocessed containers are now visible in the operation list within the operation summary.</li> <li>A total count label was added to indicate if the number of analyzed containers exceeds the total requested.</li> <li>The search icon now highlights in a different color if not all containers were analyzed, making it easier to identify incomplete operations.</li> </ul> </li> <li>Reorder the Datastore Column in the Activity Tab<ul> <li>Users can now reorder columns in the Activity tab for easier navigation and data organization.</li> </ul> </li> <li>Profile Operations<ul> <li>Users can now view added, updated, and total inferred checks within Profile operations.</li> </ul> </li> <li>Triggered by Column<ul> <li>Updated the term \"Triggered by API\" to \"Triggered by System\" for clarity.</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_4","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2024/#2024.11.1","title":"2024.11.1","text":""},{"location":"changelog-2024/#feature-enhancements_5","title":"Feature Enhancements","text":"<ul> <li> <p>Observability Enhancements</p> <ul> <li>An observability heatmap was added to the volumetric card in the Observability tab.<ul> <li>The heatmap allows users to monitor volumetric status and check for new anomalies.</li> </ul> </li> <li>Improved observability chart for clearer insights.<ul> <li>Users can now view the count of volumetric anomalies produced over time, along with the last recorded measurements for each period.</li> <li>Introduced new color indicators to help distinguish volumetric measures outside thresholds that didn\u2019t produce anomalies from those that did.</li> </ul> </li> </ul> </li> <li> <p>Editable Tags in Field Details</p> <ul> <li>Users with write permissions can now manage tags directly in the Field Details within the Explore context.</li> </ul> </li> <li> <p>Distinct Count Rule Update</p> <ul> <li>The Distinct Count rule now excludes the Coverage field for more accurate assessments.</li> </ul> </li> <li> <p>Support for Pasting into Expected Values</p> <ul> <li>Users can now paste values from spreadsheets directly into Expected Values, saving time on data entry.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_5","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2024/#2024.10.23","title":"2024.10.23","text":""},{"location":"changelog-2024/#feature-enhancements_6","title":"Feature Enhancements","text":"<ul> <li> <p>Dremio Connector</p> <ul> <li>We\u2019ve expanded our connectivity options by supporting a new connection with Dremio.</li> </ul> </li> <li> <p>Full View of Abbreviated Metrics in Operation Summary</p> <ul> <li>Users can now hover over abbreviated metrics to see the full value for better clarity.</li> </ul> </li> <li> <p>Redirect to Conflicting Check</p> <ul> <li>Added a redirect link to the conflicting check from the error message, improving navigation when addressing errors.</li> </ul> </li> <li> <p>Enhanced Visibility and Engagement for Tags and Notifications Setup</p> <ul> <li>Introduced a Call to Action to encourage users to manage Tags and Notifications for better engagement.</li> </ul> </li> <li> <p>Favorite Containers</p> <ul> <li>Users can now favorite individual containers.</li> <li>The option to favorite datastores and containers is now available in both card and list views.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_6","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2024/#2024.10.16","title":"2024.10.16","text":""},{"location":"changelog-2024/#feature-enhancements_7","title":"Feature Enhancements","text":"<ul> <li> <p>Improved Anomaly Modal</p> <ul> <li>Introduced an information icon in each failed check to display the check's description.</li> <li>Anomaly links now persist filters for sort order and displayed fields.</li> <li>Added integration details to fields in a source record.</li> </ul> </li> <li> <p>Secrets Management</p> <ul> <li>Added support for Secrets Manager in connection properties, enabling integration with Vault and other secrets management systems.</li> </ul> </li> <li> <p>Alation Data Dictionary</p> <ul> <li>Enhanced the dictionary to display friendly names in anomaly screens for improved usability.</li> <li>Added integration information to the datastore, container, and fields in the tree view footer.</li> </ul> </li> <li> <p>Tag Category</p> <ul> <li>Introduced support for tag categories to improve tag management, with sorting and filtering options based on the category field.</li> </ul> </li> <li> <p>Call to Action for Volumetric Measurements</p> <ul> <li>A call to action was added in the overview tab within the container context, and the observability page per container was added to enable volumetric measurements.</li> </ul> </li> <li> <p>Error Display for Check Operations</p> <ul> <li>Bulk operations like Edit, Activate, Update, and Template Edit now display error messages clearly when validation fails.</li> </ul> </li> <li> <p>Check Validation</p> <ul> <li>Improved check validation logic to enhance bulk check validation speed and prevent timeouts.</li> </ul> </li> <li> <p>Tag Filtering for Fields</p> <ul> <li>Users can now filter fields by tags in the field list under the datastore context.</li> </ul> </li> <li> <p>Field Remarks in Native Field Properties</p> <ul> <li>Added support for displaying field remarks alongside other native field properties.</li> </ul> </li> <li> <p>Customer Support Link</p> <ul> <li>Users can now access the Qualytics Helpdesk via the Discover menu in the main header.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_7","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2024/#2024.10.4","title":"2024.10.4","text":""},{"location":"changelog-2024/#feature-enhancements_8","title":"Feature Enhancements","text":"<ul> <li> <p>Insights Page Redesign</p> <ul> <li>Introduced a new Overview card displaying key metrics such as <code>Data Under Management</code>, <code>Source Datastores</code>, and <code>Containers</code>.</li> <li>Added a doughnut chart visualization for checks and anomalies, providing a clearer view of data health.</li> <li>Expanded available metrics to include profile runs and scan runs.</li> <li>Users can now easily navigate to Checks and Anomalies based on their current states and statuses.</li> <li>Implemented data volume visualizations to give users better insight into data trends.</li> <li>Introduced a legend option that allows users to compare specific metrics against the primary one.</li> <li>Enhanced the check distribution visualization across the platform within the overview tabs.</li> </ul> </li> <li> <p>Check Filter</p> <ul> <li>Now users can filter <code>Not Asserted</code> checks.</li> </ul> </li> <li> <p>Team Management</p> <ul> <li>Now admin users can modify the <code>Read</code> and <code>Write</code> permissions of the <code>Public</code> Team.</li> </ul> </li> <li> <p>Reapplying Clone Field</p> <ul> <li>Check cloning functionality by attempting to reapply the field from the original (source) check when a new container is selected. If the selected container matches the field and type from the original check, the cloned field will be reapplied automatically.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_8","title":"General Fixes","text":"<ul> <li> <p>Allow saving checks with attached templates as drafts</p> <ul> <li>Adjusted the behavior to allow checks attached to a template to be saved as drafts. The <code>Save as draft</code> feature now remains functional when a template is attached.</li> </ul> </li> <li> <p>Incremental identifier strange behavior</p> <ul> <li>When a user tries to modify a query in a computed table, the Incremental Modifier is set to null.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.9.25","title":"2024.9.25","text":""},{"location":"changelog-2024/#feature-enhancements_9","title":"Feature Enhancements","text":"<ul> <li> <p>Observability</p> <ul> <li>Time-series charts are presented to monitor data volume and related anomalies for each data asset.<ul> <li>Custom thresholds were added to adjust minimum and maximum volume expectations.</li> </ul> </li> <li>The Metrics tab has been moved to the Observability tab.</li> <li>The Observability tab has replaced the Freshness page.</li> </ul> </li> <li> <p>Check Category Options for Scan Operations</p> <ul> <li>Users can select one or multiple check categories when running a scan operation.</li> </ul> </li> <li> <p>Anomaly Trigger Rule Type Filter</p> <ul> <li>Added a filter by check rule types to anomaly triggers. A help component was added to the tags selector to improve clarity.</li> </ul> </li> <li> <p>Auto-Archive Anomalies</p> <ul> <li>A new Duplicate status has been introduced for anomalies.</li> <li>Users can now use Incremental Identifier ranges to auto-archive anomalies with the new Duplicate status.</li> <li>An option has been added to scan operations to automatically archive anomalies identified as duplicates if the containers analyzed have incremental identifiers configured.</li> </ul> </li> <li> <p>A dedicated tab for filtering duplicate anomalies has been added for better visibility.</p> </li> <li> <p>Tree View and Breadcrumb Context Menu</p> <ul> <li>A context menu has been added, allowing users to copy essential information and open links in new tabs.</li> <li>Users can access the context menu by right-clicking on the assets.</li> </ul> </li> <li> <p>Incremental Identifier Support</p> <ul> <li>Users can manage incremental identifiers for computed tables and computed files.</li> </ul> </li> <li> <p>Native Field Properties</p> <ul> <li>Users can now see native field properties in the field profile, displayed through an info icon next to the Type Inferred section.</li> </ul> </li> <li> <p>Qualytics CLI Update</p> <ul> <li>Users can now import check templates.</li> <li>A status filter has been added to check exports. Users can filter by <code>Active</code>, <code>Draft</code>, or <code>Archived</code> (which will include <code>Invalid</code> and <code>Discarded</code> statuses).</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_9","title":"General Fixes","text":"<ul> <li>The Oracle connector now handles invalid schemas when creating connections.</li> <li>Anomalies identified in scan operations were not counting archived statuses.</li> <li>Improved error message when a user creates a schedule name longer than 50 characters.</li> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2024/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Freshness and SLA references have been removed from user notifications and notification rules, users should migrate to Observability using volumetric checks.</li> </ul>"},{"location":"changelog-2024/#2024.9.14","title":"2024.9.14","text":""},{"location":"changelog-2024/#feature-enhancements_10","title":"Feature Enhancements","text":"<ul> <li> <p>Volumetric Measurement</p> <ul> <li>We are excited to introduce support for volumetric measurements of views, computed tables and computed files.</li> </ul> </li> <li> <p>Enhanced Source Record CSV Download</p> <ul> <li>Users can now download all source records as CSV that have been written to the enrichment datastores.</li> </ul> </li> <li> <p>Tags and Notifications Moved to Left-Side Navigation</p> <ul> <li>Users can now quickly switch between Tags, Notifications, and Data Assets through the left-side navigation.</li> <li>Access to the Settings page is restricted to admin users.</li> </ul> </li> <li> <p>Last Asserted Information in Checks</p> <ul> <li>The <code>Created Date</code> information has been replaced with <code>Last Asserted</code> to improve visibility.</li> <li>Users can hover over an info icon to view the <code>Created Date</code>.</li> </ul> </li> <li> <p>Auto-Generated Description in Check Template Dialog</p> </li> <li> <p>Descriptions are now automatically generated in the Template Dialog based on the rule type, ensuring consistency with the check form.</p> </li> <li> <p>Exposed Properties in Profile and Scan Operations</p> <ul> <li>Profile and scan operations now expose properties when listed:<ul> <li>Record Limit</li> <li>Infer As Draft</li> <li>Starting Threshold</li> <li>Enrichment Record Limit</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_10","title":"General Fixes","text":"<ul> <li>Fixed a bug where the container list would not update when a user created a computed container.</li> <li>Fixed an issue where deactivated users were not filtered on the Settings page under the Security tab.</li> <li>Improved error messages when operations fail.</li> <li>Fixed a bug where the <code>Last Editor</code> field was empty after a user was deactivated by an admin.</li> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2024/#2024.9.10","title":"2024.9.10","text":""},{"location":"changelog-2024/#feature-enhancements_11","title":"Feature Enhancements","text":"<ul> <li> <p>Add Source Datastore Modal</p> <ul> <li>Enhanced text messages and labels for better clarity and user experience.</li> </ul> </li> <li> <p>Add Datastore</p> <ul> <li>Users can now add a datastore directly from the Settings page under the Connections tab, simplifying connection management.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_11","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2024/#2024.9.6","title":"2024.9.6","text":""},{"location":"changelog-2024/#feature-enhancements_12","title":"Feature Enhancements","text":"<ul> <li>Introducing Bulk Activation on Draft Checks<ul> <li>Users can now activate and validate multiple draft checks at once, streamlining the workflow and reducing manual effort.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_12","title":"General Fixes","text":"<ul> <li> <p>Improved error message for BigQuery temporary dataset configuration exceptions.</p> </li> <li> <p>Added a retry operation for Snowflake when no active warehouse is selected in the current session.</p> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#breaking-changes_1","title":"Breaking Changes","text":"<ul> <li>API fields (<code>type</code> and <code>container_type</code>) are now mandatory in request payloads where they were previously optional.<ul> <li>POST /global-tags: <code>type</code> is now required.</li> <li>PUT /global-tags/{name}: <code>type</code> is now required.</li> <li>POST /containers: <code>container_type</code> is now required.</li> <li>PUT /containers/{id}: <code>container_type</code> is now required.</li> <li>POST /operations/schedule: <code>type</code> is now required.</li> <li>PUT /operations/schedule/{id}: <code>type</code> is now required.</li> <li>POST /operations/run: <code>type</code> is now required.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#2024.9.3","title":"2024.9.3","text":""},{"location":"changelog-2024/#feature-enhancements_13","title":"Feature Enhancements","text":"<ul> <li>Introducing Catalog Scheduling<ul> <li>Users can now schedule a Catalog operation like Profile and Scan Operations, allowing automated metadata extraction.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_13","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2024/#2024.8.31","title":"2024.8.31","text":""},{"location":"changelog-2024/#feature-enhancements_14","title":"Feature Enhancements","text":"<ul> <li> <p>New Draft Status for Checks</p> <ul> <li>Introduced a new 'draft' status for checks to enhance lifecycle management, allowing checks to be prepared and reviewed without impacting scan operations.</li> <li>Validation is only applied to active checks, ensuring draft checks remain flexible for adjustments without triggering automatic validations.</li> </ul> </li> <li> <p>Introduce Draft Check Inference in Profile Operations</p> <ul> <li>Added a new option to infer checks as drafts, offering more flexibility during data profiling.</li> </ul> </li> <li> <p>Improve Archive Capabilities for Checks and Anomalies</p> <ul> <li>Enhanced the archive capabilities for both checks and anomalies, allowing recovery of archived items.</li> <li>Introduced a hard delete option that allows permanent removal of archived items, providing greater control over their management.</li> <li>The Anomaly statuses 'Resolved' and 'Invalid' are now treated as archived states, aligning with the consistent approach used for checks.</li> </ul> </li> <li> <p>Introduce a new Volumetric Check</p> <ul> <li>Introduced the Volumetric Check to monitor and maintain data volume stability within a specified range. This check ensures that the volume of data assets does not fluctuate beyond acceptable limits based on a moving daily average.</li> <li>Automatically inferred and maintained by the system for daily, weekly, and monthly averages, enabling proactive management of data volume trends.</li> </ul> </li> <li> <p>Incremental Identifier Warning in Scan Dialog</p> <ul> <li>Enhanced the dialog to notify users when they attempt an incremental scan on containers lacking an incremental identifier, ensuring transparency and preventing unexpected full scans.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_14","title":"General Fixes","text":"<ul> <li> <p>Improve enrichment writes with queuing all writes (up to a queue threshold) for the entire scan operation. This will dramatically reduce the number of write operations performed.</p> </li> <li> <p>Explicit casting to avoid weak CSV parser support for typing.</p> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.8.19","title":"2024.8.19","text":""},{"location":"changelog-2024/#feature-enhancements_15","title":"Feature Enhancements","text":"<ul> <li> <p>Enhance Auto-Refresh Mechanism on Tree View</p> <ul> <li>The datastore and container tree footers are now automatically refreshed after specific actions, eliminating the need for manual page refreshes.</li> </ul> </li> <li> <p>Support Oracle Client-Side Encryption</p> <ul> <li>Connections with Oracle now feature end-to-end encryption. Database connection encryption adds an extra layer of protection, especially for transmissions over long-distance, insecure channels.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_15","title":"General Fixes","text":"<ul> <li> <p>UI Label on Explore Page</p> <ul> <li>Fixed an issue where the labels on the Explore page did not change based on the selected time frame.</li> </ul> </li> <li> <p>Inferred Field Type Enhancements</p> <ul> <li>Behavior updated to infer field types at data load time rather than implicitly cast them to latest profiled type. This change supports more consistent expected schema verification for delimited file types and resolves issues when comparing inferred fields to non-inferred fields in some rule types.</li> </ul> </li> <li> <p>Boolean Type Inference</p> <ul> <li>Behavior updated to align boolean inference with Spark Catalyst so that profiled types are more robustly handled during Spark based comparisons</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.8.10","title":"2024.8.10","text":""},{"location":"changelog-2024/#feature-enhancements_16","title":"Feature Enhancements","text":"<ul> <li>Introducing Profile Inference Threshold<ul> <li>This feature allows users to adjust which check types will be automatically created and updated during data profiling, enabling them to manage data quality expectations based on the complexity of inferred data quality rules.</li> </ul> </li> <li>Anomaly Source Records Retrieval Retry Option<ul> <li>Enabled users to manually retry fetching anomaly source records when the initial request fails.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_16","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2024/#2024.7.31","title":"2024.7.31","text":""},{"location":"changelog-2024/#feature-enhancements_17","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing Field Count to the Datastore Overview</p> <ul> <li>This enhancement allows users to easily view the total number of fields present in a datastore across all containers.</li> </ul> </li> <li> <p>Search Template</p> <ul> <li>Added a check filter to the templates page.</li> <li>Added a template filter to the checks page in the datastore context and explore.</li> </ul> </li> <li> <p>Driver Free Memory</p> <ul> <li>Added driver free memory information on the Health Page.</li> </ul> </li> <li> <p>Anomalous Record Count to the Anomaly Sidebar Card</p> <ul> <li>Added the anomalous record count information to the anomaly sidebar card located under the Scan Results dialog.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_17","title":"General Fixes","text":"<ul> <li> <p>Enhanced write performance on scan operations with enrichment and relaxed hard timeouts.</p> </li> <li> <p>Updated Azure Blob Storage connector to use TLS encrypted access by default.</p> </li> <li> <p>Overview Tab is not refreshing asset details automatically.</p> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.7.26","title":"2024.7.26","text":""},{"location":"changelog-2024/#feature-enhancements_18","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing Event Bus for Extended Auto-Sync with Data Catalog Integrations</p> <ul> <li>We are excited to expand our auto-sync capabilities with data catalog integrations by implementing an event bus pattern.</li> <li>Added functionality to delete any DQ values that do not meet important checks.</li> <li>Included support for a WARNING status in the Alation Data Health tab for checks that have not been asserted yet.</li> </ul> </li> <li> <p>Add Autocomplete to the Notification Form</p> <ul> <li>Improved the notification message form by implementing autocomplete. Users can now easily include internal variables when crafting custom messages, streamlining the message creation process.</li> </ul> </li> <li> <p>Redesign the Analytics Engine Functions</p> <ul> <li>The functions are now accessible through a menu, which displays the icon and full functionality.</li> <li>Added a modal to alert users before proceeding with the restart. The modal informs users that the system will be unavailable for a period during the restart process.</li> </ul> </li> <li> <p>Improve Qualytics metadata presentation in Alation</p> <ul> <li>Previously, multiple custom fields were used to persist data quality metrics measured by Qualytics. This process has been simplified by consolidating the metrics into a single rich text custom field formatted in HTML, making it easier for users to analyze the data.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_18","title":"General Fixes","text":"<ul> <li> <p>Normalize Enrichment Internal Containers</p> <ul> <li>To improve user recognition and differentiate between our internal tables and those in source systems, we now preserve the original case of table names.</li> </ul> </li> <li> <p>Validation Error on Field Search Result</p> <ul> <li>Resolved the logic for cascade deletion of dependencies on containers that have been soft deleted, ensuring proper handling of related data.</li> </ul> </li> <li> <p>Members Cannot Add Datastore on the Onboarding Screen</p> <ul> <li>Updated permissions so that members can no longer add Datastores during the onboarding process. Only Admins now have this capability.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.7.19","title":"2024.7.19","text":""},{"location":"changelog-2024/#feature-enhancements_19","title":"Feature Enhancements","text":"<ul> <li>Global Search<ul> <li>We are thrilled to introduce the \u201cGlobal Search\u201d feature into Qualytics! This enhancement is designed to streamline the search across the most crucial assets: Datastores, Containers, and Fields. It provides quick and precise search results, significantly improving navigation and user interaction.</li> <li>Navigation Update: To integrate the new global search bar seamlessly, we have relocated the main menu icons to the left side of the interface. This adjustment ensures a smoother user experience.</li> </ul> </li> <li>Teradata Connector<ul> <li>We\u2019ve expanded our connectivity options by supporting a new connection with Teradata. This enhancement allows users to connect and interact with Teradata databases directly from Qualytics, facilitating more diverse data management capabilities.</li> </ul> </li> <li>Snowflake Key-pair Authentication<ul> <li>In our ongoing efforts to enhance security, we have implemented support for Snowflake Key-pair authentication. This new feature provides an additional layer of security for our users accessing Snowflake, ensuring that data transactions are safe and reliable.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_19","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2024/#2024.7.15","title":"2024.7.15","text":""},{"location":"changelog-2024/#feature-enhancements_20","title":"Feature Enhancements","text":"<ul> <li> <p>Alation Data Catalog Integration</p> <ul> <li>We're excited to introduce integration with Alation, enabling users to synchronize and manage assets across both Qualytics and Alation.</li> <li>Metadata Customization:<ul> <li>Trust Check Flags: We now support warning flags at both the container and field levels, ensuring users are aware of deprecated items.</li> <li>Data Health: Qualytics now pushes important checks to Alation's Data Health tab, providing a comprehensive view of data health at the container level.</li> <li>Custom Fields: Quality scores and related metadata are pushed under a new section in the Overview page of Alation. This includes quality scores, quality score factors, URLs, anomaly counts, and check counts.</li> </ul> </li> </ul> </li> <li> <p>Support for Never Expiration Option for Tokens</p> <ul> <li>Users now have the option to create tokens that never expire, providing more flexibility and control over token management.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_20","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2024/#2024.7.5","title":"2024.7.5","text":""},{"location":"changelog-2024/#feature-enhancements_21","title":"Feature Enhancements","text":"<ul> <li>Enhanced Operations Listing Performance<ul> <li>Optimized the performance of operations listings and streamlined the display of container-related information dialogs. These enhancements include improved handling of operations responses and the addition of pagination for enhanced usability</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_21","title":"General Fixes","text":"<ul> <li> <p>Fix Computed Field Icon Visibility</p> <ul> <li>Resolved an issue where the computed field icon was not being displayed in the table header.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.6.29","title":"2024.6.29","text":""},{"location":"changelog-2024/#feature-enhancements_22","title":"Feature Enhancements","text":"<ul> <li> <p>Computed Field Support</p> <ul> <li>Introduced computed fields allowing users to dynamically create new virtual fields within a container by applying transformations to existing data.</li> <li>Computed fields offer three transformation options to cater to various data manipulation needs. Each transformation type is designed to address specific data characteristics:<ul> <li>Cleaned Entity Name: Automates the removal of business signifiers such as 'Inc.' or 'Corp.' from entity names, simplifying entity recognition.</li> <li>Convert Formatted Numeric: Strip formatting like parentheses (for negatives) and commas (as thousand separators) from numeric data, converting them into a clean, numerically-typed format.</li> <li>Custom Expression: Allows users to apply any valid Spark SQL expression to combine or transform fields, enabling highly customized data manipulations.</li> </ul> </li> <li>Users can define specific checks on computed fields to automatically detect anomalies during scan operations.</li> <li>Computed fields are also visible in the data preview tab, providing immediate insight into the results of the defined transformations.</li> </ul> </li> <li> <p>Autogenerated Descriptions for Authored Checks</p> <ul> <li>Implemented an auto-generation feature for check descriptions to streamline the check authoring process. This feature automatically suggests descriptions based on the selected rule type, reducing manual input and simplifying the setup of checks.</li> </ul> </li> <li> <p>Event-Driven Catalog Integrations and Sync Enhancements</p> <ul> <li>Enhanced the Atlan integration and synchronization functionalities to include event-driven support, automatically syncing assets during Profile and Scan operations. This update also refines the Sync and Integration dialogs, offering clearer control options and flexibility.</li> </ul> </li> <li> <p>Sorting by Anomalous Record Count</p> <ul> <li>Added a new sorting filter in the Anomalies tabs that allow users to sort anomalies by record count, improving the manageability and analysis of detected anomalies.</li> </ul> </li> <li> <p>Refined Tag Sorting Hierarchy:</p> <ul> <li>Updated the tag sorting logic to consistently apply a secondary alphabetical sort by name. This ensures that tags will additionally be organized by name within any primary sorting category.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_22","title":"General Fixes","text":"<ul> <li> <p>Profile Operation Support for Empty Containers</p> <ul> <li>Resolved an issue where profiling operations failed to record fields in empty containers. Now, fields are generated even if no data rows are present.</li> </ul> </li> <li> <p>Persistent Filters on the Explore Page</p> <ul> <li>Fixed a bug that caused Explore to disable when switching tabs on the Explore page. Filters now remain active and consistent, enhancing user navigation and interaction.</li> </ul> </li> <li> <p>Visibility of Scan Results Button</p> <ul> <li>Corrected the visibility issue of the 'results' button in the scan operation list at the container level. The button now correctly appears whenever at least one anomaly is detected, ensuring users have immediate access to detailed anomaly results.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.6.18","title":"2024.6.18","text":""},{"location":"changelog-2024/#feature-enhancements_23","title":"Feature Enhancements","text":"<ul> <li> <p>Improvement to Anomaly Dialog</p> <ul> <li>Enhanced the anomaly dialog to include a direct link to the operation that generated the anomaly. Users can now easily navigate from an anomaly to view other anomalies generated by the same operation directly from the Activity tab.</li> </ul> </li> <li> <p>Sorting by Duration in Activity Tab</p> <ul> <li>Introduced the ability to sort by the duration of operations in the Activity tab by ascending or descending order.</li> </ul> </li> <li> <p>Last Editor Information for Scheduled Operations</p> <ul> <li>Added visibility of which users have created or last updated scheduled operations, enhancing traceability in scheduling management.</li> </ul> </li> <li> <p>Display Total Anomalous Records for Anomalies</p> <ul> <li>Added the total count of anomalous records in the anomalies listing view.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_23","title":"General Fixes","text":"<ul> <li> <p>Performance Fixes on Computed Table Creation and Check Validation</p> <ul> <li>Optimized the processes for creating computed tables and validating checks. Users previously experiencing slow performance or timeouts during these operations will now find the processes significantly faster and more reliable.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.6.14","title":"2024.6.14","text":""},{"location":"changelog-2024/#feature-enhancements_24","title":"Feature Enhancements","text":"<ul> <li> <p>Improvements to Atlan Integration</p> <ul> <li> <p>When syncing Qualytics with Atlan, badges now display the \"Quality Score Total,\" increasing visibility and emphasizing key data quality indicators on Atlan assets.</p> </li> <li> <p>Improved performance of the synchronization operation.</p> </li> <li> <p>Implemented the propagation of external tags to checks, now automatically aligned with the container synchronization process, enabling better accuracy and relevance of data tagging.</p> </li> </ul> </li> <li> <p>Refactor Metric Check Creation</p> <ul> <li>Enhanced the encapsulated Metric Check creation flow to improve user experience and efficiency. Users can now seamlessly create computed tables and schedule operations simultaneously with the metric check creation.</li> </ul> </li> <li> <p>Support Update of Weight Modifier for External Tags</p> </li> <li> <p>Add Validation on Updated Connections</p> <ul> <li>Added support for testing the connection if there's at least one datastore attached to the connection, ensuring more reliable and accurate connection updates.</li> </ul> </li> <li> <p>Standardize Inner Tabs under the Settings Page</p> <ul> <li> <p>Tags and Notifications Improvements: The layout has been revamped for better consistency and clarity. General headers have been removed, and now each item features specific headers to enhance readability.</p> </li> <li> <p>Security Tab Improvements: The redesign features chip tabs for improved navigation and consistency. Filters have been updated to ensure they meet application standards.</p> </li> <li> <p>Tokens Tab Accessibility: Moved the action button to the top of the page to make it more accessible.</p> </li> <li> <p>Refine Connector Icons Display: Improved the display of connector icons for Datastores and Enrichments in the Connections Tab.</p> </li> </ul> </li> <li> <p>Streamlined Container Profiling and Scanning</p> <ul> <li>In the container context, the profile and scan modals have been updated to automatically display the datastore and container, eliminating the need for a selection step and streamlining the process.</li> </ul> </li> <li> <p>Swap Order During Check Creation</p> <ul> <li> <p>Rule Type Positioning: The Rule Type now appears before the container selection, making the form more intuitive.</p> </li> <li> <p>Edit Mode Header: In edit mode, the Rule Type is prominently displayed in the modal header, immediately under the check ID.</p> </li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_24","title":"General Fixes","text":"<ul> <li> <p>Address Minor Issues in the Datastore Activity Page</p> <ul> <li> <p>Operation ID Auto-Search: Restored the auto-search feature by operation ID for URL access, enhancing navigation, especially for Catalog Operations.</p> </li> <li> <p>Tree View Auto-Refresh: Implemented an auto-refresh feature for the tree view, which activates after any operation in the CTA flow (Catalog, Profile, Scan).</p> </li> </ul> </li> <li> <p>Fix \"Greater Than Field\" Quality Check</p> <ul> <li>Corrected the inclusive property of the greater than field quality check.</li> </ul> </li> <li> <p>Fix Exporting Field Profiles for Non-Admin User with Write Permission</p> <ul> <li>Resolved issues for non-admin users with write permissions to allow proper exporting of field profile metadata to enrichment.</li> </ul> </li> <li> <p>Fix \"Is Replica Of\" Quality Check validation on Field Names with Special Characters</p> <ul> <li>Improved validation logic to handle field names with special characters</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.6.7","title":"2024.6.7","text":""},{"location":"changelog-2024/#feature-enhancements_25","title":"Feature Enhancements","text":"<ul> <li> <p>Atlan Integration Improvements</p> <ul> <li>Enhanced the Atlan assets fetch and external tags syncing. </li> <li>Added support for external tag propagation to checks and anomalies. </li> <li>Merged Global and External tags section for streamlined tag management.</li> </ul> </li> <li> <p>Restart Button for Analytics Engine</p> <ul> <li>Introduced a new \"Restart\" button under the Settings - Health section, allowing admins to manually restart the Analytics Engine if it is offline or unresponsive.</li> </ul> </li> <li> <p>Interactive Tooltip Component</p> <ul> <li>Added a new interactive tooltip component that remains visible upon hovering, enhancing user interaction across various modules of the application.</li> <li>Refactored existing tooltip usage to integrate this new component for a more consistent user experience.</li> </ul> </li> <li> <p>Defaulting to Last-Used Enrichment Datastore for Check Template Exports</p> <ul> <li>Improved user experience by persisting the last selected enrichment datastore as the default option when exporting a check template.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_25","title":"General Fixes","text":"<ul> <li> <p>Shared Links Fixes</p> <ul> <li>Fixed issues with shared operation result links, ensuring that dialogs for scan/profile results and anomalies now open correctly.</li> <li>Addressed display inaccuracies in the \"Field Profiles Updated\" metrics.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.6.4","title":"2024.6.4","text":""},{"location":"changelog-2024/#feature-enhancements_26","title":"Feature Enhancements","text":"<ul> <li> <p>Atlan Data Catalog Integration</p> <ul> <li>We're excited to introduce integration with Atlan, enabling users to synchronize and manage assets across both Qualytics and Atlan:<ul> <li>Tag Sync: Sync tags assigned to data assets in Atlan with the corresponding assets in Qualytics, enabling tag-based quality score reporting, notifications, and bulk data quality operations using Atlan-managed tags.</li> <li>Metadata Sync: Automatically synchronize Atlan with Qualytics metadata, including asset URL, total score, and factor scores such as completeness, coverage, conformity, consistency, precision, timeliness, volume, and accuracy.</li> </ul> </li> </ul> </li> <li> <p>Entity Resolution Check</p> <ul> <li>We've removed the previous limitation on the maximum number of distinct entity names that could be resolved with the Entity Resolution rule type. This release includes various performance enhancements that support an unlimited number of entity names.</li> </ul> </li> <li> <p>Enhancements to Catalog Operation Results</p> <ul> <li>We've improved the catalog operation results by now including detailed information on whether tables, views, or both were involved in each catalog operation.</li> </ul> </li> <li> <p>Enhancements to 'Equal to Field' Rule Type</p> <ul> <li>The 'Equal to Field' rule now supports string values, allowing for direct comparisons between text-based data fields.</li> </ul> </li> <li> <p>Enhancements to Enrichment</p> <ul> <li>Qualytics now includes a property for anomalousRecordCount on shape anomaly, which previously was neither populated nor persisted. This aims to accurately capture and record the total number of anomalous records identified in ShapeAnomaly, regardless of the max_source_records threshold.</li> </ul> </li> <li> <p>Dynamic Meta Titles</p> <ul> <li>Pages such as Datastore Details, Container Details, and Field Details now feature dynamic meta titles that accurately describe the page content and are visible in browser tabs providing better searchability.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_26","title":"General Fixes","text":"<ul> <li> <p>Fix Trends of Quality Scores on the Insights Page</p> <ul> <li>Addressed issues with displaying trends on the Insights page. Trends now accurately reflect changes and comparisons to the previous report period, providing more reliable and insightful analytics.</li> </ul> </li> <li> <p>Resolved a bug in Entity Resolution where the distinction constraint was only applied to entity names that differed.</p> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.5.22","title":"2024.5.22","text":""},{"location":"changelog-2024/#feature-enhancements_27","title":"Feature Enhancements","text":"<ul> <li> <p>Datastore Connection Updates:</p> <ul> <li>Users can now update the connection on a datastore if the new one has the same type as the current one.</li> </ul> </li> <li> <p>Enrichment Datastore Redirection:</p> <ul> <li>Enhanced the user interface to facilitate easier redirection to enrichment datastores, streamlining the process and improving user experience.</li> </ul> </li> <li> <p>Label Enhancements for Data Completeness:</p> <ul> <li>Updated labels to better distinguish between completeness percentages and Factor Scores. The label for completeness percentage has been changed to provide clear context when viewed alongside.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_27","title":"General Fixes","text":"<ul> <li> <p>Rule Type Anomaly Corrections:</p> <ul> <li>Fixed an issue where the violation messages for record anomalies incorrectly included \"None\" for some rule types. This update ensures accurate messaging across all scenarios.</li> </ul> </li> <li> <p>Shape Anomaly Logic Adjustment:</p> <ul> <li>Revised the logic for Shape Anomalies to prevent the combination of failed checks for high-count record checks on the same field. This change ensures that displayed sample rows have definitively failed the specific checks shown, enhancing the accuracy of anomaly reporting.</li> </ul> </li> <li> <p>Entity Resolution Anomalies:</p> <ul> <li>Addressed an inconsistency where some Entity Resolution Checks did not return source records. Ongoing investigations and fixes have improved the reliability of finding source records for entity resolution checks across DFS and JDBC datastores.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.5.16","title":"2024.5.16","text":""},{"location":"changelog-2024/#feature-enhancements_28","title":"Feature Enhancements","text":"<ul> <li> <p>Entity Resolution Check</p> <ul> <li>Introduced rule \"Entity Resolution\" to determine if multiple records reference the same real-world entity. This feature uses customizable fields and similarity settings to ensure accurate and tailored comparisons.</li> </ul> </li> <li> <p>Support for Rerunning Operations</p> <ul> <li>Added an option to rerun operations from the operations listing, allowing users to reuse the configuration from previously executed operations.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_28","title":"General Fixes","text":"<ul> <li> <p>Export Operations</p> <ul> <li>Fixed metadata export operations silently failing on writing to the enrichment datastores.</li> </ul> </li> <li> <p>Computed File/Table Creation</p> <ul> <li>Resolved an issue that prevented the creation of computed files/tables with the same name as previously deleted ones, even though it is a valid action.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.5.13","title":"2024.5.13","text":""},{"location":"changelog-2024/#general-fixes_29","title":"General Fixes","text":"<ul> <li> <p>Enhanced Quality Score Factors Computation</p> <ul> <li>Addressed issues in quality score calculation and its associated factors ensuring accuracy</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.5.11","title":"2024.5.11","text":""},{"location":"changelog-2024/#feature-enhancements_29","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing Quality Score Factors</p> <ul> <li>This new feature allows users to control the quality score factor weights at the datastore and container levels.<ul> <li>Quality Score Detail Expansion: Users can now click on the quality score number to expand its details, revealing the contribution of each factor to the overall score. This enhancement aids in understanding what drives the quality score.</li> <li>Insights Page Overhaul: The Insights page has been restructured to better showcase the quality score breakdown. This redesign aims to make the page more informative and focused on quality score metrics.</li> <li>Customization of Factor Weights: Users can now customize the weights of different factors at the Datastore and Container levels. This feature is essential for adapting the quality score to meet specific user needs, such as disregarding the Timeliness factor for dimensional tables where it might be irrelevant.</li> <li>Enhanced Inferred Checks: Introduced a new property in the Check Listing schema and a feature in the Check modal that displays validity metrics, which help quantify the accuracy of inferred checks. A timezone handling issue in the last_updated property of the Check model has also been addressed.</li> </ul> </li> </ul> </li> <li> <p>Quality Score UI Enhancements</p> <ul> <li>Enhancements have been made to the user interface to provide a clearer and more detailed view of the quality score metrics, including Completeness, Coverage, Conformity, Consistency, Precision, Timeliness, Volumetrics, and Accuracy. These changes aim to provide deeper insight into the components that contribute to the overall quality score.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_30","title":"General Fixes","text":"<ul> <li> <p>Fixes to JDBC Incremental Support</p> <ul> <li>Updated the conditional logic in the catalog operation for update tables to ensure the incremental identifier is preserved if already established.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.5.2","title":"2024.5.2","text":""},{"location":"changelog-2024/#feature-enhancements_30","title":"Feature Enhancements","text":"<ul> <li> <p>Datastore Connections:</p> <ul> <li>Users can now create connections that can be shared across different datastores. This introduces a more flexible approach to managing connections, allowing users to streamline their workflow and reduce duplication of effort. With shared connections, users can easily reuse common elements such as hostname and credentials across various datastores, enhancing efficiency and simplifying management.</li> </ul> </li> <li> <p>File Container Header Configuration:</p> <ul> <li>Adds support for setting the hasHeader boolean property on File Containers, enabling users to specify whether their flat file data sources include a header row. This enhances compatibility and flexibility when working with different file formats.</li> </ul> </li> <li> <p>Improved Error Handling in Delete Dialogs:</p> <ul> <li>Error handling within delete dialogs has been revamped across the application. Error messages will now be displayed directly within the dialog itself, providing clearer feedback and preventing misleading success messages in case of deletion issues.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_31","title":"General Fixes","text":"<ul> <li> <p>Locked Template Field Editing:</p> <ul> <li>Resolves an issue where selecting a new container in the check form would reset check properties, causing problems for locked templates. The fix ensures that checks derived from templates retain their properties, allowing users to modify the field_to_compare field as needed.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.4.25","title":"2024.4.25","text":""},{"location":"changelog-2024/#feature-enhancements_31","title":"Feature Enhancements","text":"<ul> <li> <p>Profile Results Modal:</p> <ul> <li>Introducing a detailed Results Modal for each profile operation. Users can now view comprehensive statistics about the produced container profiles and their partitions, enhancing their ability to analyze data effectively.</li> </ul> </li> <li> <p>Checks Synchronized Count:</p> <ul> <li>The operations list now includes the count of synchronized checks for datastore and explore operations. This addition streamlines the identification of operations, improving user experience.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_32","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2024/#2024.4.23","title":"2024.4.23","text":""},{"location":"changelog-2024/#feature-enhancements_32","title":"Feature Enhancements","text":"<ul> <li> <p>Introduction of Comparators for Quality Checks:</p> <ul> <li>Launched new Comparator properties across several rule types, enhancing the flexibility in defining quality checks. Comparators allow users to set margins of error, accommodating slight variations in data validation:<ul> <li>Numeric Comparators: Enables numeric comparisons with a specified margin, which can be set as either a fixed absolute value or a percentage, accommodating datasets where minor numerical differences are acceptable.</li> <li>Duration Comparators: Supports time-based comparisons with flexibility in duration differences, essential for handling time-based data with variable precision.</li> <li>String Comparators: Facilitates string comparisons by allowing for variations in spacing, ideal for textual data where minor inconsistencies may occur.</li> </ul> </li> <li>Applicable to rule types such as Equal To, Equal To Field, Greater Than, Greater Than Field, Less Than, Less Than Field, and Is Replica Of.</li> </ul> </li> <li> <p>Introduced Row Comparison in the isReplicaOf Rule:</p> <ul> <li>Improved the rule to support row comparison by id, enabling more precise anomaly detection by allowing users to specify row identifiers for unique row comparison. Key updates include:<ul> <li>Revamp of the source record presentation to highlight differences between the left and right containers at the cell level, enhancing visibility into anomalies.</li> <li>New input for specifying unique row identifiers, transitioning from symmetric difference to row comparison when set.</li> <li>The original behavior of symmetric comparison remains unchanged if no row identifiers are provided.</li> </ul> </li> </ul> </li> <li> <p>New equalTo Rule Type for Direct Value Comparisons</p> <ul> <li>Introduced the equalTo rule type, enabling precise assertions that selected fields match a specified value. This new rule not only simplifies the creation of checks for constant values across datasets but also supports the use of comparators, allowing for more flexible and nuanced data validation.</li> </ul> </li> <li> <p>Redirect Links for Requested Containers in Operation Details:</p> <ul> <li>Introduced redirect links in the \"Containers Requested\" section of operation results. This enhancement provides direct links to the requested containers (such as tables or files), facilitating quicker navigation and streamlined access to relevant operational data.</li> </ul> </li> <li> <p>Enhanced Description Input with Expandable Option:</p> <ul> <li>Implemented an expandable option for the Description input in the Check Form &amp; Template Form. This enhancement allows users to more comfortably manage lengthy text entries, improving the usability of the form by accommodating extensive descriptions without compromising the interface's usability.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_33","title":"General Fixes","text":"<ul> <li> <p>Addressed Data Preview Timeout Issues:</p> <ul> <li>Tackled the timeout problems in the data preview feature, ensuring that data retrieval processes complete successfully within the new extended timeout limits.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.4.12","title":"2024.4.12","text":""},{"location":"changelog-2024/#feature-enhancements_33","title":"Feature Enhancements","text":"<ul> <li>File Pattern Overrides:<ul> <li>We have added support in the UI to override a file pattern. Now, a file pattern overwritten by a user will replace the one that the system generated during the first catalog. To have a new file pattern in the UI, users need to perform a new catalog operation without prune.</li> </ul> </li> <li>Batch Edit in the Check Templates Library::<ul> <li>We are now supporting batch edits for check templates in the Library. This enhancement will allow filters and tags.</li> </ul> </li> <li>Improved Presentation of Incremental, Remediation, and Infer Constraints:<ul> <li>We have improved the presentation of Incremental, Remediation, and Infer Constraints in the operation listing for catalog, profile, and scan operations. The Incremental, Remediation, and Infer Constraints icons have been added to the list of items, and the visualization of these items has been enhanced.</li> </ul> </li> <li>Default Placeholders for Computed File in UI:<ul> <li>We are now automatically populating the form dialog with fields from the selected container. This improvement simplifies the process for users, especially in scenarios where they wish to select or cast specific fields directly from the source container.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_34","title":"General Fixes","text":"<ul> <li> <p>Tree View Default Ordering:</p> <ul> <li>We have updated the tree view default ordering. Datastore names are now grouped and presented in alphabetical order.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.4.6","title":"2024.4.6","text":""},{"location":"changelog-2024/#breaking-changes_2","title":"Breaking Changes","text":"<ul> <li> <p>Remediation Naming Convention Update:</p> <ul> <li>Updated the naming convention for remediation to <code>{enrich_container_prefix}_remediation_{container_id}</code>, standardizing remediation identifiers.</li> </ul> </li> <li> <p>Add file extension for DFS Enrichment:</p> <ul> <li>Introduced <code>.delta</code> extension to files in the enrichment process on DFS, aligning with data handling standards.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#feature-enhancements_34","title":"Feature Enhancements","text":"<ul> <li> <p>Revamp Enrichment Datastore Main Page:</p> <ul> <li>Tree View &amp; Data Navigation: Enhanced the enrichment page with an updated tree view that now lists source datastores linked to enrichment datastores, improving navigability. A newly introduced page for enrichment datastore enables:<ul> <li>Data preview across enrichment, remediation, and metadata tables with the ability to apply \"WHERE\" filters for targeted insights.</li> <li>Direct downloading of preview data as CSV.</li> </ul> </li> <li>UI Performance Optimization: Implemented UI caching to boost performance, reducing unnecessary network requests and smoothly preserving user-inputted filters and recent data views.</li> </ul> </li> <li> <p>User Sorting by Role:</p> <ul> <li>Introduced a sorting feature in the Settings &gt; Users tab, allowing users to be sorted by their roles in ascending or descending order, facilitating easier user management.</li> </ul> </li> <li> <p>Expanded Entity Interaction Options:</p> <ul> <li>Enhanced entity lists and breadcrumbs with new direct action capabilities. Users can now right-click on an item to access useful functions: copy the entity's ID or name, open the entity's link in a new tab, and copy the entity's link. This enhancement simplifies data management by making essential actions more accessible.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_35","title":"General Fixes","text":"<ul> <li> <p>Record Quality Scores Overlap Correction:</p> <ul> <li>Resolved a problem where multiple violations could be open for the same container simultaneously, contrary to logic. This fix ensures violations for containers are uniquely recorded, eliminating parallel open violations.</li> </ul> </li> <li> <p>Anomaly Details Text Overflow:</p> <ul> <li>Corrected text overflow issues in the anomaly details' violation box, ensuring all content is properly contained and readable.</li> </ul> </li> <li> <p>Enhanced \"Not Found\" Warnings with Quick Filters:</p> <ul> <li>Improved user guidance for Checks and Anomalies list filters by adding hints for \"not found\" items, suggesting users check the \"all\" group for unfiltered search results, clarifying navigation and search results.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.3.29","title":"2024.3.29","text":""},{"location":"changelog-2024/#feature-enhancements_35","title":"Feature Enhancements","text":"<ul> <li> <p>Data Preview</p> <ul> <li>Introducing the \"Data Preview\" tab, providing users with a streamlined preview of container data within the platform. This feature aims to enhance the user experience for tasks such as debugging checks, offering a grid view showcasing up to 100 rows from the container's source.<ul> <li>Data Preview Tab: Implemented a new tab for viewing container data, limited to displaying a maximum of 100 rows for improved performance.</li> <li>Filter Support: Added functionality to apply filter clauses to the data preview, enabling users to refine displayed rows based on specific criteria.</li> <li>UI Caching: Implemented a caching layer within the UI to enhance performance and reduce unnecessary network requests, storing the latest refreshed data along with applied filters.</li> </ul> </li> </ul> </li> <li> <p>Enhanced Syntax Highlight Inputs</p> <ul> <li>Improved the syntax highlight inputs for seamless inline editing, minimizing the friction of entering expressions. This feature includes a dual-mode capability, allowing users to type directly within the input field or utilize an expanded dialog for more complex entries, significantly improving user experience.</li> </ul> </li> <li> <p>Volumetric Measurements</p> <ul> <li>Periodically measure container volumetrics for a more robust approach. This update focuses on measuring only containers without a volume measure in the last 24 hours and scheduling multiple runs of the job daily.</li> </ul> </li> <li> <p>Sort Tags by Color</p> <ul> <li>Users can now sort tags by color, visually grouping similar colors for easier navigation and management.</li> </ul> </li> <li> <p>Download Source Records</p> <ul> <li>Added a \"Download Source Records\" feature to the Anomaly view in the UI, allowing users to export data held in the enrichment store for that anomaly in CSV format.</li> </ul> </li> <li> <p>Check Templates Navigation</p> <ul> <li>Implemented a breadcrumb trail for the Check Template page to improve user navigation.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_36","title":"General Fixes","text":"<ul> <li> <p>Fix Scheduling Issues</p> <ul> <li>Resolved scheduling issues affecting specific sets of containers, particularly impacting scheduled profile and scan operations. Users must manually add new profiles after catalog operations or computed file/table creation for inclusion in existing scheduled operations.</li> </ul> </li> <li> <p>Fix Notifications Loading Issue on Large Screens</p> <ul> <li>Fixed an issue where the infinity loading feature for the user notification list was not functioning properly on large screens. The fix ensures correct triggering of infinity loading regardless of screen size, allowing all notifications to be accessed properly.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.3.15","title":"2024.3.15","text":""},{"location":"changelog-2024/#feature-enhancements_36","title":"Feature Enhancements","text":"<ul> <li>Enhanced Observability<ul> <li>Automated daily volumetric measurements for all tables and file patterns</li> <li>Time-series capture and visualizations for volume, freshness, and identified anomalies</li> </ul> </li> <li> <p>Overview Tab:</p> <ul> <li>Introduced a new \"Overview\" tab with information related to monitoring at the datastore and container level.</li> <li>This dashboard interface is designed for monitoring and managing data related to qualytics for datastore and containers.</li> <li>Users can see:<ul> <li>Totals: Quality Score, Tables, Records, Checks and Anomalies</li> <li>Total of Quality Checks grouped by Rule type</li> <li>Data Volume Over Time: A line graph that shows the total amount of data associated with the project over time.</li> <li>Anomalies Over Time: A line graph that shows the number of anomalies detected in the project over time.</li> </ul> </li> </ul> </li> <li> <p>Datastore Field List Update:</p> <ul> <li>The datastore field profiles list has been updated to match the existing list views design.</li> <li>All card-listed pages now display information in a column format, conditionally using scrolling for smaller and larger screens.</li> <li>Now the field details will show on a modal with Profiling and Histogram</li> </ul> </li> <li> <p>Heatmap Simplification:</p> <ul> <li>Simplified the heatmap to consider only operations counted.</li> </ul> </li> <li> <p>Datastore Metrics:</p> <ul> <li>Improved distinction between 0 and null values in the datastore metrics (total records, total fields, etc).</li> </ul> </li> <li> <p>Explore Page Update:</p> <ul> <li>Added new metrics to the Explore page.</li> <li>We are now adding data volume over time (records and size).</li> <li>Improved distinction between 0 and null values in metrics (total records, total fields, etc).</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_37","title":"General Fixes","text":"<ul> <li> <p>UI Wording and Display for Cataloged vs Profiled Fields:</p> <ul> <li>Addressed user confusion surrounding the display and wording used to differentiate between fields that have been cataloged versus those that have been profiled.</li> <li>Updated the messaging within the tree view and other relevant UI components to accurately reflect the state of fields post-catalog operation.</li> <li>Implemented a clear distinction between non-profiled and profiled fields in the field count indicators.</li> <li>Conducted a thorough review of the CTAs and descriptive text surrounding the Catalog, Profile, and Scan operations to improve clarity and user understanding.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.3.7","title":"2024.3.7","text":""},{"location":"changelog-2024/#general-fixes_38","title":"General Fixes","text":"<ul> <li> <p>Corrected MatchesPattern Checks Inference:</p> <ul> <li>Fixed an issue where the inference engine generated MatchesPattern checks that erroneously asserted false on more than 10% of training data. This resolution ensures all inferred checks now meet the 99% coverage criterion, aligning accurately with their training datasets.</li> </ul> </li> <li> <p>Fixed Multi-Field Check Parsing Error in DFS:</p> <ul> <li>Addressed a bug in DFS environments that caused parsing errors for checks asserting against multiple fields, such as AnyNotNull and NotNull, when selected fields contained white spaces. This resolution ensures that checks involving multiple fields with spaces are now accurately parsed and executed.</li> </ul> </li> <li> <p>Volumetric Measurements Tracking Fix:</p> <ul> <li>Addressed a bug that prevented the recording of volumetric measurements for containers without a last modified time. This fix corrects the problem by treating last_modification_time as nullable, ensuring that containers are now accurately tracked for volumetric measurements regardless of their modification date status.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.3.5","title":"2024.3.5","text":""},{"location":"changelog-2024/#feature-enhancements_37","title":"Feature Enhancements","text":"<ul> <li>Check Validation Improvement:<ul> <li>Enhanced the validation process for the \"Is Replica Of\" check. Previously, the system did not validate the field name and type, potentially leading to undetected issues until a Scan Operation was executed. Now, the validation process includes checking the field name and type, providing users with immediate feedback on any issues.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_39","title":"General Fixes","text":"<ul> <li> <p>Matches Pattern Data Quality Check Handling White Space:</p> <ul> <li>Resolved a bug in the Matches Pattern data quality check that caused white space to be ignored during training. With this fix, the system now accounts for white space during training, ensuring accurate pattern inference even with data containing significant white space. If 1% or more of the training data contains blanks, the system will derive a pattern that includes blanks as a valid value, improving data quality assessment.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.2.28","title":"2024.2.28","text":""},{"location":"changelog-2024/#feature-enhancements_38","title":"Feature Enhancements","text":"<ul> <li>User Token Management:<ul> <li>Transitioned from Generic Tokens to a more robust User Token system accessible under Settings for all users. This enhancement includes features to list, create, revoke, and delete tokens, offering granular control of API access. User activities through the API are now attributable, aligning actions with user accounts for improved accountability and traceability.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_40","title":"General Fixes","text":"<ul> <li> <p>Datetime Validation in API Requests:</p> <ul> <li>Strict validation of datetime entries in API requests has been implemented to require the Zulu datetime format. This update addresses and resolves issues where incomplete datetime entries could disrupt Scan operations, enhancing API reliability.</li> </ul> </li> <li> <p>Context-Aware Redirection Post-Operation:</p> <ul> <li>Enhanced the operation modal redirect functionality to be context-sensitive, ensuring that users are directed to the appropriate activity tab after an operation, whether at the container or datastore level. This enhancement ensures a logical and intuitive post-operation navigation experience.</li> </ul> </li> <li> <p>Template Details Page Responsiveness:</p> <ul> <li>Addressed layout issues on the Template Details page caused by long descriptions. Adjustments ensure that the description section now accommodates larger text volumes without disrupting the page layout, maintaining a clean and accessible interface.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.2.23","title":"2024.2.23","text":""},{"location":"changelog-2024/#feature-enhancements_39","title":"Feature Enhancements","text":"<ul> <li> <p>Introduction of Operations Management at the Table/File Level:</p> <ul> <li>The Activity tab has been added at the table/file level, extending its previous implementation at the source datastore level. This update provides users with the ability to view detailed information on operations for individual tables/files, including scan metrics, and histories of operation runs and schedules. It enhances the user's ability to monitor and analyze operations at a granular level.</li> </ul> </li> <li> <p>Enhanced Breadcrumb Navigation UX:</p> <ul> <li>Breadcrumb navigation has been improved for better user interaction. Users can now click on the breadcrumb representing their current context, enabling more intuitive navigation. In addition, selecting the Source Datastore breadcrumb takes users directly to the Activity tab, streamlining the flow of user interactions.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_41","title":"General Fixes","text":"<ul> <li> <p>Improved Accuracy in Profile and Scan Metrics:</p> <ul> <li>Enhanced the accuracy of metrics for profiled and scanned operations by excluding failed containers from the count. Now, metrics accurately reflect only those containers that have been successfully processed.</li> </ul> </li> <li> <p>Streamlined input display for Aggregation Comparison rule in Check/Template forms:</p> <ul> <li>Removed the \"Coverage\" input for the \"Aggregation Comparison\" rule in Check/Template Forms, as the rule does not support coverage customization. This simplification helps avoid confusion during rule configuration.</li> </ul> </li> <li> <p>Increased Backend Process Timeouts:</p> <ul> <li>In response to frequent timeout issues, the backend process timeouts have been adjusted. This change aims to reduce interruptions and improve service reliability by ensuring that processes have sufficient time to complete.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.2.19","title":"2024.2.19","text":""},{"location":"changelog-2024/#feature-enhancements_40","title":"Feature Enhancements","text":"<ul> <li> <p>Support for exporting Check Templates to the Enrichment Datastore:</p> <ul> <li>Added the ability to export Check Library metadata to the enrichment datastore. This feature helps users export their Check Library, making it easier to share and analyze check templates.</li> </ul> </li> <li> <p>File Upload Size Limit Handling:</p> <ul> <li>Implemented a user-friendly error message for file uploads that exceed the 20MB limit. This enhancement aims to improve user experience by providing clear feedback when the file size limit is breached, replacing the generic error message previously displayed.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_42","title":"General Fixes","text":"<ul> <li> <p>Resolved Parsing Errors in Expected Values Rule:</p> <ul> <li>Fixed an issue where single quotes in the list of expected values caused parsing errors in the Analytics Engine, preventing the Expected Values rule from asserting correctly. This correction ensures values, including those with quotes or special characters, are now accurately parsed and asserted.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.2.17","title":"2024.2.17","text":""},{"location":"changelog-2024/#general-fixes_43","title":"General Fixes","text":"<ul> <li> <p>Corrected Typing for Expected Values Check:</p> <ul> <li>Resolved an issue with the expectedValues rule, where numeric comparisons were inaccurately processed due to a misalignment between the API and the analytics engine. This fix ensures numeric values are correctly typed and compared, enhancing the reliability of validations.</li> </ul> </li> <li> <p>Fixed Anomaly Filtering in Scan Results dialog:</p> <ul> <li>Addressed a flaw where scan results did not consistently filter anomalies based on the operation ID. The fix guarantees that anomalies are only displayed once the operation ID parameter is accurately defined in the URL, ensuring more precise and relevant scan outcome presentations.</li> </ul> </li> <li> <p>Check Validation Sampling Behavior Adjustment:</p> <ul> <li>Fixed intermittent validation issues encountered in specific source datastore types (DB2, Microsoft SQL Server). The problem, where validation could unpredictably fail or succeed based on container size, was corrected by fine-tuning the sampling method for these technologies, leading to consistent validation performance.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.2.15","title":"2024.2.15","text":""},{"location":"changelog-2024/#feature-enhancements_41","title":"Feature Enhancements","text":"<ul> <li> <p>UX Improvements for Profile and Scan Operation Dialogs:</p> <ul> <li>Implemented significant UX enhancements to Profile &amp; Scan Operation Dialogs for improved clarity and user flow. Key improvements include:<ul> <li>Visibility of incremental fields and their current starting positions in Scan Operation dialogs.</li> <li>Logical reordering of Profile and Scan Operation steps to align with user workflows, including prioritizing container selection and clarifying the distinction between \"Starting Threshold\" and \"Limit\" settings.</li> </ul> </li> <li>Simplified operation initiation, allowing users to start operations directly before the final scheduling step, streamlining the process for immediate execution.</li> </ul> </li> <li> <p>Naming for Scheduled Operations:</p> <ul> <li>Added a name field to scheduled operations, enabling users to assign descriptive names or aliases. This feature aids in distinguishing and managing multiple scheduled operations more effectively.</li> </ul> </li> <li> <p>Container Name Filters for Operations:</p> <ul> <li>Provided filtering options for operations and scheduled operations by container name, improving the ability to quickly locate and manage specific operations.</li> </ul> </li> <li> <p>Improved Design for Field Identifiers in Tooltips:</p> <ul> <li>The design of field identifiers within tooltips has been refined for greater clarity. Enhancements focus on displaying Grouping Fields, Excluded Fields, Incremental Fields, and Partition Fields, aiming to offer users a more intuitive experience.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_44","title":"General Fixes","text":"<ul> <li> <p>External Scan Rollup Threshold Correction:</p> <ul> <li>Fixed an issue in external scans where the rollup threshold was not applied as intended. This correction ensures that anomalies exceeding the threshold are now accurately consolidated into a single shape anomaly, rather than being reported as multiple individual record anomalies.</li> </ul> </li> <li> <p>Repetitive Release Notification and Live Update Fixes:</p> <ul> <li>Resolved a recurring issue with release notifications continually prompting users to refresh despite acknowledgment. Additionally, it restored the live update notifications' functionality, ensuring users are correctly alerted to new features while actively using the system, with suggestions for a hard refresh to access the latest version.</li> </ul> </li> <li> <p>Corrected Field Input Logic in Check &amp; Template Forms:</p> <ul> <li>Addressed a logic error that incorrectly disabled field inputs for certain rules in check and template forms. This correction re-enables the necessary field input, removing a significant barrier that previously prevented users from creating checks affected by this issue.</li> </ul> </li> <li> <p>Addressed Absence of Feedback for No-Match Field Filters on Explore Page:</p> <ul> <li>Rectified the absence of feedback when field filters on the Explore Page yield no results, ensuring users receive a clear message indicating no items match the specified filter criteria.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.2.10","title":"2024.2.10","text":""},{"location":"changelog-2024/#feature-enhancements_42","title":"Feature Enhancements","text":"<ul> <li> <p>Immediate Execution Option for Scheduled Operations:</p> <ul> <li>Introduced a \"Run Now\" feature for scheduled operations, enabling users to execute operations immediately without waiting for the scheduled time. This addition provides flexibility in operation management, ensuring immediate execution as needed without altering the original schedule.</li> </ul> </li> <li> <p>Simplified Customization of Notification Messages:</p> <ul> <li>Removed the \"use custom message\" toggle from the notification form, making the message input field always editable. This change simplifies the user interface and improves usability by allowing direct editing of notification messages.</li> <li>Enhanced default messages for each notification trigger type have also been implemented to improve clarity.</li> </ul> </li> <li> <p>Performance Improvement in User Notifications Management:</p> <ul> <li>Implemented infinite scrolling pagination for the user notifications side panel. This update addresses performance issues with loading large numbers of notifications, ensuring a smoother and more responsive experience for users with extensive notification histories.</li> </ul> </li> <li> <p>Enhanced Archive Template Confirmation:</p> <ul> <li>Updated the archive dialog for templates to include information on the number of checks associated with archiving the template. This enhancement ensures users are aware of the impact of checks linked to the template, promoting informed decision-making.</li> </ul> </li> <li> <p>Improved Interaction with Computed Tables:</p> <ul> <li>Refined the Containers list UX to allow navigation to container details immediately after the creation of a computed table, addressing delays caused by background profiling. This improvement ensures users can access computed table details without waiting for the profile operation to complete, drawing inspiration from Tree View functionality for a more seamless experience.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_45","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements</li> </ul>"},{"location":"changelog-2024/#2024.2.2","title":"2024.2.2","text":""},{"location":"changelog-2024/#feature-enhancements_43","title":"Feature Enhancements","text":"<ul> <li>Excluded Fields Inclusion in Drop-downs:<ul> <li>Refined container settings to incorporate previously excluded fields in the dropdown list, enhancing user flexibility. In addition, a warning message has been added to notify users if a profile operation is required when deselecting excluded fields that were previously selected.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_46","title":"General Fixes","text":"<ul> <li> <p>Linkable Scan Results for Direct Access:</p> <ul> <li>Made Scan Results dialogs accessible via direct URL links, addressing previous issues with broken anomaly notification links. This enhancement provides users with a straightforward path to detailed scan outcomes.</li> </ul> </li> <li> <p>Property Display Refinement for Various Field Types:</p> <ul> <li>Corrected illogical property displays for specific field types like Date/Timestamp. The system now intelligently displays only properties relevant to the selected data type, eliminating inappropriate options. This update also includes renaming 'Declared Type' to 'Inferred Type' and adjusting the logic for accurate representation.</li> </ul> </li> <li> <p>Timezone Consistency in Insights and Activity Pages:</p> <ul> <li>Implemented improvements in timezone handling across Insights and Activity pages. These changes ensure that date aggregations are accurately aligned with the user's local time, eliminating previous inconsistencies compared to the Operations list results.</li> </ul> </li> <li> <p>Fixed breadcrumb display in the datastore for members with restricted permissions</p> <ul> <li>Enhanced the datastore interface to address issues faced by members with limited permissions. This update also fixes misleading breadcrumb displays and ensures that correct datastore enhancement information is visible.</li> </ul> </li> <li> <p>Resolved State Issue in Bulk Check Archive:</p> <ul> <li>Addressed a bug in the bulk selection process for archiving checks. The fix corrects an issue where the system recognized individual selections instead of the intended group selection due to an overlooked edge case.</li> </ul> </li> <li> <p>Improved Operation Modal State Management:</p> <ul> <li>Tackled state management inconsistencies in Operation Modals. Fixes include resetting the remediation strategy to its default and ensuring 'include' options do not carry over previous states erroneously.</li> </ul> </li> <li> <p>Eliminating Infinite Load for Non-Admin Enrichment Editing:</p> <ul> <li>Solved a persistent loading issue in the Enrichment form for non-admin users. Updates ensure a smoother, error-free interaction for these users, improving accessibility and functionality.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.1.30","title":"2024.1.30","text":""},{"location":"changelog-2024/#feature-enhancements_44","title":"Feature Enhancements","text":"<ul> <li> <p>Enhanced External Scan Operations:</p> <ul> <li>Improved data handling in External Scans by applying type casting to uploaded data using Spark. This update is particularly significant for date-time fields, which now expect and conform to ISO 8601 standards.</li> </ul> </li> <li> <p>Optimized DFS File Reading:</p> <ul> <li>Streamlined file reading in DFS by storing and utilizing the 'file_format' identified during the Catalog operation. This change eliminates the need for repeated format inspection on each read, significantly reducing overhead, especially for partitioned file types.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_47","title":"General Fixes","text":"<ul> <li> <p>Resolved DFS Reading Issues with Special Character Headers:</p> <ul> <li>Fixed a DFS reading issue where columns with headers containing special characters (like pipes |) adversely affected field profiling, including inaccuracies in histogram generation.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.1.26","title":"2024.1.26","text":""},{"location":"changelog-2024/#feature-enhancements_45","title":"Feature Enhancements","text":"<ul> <li> <p>Incremental Scan Starting Threshold:</p> <ul> <li>Introduced a \"Starting Threshold\" option for incremental Scans. This feature allows users to manually set a starting value for the incremental field in large tables, bypassing the need to scan the entire dataset initially. It's handy for first-time scans of massive databases, facilitating more efficient and targeted data scanning.</li> </ul> </li> <li> <p>Add Support for Archiving Anomalies:</p> <ul> <li>Implemented the capability of archiving anomalies. Users can now remove anomalies from view without permanently deleting them, providing greater control and flexibility in anomaly management.</li> </ul> </li> <li> <p>External Scan Operation for Ad hoc Processes:</p> <ul> <li>Introduced 'External Scan Operation' as a new feature enabling ad hoc data validation for all containers. This operation allows users to validate ad hoc data, such as Excel or CSV files, against a container's existing checks and enrichment configuration. The provided file's structure must align with the container's schema, ensuring a seamless validation process.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_48","title":"General Fixes","text":"<ul> <li> <p>Preventing Unrelated Entity Selection in Check Form:</p> <ul> <li>Fixed an issue in the Check Form where users could inadvertently select unrelated entities. Selecting datastores, containers, and fields is restricted during any ongoing data loading, preventing mismatched entity selections.</li> </ul> </li> <li> <p>Performance enhancements for BigQuery and Snowflake removing the need for count operations during full table analysis</p> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.1.23","title":"2024.1.23","text":""},{"location":"changelog-2024/#feature-enhancements_46","title":"Feature Enhancements","text":"<ul> <li> <p>Introduction of 'Expected Schema' Rule for Advanced Schema Validation:</p> <ul> <li>Introduced the 'Expected Schema' rule, replacing the 'Required Fields' rule. This new rule asserts that all selected fields are present and their data types match predefined expectations, offering more comprehensive schema validation. It also includes an option to validate additional fields added to the schema, allowing users to specify whether the presence of new fields should cause the check to fail.</li> </ul> </li> <li> <p>Refined Tree Navigation Experience:</p> <ul> <li>Updated the tree navigation to prevent automatic expansion of nodes upon selection and eliminated the auto-reset behavior when re-selecting an active node. These changes provide a smoother and more user-friendly navigation experience, especially in tables/files with numerous fields.</li> </ul> </li> <li> <p>Locked/Unlocked Status Filter in Library Page:</p> <ul> <li>Added a new filter feature to the Library page, enabling users to categorize and view check templates based on their Locked or Unlocked status. This enhancement simplifies the management and selection of templates.</li> </ul> </li> <li> <p>Improved Messaging for Locked Template Properties in Check Form:</p> <ul> <li>Enhanced the Check Form UX by adding informative messages explaining why certain inputs are disabled when a check is associated with a locked template. This update enhances user understanding and interaction with the form.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_49","title":"General Fixes","text":"<ul> <li> <p>Corrected Insights Metrics for Check Templates:</p> <ul> <li>Fixed an issue where check templates were incorrectly counted as checks in related metrics and counts on the Insights page. Templates are now appropriately filtered out, ensuring accurate representation of check-related data.</li> </ul> </li> <li> <p>Enabled Template Creation with Calculated Rules:</p> <ul> <li>Resolved a limitation that prevented the creation of templates using calculated rules like 'Satisfies Expression' and 'Aggregation Comparison'. This fix expands the capabilities and flexibility of template creation.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.1.11","title":"2024.1.11","text":""},{"location":"changelog-2024/#feature-enhancements_47","title":"Feature Enhancements","text":"<ul> <li> <p>Introduction of Check Templates:</p> <ul> <li>Implemented Check Templates to offer a balance between flexibility and consistency in quality check management. Checks can now be associated with templates in either a 'locked' or 'unlocked' state, allowing for synchronized properties or independent customization, respectively. This feature streamlines check management and enables efficient tracking and review of anomalies across all checks associated with a template.</li> </ul> </li> <li> <p>isType Rule Implementation:</p> <ul> <li>Replaced the previous dataType rule with the new isType rule for improved accuracy and understanding. The isType rule is now specifically tailored to assert only against string fields, enhancing its applicability and effectiveness.</li> </ul> </li> <li> <p>Enhanced Container Details Page with Identifier Icons:</p> <ul> <li>Updated the Container Details page to display icons for key container identifiers, including Partition Field, Grouping Fields, and Exclude Fields. This enhancement provides a more intuitive and informative user interface, facilitating easier identification and understanding of container characteristics.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_50","title":"General Fixes","text":"<ul> <li> <p>Notification System Reliability Improvement:</p> <ul> <li>Fixed intermittent failures in the notifications system. Users will now receive reliable notifications for identified anomalies, ensuring timely awareness and response to data irregularities.</li> </ul> </li> <li> <p>Safeguard Against Overlapping Scheduled Operations:</p> <ul> <li>Implemented a mechanism to prevent the overloading of deployments due to overlapping scheduled operations. If a scheduled operation doesn\u2019t complete before its next scheduled run, the subsequent run will be skipped, thereby avoiding potential strain on system resources.</li> </ul> </li> <li> <p>Correction of Group-by Field Display in Containers:</p> <ul> <li>Resolved an issue where selected grouping fields were not appearing in the list fields of a container. This fix ensures that user-specified fields for group-by operations are correctly displayed, maintaining the integrity of data organization and analysis.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2024/#2024.1.4","title":"2024.1.4","text":""},{"location":"changelog-2024/#feature-enhancements_48","title":"Feature Enhancements","text":"<ul> <li>Enhanced Warnings for Schema Inconsistencies in Files Profiled<ul> <li>Improved the warning message for cases where the user profiles files with different schemas under a single glob pattern. This update ensures users receive clear, helpful information when files within a glob have inconsistent structures.</li> </ul> </li> </ul>"},{"location":"changelog-2024/#general-fixes_51","title":"General Fixes","text":"<ul> <li> <p>Containers with 'Group By' settings Leading to Erroneous Profile Operation</p> <ul> <li>Fixed an issue affecting profile operations which included containers with 'Group By' settings. Previously, running a profile without inferring checks resulted in all fields being erroneously removed from the field list.</li> </ul> </li> <li> <p>General Fixes and Improvements</p> </li> </ul>"},{"location":"changelog-2025/","title":"2025","text":""},{"location":"changelog-2025/#release-notes","title":"Release Notes","text":""},{"location":"changelog-2025/#2025.12.23","title":"2025.12.23","text":""},{"location":"changelog-2025/#feature-enhancements","title":"Feature Enhancements","text":"<ul> <li> <p>Added Jira Cloud ticketing integration and enhanced ServiceNow with search and link capabilities.</p> <ul> <li>Connect with Jira Cloud or ServiceNow to streamline anomaly management through centralized ticket tracking.</li> <li>Introduced ticketing section on the anomaly page with quick actions for creating or linking tickets.</li> <li>Users can search for existing tickets and link them directly to anomalies.</li> <li>Supports retrieving ticket details by ID or key for status tracking.</li> </ul> </li> <li> <p>Introduced Group by clause support for Computed Joins and Computed Files.</p> <ul> <li>Users can now define grouping expressions when creating computed joins and computed files, enabling aggregation functions in select clauses.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements","title":"General Fixes and Improvements","text":"<ul> <li> <p>Improved Insights endpoint performance with optimized quality score queries.</p> </li> <li> <p>Enhanced Agent Q with global data asset search, async operation monitoring, and metadata management capabilities.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.12.12","title":"2025.12.12","text":""},{"location":"changelog-2025/#feature-enhancements_1","title":"Feature Enhancements","text":"<ul> <li> <p>Launched ServiceNow Ticketing System integration.</p> <ul> <li>Connect Qualytics with ServiceNow to streamline anomaly management through automated incident creation and tracking.</li> <li>Users can create ServiceNow incidents directly from anomalies via API, populating incident details automatically.</li> <li>Anomaly status changes in Qualytics are pushed to the linked ServiceNow incident as work notes.</li> <li>Comments added to anomalies are automatically synced to the corresponding ServiceNow incident.</li> </ul> </li> <li> <p>Introduced Microsoft Purview Data Catalog integration.</p> <ul> <li>Connect Qualytics with Microsoft Purview to enrich catalog assets with actionable data quality context.</li> <li>Supports importing tags from Purview to replace matching Qualytics tags, ensuring consistent tagging and classification.</li> <li>Publishes quality scores and their dimensions, along with active check counts and active anomaly counts, to corresponding Purview assets.</li> <li>Event-driven synchronization keeps Purview up to date based on Qualytics events, including scan completions, anomaly status changes, anomaly archiving, check updates, and other actions that trigger quality score recalculation.</li> </ul> </li> <li> <p>Added new beta Data Catalog integrations for Collibra and DataHub.</p> <ul> <li>Enables synchronization of Qualytics data quality metadata with external data catalog platforms.</li> <li>Supports importing tags from the catalog provider to replace matching Qualytics tags, keeping tags consistent across platforms.</li> <li>Publishes quality scores, active check counts, and anomaly metrics to corresponding catalog assets.</li> <li>Uses event-driven synchronization to automatically propagate metadata updates triggered by data quality events in Qualytics.</li> </ul> </li> <li> <p>Enhanced Flows with scheduling support and node management improvements.</p> <ul> <li>Introduced scheduled trigger type, allowing users to configure flows triggered by cron expressions.</li> <li>Flow list now displays schedule information and next trigger time for scheduled flows.</li> <li>Included filters by trigger type and deactivation status to the flow definitions page.</li> <li>Enabled bulk activate and deactivate actions for flows.</li> <li>Incorporated Activate/Deactivate toggle to the Flow Settings panel for quicker access to flow status controls.</li> <li>Users can now rearrange action nodes after creation using drag-and-drop to move nodes and their descendants to a new parent.</li> <li>Swap functionality allows exchanging a node's position with its parent in a single click while maintaining flow hierarchy.</li> <li>Visual feedback during rearrangement indicates valid and invalid drop targets for intuitive editing.</li> </ul> </li> <li> <p>Introduced Cast Type computed field.</p> <ul> <li>New computed field type allows casting fields to different data types without writing raw expressions.</li> <li>Supports standard SQL CAST function with direct JDBC dialect compatibility for improved cross-datastore portability.</li> <li>Enabled date format specification support for Cast Type computed fields when converting non-standard date and timestamp formats.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_1","title":"General Fixes and Improvements","text":"<ul> <li> <p>Corrected sampling metrics in container profiles where records processed could exceed total record count, resulting in inaccurate percentage calculations.</p> </li> <li> <p>Adjusted the \"Drop Suffix\" option to be active by default in the Computed Field dialog.</p> </li> <li> <p>Addressed Snowflake permissions error affecting enrichment operations in JDBC datastores.</p> </li> <li> <p>Fixed quality score calculation that could produce negative accuracy values in certain conditions.</p> </li> <li> <p>Removed auto-activation in check activation dialog to prevent accidental activations.</p> </li> <li> <p>Corrected scan operations to properly report enrichment failures in operation results instead of showing success status.</p> </li> <li> <p>Resolved row count mismatch between Overview and Observability charts caused by timezone alignment issues.</p> </li> <li> <p>Fixed single-value <code>each()</code> keyword not generating permutations correctly in check templates.</p> </li> <li> <p>Cleared Delta Table metadata cache issue that persisted after container operations completed.</p> </li> <li> <p>Resolved connection verification failures not returning operation results to the Controlplane.</p> </li> <li> <p>Updated Scan Settings labels and descriptions that caused confusion about threshold behavior and scanning limits.</p> </li> <li> <p>Optimized materialize operation performance with enhanced partition calculation.</p> </li> <li> <p>Added Engine information on the Settings Status page, displaying the deployment engine (Kubernetes, or Databricks).</p> </li> <li> <p>Refined error messages for sync operation timeouts to better distinguish between Dataplane availability and datastore response issues.</p> </li> <li> <p>Added visual in-progress indicator to Data Volume Over Time charts when measurements are still being recorded for the current period.</p> </li> <li> <p>Included tooltip explanation on Data Volume charts indicating when current period measurements are still in progress.</p> </li> <li> <p>Enhanced Data Volume charts to display the previous period's value as a placeholder when current period data is not yet available.</p> </li> <li> <p>Optimized scan performance by eliminating unnecessary data shuffling when applying record limits.</p> </li> <li> <p>Implemented bulk deactivate and activate actions for schedules within the datastore context.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.11.27","title":"2025.11.27","text":""},{"location":"changelog-2025/#feature-enhancements_2","title":"Feature Enhancements","text":"<ul> <li> <p>Introduced Quality Score Over Time and enhanced Insights dashboard.</p> <ul> <li>Quality Scores are now tracked historically, allowing users to monitor data quality trends over time at the datastore, container, and field levels.</li> <li>Added interactive Quality Score charts displaying score progression with dimension-level breakdown.</li> <li>Introduced a new Containers Scanned metric with trend indicators in Insights</li> <li>Added the Anomaly Rule Type Distribution histogram in Insights to identify which rule types generate the most anomalies.</li> <li>Quality Score Dimensions now include detailed explanations and recommendations for scores below 30.</li> <li>Added preview charts for field metrics, providing at-a-glance trend visualization alongside each metric.</li> <li>Reorganized Insights page layout with improved section grouping for better visual hierarchy.</li> <li>Quality Score history is available at both the total score and individual dimension levels for detailed trend analysis.</li> </ul> </li> <li> <p>Introduced automated anomaly lifecycle management with new Archive and Delete flow actions.</p> <ul> <li>Both actions target anomalies with no recent activity, including status updates, assignments, or comments within the specified inactivity timeframe.</li> <li>Archive action supports status selection, optional tag assignment, and comments to provide context during archiving.</li> <li>Anomalies can be filtered by source datastores and tags for both actions, with additional status filters available for Delete operations.</li> <li>Notification messages now support dynamic tokens such as {{ anomaly_count }} and {{ inactivity_timeframe }} for context-aware alerts.</li> <li>Archive and Delete operations can be aborted mid-execution, but any anomalies already processed will not be rolled back.</li> </ul> </li> <li> <p>Improved Check Templates with automated check permutation generation from filter arrays.</p> <ul> <li>Introduced <code>each()</code> keyword that automatically expands a single filter into multiple checks, one for each specified value in the array.</li> <li>Supports multiple <code>each()</code> clauses in a single filter to generate all possible value combinations across different dimensions.</li> <li>Template locking behavior determines check lifecycle\u2014locked templates delete checks when values are removed, while unlocked templates preserve existing checks.</li> </ul> </li> <li> <p>Added an \u201cActivate\u201d option for draft checks on the check page.</p> <ul> <li>Draft checks can now be activated directly from the gear menu on the check overview page, eliminating the need to navigate back to the field checks list.</li> <li>Includes persistent error notifications that remain visible until manually dismissed, ensuring users don't miss critical errors during activation.</li> </ul> </li> <li> <p>Improved Dry Run validation for <code>Data Diff</code> checks with filters.</p> <ul> <li>Dry Run now queries the number of records matching the filter to compare left and right side row counts, mirroring the behavior of checks without filters.</li> </ul> </li> <li> <p>Added ability to modify relative path on file patterns.</p> <ul> <li>Users can now edit the relative path field, enabling advanced globbing patterns for file selection.</li> </ul> </li> <li> <p>Added View Results button for Materialize and Export operations.</p> <ul> <li>Users can now navigate directly to the Enrichment Materialize or Export tab with a single click from the operation card.</li> <li>Enrichment container tabs are now router-based, enabling direct navigation and shareable URLs to specific tabs.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_2","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fixed history flow side panel not displaying after specific flow editing operations.</p> </li> <li> <p>Corrected Quality Score displaying as 0 when navigating between datastores.</p> </li> <li> <p>Resolved Expected Values check displaying a blank list when configured with numeric values.</p> </li> <li> <p>Fixed issue where Observability triggers were not being recorded properly.</p> </li> <li> <p>Correct operations getting stuck in running status instead of being marked as failed.</p> </li> <li> <p>Resolved flow executions getting stuck in running status when multiple actions completed concurrently.</p> </li> <li> <p>Flow execution nodes now display historical action configuration in their inline views for accurate review of past executions.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.11.6","title":"2025.11.6","text":""},{"location":"changelog-2025/#feature-enhancements_3","title":"Feature Enhancements","text":"<ul> <li> <p>Introduced Service Users and Enhanced Token Management</p> <ul> <li>Added dedicated accounts for API integrations, separating service credentials from personal user accounts.  </li> <li>Service Users include automatic token generation, making it easier to set up integrations and manage automated workflows.  </li> <li>The Tokens page has been redesigned with a new tabbed layout that separates <code>Personal Tokens</code> from <code>Service Tokens</code>, with quick navigation to user details and improved search capabilities.  </li> <li>Token generation has been improved with download-as-file support, clearer copy feedback, and the ability to generate tokens directly from the user settings dropdown.</li> </ul> </li> <li> <p>Added historical settings view for flow executions.</p> <ul> <li>Flow execution actions now display the configuration used during their execution.</li> <li>Each action's settings are preserved, enabling accurate historical review and troubleshooting of past executions.</li> <li>Actions in the flow execution view are now clickable, allowing users to inspect their historical configuration details.</li> </ul> </li> <li> <p>Introduced <code>Discarded</code> status for anomalies  </p> <ul> <li>New archive option allows marking anomalies as no longer relevant without implying resolution or error.  </li> <li>Added ability to reactivate acknowledged anomalies, allowing them to be moved back to the active state.</li> </ul> </li> <li> <p>Improved computed table creation for SQL Server, Oracle, and Redshift.</p> <ul> <li>Autocomplete suggestions now display table names with schema prefixes for these JDBC datastores.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_3","title":"General Fixes and Improvements","text":"<ul> <li> <p>Corrected issue where operation status remained unchanged after completion in container activity displays.</p> </li> <li> <p>Resolved webhook integration issues for Microsoft Teams and Slack that prevented interactive features from working properly.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.10.23","title":"2025.10.23","text":""},{"location":"changelog-2025/#feature-enhancements_4","title":"Feature Enhancements","text":"<ul> <li> <p>Expanded Command Palette with quick access to creation actions across datastores, flows, templates, and settings.</p> </li> <li> <p>Enhanced operation tracking by adding end time display in datastore activity and navigation links for container profiles and scans.</p> <ul> <li>Operation duration tooltips now show completion time for finished operations.</li> <li>Profile and scan tooltips now include clickable links in tooltips that navigate to the corresponding operation in the activity view.</li> </ul> </li> <li> <p>Added abort option for flow executions.</p> <ul> <li>New abort action available in flow execution list and details pages.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_4","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fixed volumetric chart threshold bar calculations to correctly use the configured window size in Absolute and Percentage Change comparisons.</p> </li> <li> <p>Corrected missing runtime display for fast export operations.</p> </li> <li> <p>Improved Quality Score calculation reliability and error handling.</p> </li> <li> <p>Fixed table count display showing double the actual number of tables in Snowflake and Oracle datastores when members belong to multiple teams.</p> </li> <li> <p>Added validation to prevent computed fields from being used as partition or incremental fields in container configurations.</p> </li> <li> <p>Corrected user list filtering and sorting errors when combining team filters with team-based sorting.</p> </li> <li> <p>Resolved container profile creation errors caused by concurrent operations processing inferred quality checks.</p> </li> <li> <p>Corrected materialize operation failures for glob-pattern containers by sanitizing invalid characters in output table names.</p> </li> <li> <p>Resolved Quality Score calculation failures when container rowcount</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.10.15","title":"2025.10.15","text":""},{"location":"changelog-2025/#feature-enhancements_5","title":"Feature Enhancements","text":"<ul> <li> <p>Introduced fuzzy search to form and filter inputs.</p> <ul> <li>Search results now tolerate typos and partial matches for more natural filtering.</li> </ul> </li> <li> <p>Improved Expected Values and Required Values check configuration with visual warnings for spacing issues.</p> <ul> <li>Values with trailing or leading spaces now display in warning-colored chips.</li> <li>A tooltip will show when a value containing extra spaces.</li> </ul> </li> <li> <p>Added informational message to Flow action datastores selection explaining filter criteria.</p> </li> <li> <p>Improved DFS datastore overview to display file format type.</p> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_5","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fixed data write failures in BigQuery enrichment datastores caused by message size limits exceeding API thresholds.</p> </li> <li> <p>Optimized operation triggering endpoint performance, including scheduled runs.</p> </li> <li> <p>Improved Quality Score calculation accuracy according to Quality Score Dimensions.</p> </li> <li> <p>Fixed breadcrumb navigation not updating correctly when cloning checks and changing field or container context.</p> </li> <li> <p>Improved background task processing for enhanced system reliability and performance with optimized task execution.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.10.3","title":"2025.10.3","text":""},{"location":"changelog-2025/#feature-enhancements_6","title":"Feature Enhancements","text":"<ul> <li> <p>Optimized computed tables, files, and joins creation process.</p> <ul> <li>Now the creation is much faster with optimized validation and asynchronous profiling.</li> <li>Added \"Validate\" button to check only syntax and semantics, eliminating wait times for full data profiling.</li> </ul> </li> <li> <p>Enhanced Quality Score calculation with improved clarity and transparency.</p> <ul> <li>Renamed \"Quality Score Factors\" to \"Quality Score Dimensions\" throughout the application for better conceptual understanding.</li> <li>Quality scores now better reflect data fitness for intended use cases rather than simple error counts.</li> </ul> </li> <li> <p>Introduced \"Has Logs\" filter to display operations that completed with logs.</p> </li> <li> <p>Added sort by \"Last Triggered\" option to the Flows list page for better workflow management.</p> </li> <li> <p>Improved page metadata for better link sharing across Flow, Check, Anomaly, Library, and Enrichment pages.</p> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_6","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fixed validation error when clearing filter clause fields in computed table and join forms.</p> </li> <li> <p>Corrected Ctrl+C copy functionality that was blocked by command palette shortcuts.</p> </li> <li> <p>Resolved delete button visibility for archived anomalies in both light and dark themes.</p> </li> <li> <p>Fixed inconsistent text styling for \"No inference\" label.</p> </li> <li> <p>Corrected catalog operation incorrectly identifying Iceberg metadata files as data files.</p> </li> <li> <p>Resolved bulk selection checkboxes disappearing after multiple select/deselect cycles.</p> </li> <li> <p>Fixed error when updating computed tables with excluded fields.</p> </li> <li> <p>Corrected icon alignment in collapsed datastore tree view sidebar.</p> </li> <li> <p>Resolved missing background color for \"Not Asserted\" status in Insights checks section.</p> </li> <li> <p>Fixed misleading error messages when attempting to edit comments on archived anomalies.</p> </li> <li> <p>Corrected a regression in computed joins that prevented using columns with non-normalized names in join conditions.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.9.17","title":"2025.9.17","text":""},{"location":"changelog-2025/#feature-enhancements_7","title":"Feature Enhancements","text":"<ul> <li> <p>We are happy to announce the new Keyboard Commands feature</p> <ul> <li>Execute common actions using context-sensitive shortcuts that adapt based on your current location and selection within the application.</li> <li>Quickly navigate between main pages using intuitive multi-key sequences with the new \"Go to\" shortcuts.  <ul> <li>Press <code>G</code> followed by a specific key to quickly jump to pages like: <code>G</code> + <code>E</code> for Explore, <code>G</code> + <code>L</code> for Library, <code>G</code> + <code>T</code> for Tags, and etc.</li> </ul> </li> <li>Enhance Tab navigation allows smooth movement through interface elements using <code>\u2325</code> / <code>Alt</code> + <code>\u2190</code> and <code>\u2325</code> / <code>Alt</code> + <code>\u2192</code> with clear visual focus indicators.</li> <li>Update the global search shortcut to be accessible via <code>\u2318</code>/<code>Ctrl</code> + <code>Shift</code> + <code>F</code>.</li> <li>Access the Command Palette by pressing <code>\u2318</code>/<code>Ctrl</code> + <code>K</code> to search, navigate, and execute actions without leaving your current context.</li> <li>View a complete list of all available keyboard shortcuts by pressing <code>\u2318</code>/<code>Ctrl</code> + <code>/</code> for easy reference and learning.</li> </ul> </li> <li> <p>Improved hover highlighting to Data Preview, Source Records, and Enrichment tables for consistent user experience across all data views.</p> </li> <li> <p>Enhanced queries with metadata comments for better cost tracking and operation identification in query logs.</p> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_7","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fixed profile operations failing on BigQuery tables with nested record types.</p> </li> <li> <p>Limited anomaly rollup threshold to 1,000 with visual markers to prevent system overload from excessive anomaly generation.</p> </li> <li> <p>Fixed dataDiff validation errors when using computed fields as row identifiers.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.9.12","title":"2025.9.12","text":""},{"location":"changelog-2025/#feature-enhancements_8","title":"Feature Enhancements","text":"<ul> <li> <p>We are thrilled to introduce the Profiles Over Time for tracking field-level data changes</p> <ul> <li>Users can now compare the current field profile with previous profiles to track changes.</li> <li>Visual indicators highlight metrics that changed between selected profiles.</li> <li>Interactive charts display numeric metric trends across profile history.</li> <li>Easily identify data drift and type changes at the field level.</li> <li>Special badges indicate when field types have changed between profiles.</li> </ul> </li> <li> <p>Introducing the <code>Data Diff</code> check as an enhancement of the Is Replica Of functionality.</p> </li> <li> <p>Enhanced File Profile visibility with file format display and improved handling of long names with tooltips.</p> </li> <li> <p>Improved hover contrast for list items in light mode for better visibility.</p> </li> <li> <p>Optimized slim profile logic to protect existing field typing from being overwritten by limited data samples.</p> </li> <li> <p>Added OAuth support to the Databricks connector.</p> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_8","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fixed broken enrichment datastore redirect link in the datastore tree footer.</p> </li> <li> <p>Corrected filter application in <code>Exists In</code> checks that was causing inaccurate anomaly detection.</p> </li> <li> <p>Fixed grouped inference checks to properly validate against filtered test data for each group combination.</p> </li> <li> <p>Resolved <code>Is Address</code> check failing to assert during scan.</p> </li> <li> <p>Removed <code>User Defined Function</code> check support.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.9.4","title":"2025.9.4","text":""},{"location":"changelog-2025/#feature-enhancements_9","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing Product Updates</p> <ul> <li>Users can now view feature announcements directly within the application.</li> <li>Read full posts by clicking the external link for each update.</li> </ul> </li> <li> <p>Enhanced filter clause display in readonly checks with copy functionality and improved text formatting.</p> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_9","title":"General Fixes and Improvements","text":"<ul> <li> <p>Resolved an issue where the API allowed creation of quality checks with string fields for rules that don't support them.</p> </li> <li> <p>Corrected cache logic to avoid unnecessary data refresh in insights.</p> </li> <li> <p>Fixed input field overflow when entering long filter values that caused UI layout issues.</p> </li> <li> <p>Fixed Data Preview errors when filters return no records, now properly displays empty results instead of failing with a server error.</p> </li> <li> <p>Fixed activation failures for Exists In checks that reference computed fields.</p> </li> <li> <p>Fixed issue where rule type filter options did not appear on the check list page for Member users.</p> </li> <li> <p>Updated enrichment processing for more frequent and reliable data writes.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.8.20","title":"2025.8.20","text":""},{"location":"changelog-2025/#feature-enhancements_10","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing Bulk Quality Check Creation through Templates</p> <ul> <li>Users can now create multiple quality checks from a single template.</li> <li>Select multiple target containers across different datastores and apply the same template configuration.</li> </ul> </li> <li> <p>Announcing Pause Schedule operation</p> <ul> <li>Users can now deactivate and reactivate schedules without losing configuration.</li> <li>Added a new filter option to show only deactivated schedules.</li> </ul> </li> <li> <p>Introducing Test Connection capability for existing connections</p> <ul> <li>Users can now verify connection changes before saving when editing a connection configurations.</li> </ul> </li> <li> <p>Improved isReplicaOf check to better handle incremental data comparisons.</p> </li> <li> <p>Added support for connecting to Kerberos-secured Hive datastores.</p> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_10","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fixed Data Preview filtering to properly indicate that computed fields are not supported in WHERE clauses and not exposing as auto complete options.</p> </li> <li> <p>Fixed issues related to ANSI SQL compliance in Spark 4.</p> </li> <li> <p>Fixed issues with nested data types in the Databricks connector.</p> </li> <li> <p>Corrected inconsistent formatting in operation details where containers read appeared larger than total containers.</p> </li> <li> <p>Resolved filter UI issues where selected items caused layout misalignment.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.8.8","title":"2025.8.8","text":""},{"location":"changelog-2025/#feature-enhancements_11","title":"Feature Enhancements","text":"<ul> <li> <p>Enhanced Source Record visualization</p> <ul> <li>Users can now view more source records with selectable limits (10, 100, 1000, or 10000 records).</li> <li>Added sticky headers for easier navigation when scrolling through large datasets.</li> </ul> </li> <li> <p>Introducing Quality Check Migration</p> <ul> <li>Users can now migrate quality checks from one container to another, even across different datastores.<ul> <li>Archived and inferred checks are excluded from migration.</li> </ul> </li> <li>Migrated checks are set to Draft status for users review before activation.</li> </ul> </li> <li> <p>Enhanced search functionality for datastores and flows</p> <ul> <li>Users can now search by ID or name using a single search field.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_11","title":"General Fixes and Improvements","text":"<ul> <li> <p>Updated Dataplane to Spark 4</p> </li> <li> <p>Fixed Is Replica Of dry run validation to correctly handle filtered datasets.</p> </li> <li> <p>Resolved an issue where anomaly action buttons redirect to the details page instead of performing the intended action.</p> </li> <li> <p>Corrected the navigation issue when switching between the Checks and Observability tabs that prevented lists from rendering properly.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#api-changes","title":"API Changes","text":"<ul> <li>The following endpoint <code>GET /operations</code> is affected:<ul> <li>Added <code>start_date</code> and <code>end_date</code> query parameters to filter operations by date range.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#2025.7.28","title":"2025.7.28","text":""},{"location":"changelog-2025/#feature-enhancements_12","title":"Feature Enhancements","text":"<ul> <li>Improved Spark SQL autocomplete handling for complex field names<ul> <li>SQL autocomplete now automatically adds backticks to field names, preventing errors with special characters.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_12","title":"General Fixes and Improvements","text":"<ul> <li> <p>Resolved an issue where scheduled operations were not executing reliably under high load.</p> </li> <li> <p>Fixed last asserted timestamp accuracy for all check types.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.7.23","title":"2025.7.23","text":""},{"location":"changelog-2025/#feature-enhancements_13","title":"Feature Enhancements","text":"<ul> <li> <p>Simplified Computed Join prefix management</p> <ul> <li>Select expressions automatically adjust when prefixes change, eliminating manual field name updates.</li> </ul> </li> <li> <p>Added User Guide links to Check Creation form</p> <ul> <li>Selecting a Rule Type provides a direct link to that specific rule's documentation in the User Guide.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_13","title":"General Fixes and Improvements","text":"<ul> <li> <p>Corrected Quality Check Update errors affecting <code>Greater Than Field</code>, <code>Less Than Field</code>, and <code>Equal To Field</code> rule types.</p> </li> <li> <p>Fixed source record display for high-precision decimal values</p> <ul> <li>Source records now display full decimal precision on hover for truncated numeric values.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.7.18","title":"2025.7.18","text":""},{"location":"changelog-2025/#feature-enhancements_14","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing Computed Join for creating new containers by joining data across different datastores</p> <ul> <li>Users can now create computed joins between two containers, even from different datastores, enabling cross-datastore data analysis.</li> <li>Supports multiple join types: Inner Join, Left Join, Right Join, and Full Outer Join to accommodate various data combination needs.</li> <li>Configure joins using intuitive left and right reference selections with field mapping and optional prefixes.</li> <li>Add custom SELECT expressions and WHERE clauses to refine the joined data output.</li> </ul> </li> <li> <p>Introducing Dry Run operation for draft checks</p> </li> <li> <p>Enhanced Bulk Check Operations and Management Capabilities</p> <ul> <li>Added metadata field to bulk update dialog, enabling users to update metadata across multiple checks simultaneously without opening each individually.</li> <li>Extended bulk operations to support archived checks, previously limited to active only.</li> <li>Bulk activate and draft actions now available for archived checks, expanding beyond the previous delete-only option.</li> </ul> </li> <li> <p>Added Subject Field to Email Notifications</p> <ul> <li>Email notifications now support customizable subject lines, allowing users to add meaningful context to their messages.</li> </ul> </li> <li> <p>Enhanced Record Limit Configuration</p> <ul> <li>Users can now manually input custom record limit values in Profile and Scan operations, as well as Flow operations through a text field, providing flexibility beyond the predefined options.</li> <li>A dropdown menu provides quick access to common values (1M, 10M, 100M, All).</li> </ul> </li> <li> <p>Adding Unlink Enrichment Datastore</p> <ul> <li>Users can now unlink enrichment datastores directly from the \"Enrichment Datastore Settings\" dialog.</li> </ul> </li> <li> <p>Improved Datastore Deletion Experience</p> <ul> <li>Error messages during deletion now appear directly within the confirmation dialog instead of temporary toast notifications.</li> <li>When deleting an Enrichment Datastore, the dialog now displays the number of linked source datastores and uses clear labeling to distinguish between datastore types.</li> </ul> </li> <li> <p>Enhanced catalog operation to properly recognize subdirectories within partitioned file structures, ensuring more accurate container identification for complex directory hierarchies.</p> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_14","title":"General Fixes and Improvements","text":"<ul> <li> <p>Addressed modal dismissal issues across multiple dialogs where clicking outside or pressing ESC would cause accidental closure and data loss.</p> </li> <li> <p>Fixed \"Open in new tab\" and \"Copy link\" actions for checks and anomalies that were not functioning correctly.</p> </li> <li> <p>Fixed source record formatting for 'Is Replica Of' anomalies when check configuration changes after anomaly detection.</p> </li> <li> <p>Fixed an issue where anomaly URLs generated from check side panels were not functioning correctly when shared.</p> </li> <li> <p>Fixed incorrect redirection after creating checks from templates.</p> </li> <li> <p>Fixed an issue where source records weren't displaying correctly during dry run operations.</p> </li> <li> <p>Corrected cloning behavior to preserve tags from the check being cloned.</p> </li> <li> <p>Fixed scan operations failing after deleting or unlinking enrichment datastores.</p> </li> <li> <p>Corrected failed checks information in anomaly responses to accurately reflect the historical check version at the time the anomaly was detected, rather than showing the current check version.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.7.2","title":"2025.7.2","text":""},{"location":"changelog-2025/#feature-enhancements_15","title":"Feature Enhancements","text":"<ul> <li> <p>Introduced Failed Check Version display, providing visibility into the exact check configuration that triggered each anomaly.</p> <ul> <li>Failed Check now displays the specific check properties and configuration that were active when the anomaly was generated.</li> </ul> </li> <li> <p>Enhanced Check Activities visualization.</p> <ul> <li>Users can now view historical check configurations directly from the timeline, including all properties and tags as they were at that point in time.</li> </ul> </li> <li> <p>Enhanced flow nodes with improved visual design and contextual information display for better user experience.</p> <ul> <li>Action nodes now show inline summaries with relevant details based on their type (e.g., datastore names for operation actions, channel names for Slack or Teams, URLs for webhooks, etc).</li> <li>Export nodes now display asset types in their titles (e.g., \"Export Anomalies\").</li> <li>Added filter tooltips to trigger nodes displaying applied conditions (tags, datastores, operation types) for quick configuration visibility.</li> </ul> </li> <li> <p>Supports Data Preview functionality for containers that haven't been profiled yet, removing the requirement to profile first before viewing data.</p> </li> <li> <p>Enhanced editing flexibility for Asserted Checks.</p> <ul> <li>Users can now edit SparkSQL expressions that define calculated fields.</li> <li>Row Identifiers and Passthrough Fields are now editable for Is Replica Of Check.</li> </ul> </li> <li> <p>Improving Computed Assets:</p> <ul> <li>Users can now add Additional Metadata to computed Tables/files and computed fields.</li> <li>Display the Last Editor information in the tree footer to provide context on who last modified the asset.</li> </ul> </li> <li> <p>Added Last Profile visibility in Field Overview and Field Details.</p> <ul> <li>Users can now see the last time a field was profiled, helping clarify the timeframe of the metrics shown in the Profile section.</li> </ul> </li> <li> <p>Improved Anomaly Bulk Archive with comment support.</p> <ul> <li>Similar to Acknowledge Anomaly, users can now add optional comments when bulk archiving anomalies.</li> </ul> </li> <li> <p>Improve dry run result message readability.</p> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_15","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fix a bug when user selected a date in Date Picker and this return user's timezone instead of UTC timezone.</p> </li> <li> <p>Fixed an issue with Metadata Checks Dry Run execution where status messages were not displaying properly.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.6.20","title":"2025.6.20","text":""},{"location":"changelog-2025/#feature-enhancements_16","title":"Feature Enhancements","text":"<ul> <li> <p>Added Source Records Download for Check Dry Run: Users can now download the source records as a CSV file after executing a Dry Run.</p> </li> <li> <p>Improving Source Records Performance: Implemented caching for anomaly source records, significantly reducing load times and improving in user session level.</p> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_16","title":"General Fixes and Improvements","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2025/#2025.6.13","title":"2025.6.13","text":""},{"location":"changelog-2025/#feature-enhancements_17","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing Quality Check Dry Run: Users can now quickly assess the impact of a quality check without persisting results.</p> <ul> <li>The Dry Run functionality is accessible directly from the Data Quality Check settings configuration, enabling users to test checks before scan assertion.</li> <li>A comprehensive modal displays the execution results, presenting critical metrics including Dry Run status, operation time, and sampling limits.</li> <li>Dedicated section display potential Anomalies and Source Records that would be generated by the check. When no issues are detected, users receive a clear confirmation message indicating no anomalies were identified.</li> </ul> </li> <li> <p>Enhancing Fingerprint Visualization: Users can now easily view and manage related anomalies that share the same fingerprint.</p> <ul> <li>Clicking \"View Related Anomalies\" opens a right-side panel displaying all anomalies with matching fingerprints, including the total count of related anomalies.</li> <li>The panel enables direct anomaly management, allowing users to acknowledge, archive, or click individual cards to view detailed anomaly information without navigating away.</li> </ul> </li> <li> <p>Introducing Sticky Navigation: Users can now maintain access to navigation elements and actions while scrolling through content-rich pages.</p> <ul> <li>The sticky navigation feature ensures breadcrumb information and interaction buttons remain visible and accessible as users scroll down the page.</li> </ul> </li> <li> <p>Expanding Scan Operation Support for Iceberg Tables</p> <ul> <li>Incremental scans now fully support Iceberg table formats, significantly expanding the range of asset types eligible for incremental scanning operations.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_17","title":"General Fixes and Improvements","text":"<ul> <li>Resolved an issue where metric charts failed to display data when users accessed metric details.</li> <li>Fixed a bug that incorrectly allowed users to edit settings on computed fields inherited from parent computed files.</li> <li>Corrected the rendering logic for Authored Check Details that prevented information from displaying after tag update operations.</li> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2025/#2025.6.6","title":"2025.6.6","text":""},{"location":"changelog-2025/#feature-enhancements_18","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing the new Quality Check dedicated page, enabling users to analyze check properties and metrics.</p> <ul> <li>A Check Assert Visualization is provided to analyze assertions over time, helping users monitor assertion results, with the ability to hover over a timeline point to view the latest assertion and totals.</li> <li>Displays key metrics such as Status, Rule Type, Last Asserted, Weight, Coverage, and Active Anomalies and including the check description.</li> <li>Exposes all relevant check properties to provide a comprehensive view of each check's configuration without opening the edit modal.</li> <li>Shows the full activity history for the check, including property updates, and exposes previous and new values when a check setting is modified.</li> <li>Supports inline check tag editing by clicking the tag badge, allowing users to add or remove tags without opening a modal.</li> </ul> </li> <li> <p>Announcing the Anomaly exclusive page: This new page will allow users to get detailed information about Anomaly metrics, Failed Checks and Source Records.</p> <ul> <li>Exposes detailed anomaly information, including Status, Anomalous Records, Total Failed Checks, Weight, Detected DateTime, and Scan Operation, as well as Source Datastore, Computed Table, and Location.</li> <li>Lists the Failed Checks that were violated and led to the creation of the anomaly. Clicking on a failed check opens a right-side panel with the corresponding quality check information, eliminating the need to navigate to a different page.</li> <li>Show Source Records from your data that failed the checks when available. Users can apply filters and sorting options to personalize the data display according to their preferences.</li> <li>Displays the complete activity history, including all updates made to the anomaly over time. User comments are also shown, making it easier to follow discussions and decisions.</li> <li>Similar to the dedicated Quality Check page, users can edit Anomaly Tags inline.</li> </ul> </li> <li> <p>Datastore Connection Status Visibility</p> <ul> <li>A badge attached to the datastore icon now appears in both the breadcrumb and the tree view footer, clearly indicating the connection status of the datastore.</li> </ul> </li> <li> <p>Adding support for gzipped and .txt files in Catalog Operation</p> <ul> <li>Users can now use gzipped (.gz) and .txt files in DFS Datastores for Catalog Operations.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_18","title":"General Fixes and Improvements","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2025/#2025.5.23","title":"2025.5.23","text":""},{"location":"changelog-2025/#feature-enhancements_19","title":"Feature Enhancements","text":"<ul> <li> <p>Atlan Integration</p> <ul> <li>Users can now choose whether or not to receive notifications in the Atlan platform, giving more control over their notification preferences.</li> </ul> </li> <li> <p>Freshness Heatmap</p> <ul> <li>The freshness chart has been redesigned for an improved user experience.</li> <li>Milliseconds are now displayed in a more readable date/time format for better comprehension, while the underlying data still uses milliseconds.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_19","title":"General Fixes and Improvements","text":"<ul> <li> <p>Create User</p> <ul> <li>Fixed a bug that occurred when creating a service user with automatic admin permission enabled.</li> </ul> </li> <li> <p>Rerun Operations</p> <ul> <li>Catalog, export, and materialize operations will now only display rerun operations.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.5.16","title":"2025.5.16","text":""},{"location":"changelog-2025/#feature-enhancements_20","title":"Feature Enhancements","text":"<ul> <li> <p>Anomaly</p> <ul> <li>Users can now view the Anomaly Fingerprint directly in the Anomaly Details page.</li> <li>A new button allows users to quickly copy the fingerprint value.</li> <li>A link to the User Guide has been added to explain how this feature works.</li> </ul> </li> <li> <p>Datastore Connection</p> <ul> <li>A new validation step was added to several connectors to verify if the specified schema exists.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_20","title":"General Fixes and Improvements","text":"<ul> <li> <p>Schedule Operation</p> <ul> <li>Fixed a bug in scheduled operations that allowed <code>None</code> as a value for <code>max_records_analyzed_per_partition</code> when updating.</li> </ul> </li> <li> <p>Check</p> <ul> <li>Fixed an issue where creating a metric check with a non-existent comparison value would fail..</li> <li>Fixed a bug where checks would fail if the filtered set was empty \u2014 now the check will pass in this case.</li> </ul> </li> <li> <p>Catalog Operation</p> <ul> <li>Fixed an issue in DB2 where evaluating the distribution of values caused an error.</li> </ul> </li> <li> <p>Scheduled Scan</p> <ul> <li>Fixed an issue that occurred when adding connection retries related to the Secrets Manager.</li> </ul> </li> <li> <p>Anomaly</p> <ul> <li>Fixed an issue where some triggered anomalies had no data available.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.5.9","title":"2025.5.9","text":""},{"location":"changelog-2025/#feature-enhancements_21","title":"Feature Enhancements","text":"<ul> <li> <p>Microsoft Teams Integration</p> <ul> <li>We're excited to announce a native integration with Microsoft Teams, bringing powerful collaboration features directly into your Teams workspace.</li> <li>Users can now:<ul> <li>Share Qualytics links to datastores, containers, or fields and see rich previews directly in Teams.</li> <li>Receive proactive notifications when:<ul> <li>An operation completes</li> <li>An anomalous table or file is detected</li> <li>A specific anomaly is triggered</li> </ul> </li> <li>Note that the message content and actions will adapt based on the trigger type defined in the Flow.</li> <li>Manage anomalies without leaving Teams:<ul> <li>View, acknowledge, comment on, or archive anomalies from within the Teams UI</li> <li>Click to open linked anomalies directly in Qualytics</li> </ul> </li> </ul> </li> <li>The integration must be configured by a Qualytics admin. A dedicated setup guide is available in our User Guide.</li> <li>As part of this rollout:<ul> <li>Any Flows previously configured to send Teams notifications via incoming webhooks or workflows (Teams) have been automatically migrated to the Webhook action.</li> <li>The Teams notification action is now only available through the new integration.</li> </ul> </li> </ul> </li> <li> <p>Tokens</p> <ul> <li>Users can now view the last time a token was used.</li> </ul> </li> <li> <p>Session Expiration</p> <ul> <li>Improved handling of session expiration for users who are logged in but inactive for an extended period.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_21","title":"General Fixes and Improvements","text":"<ul> <li> <p>Check Template</p> <ul> <li>Fixed an issue with the message displayed when a user archives a check that has associated checks.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.5.5","title":"2025.5.5","text":""},{"location":"changelog-2025/#feature-enhancements_22","title":"Feature Enhancements","text":"<ul> <li>Tokens<ul> <li>Users can now view the last time a token was used, providing better visibility into token activity.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_22","title":"General Fixes and Improvements","text":"<ul> <li> <p>Is Replica Of Check</p> <ul> <li>Fixed a bug where anomalies using pass-through fields on both the left and right side were not handled correctly.</li> </ul> </li> <li> <p>Action Operations</p> <ul> <li>Fixed an issue where cloning an Action Operation could exceed the allowed action limit.</li> </ul> </li> <li> <p>Group By</p> <ul> <li>Fixed a bug that occurred when users added single quotes to string columns in the Group By clause.</li> </ul> </li> <li> <p>Atlan Sync</p> <ul> <li>Updated and created custom metadata objects to align with the latest schema.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.4.24","title":"2025.4.24","text":""},{"location":"changelog-2025/#feature-enhancements_23","title":"Feature Enhancements","text":"<ul> <li> <p>Improved Filters</p> <ul> <li>Tag Filter<ul> <li>Users will only see tag options corresponding to items currently visible in the list pages (Datastore List, Container List, and Filter List).</li> <li>The same filtering behavior has been applied to Anomalies and Checks within the datastore context.</li> <li>If no visible items contain a specific tag, a <code>No option found</code> message will be displayed in the filter dropdown.</li> </ul> </li> </ul> </li> <li> <p>Scan Operation</p> <ul> <li>Scan form<ul> <li>The scan form has been reorganized to improve the user experience.</li> <li>Now, the following steps to configure the Scan Flow are Check Categories, Reading Settings and Scan Settings.</li> </ul> </li> <li>Enrichment Settings<ul> <li>The Remediation Strategy option is now in the Enrichment Datastore Settings.</li> <li>The option will be a Datastore global value.</li> <li>Also, the Anomaly Rollup Threshold and Source Record Limit can be configured as defaults.</li> <li>During scan operations, these options will be pre-filled but can still be edited within the scan form.</li> </ul> </li> </ul> </li> <li> <p>Tree View</p> <ul> <li>The tree view layout is now adjustable, allowing users to customize it to their preferences.</li> <li>The footer in the tree view can be expanded or collapsed based on user needs.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_23","title":"General Fixes and Improvements","text":"<ul> <li> <p>Filters</p> <ul> <li>Fixed an issue where filters behaved inconsistently when navigating between different datastores.</li> </ul> </li> <li> <p>The container page is not loading</p> <ul> <li>Fixed a bug that caused the container page to fail to load under certain conditions.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#api-changes_1","title":"API Changes","text":"<ul> <li>Incoming Breaking Changes<ul> <li>REQUEST PAYLOAD: The fields <code>enrich_only</code> and <code>enrichment_only</code> will be replaced by <code>enrich_container_prefix</code> and <code>enrichment_prefix</code> and will affect the following endpoints:<ul> <li><code>POST /datastores</code></li> <li><code>PUT /datastores/{id}</code></li> </ul> </li> <li>RESPONSE PAYLOAD: The fields <code>enrich_only</code> and <code>enrichment_only</code> will be replaced by <code>enrich_container_prefix</code> and <code>enrichment_prefix</code> and will affect the following endpoints:<ul> <li><code>GET /anomalies/{id}</code></li> <li><code>PUT /anomalies/{id}</code></li> <li><code>POST /containers</code></li> <li><code>GET /containers/{id}</code></li> <li><code>PUT /containers/{id}</code></li> <li><code>GET /containers/{id}/profile</code></li> <li><code>GET /containers/{id}/scan</code></li> <li><code>POST /containers/{id}/scan</code></li> <li><code>PATCH /containers/{id}/favorite</code></li> <li><code>GET /container-profiles/{id}</code></li> <li><code>GET /container-scans/{id}</code></li> <li><code>POST /datastores</code></li> <li><code>PUT /datastores/{id}</code></li> <li><code>GET /datastores/{id}</code></li> <li><code>PATCH /datastores/{id}/favorite</code></li> <li><code>PATCH /datastores/{datastore_id}/enrichment/{enrich_store_id}</code></li> <li><code>GET /field-profiles/{id}</code></li> <li><code>POST /operations/schedule</code></li> <li><code>PUT /operations/schedule/{id}</code></li> <li><code>GET /operations/schedule/{id}</code></li> <li><code>GET /operations/{id}</code></li> <li><code>POST /operations/run</code></li> <li><code>PUT /operations/run/{id}</code></li> <li><code>PUT /operations/rerun/{id}</code></li> <li><code>PUT /operations/abort/{id}</code></li> </ul> </li> <li><code>POST /operations/run</code>, <code>POST /operations/schedule</code>, <code>PUT /operations/schedule/{id}</code>, <code>POST /flows, and PUT /flows/{id}</code><ul> <li>DEPRECATE PARAMETER: <code>remediation</code> (To specify a remediation strategy going forward, use the new <code>enrichment_remediation_strategy</code> field available in the POST /datastores and PUT /datastores/{id} endpoints.)</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog-2025/#2025.4.11","title":"2025.4.11","text":""},{"location":"changelog-2025/#feature-enhancements_24","title":"Feature Enhancements","text":"<ul> <li> <p>Anomalies and Checks Filter</p> <ul> <li>The Rule filter is now dynamically populated based on the types present in the current view.</li> <li>Each rule type displayed in the filter now includes a counter showing the total number of occurrences.</li> </ul> </li> <li> <p>Status Page</p> <ul> <li>The Health page has been renamed to Status page.</li> <li>Users can view additional information like Cloud Platform and Deployment Size.</li> <li>Deployment size details are now displayed to provide better visibility into environment configurations.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_24","title":"General Fixes and Improvements","text":"<ul> <li> <p>Anomaly</p> <ul> <li>Resolved a bug when handling <code>NaN</code> values in float-type data.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.4.5","title":"2025.4.5","text":""},{"location":"changelog-2025/#general-fixes-and-improvements_25","title":"General Fixes and Improvements","text":"<ul> <li> <p>Quality Score</p> <ul> <li>Enhanced the descriptions for each Factor Weight to provide clearer guidance on how they impact the overall score.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.3.28","title":"2025.3.28","text":""},{"location":"changelog-2025/#feature-enhancements_25","title":"Feature Enhancements","text":"<ul> <li> <p>Anomalies</p> <ul> <li>The \u201cAnomalies Identified\u201d count now shows the sum of Open and Archived anomalies.</li> <li>Anomalies are now categorized into two groups:<ul> <li>Open: anomalies currently active (Active, Acknowledged).</li> <li>Archived: anomalies that have been archived (Resolved, Duplicated, Invalid).</li> </ul> </li> <li>Users can now view the total counts of Duplicate and Invalid anomalies in the Overview tab under the Explore page.</li> <li>Users can now see the total counts of Open and Archived anomalies directly in Scan Operations.</li> <li>The Scan Results modal now displays the totals for Open and Archived anomalies.<ul> <li>Users can filter anomalies by status using a new dropdown selector.</li> </ul> </li> </ul> </li> <li> <p>Quality Checks</p> <ul> <li>Users can now sort the Quality Checks list by the \u201cLast Asserted\u201d date.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_26","title":"General Fixes and Improvements","text":"<ul> <li> <p>Teams Permissions</p> <ul> <li>Fixed an issue where users with the Drafter role were unable to restore a check as a draft.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.3.23","title":"2025.3.23","text":""},{"location":"changelog-2025/#feature-enhancements_26","title":"Feature Enhancements","text":"<ul> <li> <p>Slack Integration</p> <ul> <li>We are excited to introduce a major enhancement to our integration with Slack.</li> <li>Users can now add the new Qualytics Slack App for enhanced capabilities.<ul> <li>Qualytics admins can configure the Slack Integration with two easy steps.<ul> <li>After configuring the integration, a Slack administrator must approve the Qualytics Slack App.</li> </ul> </li> </ul> </li> <li>The Qualytics Slack App supports selecting specific Slack channels for receiving Qualytics notifications in the context of a Qualytics Flow.<ul> <li>Different types of messages will be sent for each trigger in Flow operations.<ul> <li>The text and actions will vary depending on the selected trigger.</li> <li>The message state (Slack message color) will change based upon the message status.</li> </ul> </li> </ul> </li> <li>The Qualytics Slack App allows users to interact with Qualytics from the Slack interface:<ul> <li>Interact with anomalies by acknowledging, commenting, or archiving them without leaving the Slack UI where the flow notification is received.</li> <li>Click a link in Slack to be redirected to the Qualytics UI for more details regarding a specific notification.</li> <li>View anomalous tables and files detected without leaving Slack.</li> </ul> </li> </ul> </li> <li> <p>Anomaly Fingerprint</p> <ul> <li>We are thrilled to introduce support for a new feature that will begin identifying duplicate anomalies.</li> <li>Anomalies created after this release will be \"fingerprinted\" so that re-detection of that same anomaly can be readily identified as a duplicate detection.<ul> <li>New Scan Operation options allow users to define how detected duplicates should be handled.</li> <li>This feature helps maintain the history and timeline of specific data errors, allowing users to track how long a specific issue has persisted and a log of detections over time.</li> <li>Anomaly fingerprints will also be exposed in API responses and written to enrichment</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_27","title":"General Fixes and Improvements","text":"<ul> <li>External Scan<ul> <li>Users can now use and rerun external scans only from the activity listing for the targeted asset.</li> </ul> </li> <li>Check Last Asserted<ul> <li>Fixed an issue where checks were still being marked as never asserted even after producing anomalies.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#api-changes_2","title":"API Changes","text":"<ul> <li> <p>Integration Impacting Changes</p> <ul> <li>POST &amp; PUT api/integrations<ul> <li>REMOVED PARAMETERS: name and api_token</li> <li>NEW PARAMETERS: api_access_token, api_refresh_token and api_service_token</li> </ul> </li> <li>POSTs to api/flows/actions/notifications/ endpoints<ul> <li>MODIFIED PARAMETERS: tokenized_message is now a json object instead of a string</li> </ul> </li> <li>All requests to api/datastores endpoints<ul> <li>REMOVED RESPONSE PROPERTY METRIC: The metric for archived_anomalies previously returned as metrics.archived_anomalies has been removed from responses</li> </ul> </li> </ul> </li> <li> <p>Deprecation Notices</p> <ul> <li>POST api/operations/run<ul> <li>DEPRECATED PARAMETER: archive_overlapping_anomalies (migrate to the new parameter archive_duplicate_anomalies for enhanced functionality)</li> </ul> </li> <li>POST api/operations/schedule<ul> <li>DEPRECATED PARAMETER: archive_overlapping_anomalies (migrate to the new parameter archive_duplicate_anomalies for enhanced functionality)</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog-2025/#2025.3.17","title":"2025.3.17","text":""},{"location":"changelog-2025/#feature-enhancements_27","title":"Feature Enhancements","text":"<ul> <li> <p>Activity Operation</p> <ul> <li>Added a column showing the anomaly rollup threshold.</li> </ul> </li> <li> <p>Export Operations</p> <ul> <li>Added an option allowing users to schedule Export Operations.</li> </ul> </li> <li> <p>DFS Enrichment Datastore</p> <ul> <li>Normalized delta file names to lowercase.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_28","title":"General Fixes and Improvements","text":"<ul> <li> <p>Anomaly Rollup Threshold</p> <ul> <li>Fixed an issue where the field was not accepting the maximum value.</li> </ul> </li> <li> <p>Volumetric Tracking Observability</p> <ul> <li>Fixed an issue where inferred check validation errors disrupted observability measurements.</li> </ul> </li> <li> <p>Export Operation</p> <ul> <li>Updated wording on the Export dialog.</li> </ul> </li> <li> <p>Scan and External Scan</p> <ul> <li>Fixed an issue where schema checks were failing along with other checks but were not persisting in the enrichment of source records for anomalies.</li> </ul> </li> <li> <p>External Scan</p> <ul> <li>Fixed an issue where CSV data was not being properly cast for non-text fields.</li> </ul> </li> <li> <p>Enrichment Datastore</p> <ul> <li>Fixed a bug where exported tables were not appearing in the UI.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.3.5","title":"2025.3.5","text":""},{"location":"changelog-2025/#general-fixes-and-improvements_29","title":"General Fixes and Improvements","text":"<ul> <li> <p>Tag Updating</p> <ul> <li>Fixed an issue where some containers failed to receive tag changes when updating tags across multiple containers or performing bulk updates.</li> </ul> </li> <li> <p>Edit Scheduled Materialize Operation</p> <ul> <li>Fixed an issue where the modal did not appear when users attempted to edit a scheduled materialize operation.</li> </ul> </li> <li> <p>Restart Analytics Engine</p> <ul> <li>Fixed an issue where restarting the Analytics Engine did not take effect.</li> </ul> </li> <li> <p>Anomaly Count</p> <ul> <li>Fixed an inconsistency in the anomaly count.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.2.26","title":"2025.2.26","text":""},{"location":"changelog-2025/#feature-enhancements_28","title":"Feature Enhancements","text":"<ul> <li> <p>Enrichment Operations</p> <ul> <li>Introducing Materialize Operation<ul> <li>This will \"capture a snapshot\" of selected containers from your source datastore and export them to the enrichment datastore for seamless data loading.</li> <li>Users can define the maximum number of records to be materialized per table.</li> <li>A schedule option is available for users to set up and schedule the operation according to their needs.</li> </ul> </li> <li>Introducing Export Operation<ul> <li>Users can extract metadata from selected assets in their source datastore and export it to the enrichment datastore for seamless integration.</li> <li>Assets metadata options are available to export to the enrichment datastore. Users can export:</li> <li>Anomalies</li> <li>Quality checks</li> <li>Field profiles</li> </ul> </li> <li>These operations are available in Flow Action.</li> </ul> </li> <li> <p>Flows</p> <ul> <li>Introduced a cloning feature for actions in flow.<ul> <li>Users can now clone a simple action by clicking the vertical ellipses.</li> </ul> </li> </ul> </li> <li> <p>Scan Operation</p> <ul> <li>Introducing Anomaly Rollup Threshold<ul> <li>Users can now roll up anomalies that will be created per check before they are merged into a single rolled-up anomaly.</li> </ul> </li> </ul> </li> <li> <p>Error Messages</p> <ul> <li>Improved custom messages when users receive 502 and 503 status codes.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_30","title":"General Fixes and Improvements","text":"<ul> <li> <p>System Timestamp</p> <ul> <li>Standardized timestamps across the platform.</li> </ul> </li> <li> <p>External Integrations</p> <ul> <li>Fixed an issue where external tags should be updated instead of being deleted and dropped during sync.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.2.13","title":"2025.2.13","text":""},{"location":"changelog-2025/#feature-enhancements_29","title":"Feature Enhancements","text":"<ul> <li> <p>Explore View</p> <ul> <li>Users can now refresh the Insights data.</li> <li>A label will indicate when the page was last refreshed.</li> </ul> </li> <li> <p>System Appearance</p> <ul> <li>Users can now select the <code>System</code> theme.<ul> <li>This setting automatically adjusts based on the user's system theme, switching between dark and light mode.</li> </ul> </li> <li>Users can still manually select dark or light mode.</li> </ul> </li> <li> <p>Dismiss Popup Window</p> <ul> <li>Users can now dismiss popup windows by pressing the <code>Esc</code> key, improving the user experience.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_31","title":"General Fixes and Improvements","text":"<ul> <li> <p>External Scan</p> <ul> <li>Fixed an issue where attempting to run an external scan resulted in a \"Request Failed\" error message.</li> </ul> </li> <li> <p>Explore View</p> <ul> <li>Fixed an issue causing excessively long loading times.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.2.6","title":"2025.2.6","text":""},{"location":"changelog-2025/#feature-enhancements_30","title":"Feature Enhancements","text":"<ul> <li> <p>Overview Improvement</p> <ul> <li>Added inferred and authored check totals<ul> <li>Users can now view the total number of inferred and authored checks, along with a comparison timeframe.</li> <li>A redirect link allows users to access the checks directly, displaying only their current statuses.</li> </ul> </li> <li>Improved check assertion-related metrics to reflect assertions as of the report date.</li> </ul> </li> <li> <p>Team Permissions</p> <ul> <li>Manager users can now update datastore teams.<ul> <li>Requires <code>Editor</code> permission within the team.</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_32","title":"General Fixes and Improvements","text":"<ul> <li> <p>Flow Graph</p> <ul> <li>Fixed an issue where the flow graph position was randomly changing when a user updated a flow-node.</li> </ul> </li> <li> <p>Observability Chart</p> <ul> <li>Fixed an issue where threshold calculations incorrectly referenced measurement values that did not account for grouping.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Container Overview Tab<ul> <li>Refactored the Totals section to clarify that metrics are based on sampled data rather than the full dataset.<ul> <li>Added the sampling percentage next to the spark chart to indicate that derived metrics are based on this sampling percentage.</li> <li>Updated titles and labels for better clarity.</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog-2025/#2025.1.31","title":"2025.1.31","text":""},{"location":"changelog-2025/#feature-enhancements_31","title":"Feature Enhancements","text":"<ul> <li> <p>Freshness View</p> <ul> <li>We are excited to introduce the \"Freshness View\" feature in Qualytics!</li> <li>Users can now visualize both volumetric and freshness checks within the same tab.</li> <li>The displayed data includes:<ul> <li>Unit: Day, Month, Hour, etc.</li> <li>Maximum Age: Defines the maximum allowed time since the last data update.</li> <li>Last Asserted: Indicates the last time the data was validated.</li> </ul> </li> </ul> </li> <li> <p>Datastore Filter Condition in Flows</p> <ul> <li>Users can now configure datastore filter conditions in triggers for flows, enhancing control over triggered actions.</li> </ul> </li> <li> <p>Treat Empty Value as Null for DFS</p> <ul> <li>A new option allows users to enable \"Empty value as null\" as the default behavior for File Patterns, improving data consistency.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes-and-improvements_33","title":"General Fixes and Improvements","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2025/#2025.1.24","title":"2025.1.24","text":""},{"location":"changelog-2025/#feature-enhancements_32","title":"Feature Enhancements","text":"<ul> <li> <p>Introducing Freshness Tracking in Containers</p> <ul> <li>Users can now enable a freshness tracking option for containers to measure and record the last time data was added or updated in a data asset. This helps ensure data timeliness and identifies pipeline delays.</li> </ul> </li> <li> <p>Private Routes on Analytics Engine</p> <ul> <li>Customers using private routes can now view their IP addresses along with relevant messages displayed in the Analytics Engine for improved transparency.</li> </ul> </li> <li> <p>Clone a Flow</p> <ul> <li>Users can duplicate existing flows, streamlining the process of reusing and modifying flow configurations for similar scenarios.</li> </ul> </li> <li> <p>Additional Option to Execute Manual Flows</p> <ul> <li>A new \"Start a Manual Flow\" option has been added to the vertical ellipsis menu, providing users with enhanced flexibility for executing manual flows.</li> </ul> </li> <li> <p>Cancel Action for Unpublished Flows</p> <ul> <li>A \"Cancel\" action has been introduced in the flow builder, allowing users to reset the graph to its initial state for unpublished flows. This update also addresses issues related to the execute button and read-only state logic.</li> </ul> </li> <li> <p>\"Is Replica Of\" Passthrough</p> <ul> <li>The <code>IsReplicaOf</code> rule now supports a passthrough property, allowing users to exclude specific fields from assertions. Fields listed under this property are no longer flagged as anomalous.</li> </ul> </li> <li> <p>Enhancement for Volumetric Rule Type</p> <ul> <li>Volumetric checks now include a <code>comparison</code> property, ensuring consistency with metric checks and offering greater flexibility in rule configurations.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes","title":"General Fixes","text":"<ul> <li>General Fixes and Improvements.</li> </ul>"},{"location":"changelog-2025/#2025.1.20","title":"2025.1.20","text":""},{"location":"changelog-2025/#feature-enhancements_33","title":"Feature Enhancements","text":"<ul> <li> <p>Enhanced behavior for \"All\" in Schedule Operations</p> <ul> <li>The \"All\" option in Schedule Operations has been updated to include future containers automatically. Previously, if you created a schedule with the \"All\" option and added new tables or containers later, the schedule would not include these new additions.</li> </ul> </li> <li> <p>Validate Button for Enrichment Datastore Connections</p> <ul> <li>Users can now validate their data when creating or editing an enrichment datastore connection, improving reliability and confidence in datastore setups.</li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes_1","title":"General Fixes","text":"<ul> <li> <p>Inaccurate Check Assertion Details</p> <ul> <li>Resolved an issue where some checks were being marked as never asserted despite producing anomalies, ensuring more accurate reporting.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#2025.1.16","title":"2025.1.16","text":""},{"location":"changelog-2025/#feature-enhancements_34","title":"Feature Enhancements","text":"<ul> <li> <p>Flows</p> <ul> <li> <p>Introducing Flows: Users can now create automated pipelines by chaining actions and configuring triggers based on predefined events and filters.</p> <ul> <li>Triggers: Configurable based on events, filters, and operation conditions.</li> <li>Actions: Include notifications (Email, Slack, PagerDuty, etc.) and operations (catalog, profile, and scan).</li> <li>Real-Time Execution: Monitor execution history and real-time progress in the Flow Executions tab.</li> </ul> </li> <li> <p>Setup Made Simple</p> <ul> <li>Add and configure flows using the \u201cAdd Flow\u201d button in the top-right corner.</li> <li>Deactivate, delete or edit flows via the vertical ellipses or node configurations.</li> </ul> </li> <li> <p>Enhanced Triggering Options</p> <ul> <li>Operations Complete, Anomalous Table/File Detected, and Anomaly Detected triggers provide flexible, event-driven automation.</li> <li>Fine-tune triggers using filters like tags, rule types, or anomaly weights.</li> </ul> </li> <li> <p>Diverse Action Support</p> <ul> <li>Notify through in-app messages, Emails, Slack, Microsoft Teams, PagerDuty, and custom HTTP actions</li> <li>Trigger operational tasks across cataloging, profiling, and scanning.</li> </ul> </li> <li> <p>Flow Identification on Activity Tab</p> <ul> <li>Operations executed by flows are marked in the new <code>Flow</code> column, displaying the associated flow name.</li> <li>Users can navigate directly to the flow execution view from this tab.</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog-2025/#general-fixes_2","title":"General Fixes","text":"<ul> <li> <p>Duplicate Anomalies on Scan Schedule Operations</p> <ul> <li>Fixed an issue where duplicate anomalies were not being archived during scan operations despite user selection.</li> </ul> </li> <li> <p>BigQuery Message Size</p> <ul> <li>Enhanced default batch insertion size to improve performance and reliability.</li> </ul> </li> <li> <p>Anomalous Record Integer out of Range</p> <ul> <li>Updated check metrics to use BigInteger, addressing large value handling.</li> </ul> </li> <li> <p>Fix the Last Asserted Date</p> <ul> <li>Resolved inconsistencies in the Last Asserted Date logic for partition and container scans.</li> </ul> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog-2025/#breaking-changes_1","title":"Breaking Changes","text":"<ul> <li>Notification Rules Replaced by Flows.<ul> <li>Existing notification rules have been migrated to Flows and will continue to function as before.</li> <li>For new notifications, the users must create a flow leveraging the Flows functionality.</li> </ul> </li> </ul>"},{"location":"changelog/","title":"2026","text":""},{"location":"changelog/#release-notes","title":"Release Notes","text":""},{"location":"changelog/#2026.2.21","title":"2026.2.21","text":""},{"location":"changelog/#feature-enhancements","title":"Feature Enhancements","text":"<ul> <li> <p>Introduced support for Complex Datatypes for DFS.</p> <ul> <li>Profiling and quality checks now support Array and Struct field types, expanding coverage to nested and multi-valued data structures.</li> <li>Array fields are profiled with element-level context, allowing quality checks to validate each element individually using supported rule types such as Expected Values, Matches Pattern, and Min/Max Length.</li> <li>Struct fields are automatically flattened into individual scalar columns during profiling, enabling full data quality rule inference and monitoring on nested data.</li> <li>Flattening supports recursive traversal up to a configurable depth, with path-based naming conventions for generated columns.</li> <li>Quality check rule selection adapts to complex field types, showing applicable rules based on whether the check targets the array or its individual elements.</li> <li>Field tree views and profile pages display nested field hierarchies with updated iconography for clear identification of complex field types.</li> <li>Source record tables and container previews display raw complex values, allowing users to inspect nested and array data directly during anomaly investigation.</li> </ul> </li> <li> <p>Introduced MCP-powered AI assistant with built-in chat interface and Bring Your Own Key LLM support.</p> <ul> <li>Added an interactive chat interface directly in the platform, enabling users to perform data quality tasks through natural language conversations with AI-powered assistance.</li> <li>Users can configure their preferred LLM provider and API key under Settings, with support for OpenAI, Anthropic, Google, AWS Bedrock, Cohere, Groq, Mistral, and Hugging Face models.</li> <li>The assistant leverages MCP tools to explore datastores, validate queries, create computed assets, manage quality checks, investigate anomalies, and analyze quality scores within a guided workflow.</li> <li>Chat responses include real-time tool step progress indicators, displaying each action the assistant performs with expandable input and output details.</li> <li>Suggested prompts guide users toward common workflows such as building computed tables, creating quality checks, and analyzing data quality trends.</li> </ul> </li> <li> <p>Introduced version history for containers.</p> <ul> <li>Users can now view a timeline of changes made to any container directly from the container overview page, displaying the editor and timestamp for each modification.</li> <li>Computed tables include a side-by-side diff visualization of query changes, making it easy to compare before and after states for troubleshooting.</li> </ul> </li> <li> <p>Added n8n workflow integration as a flow action.</p> <ul> <li>Users can configure n8n webhook URLs and secrets directly on each flow action, enabling automated workflows triggered by flow executions.</li> <li>Supports operation-completed, anomaly, and anomaly status change triggers, sending full context payloads to n8n for downstream automation.</li> </ul> </li> <li> <p>Added Service Principal authentication support for SQL Server datastores.</p> <ul> <li>Enables Azure Active Directory OAuth-based connections using tenant ID, client ID, and client secret or certificate credentials as an alternative to username and password authentication.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fixed missing loading indicators on the Explore fields list and Activity page tabs.</p> </li> <li> <p>Corrected container creation returning a generic server error instead of a proper conflict response when a container with the same name already existed in the datastore.</p> </li> <li> <p>Resolved incremental ID timezone parsing issue that applied local timezone instead of UTC, causing inaccurate freshness metrics when incremental filtering was enabled.</p> </li> <li> <p>Corrected data catalog sync summary to only count assets that were successfully synced, preventing misleading totals when individual asset syncs failed.</p> </li> <li> <p>Fixed quality check API updates not persisting referenced fields when expressions were modified, ensuring field associations stay consistent with the check expression.</p> </li> <li> <p>Resolved deep link URLs being lost during SSO authentication, ensuring users are redirected to the intended resource after login.</p> </li> <li> <p>Enhanced ticket creation through flows to use the anomaly description as the ticket description, providing richer context for ServiceNow and Jira incidents.</p> </li> <li> <p>Improved Insights data reliability with a force refresh option, allowing users to manually trigger on-demand data updates for the latest quality metrics.</p> </li> <li> <p>Enhanced Observability tooltips with a day-over-day change indicator, displaying row count differences from the previous day for faster trend analysis.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2026.2.13","title":"2026.2.13","text":""},{"location":"changelog/#feature-enhancements_1","title":"Feature Enhancements","text":"<ul> <li> <p>Introduced Microsoft Fabric JDBC connector.</p> <ul> <li>Added read-only connectivity to Microsoft Fabric's SQL Analytics Endpoint, enabling users to profile and monitor data quality on Fabric datasets.</li> <li>Authentication is handled via Azure Active Directory Service Principal, with credentials securely managed through connection properties.</li> </ul> </li> <li> <p>Added Service Principal authentication support for ABFS datastores, allowing Azure Active Directory OAuth-based connections as an alternative to Shared Key authentication.</p> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_1","title":"General Fixes and Improvements","text":"<ul> <li> <p>Fixed missing label text color in the bulk action menu on the Flows page when using dark theme.</p> </li> <li> <p>Corrected low-contrast tooltip icon on the check assertion timeline that was difficult to see in both light and dark modes.</p> </li> <li> <p>Optimized data catalog integration sync performance by replacing verbose per-asset logging with concise per-datastore summaries, reducing storage overhead in environments with large asset catalogs.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2026.2.7","title":"2026.2.7","text":""},{"location":"changelog/#feature-enhancements_2","title":"Feature Enhancements","text":"<ul> <li> <p>Introduced license management interface under Settings.</p> <ul> <li>Administrators can now view the current license expiration date directly from the Settings Status page, with visual warnings when the license is nearing expiration.</li> <li>Added the ability to generate a license request payload to send to a Qualytics account representative for license renewal.</li> <li>Users can apply a new license directly through the interface, streamlining the license assignment process.</li> </ul> </li> <li> <p>Added bulk actions and validation improvements for flow executions.</p> <ul> <li>Users can now select and abort multiple flow executions at once, eliminating the need to handle stuck executions one by one.</li> <li>Introduced bulk delete for flow executions to streamline cleanup of completed or failed runs.</li> <li>Added filter option for aborted status in the flow executions list for easier tracking.</li> <li>Flows without actions are now validated and blocked from publishing to prevent incomplete workflow configurations.</li> </ul> </li> <li> <p>Added support for custom anomaly messages from source record fields.</p> <ul> <li>Quality checks can now use a field value from the source record as the anomaly message, replacing the auto-generated violation text with business-specific context.</li> <li>Custom anomaly message field can be configured in both individual check forms and bulk check creation workflows.</li> <li>A field selector is available when container fields are loaded, with a manual text input fallback for flexible configuration.</li> </ul> </li> <li> <p>Added rule type filter to the Observability listing.</p> <ul> <li>Users can now filter containers by observability rule type, choosing between Volumetric and Freshness checks for more focused monitoring.</li> <li>Filter availability adapts to the current context, displaying relevant filter options across Explore, Datastore, and Container views.</li> <li>Improved column alignment and heatmap display across different screen sizes for better readability.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_2","title":"General Fixes and Improvements","text":"<ul> <li> <p>Corrected auto-generated descriptions for multi-field Unique and Not Null checks to reference all selected fields instead of only the first one.</p> </li> <li> <p>Fixed anomalous field filter in the failed checks section where checkbox selections were not filtering results and layout elements were overlapping.</p> </li> <li> <p>Corrected tooltip icon contrast in the check assertion timeline that was not visible against the tooltip background in both light and dark modes.</p> </li> <li> <p>Fixed incorrect breadcrumb navigation when viewing checks that no longer reference the original field after field association changes.</p> </li> <li> <p>Improved platform reliability by automatically recovering flow executions and data catalog syncs that became stuck after environment restarts.</p> </li> <li> <p>Centralized internal field association logic to improve consistency and reliability across anomaly counts, filtering, and data catalog synchronization.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2026.1.29","title":"2026.1.29","text":""},{"location":"changelog/#feature-enhancements_3","title":"Feature Enhancements","text":"<ul> <li> <p>Introduced Anomaly Status Changed flow trigger.</p> <ul> <li>Flows can now be triggered when anomaly statuses change, enabling automated workflows for status transitions.</li> <li>Trigger settings support filtering by specific status values to target relevant transitions.</li> </ul> </li> <li> <p>Added notification support for Create Ticket and Update Ticket Status flow actions.</p> <ul> <li>Introduced Create Ticket action to automatically generate tickets in connected ticketing systems when anomalies meet defined conditions.</li> <li>Introduced Update Ticket Status action to synchronize status changes to linked tickets across integrated platforms.</li> <li>Notifications can be sent to Slack and Microsoft Teams when ticketing actions execute.</li> <li>Introduced status selection at ticket creation, allowing tickets to be created with a specific initial state.</li> <li>Ticketing actions are validated to ensure compatibility with Anomaly or Anomaly Status Change triggers.</li> <li>Improved handling when required ticketing integrations are unavailable during flow execution.</li> </ul> </li> <li> <p>Enhanced auto-generated check descriptions with context-aware, business-friendly language.</p> <ul> <li>Descriptions now include field type context such as \"numeric field\", \"timestamp field\", or \"text field\" for clearer rule interpretation.</li> <li>Expected values are summarized directly in descriptions, displaying the first three values with a total count indicator.</li> <li>Filter clauses are translated into plain English for improved readability.</li> <li>Range checks now specify inclusivity labels to clarify boundary behavior.</li> <li>Common regex patterns are described in human-readable terms instead of raw expressions.</li> <li>Cross-datastore rules include reference field and container context for better traceability.</li> <li>Distinct count checks generate comparator-aware descriptions using phrases like \"fewer than\", \"at most\", \"exactly\", \"at least\", and \"more than\" based on the configured operator.</li> </ul> </li> <li> <p>Improved computed asset management with inline editing from check interfaces and field dependency protection.</p> <ul> <li>Users can create, edit, and manage computed tables, files, joins, and fields directly from the check interface without navigating to datastore pages.</li> <li>Added a warning dialog when computed asset updates would remove fields with active checks or anomalies, with options to cancel or proceed.</li> <li>Introduced \"E\" keyboard shortcut to quickly edit computed containers from any context.</li> <li>Enhanced command palette shortcuts to preselect the current container in profile and scan dialogs.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes-and-improvements_3","title":"General Fixes and Improvements","text":"<ul> <li> <p>Improved error messages for network connectivity issues to provide clearer guidance when the API server is unreachable.</p> </li> <li> <p>Enhanced asserted check editing to support field modifications while preserving accurate quality score calculations.</p> </li> <li> <p>Improved anomaly descriptions with editable fields, version tracking, and enriched human-readable messaging.</p> </li> <li> <p>Enhanced tenant observability with build version information.</p> </li> <li> <p>Fixed tags and rule types not appearing in filter lists by aligning filter option counts with the current view perspective.</p> </li> <li> <p>Corrected distinct count anomaly messages that displayed negative values and incorrect expected values.</p> </li> <li> <p>Resolved Collibra integration authentication failures caused by OAuth token expiration with automatic token refresh.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"changelog/#2026.1.14","title":"2026.1.14","text":""},{"location":"changelog/#feature-enhancements_4","title":"Feature Enhancements","text":"<ul> <li>Introduced the new Qualytics visual identity<ul> <li>Refreshed brand experience featuring a modern color palette, updated logo, and refined typography that reflects the platform\u2019s evolution.</li> <li>Redesigned Insights dashboard with enhanced quality score cards, vibrant chart gradients, and improved dimension visualizations for clearer data storytelling.</li> <li>Modernized card layouts across datastores, containers, checks, anomalies, and templates with improved visual hierarchy and interaction states.</li> <li>Streamlined iconography with consistent outline styling and neutral tones for a cleaner, more focused interface.</li> <li>Enhanced chart styling with cyan monochromatic palettes, smooth gradients, and rounded segments across key visualizations such as data volume, observability metrics, heatmaps, and others.</li> <li>Improved accessibility with better contrast ratios, theme-aware color utilities, and consistent styling across light and dark modes.</li> <li>Refined sign-in experience with updated visual styling aligned to the new brand direction.</li> </ul> </li> </ul>"},{"location":"changelog/#general-fixes","title":"General Fixes","text":"<ul> <li> <p>Optimized container preview performance.</p> </li> <li> <p>Standardized enrichment datastore settings labels to ensure consistency with the scan operation options.</p> </li> <li> <p>Enhanced operation timeout behavior to ensure meaningful error responses for long-running sync operations.</p> </li> <li> <p>Fixed scan schedule update errors that occurred when anomaly reactivation settings were not explicitly configured.</p> </li> <li> <p>Resolved intermittent server errors affecting the Flows list page when scheduled information was present.</p> </li> <li> <p>Corrected server errors when publishing flows while action rearrangement was in progress.</p> </li> <li> <p>Resolved field exclusion behavior to properly skip problematic columns during initial data loading in SQL Server datastores.</p> </li> <li> <p>General Fixes and Improvements.</p> </li> </ul>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"glossary/#anomaly","title":"Anomaly","text":"<p>Something that deviates from the standard, normal, or expected. This can be in the form of a single data point, record, or a batch of data.</p>"},{"location":"glossary/#accuracy","title":"Accuracy","text":"<p>The data represents the real-world values they are expected to model.</p>"},{"location":"glossary/#catalog-operation","title":"Catalog Operation","text":"<p>Used to read fundamental metadata from a Datastore required for the proper functioning of subsequent Operations such as Profile, Hash, and Scan.</p>"},{"location":"glossary/#comparison","title":"Comparison","text":"<p>An evaluation to determine if the structure and content of the source and target Datastores match.</p>"},{"location":"glossary/#comparison-runs","title":"Comparison Runs","text":"<p>An action to perform a comparison.</p>"},{"location":"glossary/#completeness","title":"Completeness","text":"<p>Required fields are fully populated.</p>"},{"location":"glossary/#conformity","title":"Conformity","text":"<p>Alignment of the content to the required standards, schemas, and formats.</p>"},{"location":"glossary/#connectors","title":"Connectors","text":"<p>Components that can be easily connected to and used to integrate with other applications and databases. Common uses include sending and receiving data.</p> <p>Info</p> <p>We can connect to any Apache Spark accessible datastore. If you have a datastore we don\u2019t yet support, talk to us! We currently support: Files (CSV, JSON, XLSX, Parquet) on Object Storage (S3, Azure Blob, GCS); ETL/ELT Providers (Fivetran, Stitch, Airbyte, Matillion \u2013 and any of their connectors!); Data Warehouses (BigQuery, Snowflake, Redshift); Data Pipelining (Airflow, DBT, Prefect); Databases (MySQL, PostgreSQL, MSSQL, SQLite, etc.) and any other JDBC source.</p>"},{"location":"glossary/#consistency","title":"Consistency","text":"<p>The value is the same across all datastores within the organization.</p>"},{"location":"glossary/#container-of-a-datastore","title":"Container (of a Datastore)","text":"<p>The uniquely named abstractions within a Datastore that hold data adhering to a known schema. The Containers within a RDBMS are tables, the containers in a filesystem are well formatted files, etc.</p>"},{"location":"glossary/#data-at-rest","title":"Data-at-rest","text":"<p>Data that is stored in a database, warehouse, file system, data lake, or other datastore.</p>"},{"location":"glossary/#data-drift","title":"Data Drift","text":"<p>Changes in a data set\u2019s properties or characteristics over time.</p>"},{"location":"glossary/#data-in-flight","title":"Data-in-flight","text":"<p>Data that is on the move, transporting from one location to another, such as through a message queue, API, or other pipeline.</p>"},{"location":"glossary/#data-lake","title":"Data Lake","text":"<p>\u200b\u200bA centralized repository that allows you to store all your structured and unstructured data at any scale.</p>"},{"location":"glossary/#data-quality","title":"Data Quality","text":"<p>Ensuring data is free from errors, including duplicates, inaccuracies, inappropriate fields, irrelevant data, missing elements, non-conforming data, and poor data entry.</p>"},{"location":"glossary/#data-quality-check","title":"Data Quality Check","text":"<p>aka \"Check\" is an expression regarding the values of a Container that can be evaluated to determine whether the actual values are expected or not.</p>"},{"location":"glossary/#datastore","title":"Datastore","text":"<p>Where data is persisted in a database, file system, or other connected retrieval systems. You can check more in Datastore Overview.</p>"},{"location":"glossary/#data-warehouse","title":"Data Warehouse","text":"<p>A system that aggregates data from different sources into a single, central, consistent datastore to support data analysis, data mining, artificial intelligence (AI), and machine learning.</p>"},{"location":"glossary/#distinctness-of-a-field","title":"Distinctness (of a Field)","text":"<p>The fraction of distinct values (appear at least once) to total values that appear in a Field.</p>"},{"location":"glossary/#enrichment-datastore","title":"Enrichment Datastore","text":"<p>Additional properties that are added to a data set to enhance its meaning. Qualytics enrichment includes whether a record is anomalous, what caused it to be an anomaly, what characteristics it was expected to have, and flags that allow other systems to act upon the data.</p>"},{"location":"glossary/#favorite","title":"Favorite","text":"<p>Users can mark instances of an abstraction (Field, Container, Datastore, Check, Anomaly, etc.) as a personalized favorite to ensure it ranks higher in default ordering and is prioritized in other personalized views &amp; workflows.</p>"},{"location":"glossary/#compute-daemon","title":"Compute Daemon","text":"<p>An application that protects a system from contamination due to inputs, reducing the likelihood of contamination from an outside source. The Compute Daemon will quarantine data that is problematic, allowing the user to act upon quarantined items.</p>"},{"location":"glossary/#incremental-identifier","title":"Incremental Identifier","text":"<p>A Field that can be used to group the records in the Table Container into distinct ordered Qualytics Partitions in support of incremental operations upon those partitions:</p> <ul> <li>a whole number - then all records with the same partition_id value are considered part of the same partition.</li> <li>a float or timestamp - then all records between two defined values are considered part of the same partition (the defining values will be set by incremental scan/profile business logic). Since Qualytics Partitions are required to support Incremental Operations, an Incremental Identifier is required for a Table Container to support incremental Operations.</li> </ul>"},{"location":"glossary/#incremental-scan-operation","title":"Incremental Scan Operation","text":"<p>A Scan Operation where only new records (inserted since the last Scan Operation) are analyzed. The underlying Container must support determining which records are new for incremental scanning to be a valid option for it.</p>"},{"location":"glossary/#inference-engine","title":"Inference Engine","text":"<p>After Compute Daemon gathers all the metadata generated by a profiling operation, it feeds that metadata into our Inference Engine. The inference engine then initiates a \"true machine learning\" (specifically, this is referred to as Inductive Learning) process whereby the available customer data is partitioned into a training set and a testing set. The engine applies numerous machine learning models &amp; techniques to the training data in an effort to discover well-fitting data quality constraints. Those inferred constraints are then filtered by testing them against the held out testing set &amp; only those that assert true are converted to inferred data quality Checks.</p>"},{"location":"glossary/#metadata","title":"Metadata","text":"<p>Data about other data, including descriptions and additional information.</p>"},{"location":"glossary/#object-storage","title":"Object Storage","text":"<p>A type of data storage used for handling large amounts of unstructured data managed as objects.</p>"},{"location":"glossary/#operation","title":"Operation","text":"<p>The asynchronous (often long-running) tasks that operate on Datastores are collectively referred to as \"Operations\". Examples include Catalog, Profile, Hash, and Scan.</p>"},{"location":"glossary/#partition-identifier","title":"Partition Identifier","text":"<p>A Field that can be used by Spark to group the records in a Dataframe into smaller sets that fit within our Spark worker\u2019s memory. The ideal Partition Identifier is an Incremental Identifier of type datetime since that can serve as both but we identify alternatives should that not be available.</p>"},{"location":"glossary/#pipeline","title":"Pipeline","text":"<p>A workflow that processes and moves data between systems.</p>"},{"location":"glossary/#precision","title":"Precision","text":"<p>Your data is the resolution that is expected- How tightly can you define your data?</p>"},{"location":"glossary/#profile-operation","title":"Profile Operation","text":"<p>An operation that generates metadata describing the characteristics of your actual data values.</p>"},{"location":"glossary/#profiling","title":"Profiling","text":"<p>The process of collecting statistics on the characteristics of a dataset involving examining, analyzing, and reviewing the data.</p>"},{"location":"glossary/#proprietary-algorithms","title":"Proprietary Algorithms","text":"<p>A procedure utilizing a combination of processes, tools, or systems of interrelated connections that are the property of a business or individual in order to solve a problem.</p>"},{"location":"glossary/#quality-score","title":"Quality Score","text":"<p>A measure of data quality calculated at the Field, Container, and Datastore level. Quality Scores are recorded as time-series enabling you to track movement over time. You can read more in Quality Scoring.</p>"},{"location":"glossary/#qualytics-app","title":"Qualytics App","text":"<p>aka \"App\" this is the user interface for our Product delivered as a web application.</p>"},{"location":"glossary/#qualytics-deployment","title":"Qualytics Deployment","text":"<p>A single instance of our product (the k8s cluster, postgres database, controlplane/app/compute daemon pods, etc).</p>"},{"location":"glossary/#qualytics-compute-daemon","title":"Qualytics Compute Daemon","text":"<p>aka \"Compute Daemon\" this is the layer of our Product that connects to Datastores and directly operates on users\u2019 data.</p>"},{"location":"glossary/#qualytics-implementation","title":"Qualytics Implementation","text":"<p>A customer\u2019s Deployment plus any associated integrations.</p>"},{"location":"glossary/#controlplane","title":"Controlplane","text":"<p>aka \"controlplane\" this is the layer of our Product that exposes an Application Programming Interface (API).</p>"},{"location":"glossary/#qualytics-partition","title":"Qualytics Partition","text":"<p>The smallest grouping of records that can be incrementally processed. For DFS datastores, each file is a Qualytics Partition. For JDBC datastores, partitions are defined by each table\u2019s incremental identifier values.</p>"},{"location":"glossary/#record-of-a-container","title":"Record (of a Container)","text":"<p>A distinct set of values for all Fields defined for a Container (e.g. a row of a table).</p>"},{"location":"glossary/#schema","title":"Schema","text":"<p>The organization of data in a datastore. This could be the columns of a table, the header of a CSV file, the fields in a JSON file, or other structural constraints.</p>"},{"location":"glossary/#schema-differences","title":"Schema Differences","text":"<p>Differences in the organization of information between two datastores that are supposed to hold the same content.</p>"},{"location":"glossary/#source","title":"Source","text":"<p>The origin of data in a pipeline, migration, or other ELT/ETL process. It\u2019s where data gets extracted.</p>"},{"location":"glossary/#tag","title":"Tag","text":"<p>Users can assign Tags to Datastores, Profiles (Files, Tables, Containers), Checks and Anomalies. Add a Description and Assign a Weight. The weight value directly correlates with the level of importance, where a higher weight indicates higher significance.</p>"},{"location":"glossary/#target","title":"Target","text":"<p>The destination of data in a pipeline, migration, or other ELT/ETL process. It\u2019s where data gets loaded.</p>"},{"location":"glossary/#third-party-data","title":"Third-party data","text":"<p>Data acquired from a source outside of your company which may not be controlled by the same data quality processes. You may not have the same level of confidence in the data and it may not be as trustworthy as internally vetted datasets.</p>"},{"location":"glossary/#timeliness","title":"Timeliness","text":"<p>It can be calculated as the time between when information should be available and when it is actually available, focused on if data is available when it\u2019s expected.</p>"},{"location":"glossary/#volumetrics","title":"Volumetrics","text":"<p>Data has the same size and shape across similar cycles. It includes statistics about the size of a data set including calculations or predictions on the rate of change over time.</p>"},{"location":"glossary/#weight","title":"Weight","text":"<p>The weight value directly correlates with the level of importance, where a higher weight indicates higher significance.</p>"},{"location":"keyboard-shortcuts/","title":"Keyboard Shortcuts","text":"<p>Qualytics offers a comprehensive set of keyboard shortcuts to reduce mouse usage and accelerate everyday tasks. These shortcuts enable you to navigate the platform, run operations, manage checks, and update entities directly from the keyboard.</p> <p>Let\u2019s get started \ud83d\ude80</p> <p>Step 1: Log in to your Qualytics account and click on the Search bar at the top of the dashboard.</p> <p>Alternatively, you can press Ctrl + K on Windows/Linux or \u2318 + K on macOS to open the Search bar directly.</p> <p></p> <p>A popup window will appear, presenting various search options.</p> <p></p> <p>Step 2: Click on Search Keyboard Shortcuts to see all the shortcuts in one place.</p> <p></p> <p>A window will appear, displaying a comprehensive list of all shortcuts organized by category.</p> <p></p> <p>Step 3: You can now either search for a specific shortcut or scroll through the list to see all available options.</p> <p></p> <p>Qualytics supports a wide range of keyboard shortcut categories, including but not limited to:</p> No. Shortcut Category 1. Navigation 2. Anomaly 3. Check 4. Checks 5. Container 6. Datastore 7. Enrichment 8. Field 9. Flow 10. Flow Execution 11. Interface 12. Operations 13. Search 14. Tags 15. Template <p>The table below shows the available shortcuts and the keys you can use to perform actions with your keyboard:</p>"},{"location":"keyboard-shortcuts/#search","title":"Search","text":"<p>The Search Shortcuts allow you to quickly find datastores, containers, fields, and even view the complete list of shortcuts without navigating through menus.</p> Action Windows/Linux macOS Search datastores, containers, and fields Ctrl + \u2191 + F \u2318 + \u2191 + F Search keyboard shortcuts Ctrl + / \u2318 + /"},{"location":"keyboard-shortcuts/#navigation","title":"Navigation","text":"<p>The Navigation Shortcuts make it easy to move between pages, switch tabs, and access core areas of the Qualytics platform efficiently.</p> Action Windows/Linux macOS Go back to previous page G then B G then B Go forward to next page G then N G then N Go to explore G then E G then E Go to flows G then F G then F Go to library G then L G then L Go to settings G then S G then S Go to tags G then T G then T Select next tab Alt + \u2192 \u2325 + \u2192 Select previous tab Alt + \u2190 \u2325 + \u2190"},{"location":"keyboard-shortcuts/#anomaly","title":"Anomaly","text":"<p>The Anomaly Shortcuts provide quick actions for archiving, deleting, or tagging anomalies, helping you manage data issues faster.</p> Action Windows/Linux macOS Archive anomaly Ctrl + E \u2318 + E Assign tags A then T A then T Delete anomaly Ctrl + Del \u2318 + Del"},{"location":"keyboard-shortcuts/#check","title":"Check","text":"<p>The Check Shortcuts give you instant access to actions like editing, archiving, tagging, or favoriting checks directly from the keyboard.</p> Action Windows/Linux macOS Archive check Ctrl + E \u2318 + E Assign tags A then T A then T Delete check Ctrl + Del \u2318 + Del Edit check E E Mark as favorite Alt + F \u2325 + F"},{"location":"keyboard-shortcuts/#checks","title":"Checks","text":"<p>The Checks Shortcuts are focused on creating new checks, whether from scratch or from templates, to speed up your validation process.</p> Action Windows/Linux macOS Add multiple checks from template Ctrl + Shift + C \u2318 + Shift + C Add new check from template Shift + C Shift + C Add new check C C"},{"location":"keyboard-shortcuts/#container","title":"Container","text":"<p>The Container Shortcuts let you manage containers efficiently by assigning tags, editing, deleting, or marking them as favorites.</p> Action Windows/Linux macOS Assign tags A then T A then T Delete container Ctrl + Del \u2318 + Del Mark as favorite Alt + F \u2325 + F Open container settings E E"},{"location":"keyboard-shortcuts/#datastore","title":"Datastore","text":"<p>The Datastore Shortcuts simplify datastore management, allowing you to quickly assign tags, edit, delete, or favorite a datastore.</p> Action Windows/Linux macOS Assign tags A then T A then T Delete datastore Ctrl + Del \u2318 + Del Edit datastore E E Mark as favorite Alt + F \u2325 + F"},{"location":"keyboard-shortcuts/#enrichment","title":"Enrichment","text":"<p>The Enrichment Shortcuts provide fast access to edit or delete enrichment configurations without leaving the keyboard.</p> Action Windows/Linux macOS Delete enrichment Ctrl + Del \u2318 + Del Edit enrichment E E"},{"location":"keyboard-shortcuts/#field","title":"Field","text":"<p>The Field Shortcuts help you manage fields by assigning tags, editing details, or deleting computed fields directly from the keyboard.</p> Action Windows/Linux macOS Assign tags A then T A then T Delete computed field Ctrl + Del \u2318 + Del Edit field E E"},{"location":"keyboard-shortcuts/#flow","title":"Flow","text":"<p>The Flow Shortcuts allow you to clone, delete, execute, or publish flows efficiently, reducing time spent in menus.</p> Action Windows/Linux macOS Clone flow Ctrl + D \u2318 + D Delete flow Ctrl + Del \u2318 + Del Execute flow Ctrl + Enter \u2318 + Enter Publish flow P P"},{"location":"keyboard-shortcuts/#flow-execution","title":"Flow Execution","text":"<p>The Flow Execution Shortcuts are designed for quick deletion of flow execution records to keep your workspace clean.</p> Action Windows/Linux macOS Delete flow execution Ctrl + Del \u2318 + Del"},{"location":"keyboard-shortcuts/#interface","title":"Interface","text":"<p>The Interface Shortcuts let you control the look and feel of the platform, such as collapsing the sidebar or switching themes.</p> Action Windows/Linux macOS Collapse navigation sidebar Ctrl + B \u2318 + B Switch theme Ctrl + Shift + L \u2318 + Shift + L"},{"location":"keyboard-shortcuts/#operations","title":"Operations","text":"<p>The Operations Shortcuts provide fast commands for running catalog, export, materialize, profile, and scan operations on datastores.</p> Action Windows/Linux macOS Run catalog operation R then C R then C Run export operation R then E R then E Run materialize operation R then M R then M Run profile operation R then P R then P Run scan operation R then S R then S"},{"location":"keyboard-shortcuts/#tags","title":"Tags","text":"<p>The Tags Shortcuts let you quickly add new tags to classify and organize your data assets.</p> Action Windows/Linux macOS Add new tag T T"},{"location":"keyboard-shortcuts/#template","title":"Template","text":"<p>The Template Shortcuts cover editing and archiving templates, helping you maintain reusable patterns with ease.</p> Action Windows/Linux macOS Archive template Ctrl + E \u2318 + E Edit template E E"},{"location":"onboarding/","title":"Onboarding","text":"<p>Qualytics is a comprehensive data quality management solution that helps enterprises proactively manage their full data quality lifecycle at scale. Through automated profiling, contextual quality checks, rule inference, anomaly detection, remediation, and tailored notifications, Qualytics transforms how organizations approach data quality.</p> <p>This guide will walk you through getting started with Qualytics, ensuring a smooth and efficient onboarding experience.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"onboarding/#onboarding-process","title":"Onboarding Process","text":"<p>Your Qualytics journey begins with understanding your enterprise's specific requirements. We'll work with you to create a tailored approach based on your data needs and goals.</p>"},{"location":"onboarding/#1-screening-criteria-gathering","title":"1. Screening &amp; Criteria Gathering","text":"<p>Schedule a demo with our team to help us understand your enterprise data needs. During this session, we'll: - Create a detailed plan - Identify key success criteria - Tailor the deployment to your specific requirements - Explore relevant use cases for your business</p>"},{"location":"onboarding/#2-user-invitations","title":"2. User Invitations","text":"<p>Once your deployment setup is complete, we'll send invitations to your team members' email addresses. These invitations include: - Instructions for accessing the platform - Role assignments (admin or member) based on your preferences - Access configuration details</p> <p>Admins receive full platform configuration and management capabilities, while members receive access based on admin-defined permissions.</p>"},{"location":"onboarding/#deployment-options","title":"Deployment Options","text":"<p>Qualytics offers flexible deployment options to seamlessly integrate with your existing data infrastructure:</p>"},{"location":"onboarding/#1-saas-deployment-default","title":"1. SaaS Deployment (Default)","text":"<p>Our Software as a Service (SaaS) deployment provides a fully managed experience hosted by Qualytics. This option offers: - Minimal maintenance requirements - Rapid scalability - Automatic updates and improvements - Focus on data quality rather than infrastructure</p>"},{"location":"onboarding/#2-on-premise-deployment","title":"2. On-Premise Deployment","text":"<p>For organizations that prefer complete control over their data environment, our on-premise deployment option allows you to: - Maintain data within your own data centers - Ensure compliance with internal policies and regulations - Exercise complete control over your data and security</p> <p>Tip</p> <p>This deployment option is recommended for customers with sensitive data</p>"},{"location":"onboarding/#frequent-asked-questions-faqs","title":"Frequent Asked Questions (FAQs)","text":"<p>Q 1: What type of support is provided during a POC?</p> <p>A 1: A dedicated Customer Success Manager, with mandatory weekly check-ins.</p> <p>Q 2: What are the deployment options for POC?</p> <p>A 2: Qualytics offers deployment options for Proof of Concept (POC) primarily as a Software as a Service (SaaS) solution.</p> <p>Q 3: What type of data should we use for a POC?</p> <p>A 3: In most cases, potential customers use their actual data during a POC for the most realistic evaluation. Some customers opt to use cleaned data (removing PII) or sample test data.</p> <p>Q 4: Are there limitations to data size for POC?</p> <p>A 4: There are no limitations to data size for a Proof of Concept (POC).</p> <p>Q 5: What type of support is provided during the Onboarding process?</p> <p>A 5: A dedicated Customer Success Manager, with mandatory weekly check-ins.</p> <p>Q 6: What types of data stacks does Qualytics support?</p> <p>A 6: Qualytics supports both modern solutions and legacy systems:</p> <ul> <li> <p>Modern Solutions</p> <p>Qualytics seamlessly integrates with modern data platforms like Snowflake, Amazon S3, BigQuery, and more to ensure robust data quality management.</p> </li> <li> <p>Legacy Systems</p> <p>We maintain high data quality standards across legacy systems including MySQL, Microsoft SQL Server, and other reliable relational database management systems.</p> </li> </ul> <p>For detailed integration instructions, please refer to the quick start guide.</p> <p>Q 7: What types of database technology can you connect in Qualytics?</p> <p>A 7: Qualytics supports any Apache Spark-compatible datastore, including: - Relational databases (RDBMS) - Raw file formats (CSV, XLSX, JSON, Avro, Parquet)</p> <p>Q 8: What is an enrichment datastore?</p> <p>A 8: An Enrichment Datastore is a user-managed storage location where Qualytics records and accesses metadata through system-defined tables. It's specifically designed to capture metadata generated during profiling and scanning operations.</p> <p>Q 9: Can I download my metadata and data quality checks?</p> <p>A 9: Yes, Qualytics's metadata export feature captures the mutable states of various data entities. You can export Quality Checks, Field Profiles, and Anomalies metadata from selected profiles into your designated enrichment datastore.</p> <p>Q 10: How is the Quality Score calculated?</p> <p>A 10: Quality Scores measure data quality at the field, container, and datastore levels, recorded as a time series to track improvements. Scores range from 0-100, with higher scores indicating better quality.</p> <p>Q 11: What is a catalog operation?</p> <p>A 11: A Catalog Operation scans your datastore to import named collections (tables, views, files). It automatically identifies optimal approaches for: - Incremental scanning - Data partitioning - Record identification</p> <p>Q 12: What is a profiling operation?</p> <p>A 12: A Profile Operation analyzes every available record across all containers in a datastore. Full Profiles deliver 100% fidelity metadata at the cost of maximum compute time.</p> <p>Q 13: What is a scan operation?</p> <p>A 13: The Scan Operation evaluates data quality checks across your datastore's collections, producing: - Record anomalies for individual anomalous values - Shape anomalies for multi-record anomalies - Detailed analysis in your Enrichment Datastore</p>"},{"location":"printing/","title":"Printing This Guide","text":"<p>This guide changes often, so we can't recommend printing it or saving it offline. However, we recognize that there are  circumstances beyond some users' control that make doing so more convenient. Thus, we are providing this link to our  userguide as a single page appropriate for saving as a PDF. If you find this useful, please let us know. </p>"},{"location":"quick-start-guide/","title":"Quick Start Guide","text":"<p>Welcome to Qualytics! This guide will help you quickly get up and running with the platform, from initial setup through your first data quality operations. Whether you're a business user or technical administrator, you'll find everything needed to start managing data quality at scale.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"quick-start-guide/#deployment-access","title":"Deployment Access","text":"<p>Each Qualytics deployment is a single-tenant, dedicated cloud instance, configured to your organization's requirements. Your deployment will be accessible via a custom URL (e.g., <code>https://acme.qualytics.io</code>), with corresponding API documentation at <code>/api/docs</code>.</p>"},{"location":"quick-start-guide/#onboarding-process","title":"Onboarding Process","text":"<p>The Qualytics onboarding process ensures your environment is perfectly tailored to your needs:</p>"},{"location":"quick-start-guide/#1-screening-and-criteria-gathering","title":"1. Screening and Criteria Gathering","text":"<p>Our team works with you to understand your specific needs, including:</p> <ul> <li>Evaluating sample data requirements.</li> <li>Identifying primary success criteria.</li> <li>Exploring relevant use cases for your environment.</li> <li>Determining deployment specifications.</li> </ul>"},{"location":"quick-start-guide/#2-environment-setup","title":"2. Environment Setup","text":"<p>Based on your requirements, we:</p> <ul> <li>Create your custom deployment URL.</li> <li>Configure your preferred cloud provider and region.</li> <li>Set up initial security parameters.</li> <li>Establish integration endpoints.</li> </ul>"},{"location":"quick-start-guide/#3-user-access","title":"3. User Access","text":"<p>Once deployment is complete:</p> <ul> <li>Team members receive email invitations.</li> <li>Roles are assigned based on your specifications.</li> <li>Access credentials are securely distributed.</li> </ul> <p>Tip</p> <p>Please check your spam folder if you don't see the invite.</p> <p>See our onboarding page for a more detailed view of what to expect during onboarding!</p>"},{"location":"quick-start-guide/#signing-in","title":"Signing In","text":"<p>Qualytics supports two authentication methods:</p>"},{"location":"quick-start-guide/#method-1-direct-credentials","title":"Method 1: Direct Credentials","text":"<p>Ideal for:</p> <ul> <li>Initial platform evaluation.</li> <li>Proof of Concept (POC) phases.</li> <li>Environments without SSO integration.</li> </ul> <p></p>"},{"location":"quick-start-guide/#method-2-enterprise-sso","title":"Method 2: Enterprise SSO","text":"<p>For production deployments:</p> <ul> <li>Integrates with your organization's Identity Provider.</li> <li>Supports standard SSO protocols.</li> <li>Provides seamless access management.</li> </ul> <p></p>"},{"location":"quick-start-guide/#getting-started-checklist","title":"Getting Started Checklist","text":"<p>To begin using Qualytics, you'll complete these key steps:</p> <ol> <li>Connect Your First Datastore.</li> <li>Run Initial Profile Operation.</li> <li>Review Generated Quality Checks.</li> <li>Configure Monitoring &amp; Alerts.</li> </ol> <p>Let's walk through each step in detail.</p>"},{"location":"quick-start-guide/#understanding-datastores","title":"Understanding Datastores","text":"<p>In Qualytics, a Datastore represents your data source connection. Qualytics supports any Apache Spark-compatible data source, including:</p>"},{"location":"quick-start-guide/#jdbc-datastores","title":"JDBC Datastores","text":"<ul> <li>Traditional relational databases (RDBMS).</li> <li>Data warehouses.</li> <li>Analytical databases.</li> </ul>"},{"location":"quick-start-guide/#distributed-file-system-dfs-datastores","title":"Distributed File System (DFS) Datastores","text":"<ul> <li>Cloud storage (AWS S3, Azure Blob, GCP).</li> <li>Raw files (CSV, XLSX, JSON, Avro, Parquet).</li> <li>Local file systems.</li> </ul>"},{"location":"quick-start-guide/#connecting-your-first-datastore","title":"Connecting Your First Datastore","text":""},{"location":"quick-start-guide/#adding-a-source-datastore","title":"Adding a Source Datastore","text":"<ol> <li>From the main menu, select \"Add Source Datastore\":</li> </ol> <ol> <li>Select your datastore type.</li> <li>Provide connection details.</li> <li>Test connectivity.</li> <li>Configure an Enrichment Datastore (strongly recommended).</li> </ol> <p>Warning</p> <p>While optional, not configuring an Enrichment Datastore limits platform capabilities.</p>"},{"location":"quick-start-guide/#enrichment-datastores","title":"Enrichment Datastores","text":"<p>An Enrichment Datastore serves as the storage location for:</p> <ul> <li>Anomaly detection results.</li> <li>Metadata and profiling information.</li> <li>Quality check outcomes.</li> <li>Historical analysis data.</li> </ul> <p>You can either:</p> <ol> <li>Configure a new Enrichment Datastore.</li> <li>Select an existing Enrichment Datastore from the dropdown.</li> </ol>"},{"location":"quick-start-guide/#core-operations","title":"Core Operations","text":"<p>After connecting your datastore, three fundamental operations manage data quality:</p>"},{"location":"quick-start-guide/#1-catalog-operation","title":"1. Catalog Operation","text":"<p>The first step in understanding your data:</p> <ul> <li>Systematically collects data structures.</li> <li>Analyzes existing metadata.</li> <li>Prepares for profiling and scanning.</li> <li>Runs automatically on datastore creation.</li> </ul> <p></p>"},{"location":"quick-start-guide/#2-profile-operation","title":"2. Profile Operation","text":"<p>The Profile operation performs deep analysis of your data:</p> <ul> <li>Generates comprehensive metadata.</li> <li>Calculates statistical measures:<ul> <li>Basic metrics (type, min/max, and lengths).</li> <li>Advanced analytics (skewness, kurtosis, and correlations).</li> <li>Value distributions and patterns.</li> </ul> </li> <li>Automatically infers data quality rules.</li> <li>Uses machine learning for pattern detection.</li> </ul> <p></p> <p>Our profiling engine analyzes:</p> <ul> <li>Field types and patterns.</li> <li>Value distributions.</li> <li>Statistical relationships.</li> <li>Data quality patterns.</li> <li>Structural consistency.</li> </ul> <p>The engine uses machine learning to:</p> <ul> <li>Identify column data types.</li> <li>Discover relationships.</li> <li>Generate quality rules.</li> <li>Detect anomaly patterns.</li> </ul>"},{"location":"quick-start-guide/#3-scan-operation","title":"3. Scan Operation","text":"<p>The Scan operation actively monitors data quality:</p> <ul> <li>Asserts all defined quality checks.</li> <li>Identifies anomalies and violations.</li> <li>Records results in the Enrichment Datastore.</li> <li>Generates quality scores.</li> </ul> <p></p> <p>The first scan runs as a \"Full\" scan to establish baselines. After completion, you can review:</p> <ul> <li>Start and finish times.</li> <li>Records processed.</li> <li>Anomalies detected.</li> <li>Quality scores.</li> </ul>"},{"location":"quick-start-guide/#managing-data-quality","title":"Managing Data Quality","text":""},{"location":"quick-start-guide/#quality-checks","title":"Quality Checks","text":"<p>Qualytics uses two types of quality checks:</p>"},{"location":"quick-start-guide/#1-inferred-checks","title":"1. Inferred Checks","text":"<ul> <li>Automatically generated during profiling.</li> <li>Cover 80-90% of common quality rules.</li> <li>Based on statistical analysis and ML.</li> <li>Continuously refined through operation.</li> </ul>"},{"location":"quick-start-guide/#2-authored-checks","title":"2. Authored Checks","text":"<ul> <li>Manually created by users.</li> <li>Support complex business rules.</li> <li>Use Spark SQL or Scala UDFs.</li> <li>Can be templated and shared.</li> </ul>"},{"location":"quick-start-guide/#platform-navigation","title":"Platform Navigation","text":""},{"location":"quick-start-guide/#explore-dashboard","title":"Explore Dashboard","text":"<p>The Explore interface provides comprehensive visibility:</p>"},{"location":"quick-start-guide/#1-insights","title":"1. Insights","text":"<ul> <li>Overview of anomaly detection.</li> <li>Quality monitoring metrics.</li> <li>Filterable by source, tags, and dates.</li> </ul>"},{"location":"quick-start-guide/#2-activity","title":"2. Activity","text":"<ul> <li>Operation history and status.</li> <li>Data volume heatmaps.</li> <li>Anomaly tracking.</li> </ul>"},{"location":"quick-start-guide/#3-profiles","title":"3. Profiles","text":"<p>Unified view of all data assets:</p> <ul> <li>Tables and Views.</li> <li>Computed Assets.</li> <li>Field-level Details.</li> </ul> <p></p>"},{"location":"quick-start-guide/#4-observability","title":"4. Observability","text":"<p>Monitor platform health and performance:</p> <ul> <li>Volume metrics.</li> <li>Quality trends.</li> <li>System health.</li> </ul> <p></p>"},{"location":"quick-start-guide/#5-checks","title":"5. Checks","text":"<p>Unified view of all data quality validations across datastores:</p> <ul> <li>Active, Draft, Favorite, and Archived checks.</li> <li>Filter by Source Datastore, Tags, or Importance.</li> <li>View validation results by table and field, including pass/fail status and anomaly counts.</li> </ul> <p></p>"},{"location":"quick-start-guide/#6-anomalies","title":"6. Anomalies","text":"<p>Centralized view of all detected data issues across datastores:</p> <ul> <li>Filter anomalies by status \u2014 Open, Active, Acknowledged, or Archived.</li> <li>View details including datastore, table, affected fields, rules triggered, and detection date.</li> <li>Track anomaly trends and weights to prioritize investigation and resolution.</li> </ul> <p></p>"},{"location":"quick-start-guide/#configuration-management","title":"Configuration &amp; Management","text":""},{"location":"quick-start-guide/#tags","title":"Tags","text":"<p>Organize and prioritize:</p> <ul> <li>Categorize data assets.</li> <li>Drive notifications.</li> <li>Weight importance.</li> </ul> <p></p>"},{"location":"quick-start-guide/#flows","title":"Flows","text":"<p>Automate and streamline:</p> <ul> <li>Trigger actions based on specific events.</li> <li>Manage workflows efficiently.</li> <li>Monitor and track execution status.</li> </ul> <p></p>"},{"location":"quick-start-guide/#platform-settings","title":"Platform Settings","text":"<p>Access key configuration areas:</p> <ol> <li> <p>Connections</p> <ul> <li>Manage datastores.</li> <li>Configure integrations.</li> </ul> <p></p> </li> <li> <p>Security</p> <ul> <li>User management.</li> <li>Role assignments.</li> </ul> <p></p> </li> <li> <p>Integrations</p> <ul> <li>External tool setup.</li> <li>API configuration.</li> </ul> <p></p> </li> <li> <p>Status</p> <ul> <li>Deployment status.</li> <li>Analytics engine management.</li> </ul> <p></p> </li> </ol>"},{"location":"quick-start-guide/#next-steps","title":"Next Steps","text":"<p>Now that you're familiar with the Qualytics basics, consider:</p> <ol> <li>Setting up additional datastores.</li> <li>Creating custom quality checks.</li> <li>Configuring notifications.</li> <li>Exploring advanced features.</li> </ol> <p>For detailed information on any topic, explore the relevant sections in our documentation.</p>"},{"location":"web-app/","title":"Web Application","text":"<p>Upon signing in to Qualytics, users are greeted with a thoughtfully designed web application that offers intuitive navigation and quick access to essential features and datasets, ensuring an efficient and comprehensive data quality management experience.</p> <p>In this documentation, we will explore every component of the Qualytics web application.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"web-app/#global-search","title":"Global Search","text":"<p>The Global Search feature in Qualytics is designed to streamline the process of finding crucial assets such as Datastores, Containers, and Fields. This enhancement provides quick and precise search results, significantly improving navigation and user interaction. By entering keywords in the search bar located at the top of the dashboard, users can efficiently locate specific data elements, facilitating better data management and access. This functionality is especially useful for large datasets, ensuring users can swiftly find the information they need without navigating through multiple layers of the interface.</p> <p>Tip</p> <p>Press the shortcut key: Ctrl+K for quick access to Global Search.</p> <p></p>"},{"location":"web-app/#in-app-notifications","title":"In-App Notifications","text":"<p>In Qualytics, notifications keep users updated on flow executions in real time. When a flow is triggered, users receive alerts with details like the flow name, status (success or failure), completion time, and actions performed. Clicking on a notification provides more details, including any detected anomalies. These notifications help users monitor workflows efficiently and respond quickly to important updates.</p> <p></p>"},{"location":"web-app/#discover","title":"Discover","text":"<p>The Discover option in Qualytics features a dropdown menu that provides access to various resources and tools to help users navigate and utilize the platform effectively. The menu includes the following options:</p> <p>Resources:</p> <ul> <li> <p>User Guide: Opens the comprehensive user guide for Qualytics, which provides detailed instructions and information on how to use the platform effectively.</p> </li> <li> <p>SparkSQL: Directs users to resources or documentation related to using SparkSQL within the Qualytics platform, aiding in advanced data querying and analysis.</p> </li> </ul> <p>API:</p> <ul> <li> <p>Docs: Opens the API documentation, offering detailed information on how to interact programmatically with the Qualytics platform. This is essential for developers looking to integrate Qualytics with other systems or automate tasks.</p> </li> <li> <p>Playground: Provides access to an interactive environment where users can test and experiment with API calls. This feature is particularly useful for developers who want to understand how the API works and try out different queries before implementing them in their applications.</p> </li> </ul> <p>Support:</p> <ul> <li>Qualytics Helpdesk: Provides users with access to a support environment where they can get assistance with any issues or questions related to the platform.</li> </ul> <p></p>"},{"location":"web-app/#theme","title":"Theme","text":"<p>Qualytics offers both dark mode and light mode to enhance user experience and cater to different preferences and environments.</p> <p>Light Mode:</p> <ul> <li> <p>This is the default visual theme of Qualytics, featuring a light background with dark text.</p> </li> <li> <p>It provides a clean and bright interface, which is ideal for use in well-lit environments.</p> </li> <li> <p>To switch from dark mode to light mode, click the Light Mode button.</p> </li> </ul> <p>Dark Mode:</p> <ul> <li> <p>Dark mode features a dark background with light text, reducing eye strain and glare, especially in low-light environments.</p> </li> <li> <p>It is designed to be easier on the eyes during prolonged usage and can help save battery life on devices.</p> </li> <li> <p>To activate dark mode, click the Dark Mode button.</p> </li> </ul> <p>System Appearance:</p> <ul> <li> <p>The system theme automatically adjusts based on the user\u2019s device settings.</p> </li> <li> <p>When enabled, Qualytics will switch between light and dark mode based on the system preference.</p> </li> <li> <p>It provides a seamless experience by adapting to the user\u2019s environment without manual adjustments.</p> </li> </ul> <p>Tip</p> <p>Users can still manually select dark or light mode if they prefer a fixed theme.</p> <p></p>"},{"location":"web-app/#view-mode","title":"View Mode","text":"<p>In Qualytics, users have the option to switch between two display modes: List View and Card View. These modes are available on the Source Datastore page, Enrichment Datastore page, and Library page, allowing users to choose their preferred method of displaying information.</p> <ul> <li> <p>List View: List View arranges items in a linear, vertical list format. This mode focuses on providing detailed information in a compact and organized manner. To activate List View, click the \"List View\" button (represented by an icon with three horizontal lines) located at the top of the page.</p> </li> <li> <p>Card View: Card View displays items as individual cards arranged in a grid. Each card typically includes a summary of the most important information about the item. To switch to Card View, click the \"Card View\" button (represented by an icon with a grid of squares) located at the top of the page.</p> </li> </ul> <p></p>"},{"location":"web-app/#product-updates","title":"Product Updates","text":"<p>In Qualytics, the Product Updates feature helps users stay up to date with the latest changes. They can see new features, bug fixes, and improvements directly in the app, with links to full release notes for more details.</p> <p></p>"},{"location":"web-app/#user-profile","title":"User Profile","text":"<p>The user profile section in Qualytics provides essential information and settings related to the user's account. Here's an explanation of each element:</p> <ul> <li> <p>Name: Displays the user's email address used as the account identifier.</p> </li> <li> <p>Role: Indicates the user's role within the Qualytics platform (e.g., Admin), which defines their level of access and permissions.</p> </li> <li> <p>Teams: Shows the teams to which the user belongs (e.g., Public), helping organize users and manage permissions based on group membership.</p> </li> <li> <p>Preview Features: A toggle switch that enables or disables preview features. When turned on, it adds an AI Readiness Benchmark for the Quality Score specifically on the Explore page.</p> </li> <li> <p>Logout: A button that logs the user out of their Qualytics account, ending the current session and returning them to the login page.</p> </li> <li> <p>Version: Displays the current version of the Qualytics platform being used, which is helpful for troubleshooting and ensuring compatibility with other tools and features.</p> </li> </ul> <p></p>"},{"location":"web-app/#navigation-menu-left-sidebar","title":"Navigation Menu (Left Sidebar)","text":"<p>The left sidebar of the app displays the primary navigation menu, which allows users to quickly access various functionalities of the Qualytics platform. The menu items include:</p>"},{"location":"web-app/#source-datastores-default-view","title":"Source Datastores (Default View)","text":"<p>Lists all the source datastores connected to Qualytics in the left sidebar. It also provides the option to:</p> <ul> <li> <p>Add a new source datastore.</p> </li> <li> <p>Search from existing source datastores.</p> </li> <li> <p>Sort existing datastores based on the name, records, checks, etc.</p> </li> <li> <p>Filter source datastores.</p> </li> </ul> <p></p>"},{"location":"web-app/#enrichment-datastores","title":"Enrichment Datastores","text":"<p>Lists all the enrichment datastores connected to Qualytics in the left sidebar. It also provides options to:</p> <ul> <li>Add an enrichment datastore.</li> <li>Search from existing enrichment datastores.</li> <li>Sort existing datastores based on the name, records, checks, etc.</li> </ul> <p></p>"},{"location":"web-app/#explore","title":"Explore","text":"<p>The Explore section in Qualytics enables effective data management and analysis through several key sections:</p> <ul> <li> <p>Insights: Offers an overview of anomaly detection and data monitoring, allowing users to filter by source datastores, tags, and dates. It displays profile data, applied checks, quality scores, records scanned, and more. Moreover, you can also export the insight reports in PDF format.  </p> </li> <li> <p>Activity: Provides a detailed view of operations (catalog, profile, and scan) across source datastores with a heatmap to visualize daily activities and detected anomalies.</p> </li> <li> <p>Profiles: Unifies all containers, including tables, views, computed tables, computed files, and fields, with search, sort, and filter functionalities.</p> </li> <li> <p>Observability: Observability gives users an easy way to track changes in data volume over time. It introduces two types of checks: Volumetric and Metric.</p> </li> <li> <p>Checks: Shows all applied checks, both inferred and authored, across source datastores to monitor and manage data quality rules.</p> </li> <li> <p>Anomalies: Lists all detected anomalies across source datastores for quick identification and resolution of issues.</p> </li> </ul> <p></p>"},{"location":"web-app/#library","title":"Library","text":"<p>The library section allows for managing check templates and editing applied checks in source datastores with two main functionalities:</p> <ul> <li> <p>Add Check Templates: Easily add new templates to apply standardized checks across datastores.</p> </li> <li> <p>Export Check Templates: Export template metadata to a specified Enrichment datastore.</p> </li> </ul> <p>Tip</p> <p>You can also search, sort, and filter checks across the source datastores.</p> <p></p>"},{"location":"web-app/#tags","title":"Tags","text":"<p>Tags help users organize and prioritize data assets by categorizing them. They can be applied to Datastores, Profiles, Fields, Checks, and Anomalies, improving data management and workflows.</p> <p></p>"},{"location":"web-app/#flows","title":"Flows","text":"<p>Qualytics allows users to set up flows, enabling them to create pipelines by chaining actions and configuring how they are triggered. Triggers can be set based on predefined events and filters, providing a flexible and efficient way to automate processes. These actions can include notifications or operations, allowing users to notify various channels or execute tasks based on specific operations.</p> <p></p>"},{"location":"web-app/#global-settings","title":"Global Settings","text":"<p>Manage global configurations with the following options:</p> <ul> <li> <p>Connection: Manage datastore sources (add, edit, delete).</p> </li> <li> <p>Integration: Configure parameters for integrating external tools.</p> </li> <li> <p>Security: Manage teams, roles, and user access.</p> </li> <li> <p>Tokens: Create tokens for secure API interactions.</p> </li> <li> <p>Status: Monitor and restart the Qualytics deployment.</p> </li> </ul> <p></p>"},{"location":"add-datastores/amazon-s3/","title":"Amazon S3","text":"<p>Adding and configuring an Amazon S3 connection within Qualytics empowers the platform to build a symbolic link with your file system to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Amazon S3 as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Amazon S3 environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/amazon-s3/#amazon-s3-setup-guide","title":"Amazon S3 Setup Guide","text":"<p>This section provides a simple walkthrough for setting up Amazon S3, including retrieving URIs. It also explains how to retrieve the Access Key and Secret Key to configure datastore permissions.</p> <p>By following the Amazon S3 setup process, you will ensure secure and efficient access to your stored data, allowing seamless datastore integration and proper access management in Qualytics.</p>"},{"location":"add-datastores/amazon-s3/#retrieve-the-uri","title":"Retrieve the URI","text":"<p>The S3 URI is the unique resource identifier within the context of the S3 protocol. They follow this naming convention: <code>S3://bucket-name/key-name</code></p> <p>To retrieve the URL of an S3 object via the AWS Console, follow these steps:</p> <ol> <li>Navigate to the AWS S3 console and click on your bucket's name (use the search input to find the object if necessary).</li> <li>Click on the checkbox next to the object's name</li> <li>Click on the Copy S3 URI button</li> </ol> <p></p>"},{"location":"add-datastores/amazon-s3/#retrieve-the-access-key-and-secret-key","title":"Retrieve the Access Key and Secret Key","text":"<p>The access keys are long-term credentials for an IAM user or the AWS account root user. You can use these keys to sign programmatic requests to the AWS CLI or AWS API (directly or using the AWS SDK).</p> <p>To retrieve the Access Key and Secret Access Key, follow these steps:</p> <ol> <li>Open the IAM console.</li> <li>From the navigation menu, click on the Users.</li> <li>Select your IAM user name.</li> <li>Click on the User Actions, and then click on the Manage Access Keys.</li> <li>Click on the Create Access Key.</li> <li>Your keys will look something like this:<ol> <li>Access key ID example: <code>AKIAIOSFODNN7EXAMPLE</code>.</li> <li>Secret access key example: <code>wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</code>.</li> </ol> </li> <li>Click on the Download Credentials, and store the keys in a secure location.</li> </ol> <p>Warning</p> <p>Your Secret Access Key will be visible only once at the time of creation. Please ensure you copy and securely store it for future use.</p>"},{"location":"add-datastores/amazon-s3/#datastore-privileges","title":"Datastore Privileges","text":"<p>If you are using a private bucket, authentication is required for the connection.</p>"},{"location":"add-datastores/amazon-s3/#source-datastore-permissions-read-only","title":"Source Datastore Permissions (Read-Only)","text":"<p>To create a policy, follow these steps:</p> <ol> <li>Open the IAM console.</li> <li>Navigate to Policies in the IAM dashboard and select Create Policy.</li> <li>Go to the JSON tab and paste the provided JSON into the Policy editor.</li> </ol> <p>Tip</p> <p>Ensure you replace <code>&lt;bucket/path&gt;</code> with your specific resource.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:ListBucketMultipartUploads\",\n                \"s3:Get*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket&gt;/*\",\n                \"arn:aws:s3:::&lt;bucket&gt;\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Warning</p> <p>Currently, object-level permissions alone are insufficient to authenticate the connection. Please ensure you also include bucket-level permissions as demonstrated in the example above.</p>"},{"location":"add-datastores/amazon-s3/#enrichment-datastore-permissions-read-write","title":"Enrichment Datastore Permissions (Read-Write)","text":"<p>To create a policy, follow these steps:</p> <ol> <li>Open the IAM console.</li> <li>Navigate to Policies in the IAM dashboard and select Create Policy.</li> <li>Go to the JSON tab and paste the provided JSON into the Policy editor.</li> </ol> <p>Tip</p> <p>Ensure you replace <code>&lt;bucket/path&gt;</code> with your specific resource.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:Get*\",\n                \"s3:ListBucket\",\n                \"s3:ListBucketMultipartUploads\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\",\n                \"s3:AbortMultipartUpload\",\n                \"s3:ListMultipartUploadParts\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket&gt;/*\",\n                \"arn:aws:s3:::&lt;bucket&gt;\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Warning</p> <p>Currently, object-level permissions alone are insufficient to authenticate the connection. Please ensure you also include bucket-level permissions as demonstrated in the example above.</p>"},{"location":"add-datastores/amazon-s3/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect and access data from external sources. Amazon S3 is an example of a source datastore, specifically a type of Distributed File System (DFS) datastore that is designed to handle data stored in distributed file systems. Configuring a DFS datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore (e.g., The specified name will appear on the datastore cards.). 2. Toggle Button Toggle ON to create a new source datastore from scratch or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select Amazon S3 from the dropdown list."},{"location":"add-datastores/amazon-s3/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Amazon S3 connector from the dropdown list and add connection details such as Secrets Management, URI, access key, secret key, root path, and teams.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. URI (Required) Enter the Uniform Resource Identifier (URI) of Amazon S3. 2. Access Key (Required) Input the access key provided for secure access. 3. Secret Key (Required) Input the secret key associated with the access key for secure authentication. 4. Root Path (Required) Specify the root path where the data is stored. 5. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 6. Initiate Cataloging (Optional) Check the checkbox to automatically perform a catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/amazon-s3/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Root Path, Teams, and initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to check and verify the source data connection. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/amazon-s3/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). This datastore is used to store the analyzed results, including any anomalies and additional metadata in files. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/amazon-s3/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> REF. FIELDS ACTIONS 1. URI (Required) Enter the Uniform Resource Identifier (URI) for the Amazon S3. 2. Access Key (Required) Input the access key provided for secure access. 3. Secret Key (Required) Input the secret key associated with the access key for secure authentication. 4. Root Path (Required) Specify the root path where the data is stored. 5. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 5: Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/amazon-s3/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use Enrichment Datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example - Marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>URI: The Uniform Resource Identifier (URI) points to the specific location of the source data and should be formatted accordingly (e.g., <code>s3://bucket-name</code> for Amazon S3).</p> </li> <li> <p>Root Path: Specify the root path where the data is stored. This path defines the base directory or folder from which all data operations will be performed.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/amazon-s3/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/amazon-s3/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating the Amazon S3 datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"trigger_catalog\": true,\n    \"root_path\": \"/s3_root_path\",\n    \"enrich_only\": false,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"s3\",\n        \"uri\": \"s3://&lt;bucket_name&gt;\",\n        \"access_key\": \"s3_access_key\",\n        \"secret_key\": \"s3_secret_key\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"trigger_catalog\": true,\n    \"root_path\": \"/s3_root_path\",\n    \"enrich_only\": false,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/amazon-s3/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"trigger_catalog\": true,\n    \"root_path\": \"/s3_root_path\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"s3\",\n        \"uri\": \"s3://&lt;bucket_name&gt;\",\n        \"access_key\": \"s3_access_key\",\n        \"secret_key\": \"s3_secret_key\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"trigger_catalog\": true,\n    \"root_path\": \"/s3_root_path\",\n    \"enrich_only\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/amazon-s3/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore:</p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/athena/","title":"Athena","text":"<p>Adding and configuring an Amazon Athena connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.  </p> <p>This documentation provides a step-by-step guide on adding Athena as a source datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Athena environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.  </p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/athena/#add-the-source-datastore","title":"Add the Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Athena is an example of such a datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the Athena datastore allows the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name Specify the name of the datastore (e.g., The specified name will appear on the datastore cards). 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector Select Athena from the dropdown list."},{"location":"add-datastores/athena/#option-i-create-a-datastore-with-a-new-connection","title":"Option I: Create a Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Athena connector from the dropdown list and add connection properties such as Secrets Management, host, port, username, and password, along with datastore properties like catalog, database, etc.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get Hostname from your Athena account and add it to this field. 2. Port Specify the Port number. 3. User Enter the User ID to connect. 4. Password Enter the password to connect to the database. 5. S3 Output Location Define the S3 bucket location where the output will be stored. This is specific to AWS Athena and specifies where query results are saved. 6. Catalog Enter the catalog name. In AWS Athena, this refers to the data catalog that contains database and table metadata. 7. Database Specify the database name. 8. Teams Select one or more teams from the dropdown to associate with this source datastore. 9. Initiate Cataloging Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified. </p>"},{"location":"add-datastores/athena/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials. </p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Catalog, Database, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to check and verify the source data connection. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>Click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/athena/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>After successfully testing and verifying your source datastore connection, you have the option to add an enrichment datastore (recommended). This datastore is used to store analyzed results, including any anomalies and additional metadata in tables. This setup provides comprehensive visibility into your data quality, enabling you to manage and improve it effectively.  </p> <p>Warning</p> <p>Qualytics does not support the Athena connector as an enrichment datastore, but you can point to a different enrichment datastore.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/athena/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for Add New Connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p>Note</p> <p>Qualytics does not support Athena as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using BigQuery as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p></p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection.</p> <p></p> <p>If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a success notification appears on the screen indicating that the datastore was added successfully.</p> <p>Step 5: Close the success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/athena/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Use an existing enrichment datastore is turned on, you will be prompted to configure the enrichment datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p>Note</p> <p>Qualytics does not support Athena as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using BigQuery as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example- Marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment.</p> </li> <li> <p>Database:  Refers to the specific database within the enrichment datastore environment where the data is stored.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a success notification appears on the screen indicating that the datastore was added successfully. </p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/athena/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"add-datastores/athena/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides a sample payload for creating an Athena datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint (Post): <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"athena_catalog\",\n    \"schema\": \"athena_database\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"host\": \"athena_host\",\n        \"port\": 443,\n        \"username\": \"athena_user\",\n        \"password\": \"athena_password\",\n        \"parameters\": { \"output\": \"s3://&lt;bucket_name&gt;\" },\n        \"type\": \"athena\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"athena_catalog\",\n    \"schema\": \"athena_database\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": connection_id\n}\n</code></pre>"},{"location":"add-datastores/athena/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/azure-datalake-storage/","title":"Azure Datalake Storage","text":"<p>Adding and configuring an Azure Datalake Storage connection within Qualytics empowers the platform to build a symbolic link with your file system to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Azure Datalake Storage as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Azure Datalake Storage environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/azure-datalake-storage/#azure-datalake-storage-setup-guide","title":"Azure Datalake Storage Setup Guide","text":"<p>This setup guide details the process for retrieving the credentials needed to configure your Azure Datalake Storage account in Qualytics. Azure Datalake Storage supports two authentication methods:</p> <ul> <li>Access Key Authentication \u2014 Uses the storage account name and access key.</li> <li>Service Principal Authentication \u2014 Uses a Microsoft Entra ID (Azure AD) service principal with Client ID, Client Secret, and Tenant ID.</li> </ul>"},{"location":"add-datastores/azure-datalake-storage/#azure-datalake-storage-uri","title":"Azure Datalake Storage URI","text":"<p>The Uniform Resource Identifier (URI) for Azure Datalake Storage is structured to uniquely identify resources within your storage account. The format of the URI is as follows:</p> <pre><code>abfs[s]://&lt;file_system&gt;@&lt;account_name&gt;.dfs.core.windows.net/&lt;path&gt;\n</code></pre> <ul> <li><code>abfs[s]</code>: The <code>abfs</code> or <code>abfss</code> protocol is used as the scheme identifier.</li> <li><code>\\&lt;file_system&gt;</code>: The parent location that holds the files and folders. This is similar to containers in the Azure Storage Blobs service.</li> <li><code>&lt;account-name&gt;</code>: The name assigned to your storage account during creation.</li> <li><code>&lt;path&gt;</code>: A forward slash delimited (/) representation of the directory structure.</li> </ul>"},{"location":"add-datastores/azure-datalake-storage/#retrieve-the-account-name-and-access-key","title":"Retrieve the Account Name and Access Key","text":"<p>To configure Azure Datalake Storage Datastore in Qualytics, you need the account name and access key. Follow these steps to retrieve them:</p> <ol> <li> <p>To get the <code>account_name</code> and <code>access_key</code> you need to access your local storage in Azure.</p> </li> <li> <p>Click on Access Keys tab and copy the values.</p> </li> </ol> <p></p> <p>Tip</p> <p>Refer to the Azure Datalake Storage documentation for more information on how to retrieve the account name and access key of your storage account.</p>"},{"location":"add-datastores/azure-datalake-storage/#service-principal-authentication-alternative","title":"Service Principal Authentication (Alternative)","text":"<p>As an alternative to access key authentication, you can use a Service Principal to authenticate with Azure Datalake Storage. This method is recommended for organizations that prefer identity-based access control via Microsoft Entra ID.</p> <p>To set up service principal authentication, you need to:</p> <ol> <li>Register an application in Microsoft Entra ID.</li> <li>Create a Client Secret for the application.</li> <li>Assign the appropriate RBAC role (e.g., Storage Blob Data Reader or Storage Blob Data Contributor) to the service principal on your storage account or container.</li> </ol> <p>After completing the setup, you will have the following credentials:</p> Credential Description Client ID The Application (Client) ID from the app registration. Client Secret A secret key generated for the application. Tenant ID The Directory (Tenant) ID of your Azure AD tenant. <p>Warning</p> <p>If the service principal is assigned the ViewOnly role, it must also be granted the ReadAll permission at the lakehouse level. Without this additional permission, the service principal will not have sufficient access to read data from the lakehouse.</p> <p>Tip</p> <p>For detailed step-by-step instructions on creating a service principal in the Azure Portal, refer to the Microsoft documentation.</p>"},{"location":"add-datastores/azure-datalake-storage/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect and access data from external sources. Azure Datalake Storage is an example of a source datastore, specifically a type of Distributed File System (DFS) datastore that is designed to handle data stored in distributed file systems. Configuring a DFS datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore. (e.g., The specified name will appear on the datastore cards.) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select Azure Datalake Storage from the dropdown list."},{"location":"add-datastores/azure-datalake-storage/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Azure Datalake Storage connector from the dropdown list and add connection details such as Secrets Management, URI, account name, access key, root path, and teams.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF FIELDS ACTIONS 1. URI (Required) Enter the Uniform Resource Identifier (URI) of the Azure Datalake Storage. 2. Root Path (Required) Specify the root path where the data is stored. 3. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 4. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Authentication: Select the authentication method to connect to Azure Datalake Storage. You can choose between Shared Key (default) or Service Principal.</p> Shared KeyService Principal REF FIELDS ACTIONS 1. Account Name (Required) Input the account name to access the Azure Datalake Storage. 2. Access Key (Required) Input the access key provided for secure access. REF FIELDS ACTIONS 1. Client ID (Required) Enter the Application (Client) ID from your Microsoft Entra ID app registration. 2. Client Secret (Required) Enter the Client Secret value generated in your app registration. 3. Tenant ID (Required) Enter the Directory (Tenant) ID from your Microsoft Entra ID app registration. <p>Tip</p> <p>For detailed instructions on setting up a service principal and assigning RBAC roles, refer to the Microsoft documentation.</p> <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/azure-datalake-storage/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Root Path, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/azure-datalake-storage/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). This datastore is used to store the analyzed results, including any anomalies and additional metadata in files. This setup provides full visibility into your data quality, helping you manage and improve it effectively. </p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2:  A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/azure-datalake-storage/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window - Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for Add New Connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> REF FIELDS ACTIONS 1. URI (Required) Enter the Uniform Resource Identifier (URI) of the Azure Datalake Storage. 2. Root Path (Required) Specify the root path where the data is stored. 3. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. <p>Authentication: Select the authentication method. You can choose between Shared Key (default) or Service Principal.</p> Shared KeyService Principal REF FIELDS ACTIONS 1. Account Name (Required) Input the account name to access the Azure Datalake Storage. 2. Access Key (Required) Input the access key provided for secure access. REF FIELDS ACTIONS 1. Client ID (Required) Enter the Application (Client) ID from your Microsoft Entra ID app registration. 2. Client Secret (Required) Enter the Client Secret value generated in your app registration. 3. Tenant ID (Required) Enter the Directory (Tenant) ID from your Microsoft Entra ID app registration. <p>Note</p> <p>When using Service Principal for enrichment, ensure the service principal has the Storage Blob Data Contributor role (not just Reader) to allow write operations.</p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p></p> <p>Step 5: Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/azure-datalake-storage/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Use an existing enrichment datastore is turned on, you will be prompted to configure the enrichment datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example - Marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>URI: Uniform Resource Identifier (URI) points to the specific location of the source data and should be formatted accordingly (e.g., <code>abfss://storage-url</code> for Azure Datalake Storage).</p> </li> <li> <p>Root Path: Specify the root path where the data is stored. This path defines the base directory or folder from which all data operations will be performed.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p></p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/azure-datalake-storage/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/azure-datalake-storage/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating the Azure Datalake Storage datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Shared Key - New ConnectionService Principal - New ConnectionExisting Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"trigger_catalog\": true,\n    \"root_path\": \"/azure_root_path\",\n    \"enrich_only\": false,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"abfs\",\n        \"uri\": \"abfs://&lt;container&gt;@&lt;account_name&gt;.dfs.core.windows.net\",\n        \"access_key\": \"azure_account_name\",\n        \"secret_key\": \"azure_access_key\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"trigger_catalog\": true,\n    \"root_path\": \"/azure_root_path\",\n    \"enrich_only\": false,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"abfs\",\n        \"uri\": \"abfs://&lt;container&gt;@&lt;account_name&gt;.dfs.core.windows.net\",\n        \"access_key\": \"your_client_id\",\n        \"secret_key\": \"your_client_secret\",\n        \"parameters\": {\n            \"authentication_type\": \"SERVICE_PRINCIPAL\",\n            \"tenant_id\": \"your_tenant_id\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"trigger_catalog\": true,\n    \"root_path\": \"/azure_root_path\",\n    \"enrich_only\": false,\n    \"connection_id\": \"connection-id\"\n}\n</code></pre>"},{"location":"add-datastores/azure-datalake-storage/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Shared Key - New ConnectionService Principal - New ConnectionExisting Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"trigger_catalog\": true,\n    \"root_path\": \"/azure_root_path\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"abfs\",\n        \"uri\": \"abfs://&lt;container&gt;@&lt;account_name&gt;.dfs.core.windows.net\",\n        \"access_key\": \"azure_account_name\",\n        \"secret_key\": \"azure_access_key\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"trigger_catalog\": true,\n    \"root_path\": \"/azure_root_path\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"abfs\",\n        \"uri\": \"abfs://&lt;container&gt;@&lt;account_name&gt;.dfs.core.windows.net\",\n        \"access_key\": \"your_client_id\",\n        \"secret_key\": \"your_client_secret\",\n        \"parameters\": {\n            \"authentication_type\": \"SERVICE_PRINCIPAL\",\n            \"tenant_id\": \"your_tenant_id\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"trigger_catalog\": true,\n    \"root_path\": \"/azure_root_path\",\n    \"enrich_only\": true,\n    \"connection_id\": \"connection-id\"\n}\n</code></pre>"},{"location":"add-datastores/azure-datalake-storage/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore:</p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/bigquery/","title":"BigQuery","text":"<p>Adding and configuring a BigQuery connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.  </p> <p>This documentation provides a step-by-step guide on adding BigQuery as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their BigQuery environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"add-datastores/bigquery/#bigquery-setup-guide","title":"BigQuery Setup Guide","text":"<p>This guide explains how to create and use a temporary dataset with an expiration time in BigQuery. This dataset helps manage intermediate query results and temporary tables when using the Google BigQuery JDBC driver.</p> <p>It is recommended for efficient data management, performance optimization, and automatic reduction of storage costs by deleting data when it is no longer needed.</p>"},{"location":"add-datastores/bigquery/#access-the-bigquery-console","title":"Access the BigQuery Console","text":"<p>Step 1: Navigate to the BigQuery console within your Google Cloud Platform (GCP) account.</p> <p></p> <p>Step 2: Click on the vertical ellipsis, it will open a popup menu for creating a dataset. Click on the Create dataset to set up a new dataset.</p> <p></p> <p>Step 3: Fill details for the following fields to create a new dataset.</p> <p>Info</p> <ul> <li>Dataset Location: Select the location that aligns with where your other datasets reside to minimize data transfer delays.</li> <li>Default Table Expiration: Set the expiration to <code>1 day</code> to ensure any table created in this dataset is automatically deleted one day after its creation.</li> </ul> <p></p> <p>Step 4: Click the Create Dataset button to apply the configuration and create the dataset.</p> <p></p> <p>Step 5: Navigate to the created dataset and find the Dataset ID in the Dataset Info.</p> <p></p> <p>The Dataset info section contains the Dataset ID and other information related to the created dataset. This generated Dataset ID is used to configure the BigQuery datastore.</p>"},{"location":"add-datastores/bigquery/#bigquery-roles-and-permissions","title":"BigQuery Roles and Permissions","text":"<p>This section explains the roles required for viewing, editing, and running jobs in BigQuery. To integrate BigQuery with Qualytics, you need specific roles and permissions.</p> <p>Assigning these roles ensures Qualytics can perform data discovery, management, and analytics tasks efficiently while maintaining security and access control.</p>"},{"location":"add-datastores/bigquery/#bigquery-roles","title":"BigQuery Roles","text":"<ul> <li> <p>BigQuery Data Editor (<code>roles/bigquery.dataEditor</code>)     Allows modification of data within BigQuery, including adding new tables and changing table schemas. It is suitable if you want to regularly update or insert data.</p> </li> <li> <p>BigQuery Data Viewer (<code>roles/bigquery.dataViewer</code>)     Enables viewing datasets, tables, and the contents. It is essential if you need to read data structures and information.</p> </li> <li> <p>BigQuery Job User (<code>roles/bigquery.jobUser</code>)     Allows creating and managing jobs in BigQuery, such as queries, data imports, and data exports. It is important if you want to run automated queries.</p> </li> <li> <p>BigQuery Read Session User (<code>roles/bigquery.readSessionUser</code>)     Allows usage of the BigQuery Storage API for efficient retrieval of large data volumes. It provides capabilities to create and manage read sessions, facilitating large-scale data transfers.</p> </li> </ul> <p>Warning</p> <p>If a temporary dataset already exists in BigQuery and users want to use it when creating a new datastore connection, the service account must have the <code>bigquery.tables.create</code> permission to perform the test connection and proceed to the datastore creation.</p>"},{"location":"add-datastores/bigquery/#datastore-bigquery-privileges","title":"Datastore BigQuery Privileges","text":"<p>The following table outlines the privileges associated with BigQuery roles when configuring datastore connections in Qualytics:</p>"},{"location":"add-datastores/bigquery/#source-datastore-permissions-read-only","title":"Source Datastore Permissions (Read-Only)","text":"<p>Provides read access to view table data and metadata:</p> REF. READ-ONLY PERMISSIONS DESCRIPTION 1. <code>roles/bigquery.dataViewer</code> Allows viewing of datasets, tables, and their data. 2. <code>roles/bigquery.jobUser</code> Enables running of jobs such as queries and data loading. 3. <code>roles/bigquery.readSessionUser</code> Facilitates the creation of read sessions for efficient data retrieval."},{"location":"add-datastores/bigquery/#enrichment-datastore-permissions-read-write","title":"Enrichment Datastore Permissions (Read-Write)","text":"<p>Grants read and write access for data editing and management:</p> REF. WRITE-ONLY PERMISSIONS DESCRIPTION 1. <code>roles/bigquery.dataEditor</code> Provides editing permissions for table data and schemas. 2. <code>roles/bigquery.dataViewer</code> Allows viewing of datasets, tables, and their data. 3. <code>roles/bigquery.jobUser</code> Enables running of jobs such as queries and data loading. 4. <code>roles/bigquery.readSessionUser</code> Facilitates the creation of read sessions for efficient data retrieval."},{"location":"add-datastores/bigquery/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. BigQuery is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore (e.g. The specified name will appear on the datastore cards). 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select BigQuery from the dropdown list."},{"location":"add-datastores/bigquery/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the BigQuery connector from the dropdown list and add connection details such as temp dataset ID, service account key, project ID, and dataset ID.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use <code>${key}</code> in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Temp Dataset ID (Optional) Enter a temporary Dataset ID for intermediate data storage during BigQuery operations. 2. Service Account Key (Required) Upload a JSON file that contains the credentials required for accessing BigQuery. 3. Project ID (Required) Enter the Project ID associated with BigQuery. 4. Dataset ID (Required) Enter the Dataset ID (schema name) associated with BigQuery. 5. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 6. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/bigquery/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New Connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Project ID, Dataset ID, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/bigquery/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata for your selected datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/bigquery/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for Add New Connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>Once the HashiCorp Vault is set up, use the <code>${key}</code> format in Connection form to reference a Vault secret.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 3: The configuration form, requesting credential details after selecting enrichment datastore connector.</p> <p></p> REF. FIELD ACTIONS 1. Temp Dataset ID (Optional) Enter a temporary Dataset ID for intermediate data storage during BigQuery operations. 2. Service Account Key (Required) Upload a JSON file that contains the credentials required for accessing BigQuery. 3. Project ID (Required) Enter the Project ID associated with BigQuery. 4. Dataset ID (Required) Enter the Dataset ID (schema name) associated with BigQuery. 5. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. <p>Step 4: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the enrichment datastore has been successfully verified.</p> <p></p> <p>Step 5: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a success notification appears on the screen indicating that the datastore was added successfully.</p> <p>Step 6: Close the success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/bigquery/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use Enrichment Datastore option is selected from the dropdown menu, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example- Marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>Host: This is the server address where the BigQuery instance is hosted. It is the endpoint used to connect to the BigQuery environment.</p> </li> <li> <p>Database: Refers to the specific database within the BigQuery environment where the data is stored.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a success notification appears on the screen indicating that the datastore was added successfully.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/bigquery/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/bigquery/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a BigQuery datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"your_project_id\",\n    \"schema\": \"your_dataset_id\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"bigquery\",\n        \"password\": \"your_service_account_key\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"your_project_id\",\n    \"schema\": \"your_dataset_id\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/bigquery/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an Existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"your_project_id\",\n    \"schema\": \"your_enrichment_dataset_id\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"bigquery\",\n        \"password\": \"your_service_account_key\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"your_project_id\",\n    \"schema\": \"your_enrichment_dataset_id\",\n    \"enrich_only\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/bigquery/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore: </p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/databricks/","title":"Databricks","text":"<p>Adding and configuring a Databricks connection within Qualytics empowers the platform to build a symbolic link with your database to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.  </p> <p>This documentation provides a step-by-step guide on how to add Databricks as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Databricks environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"add-datastores/databricks/#databricks-setup-guide","title":"Databricks Setup Guide","text":"<p>This guide provides a comprehensive walkthrough for setting up Databricks. It highlights the distinction between SQL Warehouses and All-Purpose Compute, the functionality of node pools, and the enhancements they offer.</p> <p>Additionally, it details the process for attaching compute resources to node pools and explains the minimum requirements for effective operation.</p>"},{"location":"add-datastores/databricks/#understanding-sql-warehouses-and-all-purpose-compute","title":"Understanding SQL Warehouses and All-Purpose Compute","text":""},{"location":"add-datastores/databricks/#sql-warehouses-serverless","title":"SQL Warehouses (Serverless)","text":"<p>SQL Warehouses (Serverless) in Databricks utilize serverless SQL endpoints for running SQL queries.</p> REF. ATTRIBUTE DESCRIPTION 1. Cost-effectiveness Serverless SQL endpoints allow you to pay only for the queries you execute, without the need to provision or manage dedicated infrastructure, making it more cost-effective for ad-hoc or sporadic queries. 2. Scalability Serverless architectures automatically scale resources based on demand, ensuring optimal performance for varying workloads. 3. Simplified Management With serverless SQL endpoints, you don't need to manage clusters or infrastructure, reducing operational overhead. 4. Minimum Requirements The minimum requirements for using SQL Warehouse with serverless typically include access to a Databricks workspace and appropriate permissions to create and run SQL queries."},{"location":"add-datastores/databricks/#all-purpose-compute","title":"All-Purpose Compute","text":"<p>All-purpose compute in Databricks refers to clusters that are not optimized for specific tasks. While they offer flexibility, they may not provide the best performance or cost-effectiveness for certain workloads.</p> REF. ATTRIBUTE DESCRIPTION 1. Slow Spin-up Time All-purpose compute clusters may take longer to spin up compared to specialized clusters, resulting in delays before processing can begin. 2. Timeout Connections Due to longer spin-up times, there's a risk of timeout connections, especially for applications or services that expect quick responses."},{"location":"add-datastores/databricks/#node-pool-and-its-usage","title":"Node Pool and Its Usage","text":"<p>A node pool in Databricks is a set of homogeneous virtual machines (VMs) within a cluster. It allows you to have a fixed set of instances dedicated to specific tasks, ensuring consistent performance and resource isolation.</p> REF. ATTRIBUTE DESCRIPTION 1. Resource Isolation Node pools provide resource isolation, allowing different workloads or applications to run without impacting each other's performance. 2. Optimized Performance By dedicating specific nodes to particular tasks, you can optimize performance for those workloads. 3. Cost-effectiveness Node pools can be more cost-effective than using all-purpose compute for certain workloads, as you can scale resources according to the specific requirements of each task."},{"location":"add-datastores/databricks/#improving-all-purpose-compute-with-node-pools","title":"Improving All-Purpose Compute with Node Pools","text":"<p>To improve the performance of all-purpose compute using node pools, you can follow these steps:</p> REF. ATTRIBUTE DESCRIPTION 1. Define Workload-Specific Node Pools Identify the specific tasks or workloads that require optimized performance and create dedicated node pools for them. 2. Specify Minimum Requirements Determine the minimum resources (such as CPU, memory, and disk) required for each workload and configure the node pools accordingly. 3. Monitor and Adjust Continuously monitor the performance of your node pools and adjust resource allocations as needed to ensure optimal performance. <p>Step 1: Configure details for Qualytics Node Pool.</p> <p></p> <p>Step 2: Attach Compute details with the Node Pool.</p> <p></p>"},{"location":"add-datastores/databricks/#retrieve-the-connection-details","title":"Retrieve the Connection Details","text":"<p>This section explains how to retrieve the connection details that you need to connect to Databricks.</p>"},{"location":"add-datastores/databricks/#credentials-to-connect-with-qualytics","title":"Credentials to Connect with Qualytics","text":"<p>To configure Databricks, you need the following credentials:</p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your Databricks account and add it to this field. 2. HTTP Path (Required) Add HTTP Path (web address) to fetch data from your Databricks account. 3. Catalog (Required) Add a Catalog to fetch data structures and metadata from Databricks. 4. Database (Required) Specify the database name to be accessed. 5. Personal Access Token (Required) Generate a Personal Access Token from your Databricks account and add it for authentication."},{"location":"add-datastores/databricks/#get-connection-details-for-the-sql-warehouse","title":"Get Connection Details for the SQL Warehouse","text":"<p>Follow the given steps to get the connection details for the SQL warehouse:</p> <ol> <li>Click on the SQL Warehouses in the sidebar.</li> <li>Choose a warehouse to connect to.</li> <li>Navigate to the Connection Details tab.</li> <li>Copy the connection details.</li> </ol> <p></p>"},{"location":"add-datastores/databricks/#get-connection-details-for-the-cluster","title":"Get Connection Details for the Cluster","text":"<p>Follow the given steps to get the connection details for the cluster:</p> <ol> <li>Click on the Compute in the sidebar.</li> <li>Choose a cluster to connect to.</li> <li>Navigate to the Advanced Options.</li> <li>Click on the JDBC/ODBC tab.</li> <li>Copy the connection details.</li> </ol> <p></p>"},{"location":"add-datastores/databricks/#get-the-access-token","title":"Get the Access Token","text":"<p>Step 1: In your Databricks workspace, click your Databricks username in the top bar, and then select User Settings from the dropdown menu.</p> <p></p> <p>Note</p> <p>Refer to the Databricks Official Docs to generate the Access Token.</p> <p>Step 2: In the Settings page, select the Developer option in the User section.</p> <p></p> <p>Step 3: In the Developer page, click on Manage in Access Tokens.</p> <p></p> <p>Step 4: In the Access Tokens page, click on the Generate new token button.</p> <p></p> <p>Step 5: You will see a modal to add a description and validation time (in days) for the token.</p> <p></p> <p>Step 6: After adding the contents, click on Generate, and it will show the token.</p> <p></p> <p>Warning</p> <p>Before closing the modal window by clicking on the Done button, ensure the Personal Access Token is saved to a secure location.</p> <p>Step 7: You can see the new token on the Access Tokens page.</p> <p></p> <p>You can also revoke a token on the Access Tokens page by clicking on the Revoke token button.</p> <p></p>"},{"location":"add-datastores/databricks/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Databricks is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the datastore name (e.g., This name will appear on the datastore cards). 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select Databricks from the dropdown list."},{"location":"add-datastores/databricks/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Databricks connector from the dropdown list and add connection details such as Secrets Management, host, HTTP path, database, and personal access token.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use <code>${key}</code> in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELD ACTIONS 1. Host (Required) Get the hostname from your Databricks account and add it to this field. 2. HTTP Path (Required) Add the HTTP Path (web address) to fetch data from your Databricks account. 3. Personal Access Token (Required) Generate a Personal Access Token from your Databricks account and add it for authentication. 4. Catalog (Required) Add a Catalog to fetch data structures and metadata from the Databricks. 5. Database (Optional) Specify the database name to be accessed. 6. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 7. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/databricks/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New Connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Catalog, Database, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/databricks/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down button to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/databricks/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for Add New Connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>Once the HashiCorp Vault is set up, use the <code>${key}</code> format in Connection form to reference a Vault secret.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 3: The configuration form, requesting credential details after selecting enrichment datastore connector.</p> <p></p> REF. FIELD ACTIONS 1. Host (Required) Get the hostname from your Databricks account and add it to this field. 2. HTTP Path (Required) Add the HTTP Path (web address) to fetch data from your Databricks account. 3. Personal Access Token (Required) Generate a Personal Access Token from your Databricks account and add it for authentication. 4. Catalog (Required) Add a Catalog to fetch data structures and metadata from Databricks. 5. Database (Optional) Specify the database name. 6. Teams (Required) Select one or more teams from the dropdown to associate with this enrichment datastore. <p>Step 4: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the enrichment datastore has been successfully verified.</p> <p></p> <p>Step 5: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a success notification appears on the screen indicating that the datastore was added successfully.</p> <p>Step 6: Close the success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/databricks/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use Enrichment Datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example- Marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>Host: This is the server address where the Databricks instance is hosted. It is the endpoint used to connect to the Databricks environment.</p> </li> <li> <p>Database: Refers to the specific database within the Databricks environment where the data is stored.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a success notification appears on the screen indicating that the datastore was added successfully.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/databricks/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/databricks/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a Databricks datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"databricks_database\",\n    \"schema\": \"databricks_catalog\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"databricks\",\n        \"host\": \"databricks_host\",\n        \"password\": \"databricks_token\",\n        \"parameters\": {\n            \"path\": \"databricks_http_path\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"databricks_database\",\n    \"schema\": \"databricks_catalog\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/databricks/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an Existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"databricks_database\",\n    \"schema\": \"databricks_enrichment_catalog\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"databricks\",\n        \"host\": \"databricks_host\",\n        \"password\": \"databricks_token\",\n        \"parameters\": {\n            \"path\": \"databricks_http_path\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"databricks_database\",\n    \"schema\": \"databricks_enrichment_catalog\",\n    \"enrich_only\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/databricks/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore: </p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/db2/","title":"DB2","text":"<p>Adding and configuring a DB2 connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.  </p> <p>This documentation provides a step-by-step guide on how to add DB2 as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration. </p> <p>By following these instructions, enterprises can ensure their DB2 environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle. </p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/db2/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. DB2 is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface. </p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the datastore name (e.g., This name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select DB2 from the dropdown list."},{"location":"add-datastores/db2/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the DB2 connector from the dropdown list and add connection details such as Secrets Management, host, port, user, password, SSL connection, database, and schema.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your DB2 account and add it to this field. 2. Port (Required) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the password to connect to the database. 5. SSL Connection Enable the SSL connection to ensure secure communication between Qualytics and the selected datastore. 6. Database (Required) Specify the database name. 7. Schema (Required) Define the schema within the database that should be used. 8. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 9. Initial Catalog Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified. </p>"},{"location":"add-datastores/db2/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams, and Initiate Cataloging. </p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/db2/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). This datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window- Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1 Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2 Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3 Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/db2/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>Once the HashiCorp Vault is set up, use the ${key} format in Connection form to reference a Vault secret.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 3: The configuration form, requesting credential details after selecting the enrichment datastore connector.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get Hostname from your DB2 account and add it to this field. 2. Port Specify the Port number. 3. User Enter the User to connect. 4. Password Enter the password to connect to the database. 5. SSL Connection Enable the SSL connection to ensure secure communication between Qualytics and the selected datastore. 6. Database Specify the database name. 7. Schema Define the schema within the database that should be used. 8. Teams Select one or more teams from the dropdown to associate with this datastore. <p>Step 4:  Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the enrichment datastore has been successfully verified. </p> <p></p> <p>Step 5: Click on the Finish button to complete the configuration process. </p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 6: Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/db2/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details. </p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1 Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2 Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment: </p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example- Marked as Public means that this datastore is accessible to all the users. </p> </li> <li> <p>Host: This is the server address where the DB2 instance is hosted. It is the endpoint used to connect to the DB2 environment. </p> </li> <li> <p>Database: Refers to the specific database within the DB2 environment where the data is stored.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/db2/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/db2/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a DB2 datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"db2_database\",\n    \"schema\": \"db2_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"db2\",\n        \"host\": \"db2_host\",\n        \"port\": \"db2_port\",\n        \"username\": \"db2_username\",\n        \"password\": \"db2_password\",\n        \"parameters\": {\n            \"ssl\": true\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"db2_database\",\n    \"schema\": \"db2_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection_id\n}\n</code></pre>"},{"location":"add-datastores/db2/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"db2_database\",\n    \"schema\": \"db2_enrichment_schema\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"db2\",\n        \"host\": \"db2_host\",\n        \"port\": \"db2_port\",\n        \"username\": \"db2_username\",\n        \"password\": \"db2_password\",\n        \"parameters\": {\n            \"ssl\": true\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"db2_database\",\n    \"schema\": \"db2_enrichment_schema\",\n    \"enrich_only\": true,\n    \"connection_id\": connection_id\n}\n</code></pre>"},{"location":"add-datastores/db2/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore: </p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/dremio/","title":"Dremio","text":"<p>Adding and configuring a Dremio connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on adding Dremio as a source datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Dremio environment is properly connected with Qualytics, unlocking the platform\u2019s potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/dremio/#add-the-source-datastore","title":"Add the Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Dremio is an example of such a datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the Dremio datastore allows the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name Specify the name of the datastore (e.g., the specified name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection 3. Connector Select Dremio from the dropdown list."},{"location":"add-datastores/dremio/#option-i-create-a-datastore-with-a-new-connection","title":"Option I: Create a Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Dremio connector from the dropdown list and add connection properties such as Secrets Management, host, port, username, and password, along with datastore properties like catalog, database, etc.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get the Hostname from your Dremio account and add it to this field. 2. Port Specify the Port number. 3. Project ID Enter the Project ID associated with Dremio. 4. SSL Connection Enable the SSL connection to ensure secure communication between Qualytics and the selected datastore. 5. Authentication You can choose between Basic authentication or Access Token for validating and securing the connection to your Dremio instance.  Basic Authentication: This method uses a username and password combination for authentication. It is a straightforward method where the user's credentials are directly used to access Dremio. <ul><li>Type: Select the authentication type from the dropdown menu.</li><li>User: Enter the username that Qualytics will use to connect to Dremio.</li><li>Password: Enter the password associated with the specified user account.</li></ul>Access Token Authentication: This method uses an access token for authentication. This is a more secure method compared to basic authentication.<ul><li> Personal Access Token: Enter the personal access token here to authenticate and access the resources securely.</li> </ul> 6. Schema Define the schema within the database that should be used. 7. Teams Select one or more teams from the dropdown to associate with this source datastore. 8. Initial Cataloging Tick the checkbox to automatically perform a catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/dremio/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Schema, Teams and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to check and verify the source data connection. If the connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>Click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/dremio/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>After successfully testing and verifying your source datastore connection, you have the option to add an enrichment datastore (recommended). This datastore is used to store analyzed results, including any anomalies and additional metadata in tables. This setup provides comprehensive visibility into your data quality, enabling you to manage and improve it effectively.</p> <p>Warning</p> <p>Qualytics does not support the Dremio connector as an enrichment datastore, but you can point to a different enrichment datastore.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/dremio/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTION 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment datastore from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p>Note</p> <p>Qualytics does not support Dremio as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using DB2 as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.  </p> <p></p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.  </p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 5: Close the Success dialogue and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/dremio/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p>Note</p> <p>Qualytics does not support Dremio as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using DB2 as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example - Marked as Public means that this datastore is accessible to all the users.  </p> </li> <li> <p>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment.</p> </li> <li> <p>Database: Refers to the specific database within the enrichment datastore environment where the data is stored.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/dremio/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/dremio/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a Dremio datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint (Post): <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an Existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"dremio_database\",\n    \"schema\": \"dremio_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"dremio\",\n        \"host\": \"dremio_host\",\n        \"port\": 443,\n        \"project_id\": \"dremio_id\",\n        \"ssl\": true,\n        \"authentication\": {\n            \"type\": \"access_token\",\n            \"personal_access_token\": \"your_personal_access_token\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"dremio_database\",\n    \"schema\": \"dremio_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection_id\n}\n</code></pre>"},{"location":"add-datastores/dremio/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/fabric-analytics/","title":"Fabric Analytics","text":"<p>Adding and configuring a Fabric Analytics connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Fabric Analytics as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Fabric Analytics environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"add-datastores/fabric-analytics/#fabric-analytics-setup-guide","title":"Fabric Analytics Setup Guide","text":"<p>To connect Fabric Analytics with Qualytics, you need to configure a Service Principal in Microsoft Entra ID (Azure AD) and grant it access to your Fabric workspace. This section walks you through the required prerequisites and credential retrieval process.</p>"},{"location":"add-datastores/fabric-analytics/#prerequisites","title":"Prerequisites","text":"<p>Before configuring the Fabric Analytics datastore in Qualytics, ensure the following requirements are met:</p> Requirement Description Microsoft Entra ID (Azure AD) Access to your organization's Entra ID tenant to create or manage app registrations. Fabric Workspace A Fabric workspace with a Lakehouse or Warehouse that you want to connect to. Service Principal An app registration in Entra ID with a Client ID and Client Secret. Tenant Admin Setting The Fabric tenant setting \"Service principals can use Fabric APIs\" must be enabled. Workspace Access The service principal must be added as a Contributor (or higher) to the target Fabric workspace."},{"location":"add-datastores/fabric-analytics/#step-1-register-an-application-in-microsoft-entra-id","title":"Step 1: Register an Application in Microsoft Entra ID","text":"<ol> <li> <p>Navigate to the Azure Portal and sign in.</p> <p></p> </li> <li> <p>Go to Microsoft Entra ID &gt; App registrations.</p> <p></p> </li> <li> <p>Click + New registration.</p> <p></p> </li> <li> <p>Enter a Name for the application (e.g., <code>qualytics-fabric</code>).</p> </li> <li> <p>Select the appropriate Supported account types (typically \"Accounts in this organizational directory only\").</p> </li> <li> <p>Click Register.     </p> </li> </ol>"},{"location":"add-datastores/fabric-analytics/#step-2-retrieve-the-client-id-and-tenant-id","title":"Step 2: Retrieve the Client ID and Tenant ID","text":"<ol> <li> <p>After registration, you will be taken to the app's Overview page.</p> <p></p> </li> <li> <p>Copy the Application (client) ID \u2014 this is your Client ID.</p> </li> <li> <p>Copy the Directory (tenant) ID \u2014 this is your Tenant ID.</p> </li> </ol>"},{"location":"add-datastores/fabric-analytics/#step-3-create-a-client-secret","title":"Step 3: Create a Client Secret","text":"<ol> <li> <p>In the app registration, navigate to Certificates &amp; secrets.</p> </li> <li> <p>Click + New client secret, provide a description, select an expiration period, and click Add.</p> <p></p> <p></p> </li> <li> <p>Copy the Value of the newly created secret immediately \u2014 this is your Client Secret (it will not be shown again).</p> </li> </ol> <p>Warning</p> <p>Make sure to copy the Client Secret value immediately after creation. It will not be displayed again once you navigate away from the page.</p>"},{"location":"add-datastores/fabric-analytics/#step-4-enable-service-principal-access-in-fabric","title":"Step 4: Enable Service Principal Access in Fabric","text":"<ol> <li> <p>Sign in to Microsoft Fabric as a Fabric administrator.</p> </li> <li> <p>Navigate to Settings (gear icon) &gt; Admin portal.</p> <p></p> </li> <li> <p>Under Tenant settings, find \"Service principals can call Fabric APIs\".</p> <p></p> </li> <li> <p>Enable the setting and specify the security group containing your service principal (or allow the entire organization).</p> </li> </ol>"},{"location":"add-datastores/fabric-analytics/#step-5-grant-workspace-access-to-the-service-principal","title":"Step 5: Grant Workspace Access to the Service Principal","text":"<ol> <li> <p>In Microsoft Fabric, navigate to your target Workspace.</p> </li> <li> <p>Click Manage access (or the ... menu &gt; Manage access).</p> </li> </ol> <p></p> <ol> <li>Click + Add people or groups.</li> </ol> <p></p> <ol> <li> <p>Search for your registered application name (e.g., <code>qualytics-fabric</code>).</p> </li> <li> <p>Assign the Contributor role (or higher).</p> </li> </ol> <p></p> <ol> <li>Click Add.</li> </ol>"},{"location":"add-datastores/fabric-analytics/#retrieve-the-sql-analytics-endpoint","title":"Retrieve the SQL Analytics Endpoint","text":"<ol> <li> <p>In your Fabric workspace, open the Lakehouse or Warehouse you want to connect to.</p> </li> <li> <p>Copy the SQL analytics endpoint from the connection string area. The format is:</p> <pre><code>&lt;workspace-guid&gt;.datawarehouse.fabric.microsoft.com\n</code></pre> <p> </p> </li> </ol> <p>Tip</p> <p>Refer to the Microsoft Fabric documentation for more information on connectivity and SQL analytics endpoints.</p>"},{"location":"add-datastores/fabric-analytics/#add-the-source-datastore","title":"Add the Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Fabric Analytics is an example of such a datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the Fabric Analytics datastore allows the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name Specify the name of the datastore (e.g., The specified name will appear on the datastore cards). 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector Select Fabric Analytics from the dropdown list."},{"location":"add-datastores/fabric-analytics/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Fabric Analytics connector from the dropdown list and add connection details such as Secrets Management, host, port, client ID, client secret, tenant ID, database, and schema.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Enter the SQL analytics endpoint of your Fabric Lakehouse or Warehouse (e.g., <code>&lt;workspace-guid&gt;.datawarehouse.fabric.microsoft.com</code>). 2. Port Specify the Port number. The default port for Fabric Analytics is <code>1433</code>. 3. Client ID Enter the Application (Client) ID from your Microsoft Entra ID app registration. 4. Client Secret Enter the Client Secret value generated in your app registration. 5. Tenant ID Enter the Directory (Tenant) ID from your Microsoft Entra ID app registration. 6. Database Specify the name of your Fabric Lakehouse or Warehouse. 7. Schema Define the schema within the database that should be used (default: <code>dbo</code>). 8. Teams Select one or more teams from the dropdown to associate with this source datastore. 9. Initial Cataloging Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/fabric-analytics/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New Connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If the connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/fabric-analytics/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Warning</p> <p>Qualytics does not support the Fabric Analytics connector as an enrichment datastore, but you can point to a different enrichment datastore.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/fabric-analytics/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add New Connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for Add New Connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Note</p> <p>Qualytics does not support Fabric Analytics as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using Microsoft SQL Server as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/fabric-analytics/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p>Note</p> <p>Qualytics does not support Fabric Analytics as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using Microsoft SQL Server as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. For example, Marked as Public means that this datastore is accessible to all the users.</li> <li>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment.</li> <li>Database: Refers to the specific database within the enrichment datastore environment where the data is stored.</li> <li>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/fabric-analytics/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"add-datastores/fabric-analytics/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides a sample payload for creating a Fabric Analytics datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint (Post): <code>/api/datastores (post)</code></p> Creating a source datastore with a new connectionCreating a datastore with an existing connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"fabric_database\",\n    \"schema\": \"dbo\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"fabric\",\n        \"host\": \"your-workspace-guid.datawarehouse.fabric.microsoft.com\",\n        \"port\": \"1433\",\n        \"username\": \"your_client_id\",\n        \"password\": \"your_client_secret\",\n        \"parameters\": {\n            \"tenant_id\": \"your_tenant_id\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"fabric_database\",\n    \"schema\": \"dbo\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": \"connection-id\"\n}\n</code></pre>"},{"location":"add-datastores/fabric-analytics/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/google-cloud-storage/","title":"Google Cloud Storage","text":"<p>Adding and configuring a Google Cloud Storage connection within Qualytics empowers the platform to build a symbolic link with your file system to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Google Cloud Storage as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Google Cloud Storage environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/google-cloud-storage/#google-cloud-storage-setup-guide","title":"Google Cloud Storage Setup Guide","text":"<p>This guide will walk you through the steps to set up Google Cloud Storage, including how to retrieve the necessary URIs, access keys, and secret keys, which are essential for integrating this datastore into Qualytics.</p>"},{"location":"add-datastores/google-cloud-storage/#retrieve-the-google-cloud-storage-uri","title":"Retrieve the Google Cloud Storage URI","text":"<p>To retrieve the Cloud Storage URI, follow the given steps:</p> <ol> <li>Go to the Cloud Storage Console.</li> <li>Navigate to the location of the object (file) that holds the source data.</li> <li>At the top of the Cloud Storage console, locate and note down the path to the object.</li> <li>Create the URI using the following format: </li> </ol> <pre><code>gs://bucket/file\n</code></pre> <ul> <li> <p><code>bucket</code> is the name of the Cloud Storage bucket.</p> </li> <li> <p><code>file</code> is the name of the object (file) containing the data.</p> </li> </ul>"},{"location":"add-datastores/google-cloud-storage/#retrieve-the-access-key-and-secret-key","title":"Retrieve the Access Key and Secret Key","text":"<p>You need these keys when integrating Google Cloud Storage with other applications or services, such as when adding it as a datastore in Qualytics. The keys allow you to reuse existing code to access Google Cloud Storage without needing to implement a different authentication mechanism.</p> <p>To retrieve the access key and secret key in the Google Cloud Storage Console account, follow the given steps: </p> <p>Step 1: Log in to the Google Cloud Console, navigate to the Google Cloud Storage settings, and this will redirect you to the Settings page.</p> <p></p> <p>Step 2: Click on the Interoperability tab. </p> <p></p> <p>Step 3: Scroll down the Interoperability  page and under Access keys for your user account, click the CREATE A KEY button to generate a new Access Key and Secret Key.</p> <p></p> <p>Step 4: Use these generated Access Key and Secret Key values when adding your Google Cloud Storage account to SimpleBackups.</p> <p></p> <p>For example, once you generate the keys, they might look like this:</p> <ul> <li> <p>Access Key: <code>GOOG1234ABCDEFGH5678</code></p> </li> <li> <p>Secret Key: <code>abcd1234efgh5678ijklmnopqrstuvwx</code></p> </li> </ul> <p>Warning</p> <p>Make sure to store these keys securely, as they provide access to your Google Cloud Storage resources. </p>"},{"location":"add-datastores/google-cloud-storage/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect and access data from external sources. Google Cloud Storage is an example of a source datastore, specifically a type of Distributed File System (DFS) datastore that is designed to handle data stored in distributed file systems. Configuring a DFS datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore (e.g., The specified name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection 3. Connector (Required) Select Google Cloud Storage from the dropdown list."},{"location":"add-datastores/google-cloud-storage/#option-i-create-a-datastore-with-a-new-connection","title":"Option I: Create a Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Google Cloud Storage connector from the dropdown list and add connection details such as Secrets Management, URI, service account key, root path, and teams.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. URI (Required) Enter the Uniform Resource Identifier (URI) of the Google Cloud Storage. 2. Service Account Key (Required) Upload a JSON file that contains the credentials required for accessing the Google Cloud Storage. 3. Root Path (Required) Specify the root path where the data is stored. 4. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 5. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified. </p>"},{"location":"add-datastores/google-cloud-storage/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Root Path, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to check and verify the source data connection. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/google-cloud-storage/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). This datastore is used to store the analyzed results, including any anomalies and additional metadata files. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/google-cloud-storage/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window - Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for Add New Connection Toggle ON to create a new enrichment datastore from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> REF. FIELDS ACTIONS 1. URI (Required) Enter the Uniform Resource Identifier (URI) for the Google Cloud Storage. 2. Service Account Key (Required) Upload a JSON file that contains the credentials required for accessing the Google Cloud Storage. 3. Root Path (Required) Specify the root path where the data is stored. 4. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified. </p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process. </p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p></p> <p>Step 5: Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/google-cloud-storage/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Use an existing enrichment datastore is turned on, you will be prompted to configure the enrichment datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment: </p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example: Marked as Public means that this datastore is accessible to all the users. </p> </li> <li> <p>URI: Uniform Resource Identifier (URI) points to the specific location of the source data and should be formatted accordingly (e.g., <code>gs://bucket/file</code> for Google Cloud Storage).</p> </li> <li> <p>Root Path: Specify the root path where the data is stored. This path defines the base directory or folder from which all data operations will be performed.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p></p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/google-cloud-storage/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/google-cloud-storage/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating the Google Cloud Storage datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>        {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"trigger_catalog\": true,\n        \"root_path\": \"/gcs_root_path\",\n        \"enrich_only\": false,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"gcs\",\n            \"uri\": \"gs://&lt;bucket_name&gt;\",\n            \"secret_key\": \"gcs_service_account_key\"\n        }\n    }\n</code></pre> <pre><code>   {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"trigger_catalog\": true,\n        \"root_path\": \"/gcs_root_path\",\n        \"enrich_only\": false,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/google-cloud-storage/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an existing Connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"trigger_catalog\": true,\n        \"root_path\": \"/gcs_root_path\",\n        \"enrich_only\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"gcs\",\n            \"uri\": \"gs://&lt;bucket_name&gt;\",\n            \"secret_key\": \"gcs_service_account_key\"\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"trigger_catalog\": true,\n        \"root_path\": \"/gcs_root_path\",\n        \"enrich_only\": true,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/google-cloud-storage/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore:</p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/hive/","title":"Hive","text":"<p>Adding and configuring a Hive connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Hive as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Hive environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/hive/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Hive is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the datastore name (e.g., This name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select Hive from the dropdown list."},{"location":"add-datastores/hive/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Hive connector from the dropdown list and add the connection details.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically. </p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> REF. FIELDS ACTIONS 1. Host(Required) Get Hostname from your Hive account and add it to this field. 2. Port(Required) Specify the Port number. 3. Authentication You can choose between Basic Authentication and Kerberos Authentication for validating and securing the connection to your Hive instance.  Basic Authentication: This method uses a username and password combination for authentication. It is a straightforward method where the user's credentials are directly used to access Hive. <ul><li>Type: Select the authentication type from the dropdown menu.</li><li>User: Enter the username that Qualytics will use to connect to Hive.</li><li>Password: Enter the password associated with the specified user account.</li></ul> Kerberos Authentication: This method uses Kerberos tickets for authentication. It relies on a secure, ticket-based mechanism managed by your environment\u2019s Kerberos configuration. <ul> <li>Type: Select Kerberos from the authentication type dropdown.</li><li>Principal: Enter the Kerberos principal (for example: <code>hive/_HOST@DOMAIN.COM</code>) that Qualytics will use to connect to Hive. </li> </ul> 4. Schema(Required) Define the schema within the database that should be used. 5. Teams(Required) Select one or more teams from the dropdown to associate with this source datastore. 6. Initial Cataloging(Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p></p> <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/hive/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add new connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams, and Initiate Cataloging. </p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step. </p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page. </p>"},{"location":"add-datastores/hive/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Warning</p> <p>Qualytics does not support the Hive connector as an enrichment datastore, but you can point to a different enrichment datastore. </p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window- Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/hive/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p>Note</p> <p>Qualytics does not support Hive as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using BigQuery as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration. </p> <p></p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/hive/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example- Marked as Public means that this datastore is accessible to all the users.  </li> <li>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment. </li> <li>Database: Refers to the specific database within the enrichment datastore environment where the data is stored.  </li> <li>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/hive/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API.</p> <p>Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/hive/#creating-a-datastore","title":"Creating a Datastore","text":"<p>This section provides a sample payload for creating a datastore. Replace the placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/hive/#endpoint-post","title":"Endpoint (Post)","text":"<p><code>/api/datastores</code> (post)</p> Creating a datastore with a new connectionCreating a datastore with an existing connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"hive_database\",\n        \"schema\": \"hive_schema\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"hive\",\n            \"host\": \"hive_host\",\n            \"port\": \"hive_port\",\n            \"username\": \"hive_username\",\n            \"password\": \"hive_password\",\n            \"parameters\": {\n                \"zookeeper\": false\n            }\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"hive_database\",\n        \"schema\": \"hive_schema\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection_id\": connection_id\n    }\n</code></pre>"},{"location":"add-datastores/hive/#linking-datastore-to-an-enrichment-datastore-through-api","title":"Linking Datastore to an Enrichment Datastore through API","text":""},{"location":"add-datastores/hive/#endpoint-patch","title":"Endpoint (Patch)","text":"<p><code>/api/datastores/{datastore-id}/enrichment/{enrichment-id}</code> (patch)</p>"},{"location":"add-datastores/maria-db/","title":"MariaDB","text":"<p>Adding and configuring a MariaDB connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add MariaDB as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their MariaDB environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/maria-db/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. MariaDB is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the datastore name (e.g., this name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select MariaDB from the dropdown list."},{"location":"add-datastores/maria-db/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the MariaDB connector from the dropdown list and add connection details such as Secrets Management, host, port, user, password, SSL connection, database, and schema.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> <p></p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get Hostname from your MariaDB account and add it to this field. 2. Port Specify the Port number. 3. User Enter the User ID to connect. 4. Password Enter the password to connect to the database. 5. Database Specify the database name. 6. Teams Select one or more teams from the dropdown to associate with this source datastore. 7. Initial Cataloging Check the checkbox to automatically perform a catalog operation on the configured source to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/maria-db/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/maria-db/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). This datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/maria-db/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Enter a name for the enrichment datastore. 3. Toggle Button For Add New Connection Toggle ON to create a new enrichment datastore from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 5: Close the success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/maria-db/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. For example, marked as Public means that this datastore is accessible to all the users.  </li> <li>Host: This is the server address where the MariaDB instance is hosted. It is the endpoint used to connect to the MariaDB environment.  </li> <li>Database: Refers to the specific database within the MariaDB environment where the data is stored.  </li> <li>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal window will display and a success flash message stating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/maria-db/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"add-datastores/maria-db/#creating-a-datastore","title":"Creating a Datastore","text":"<p>This section provides a sample payload for creating a datastore. Replace the placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/maria-db/#endpoint-post","title":"Endpoint (Post)","text":"<p><code>/api/datastores</code> (post)</p> Creating a datastore with a new connectionCreating a datastore with an existing connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"mariadb_database\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"mariadb\",\n            \"host\": \"mariadb_host\",\n            \"port\": \"mariadb_port\",\n            \"username\": \"mariadb_username\",\n            \"password\": \"mariadb_password\"\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"mariadb_database\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/maria-db/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":""},{"location":"add-datastores/maria-db/#endpoint-post_1","title":"Endpoint (Post)","text":"<p><code>/api/datastores</code> (post)</p> <p>This section provides a sample payload for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> Creating an enrichment datastore with a new connectionCreating an enrichment datastore with an existing connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"mariadb_database\",\n        \"enrich_only\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"mariadb\",\n            \"host\": \"mariadb_host\",\n            \"port\": \"mariadb_port\",\n            \"username\": \"mariadb_username\",\n            \"password\": \"mariadb_password\"\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"mariadb_database\",\n        \"enrich_only\": true,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/maria-db/#linking-datastore-to-an-enrichment-datastore-through-api","title":"Linking Datastore to an Enrichment Datastore through API","text":""},{"location":"add-datastores/maria-db/#endpoint-patch","title":"Endpoint (Patch)","text":"<p><code>/api/datastores/{datastore-id}/enrichment/{enrichment-id}</code> (patch)</p>"},{"location":"add-datastores/microsoft-sql-server/","title":"Microsoft SQL Server","text":"<p>Adding and configuring Microsoft SQL Server connection within Qualytics empowers the platform to build a symbolic link with your database to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on adding Microsoft SQL Server as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Microsoft SQL Server environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/microsoft-sql-server/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Microsoft SQL Server is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface. </p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the datastore name (e.g., This name will appear on the datastore cards) 2. Toggle Button Toggle ON to  create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select Microsoft SQL Server from the dropdown list."},{"location":"add-datastores/microsoft-sql-server/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Microsoft SQL Server connector from the dropdown list and add connection details such as Secret Management, host, port, username, password, and database.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your Microsoft SQL Server account and add it to this field. 2. Port (Optional) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the password to connect to the database. 5. Database (Required) Specify the database name. 6. Schema (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 8. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified. </p>"},{"location":"add-datastores/microsoft-sql-server/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add new connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/microsoft-sql-server/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window- Link Enrichment Datastore will appear, providing you with the options to configure to add an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/microsoft-sql-server/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>Once the HashiCorp Vault is set up, use the ${key} format in Connection form to reference a Vault secret.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 3: The configuration form, requesting credential details after selecting the enrichment datastore connector.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your Microsoft SQL Server account and add it to this field. 2. Port (Optional) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the Password to connect to the database. 5. Database (Required) Specify the database name. 6. Schema (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this datastore. <p>Step 4: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified. </p> <p></p> <p>Step 5: Click on the Finish button to complete the configuration process. </p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/microsoft-sql-server/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment: </p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example- Marked as Public means that this datastore is accessible to all the users. </p> </li> <li> <p>Host: This is the server address where the Microsoft SQL Server instance is hosted. It is the endpoint used to connect to the Microsoft SQL Server environment. </p> </li> <li> <p>Database: Refers to the specific database within the Microsoft SQL Server environment where the data is stored. </p> </li> <li> <p>Schema:  The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/microsoft-sql-server/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. </p> <p>Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/microsoft-sql-server/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a Microsoft SQL Server datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"sqlserver_database\",\n    \"schema\": \"sqlserver_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"sqlserver\",\n        \"host\": \"sqlserver_host\",\n        \"port\": \"sqlserver_port\",\n        \"username\": \"sqlserver_username\",\n        \"password\": \"sqlserver_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"sqlserver_database\",\n    \"schema\": \"sqlserver_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection_id\n}\n</code></pre>"},{"location":"add-datastores/microsoft-sql-server/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an Existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"sqlserver_database\",\n    \"schema\": \"sqlserver_enrichment_schema\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"sqlserver\",\n        \"host\": \"sqlserver_host\",\n        \"port\": \"sqlserver_port\",\n        \"username\": \"sqlserver_username\",\n        \"password\": \"sqlserver_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"sqlserver_database\",\n    \"schema\": \"sqlserver_enrichment_schema\",\n    \"enrich_only\": true,\n    \"connection_id\": connection_id\n}\n</code></pre>"},{"location":"add-datastores/microsoft-sql-server/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore:</p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/mysql/","title":"MySQL","text":"<p>Adding and configuring a MySQL connection within Qualytics empowers the platform to build a symbolic link with your database to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add MySQL as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their MySQL environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/mysql/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. MySQL is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the datastore name (e.g., This name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select MySQL from the dropdown list."},{"location":"add-datastores/mysql/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the MySQL connector from the dropdown list and add connection details such as Secrets Management, host, port, username, password, and database.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your MySQL account and add it to this field. 2. Port (Required) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the password to connect to the database. 5. Database (Required) Specify the database name. 6. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 7. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/mysql/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add new connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Teams and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/mysql/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window- Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1 Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2 Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3 Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/mysql/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>Once the HashiCorp Vault is set up, use the ${key} format in Connection form to reference a Vault secret.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 3: The configuration form, requesting credential details after selecting the enrichment datastore connector.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your MySQL account and add it to this field. 2. Port (Required) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the password to connect to the database. 5. Database (Required) Specify the database name. 6. Teams (Required) Select one or more teams from the dropdown to associate with this datastore. <p>Step 4: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 5: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/mysql/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Team: The team associated with managing the enrichment datastore is based on the role of public or private. Example- Marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>Host: This is the server address where the MySQL  instance is hosted. It is the endpoint used to connect to the MySQL environment.</p> </li> <li> <p>Database: Refers to the specific database within the MySQL environment where the data is stored.</p> </li> </ul> <p></p> <p>Step 3: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/mysql/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API.</p> <p>Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/mysql/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a MySQL datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"mysql_database\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"mysql\",\n        \"host\": \"mysql_host\",\n        \"port\": \"mysql_port\",\n        \"username\": \"mysql_username\",\n        \"password\": \"mysql_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"mysql_database\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/mysql/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"mysql_database\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"mysql\",\n        \"host\": \"mysql_host\",\n        \"port\": \"mysql_port\",\n        \"username\": \"mysql_username\",\n        \"password\": \"mysql_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"mysql_database\",\n    \"enrich_only\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/mysql/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore:</p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/oracle/","title":"Oracle","text":"<p>Adding and configuring an Oracle connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Oracle as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Oracle environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/oracle/#add-the-source-datastore","title":"Add the Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Oracle, for example, is a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the Oracle datastore allows the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> Step FIELDS Description 1. Name Specify the name of the datastore (e.g., The specified name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector Select \u201cOracle\u201d from the dropdown list."},{"location":"add-datastores/oracle/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add new connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Oracle connector from the dropdown list and add connection details such as Secret Management, host, port, username, sid, and schema.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get \u201cHostname\u201d from your Oracle account and add it to this field. 2. Port Specify the \u201cPort\u201d number. 3. Protocol Specifies the connection protocol used for communicating with the database. Choose between TCP or TCPS. 4. Connect By You can choose between SID or Service Name to establish a connection with the Oracle database, depending on how your database instance is configured. 5. User Enter the \u201cUser ID\u201d to connect. 6. Password Enter the \u201cpassword\u201d to connect to the database. 7. Schema Define the schema within the database that should be used. 8. Teams Select one or more teams from the dropdown to associate with this source data store. 9. Initial Cataloging Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/oracle/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for add new connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If the connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/oracle/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Warning</p> <p>Qualytics does not support the Oracle connector as an enrichment datastore, but you can point to a different enrichment datastore.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/oracle/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Note</p> <p>Qualytics does not support Oracle as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using Microsoft SQL Server as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success  message indicating that your datastore has been successfully added.</p> <p>Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/oracle/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p>Note</p> <p>Qualytics does not support Oracle as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using  Microsoft SQL Server as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. For example, Marked as Public means that this datastore is accessible to all the users.</li> <li>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment.</li> <li>Database: Refers to the specific database within the enrichment datastore environment where the data is stored.</li> <li>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects(tables, views, etc.).Each schema belongs to a single database.</li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added**.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/oracle/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"add-datastores/oracle/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides a sample payload for creating an Oracle datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint (Post): <code>/api/datastores (post)</code></p> Creating a source datastore with a new connectionCreating a datastore with an existing connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"oracle_database\",\n    \"schema\": \"oracle_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"oracle\",\n        \"host\": \"oracle_host\",\n        \"port\": \"oracle_port\",\n        \"username\": \"oracle_username\",\n        \"password\": \"oracle_password\",\n        \"parameters\": {\n            \"sid\": \"orcl\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"oracle_database\",\n    \"schema\": \"oracle_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": \"connection-id\"\n}\n</code></pre>"},{"location":"add-datastores/oracle/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/overview-of-a-datastore/","title":"Source Datastore","text":"<p>Qualytics connects to source datastores using \"Datastores,\" a framework that enables organizations to:</p> <ul> <li> <p>Connect with Apache Spark-compatible source datastores.</p> </li> <li> <p>Support both traditional databases and modern object storage.</p> </li> <li> <p>Profile and monitor structured data across systems.</p> </li> <li> <p>Ensure secure and fast access to data.</p> </li> <li> <p>Scale data quality operations across platforms.</p> </li> <li> <p>Manage data quality centrally across all sources.</p> </li> </ul> <p>These source datastore integrations ensure comprehensive quality management across your entire data landscape, regardless of where your data resides.</p>"},{"location":"add-datastores/overview-of-a-datastore/#understanding-datastores","title":"Understanding Datastores","text":"<p>A Datastore in Qualytics represents any structured source datastore, such as:</p> <ul> <li> <p>Relational databases (RDBMS)</p> </li> <li> <p>Raw file formats like CSV, XLSX, JSON, Avro, or Parquet</p> </li> <li> <p>Cloud storage platforms like AWS S3, Azure Blob Storage, or GCP Cloud Storage</p> </li> </ul> <p>Qualytics integrates with these source datastores through a layered architecture:</p> <p></p>"},{"location":"add-datastores/overview-of-a-datastore/#configuring-source-datastores","title":"Configuring Source Datastores","text":"<p>Configure your source datastores in Qualytics by connecting them through a new datastore.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window, Add Datastore, will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name Specify the name of the datastore (e.g., the specified name will appear on the datastore cards). 2. Toggle Button Toggle on to create a new source datastore from scratch, or toggle off to reuse credentials from an existing connection. 3. Connector Select Any source datastore from the dropdown list."},{"location":"add-datastores/overview-of-a-datastore/#available-datastore-connectors","title":"Available Datastore Connectors","text":"<p>Qualytics supports a range of source datastores, including but not limited to:</p> <p>Tip</p> <p>Want to check which datastores have Enrichment support?  See the Supported Enrichment Datastores </p> No. Source Datastores 1. Amazon Redshift 2. Amazon S3 3. Athena 4. Azure Datalake Storage (ABFS) 5. Big Query 6. Databricks 7. DB2 8. Dremio 9. Fabric Analytics 10. Google Cloud Storage 11. Hive 12. MariaDB 13. Microsoft SQL Server 14. MySQL 15. Oracle 16. PostgreSQL 17. Presto 18. Snowflake 19. Synapse 20. Teradata 21. Timescale DB 22. Trino"},{"location":"add-datastores/overview-of-a-datastore/#connection-management","title":"Connection Management","text":"<p>To connect to a datastore, users must provide the required connection details, such as Host/Port or URI. These fields may vary depending on the datastore and are essential for establishing a secure and reliable connection to the target database.</p> <p>For demonstration purposes, we have selected the Snowflake connector.</p>"},{"location":"add-datastores/overview-of-a-datastore/#option-i-create-a-datastore-with-a-new-connection","title":"Option I: Create a Datastore with a New Connection","text":"<p>If the toggle for Add New Connection is turned on, this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select any connector (as we are selecting the Snowflake connector) from the dropdown list and add connection properties such as Secrets Management, host, port, username, and password, along with datastore properties like catalog, database, etc.</p> <p></p> <p>For the next steps, refer to the \"Add Source Datastore\" section in the Snowflake Datastore documentation.</p> <p>Once a datastore is verified and created, it appears in your source datastores.</p> <p></p>"},{"location":"add-datastores/overview-of-a-datastore/#datastore-operations","title":"Datastore Operations","text":"<p>Once a datastore is added in Qualytics, you can perform three key operations to manage and ensure data quality effectively:</p> <p>1. Catalog Operation</p> <p>This operation imports named data collections such as tables, views, and files into the source datastore. It identifies incremental fields for scans and allows you to recreate or delete containers, streamlining data organization and enhancing discovery.  </p> <p>For more details about the catalog operation, refer to the \"Catalog Operation\" document.</p> <p>2. Profile Operation</p> <p>After cataloging, the Profile Operation analyzes each record within the collections to assess and improve data quality. By generating detailed metadata and interacting with the Qualytics Inference Engine, it identifies quality issues and refines checks for maintaining data integrity.</p> <p>For more details about the profile operation, refer to the \"Profile Operation\" document.</p> <p>3. Scan Operation</p> <p>Finally, the Scan Operation enforces data quality checks on the collections. It identifies anomalies at the record and schema levels, highlights structural issues, and records all findings for further analysis. Flexible options allow for incremental scans, specific table/file scans, and scheduling future scans.</p> <p>For more details about the scan operation, refer to the \"Scan Operation\" document.</p> <p>By performing these operations sequentially, you can efficiently manage and ensure the quality of your data in Qualytics.</p>"},{"location":"add-datastores/overview-of-a-datastore/#view-operation","title":"View Operation","text":"<p>Once the datastores are connected, you can run operations on the selected datastore. To track the progress, simply navigate to the Activity tab, where you can view the running operation.</p> <p>Step 1: Simply click to open the datastore on which you ran the operation.</p> <p></p> <p>Step 2: After clicking on the datastore, select the \"Activity\" tab to view the ongoing operation.</p> <p></p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/","title":"DFS Datastore Overview","text":"<p>The DFS (Distributed File System) Datastore feature in Qualytics is designed to handle data stored in distributed file systems. </p> <p>This includes file systems like Hadoop Distributed File System (HDFS) or similar distributed storage solutions.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#supported-distributed-file-systems","title":"Supported Distributed File Systems","text":"<p>Qualytics supports DFS Datastores, catering to distributed file systems like:</p> <ul> <li>Amazon S3</li> <li>Google Cloud Storage</li> <li>Windows Azure Storage Blob</li> </ul>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#connection-details","title":"Connection Details","text":"<p>Users provide connection details for DFS Datastores, allowing Qualytics to establish a connection to the distributed file system.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#catalog-operation","title":"Catalog Operation","text":"<p>The Catalog operation involves walking the directory tree, reading files with supported filename extensions, and creating containers based on file metadata.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#supported-file-formats","title":"Supported File Formats","text":"<p>Qualytics supports the following file formats in the DFS Datastore.</p> No File Format Extension Example 1 Avro <code>.avro</code> 2 CSV <code>.csv</code>, <code>.csv.gz</code> 3 TSV <code>.tsv</code>, <code>.tsv.gz</code> 4 TXT <code>.txt</code>, <code>.txt.gz</code> 5 PSV <code>.psv</code>, <code>.psv.gz</code> 6 SKV <code>.skv</code>, <code>.skv.gz</code> 7 JSON <code>.json</code>, <code>.json.gz</code> 8 ORC <code>.orc</code> 9 Delta <code>.delta</code> 10 Iceberg <code>.iceberg</code> 11 Parquet <code>.parquet</code> 12 XLS <code>.xls</code> 13 XLSX <code>.xlsx</code> 14 XLSM <code>.xlsm</code>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#data-quality-and-profiling","title":"Data Quality and Profiling","text":"<p>DFS Datastores support the initiation of Profile Operations, allowing users to understand the structure and characteristics of the data stored in the distributed file system.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#containers-overview","title":"Containers Overview","text":"<p>For a more detailed understanding of how Qualytics manages and interacts with containers in DFS Datastores, please refer to the Containers section in our comprehensive user guide. </p> <p>This section covers topics such as container deletion, field deletion, and the initial profile of a Datastore's containers.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#multi-token-filename-globbing-and-container-formation","title":"Multi-Token Filename Globbing and Container Formation","text":"<p>Filenames with similar structures in the same folder are automatically included in a single globbed container during the Catalog operation.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#use-folders-for-precise-file-grouping","title":"Use Folders for Precise File Grouping","text":"<p>Organizing files within distinct folders is a straightforward and effective strategy in Distributed File Systems (DFS). </p> <p>When all files in a folder share a common schema, it simplifies the process of grouping and managing them. </p> <p>This approach ensures precise file grouping without relying on complex glob patterns.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#how-to-use-folders-for-shared-schema","title":"How to Use Folders for Shared Schema","text":""},{"location":"add-datastores/overview-of-a-dfs-datastore/#1-create-a-folder","title":"1. Create a Folder:","text":"<p>Begin by creating a new folder in your distributed file system.</p> <ul> <li> <p>Suppose you have order data files with filenames like <code>orders_20240229.csv</code>, <code>orders-20240228.csv</code>, <code>orders-20240227.csv</code>.</p> </li> <li> <p>Create a folder named <code>Orders</code> to group these files.</p> </li> </ul> <p>Qualytics Pattern: Qualytics will automatically create the container <code>orders_*.csv</code> based on the filenames.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#2-place-related-files-in-the-folder","title":"2. Place Related Files in the Folder:","text":"<p>Move or upload files that share a common schema into the created folder.</p> <ul> <li>Move the order data files into the <code>Orders</code> folder.</li> </ul>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#3-repeat-for-each-schema","title":"3. Repeat for Each Schema:","text":"<p>Create separate folders for different schemas, and organize files accordingly.</p> <ul> <li>Suppose you have customer data files with filenames like <code>customers_us.csv</code>, <code>customers_eu.csv</code>.</li> <li>Create a folder named <code>Customers</code> to group these files.</li> </ul> <p>Qualytics Pattern: Qualytics will automatically create the pattern <code>customers_*.csv</code> based on the filenames.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#4-naming-conventions","title":"4. Naming Conventions:","text":"<p>Consider adopting clear and consistent naming conventions for folders to enhance organization.</p> <ul> <li>Use descriptive names for folders, such as <code>Orders</code>, <code>Customers</code>, to make it easier to identify the contents.</li> </ul>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#flowchart-using-folders-for-shared-schema","title":"Flowchart: Using Folders for Shared Schema","text":"<pre><code>graph TD\n  A[Start] --&gt;|Create a Folder| B(Create Folder)\n  B --&gt;|Place Related Files| C(Move or Upload Files)\n  C --&gt;|Repeat for Each Schema| D(Create Separate Folders)\n  D --&gt;|Naming Conventions| E(Consider Clear Naming)\n  E --&gt; F[End]</code></pre>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#use-filename-conventions-for-posix-globs","title":"Use Filename Conventions for Posix Globs:","text":"<p>This option leverages filename conventions that align with POSIX globs, allowing the system to automatically organize files for you. </p> <p>The system intelligently analyzes filename patterns, making the process seamless and efficient.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#how-to-use-filename-conventions-for-posix-globs","title":"How to Use Filename Conventions for Posix Globs","text":""},{"location":"add-datastores/overview-of-a-dfs-datastore/#1-follow-clear-filename-conventions","title":"1. Follow Clear Filename Conventions:","text":"<p>Adopt clear and consistent filename conventions that lend themselves to POSIX globs.</p> <ul> <li>Suppose you have log files with filenames like <code>app_log_20240229.txt</code>, <code>app_log_20240228.txt</code>, <code>app_log_20240227.txt</code>.</li> <li>Use a consistent naming convention like <code>app_log_*.txt</code>, where <code>*</code> serves as a placeholder for varying elements.</li> <li>The <code>*</code> in the convention acts as a wildcard, representing any variation in the filename. In this example, it matches the date part (<code>20240229</code>, <code>20240228</code>, etc.).</li> </ul>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#2-upload-or-move-files","title":"2. Upload or Move Files:","text":"<p>Upload or move files with filenames following the adopted conventions to your distributed file system.</p> <ul> <li>Move log files with the specified naming convention to your DFS.</li> </ul>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#3-system-analysis","title":"3. System Analysis:","text":"<p>The system will automatically detect and analyze the filename conventions, creating appropriate glob patterns.</p> <ul> <li>With filenames like <code>app_log_20240229.txt</code>, <code>app_log_20240228.txt</code>, the system will create the pattern <code>app_log_*.txt</code>.</li> </ul>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#flowchart-using-folders-for-filename-conventions","title":"Flowchart: Using Folders for Filename Conventions","text":"<pre><code>graph TD\n  A[Start] --&gt;|Follow Clear Conventions| B(Adopt Consistent Conventions)\n  B --&gt;|Upload or Move Files| C(Move Files to DFS)\n  C --&gt;|System Analysis| D(Automatic Pattern Creation)\n  D --&gt; E[End]</code></pre>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#why-not-manually-creating-your-own-globs","title":"Why not manually creating your own Globs?","text":"<p>While the system offers powerful features to automate file organization, manually creating globs is strongly discouraged.</p> <p>This option may lead to errors, inconsistencies, and hinder the efficiency of the system.</p> <p>It is recommended to leverage the automated tools for a seamless and error-free experience.</p>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#complexity-and-error-prone","title":"Complexity and Error-Prone:","text":"<p>Manually creating globs can be complex, prone to typos, and susceptible to errors in pattern formation.</p> <ul> <li>Suppose you want to group log files with the pattern <code>app_log_*.txt</code>. A manual attempt might result in mistakes like <code>app_log_202*.txt</code> or <code>app_log_*.tx</code>.</li> </ul>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#inconsistencies-across-files","title":"Inconsistencies Across Files:","text":"<p>Manual glob creation may lead to inconsistencies across different files, making it challenging to establish a uniform file organization.</p> <ul> <li>Trying to manually create globs for order data files with varying date formats (<code>orders_20240229.csv</code>, <code>orders-20240228.csv</code>) can result in inconsistent patterns.</li> </ul>"},{"location":"add-datastores/overview-of-a-dfs-datastore/#explore-deeper-knowledge","title":"Explore Deeper Knowledge","text":"<p>If you want to go deeper into the knowledge or if you are curious and want to learn more about DFS filename globbing, you can explore our comprehensive guide here: How DFS Filename Globbing Works.</p>"},{"location":"add-datastores/overview-of-a-jdbc-datastore/","title":"JDBC Datastore Overview","text":"<p>JDBC Datastore in Qualytics allows you to easily integrate and manage data from relational databases. Using the Java Database Connectivity (JDBC) API, you can securely connect to databases, analyze data, and perform data profiling. This feature supports a wide range of relational databases, providing you with a flexible solution for data discovery and quality checks.</p>"},{"location":"add-datastores/overview-of-a-jdbc-datastore/#adding-jdbc-datastore","title":"Adding JDBC Datastore","text":"<p>Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>For detailed steps on adding a JDBC Datastore, refer to the Add the Source Datastore section of the documentation.</p>"},{"location":"add-datastores/overview-of-a-jdbc-datastore/#supported-jdbc-databases","title":"Supported JDBC Databases","text":"<p>Qualytics supports a range of relational databases, including but not limited to:</p> <ul> <li>Athena </li> <li>Databricks</li> <li>DB2</li> <li>Fabric Analytics</li> <li>Hive </li> <li>MariaDB </li> <li>Microsoft SQL Server </li> <li>MySQL </li> <li>Oracle </li> <li>PostgreSQL </li> <li>Presto </li> <li>Amazon Redshift </li> <li>Snowflake </li> <li>Synapse </li> <li>Timescale DB </li> <li>Trino</li> </ul>"},{"location":"add-datastores/overview-of-a-jdbc-datastore/#connection-details","title":"Connection Details","text":"<p>To connect to a JDBC datastore, users must provide the required connection details, such as Host/Port or URI. These fields may vary depending on the datastore and are essential for establishing a secure and reliable connection to the target database.</p> <p>For more information about connections, refer to the Connection Overview documentation.</p>"},{"location":"add-datastores/overview-of-a-jdbc-datastore/#catalog-operation","title":"Catalog Operation","text":"<p>After adding a JDBC Datastore, you can initiate a Catalog operation to extract key metadata from the database. This operation provides:</p> <ul> <li>A list of containers (schemas, tables, or views).  </li> <li>Field names within each container.  </li> <li>Record counts for data analysis and profiling.</li> </ul> <p></p> <p>For more information about how to run catalog operation, refer to the Catalog Operation documentation.</p>"},{"location":"add-datastores/overview-of-a-jdbc-datastore/#field-types-inference","title":"Field Types Inference","text":"<p>Qualytics employs weighted histogram analysis during the Catalog operation to infer field types automatically. This advanced method ensures accurate detection of data types within the JDBC Datastore, enhancing the precision of data profiling.</p>"},{"location":"add-datastores/overview-of-a-jdbc-datastore/#containers-overview","title":"Containers Overview","text":"<p>Containers are fundamental entities representing structured data sets. These containers could manifest as tables in JDBC datastores or as files within DFS datastores. They play a pivotal role in data organization, profiling, and quality checks within the Qualytics application. For a more detailed understanding of how Qualytics manages and interacts with containers in JDBC Datastores, please refer to the Containers overview documentation.</p>"},{"location":"add-datastores/postgresql/","title":"PostgreSQL","text":"<p>Adding and configuring a PostgreSQL connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add PostgreSQL as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their PostgreSQL environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/postgresql/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. PostgreSQL is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore. (e.g., The specified name will appear on the datastore cards.) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select PostgreSQL from the dropdown list."},{"location":"add-datastores/postgresql/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add new existing connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the PostgreSQL connector from the dropdown list and add connection details such as Secrets Management, host, port, username, database, and schema.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your PostgreSQL account and add it to this field. 2. Port (Required) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the password to connect to the database. 5. Database (Required) Specify the database name. 6. Schema (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 8. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/postgresql/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add new connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Info</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/postgresql/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1 Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2 Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3 Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/postgresql/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>Once the HashiCorp Vault is set up, use the ${key} format in Connection form to reference a Vault secret.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 3: The configuration form will expand, requesting credential details after the selected enrichment datastore connector is chosen.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your PostgreSQL account and add it to this field. 2. Port (Required) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the password associated with the PostgreSQL user account. 5. Database (Required) Specify the database name to be accessed. 6. Schema (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this datastore. <p>Step 4: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 5: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/postgresql/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment datastore:</p> <ul> <li> <p>Team: The team associated with managing the enrichment datastore is based on the role of public or private. Example- Marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>Host: This is the server address where the PostgreSQL instance is hosted. It is the endpoint used to connect to the PostgreSQL environment.</p> </li> <li> <p>Database: Refers to the specific database within the PostgreSQL environment where the data is stored.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/postgresql/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/postgresql/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a PostgreSQL datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"postgresql_database\",\n    \"schema\": \"postgresql_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"postgresql\",\n        \"host\": \"postgresql_host\",\n        \"port\": \"postgresql_port\",\n        \"username\": \"postgresql_username\",\n        \"password\": \"postgresql_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"postgresql_database\",\n    \"schema\": \"postgresql_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/postgresql/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"postgresql_database\",\n    \"schema\": \"postgresql_schema\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"postgresql\",\n        \"host\": \"postgresql_host\",\n        \"port\": \"postgresql_port\",\n        \"username\": \"postgresql_username\",\n        \"password\": \"postgresql_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"postgresql_database\",\n    \"schema\": \"postgresql_schema\",\n    \"enrich_only\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/postgresql/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore:</p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/presto/","title":"Presto","text":"<p>Adding and configuring a Presto connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Presto as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Presto environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/presto/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Presto is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore. (e.g., The specified name will appear on the datastore cards.) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select Presto from the dropdown list."},{"location":"add-datastores/presto/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New existing connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Presto connector from the dropdown list and add connection details such as Secrets Management, host, port, username, database, and schema.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically. </p> REF. FIELDS ACTION 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your Presto account and add it to this field. 2. Port (Required) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the Password to connect to the database. 5. Catalog (Required) Add a Catalog to fetch data structures and metadata from Presto. 6. Schema (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 8. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.  </p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/presto/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add new connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams, and Initiate Cataloging. </p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.  </p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step. </p> <p>Info</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page. </p>"},{"location":"add-datastores/presto/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Warning</p> <p>Qualytics does not support the Presto connector as an enrichment datastore, but you can point to a different enrichment datastore. </p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/presto/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment datastore from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Note</p> <p>Qualytics does not support Presto as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using Microsoft SQL Server as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration. </p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 5: Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/presto/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> <p>Note</p> <p>Qualytics does not support Presto as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using Microsoft SQL Server as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration. </p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. For example, marked as Public means that this datastore is accessible to all users.  </li> <li>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment.  </li> <li>Database: Refers to the specific database within the enrichment datastore environment where the data is stored.  </li> <li>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.  </p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.  </p> <p></p>"},{"location":"add-datastores/presto/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API.</p> <p>Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/presto/#creating-a-datastore","title":"Creating a Datastore","text":"<p>This section provides a sample payload for creating a datastore. Replace the placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/presto/#endpoint-post","title":"Endpoint (Post)","text":"<p><code>/api/datastores</code> (post)</p> Creating a datastore with a new connectionCreating a datastore with an existing connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"presto_database\",\n        \"schema\": \"presto_schema\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"presto\",\n            \"host\": \"presto_host\",\n            \"port\": \"presto_port\",\n            \"username\": \"presto_username\",\n            \"password\": \"presto_password\",\n            \"parameters\":{\n                \"ssl_truststore\":\"truststore.jks\"\n            }\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"presto_database\",\n        \"schema\": \"presto_schema\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/presto/#linking-datastore-to-an-enrichment-datastore-through-api","title":"Linking Datastore to an Enrichment Datastore through API","text":""},{"location":"add-datastores/presto/#endpoint-patch","title":"Endpoint (Patch)","text":"<p><code>/api/datastores/{datastore-id}/enrichment/{enrichment-id}</code> (patch)</p>"},{"location":"add-datastores/redshift/","title":"Redshift","text":"<p>Adding and configuring a Redshift connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Redshift as both a source and an enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Redshift environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/redshift/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Redshift is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore. (e.g., The specified name will appear on the datastore cards.) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select Redshift from the dropdown list."},{"location":"add-datastores/redshift/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Redshift connector from the dropdown list and add connection details such as Secrets Management, port, host, password, database, and schema.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your Redshift account and add it to this field. 2. Port (Required) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the password associated with the Redshift user account. 5. Database (Required) Specify the database name. 6. Schema (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 8. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/redshift/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add new connection is turned off, then this will prompt you to configure the source datastore using existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Info</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/redshift/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>Once you have successfully tested and verified your source datastore connection, you can add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1 Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2 Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3 Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/redshift/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Enter a name for the enrichment datastore. 3. Toggle Button for Add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>Once the HashiCorp Vault is set up, use the ${key} format in Connection form to reference a Vault secret.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 3: The configuration form, requesting credential details after selected enrichment datastore connector.</p> <p></p> REF. FIELDS ACTIONS 1. Host (Required) Get Hostname from your Redshift account and add it to this field. 2. Port (Required) Specify the Port number. 3. User (Required) Enter the User to connect. 4. Password (Required) Enter the password associated with the Redshift user account. 5. Database (Required) Specify the database name to be accessed. 6. Schema  (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this datastore. <p>Step 4: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 5: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 6: Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/redshift/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2:  A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Team: The team associated with managing the enrichment datastore is based on the role of public or private. Example - Marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>Host: This is the server address where the Redshift instance is hosted. It is the endpoint used to connect to the Redshift environment.</p> </li> <li> <p>Database: Refers to the specific database within the Redshift environment where the data is stored.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/redshift/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/redshift/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a Redshift datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"redshift_database\",\n    \"schema\": \"redshift_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"redshift\",\n        \"host\": \"redshift_host\",\n        \"port\": \"redshift_port\",\n        \"username\": \"redshift_username\",\n        \"password\": \"redshift_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"redshift_database\",\n    \"schema\": \"redshift_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/redshift/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"redshift_database\",\n    \"schema\": \"redshift_schema\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"redshift\",\n        \"host\": \"redshift_host\",\n        \"port\": \"redshift_port\",\n        \"username\": \"redshift_username\",\n        \"password\": \"redshift_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"redshift_database\",\n    \"schema\": \"redshift_schema\",\n    \"enrich_only\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/redshift/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore:</p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/snowflake/","title":"Snowflake","text":"<p>Adding and configuring a Snowflake connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on how to add Snowflake as both a source and enrichment datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Snowflake environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/snowflake/#snowflake-setup-guide","title":"Snowflake Setup Guide","text":"<p>The Snowflake Setup Guide provides step-by-step instructions for configuring warehouses and roles, ensuring efficient data management and access control. It explains how to create a warehouse with minimal requirements and the setup of a default warehouse for a user. It also explains how to create custom read-only and read-write roles and grant the necessary privileges for data access and modification. </p> <p>This guide is designed to help you optimize your Snowflake environment for performance and security, whether setting it up for the first time or refining your configuration.</p>"},{"location":"add-datastores/snowflake/#warehouse-role-configuration","title":"Warehouse &amp; Role Configuration","text":"<p>This section provides instructions for configuring Snowflake warehouses and roles. It includes creating a warehouse with minimal requirements, assigning a default warehouse for a user, creating custom read-only and read-write roles, and granting privileges to these roles for data access and modification.</p>"},{"location":"add-datastores/snowflake/#create-a-warehouse","title":"Create a Warehouse","text":"<p>Use the following command to create a warehouse with minimal requirements:</p> <pre><code>CREATE WAREHOUSE qualytics_wh\nWITH\n    WAREHOUSE_SIZE = 'XSMALL'\n    AUTO_SUSPEND = 60\n    AUTO_RESUME = TRUE;\n</code></pre> <p>Set a specific warehouse as the default for a user:</p> <pre><code>ALTER USER &lt;username&gt; SET DEFAULT_WAREHOUSE = qualytics_wh;\n</code></pre>"},{"location":"add-datastores/snowflake/#source-datastore-privileges-and-permissions","title":"Source Datastore Privileges and Permissions","text":"<p>Create a new role called <code>qualytics_read_role</code> and grant it privileges:</p> <pre><code>CREATE ROLE qualytics_read_role;\nGRANT USAGE ON WAREHOUSE qualytics_wh TO ROLE qualytics_read_role;\nGRANT USAGE ON DATABASE &lt;database_name&gt; TO ROLE qualytics_read_role;\nGRANT USAGE ON SCHEMA &lt;database_name&gt;.&lt;schema_name&gt; TO ROLE qualytics_read_role;\nGRANT SELECT ON TABLE &lt;database_name&gt;.&lt;schema_name&gt;.&lt;table_name&gt; TO ROLE qualytics_read_role;\nGRANT SELECT ON ALL TABLES IN SCHEMA &lt;database_name&gt;.&lt;schema_name&gt; TO ROLE qualytics_read_role;\nGRANT SELECT ON ALL VIEWS IN SCHEMA &lt;database_name&gt;.&lt;schema_name&gt; TO ROLE qualytics_read_role;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA &lt;database_name&gt;.&lt;schema_name&gt; TO ROLE qualytics_read_role;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA &lt;database_name&gt;.&lt;schema_name&gt; TO ROLE qualytics_read_role;\nGRANT ROLE qualytics_read_role TO USER &lt;user_name&gt;;\n</code></pre>"},{"location":"add-datastores/snowflake/#enrichment-datastore-privileges-and-permissions","title":"Enrichment Datastore Privileges and Permissions","text":"<p>Create a new role called <code>qualytics_readwrite_role</code> and grant it privileges:</p> <pre><code>CREATE ROLE qualytics_readwrite_role;\nGRANT USAGE ON WAREHOUSE qualytics_wh TO ROLE qualytics_readwrite_role;\nGRANT USAGE, MODIFY ON DATABASE &lt;database_name&gt; TO ROLE qualytics_readwrite_role;\nGRANT USAGE, MODIFY ON SCHEMA &lt;database_name&gt;.&lt;qualytics_schema&gt; TO ROLE qualytics_readwrite_role;\nGRANT CREATE TABLE ON SCHEMA &lt;database_name&gt;.&lt;qualytics_schema&gt; TO ROLE qualytics_readwrite_role;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA &lt;database_name&gt;.&lt;qualytics_schema&gt; TO ROLE qualytics_readwrite_role;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA &lt;database_name&gt;.&lt;qualytics_schema&gt; TO ROLE qualytics_readwrite_role;\nGRANT SELECT ON ALL TABLES IN SCHEMA &lt;database_name&gt;.&lt;qualytics_schema&gt; TO ROLE qualytics_readwrite_role;\nGRANT SELECT ON ALL VIEWS IN SCHEMA &lt;database_name&gt;.&lt;qualytics_schema&gt; TO ROLE qualytics_readwrite_role;\nGRANT ROLE qualytics_readwrite_role TO USER &lt;user_name&gt;;\n</code></pre>"},{"location":"add-datastores/snowflake/#authentication-changes-in-snowflake","title":"Authentication Changes in Snowflake","text":"<p>Snowflake has announced a migration plan to phase out Basic authentication (username and password) for service accounts in favor of Key-Pair authentication. While basic authentication is still supported, organizations should begin planning their migration to ensure uninterrupted service.</p>"},{"location":"add-datastores/snowflake/#user-type-classification","title":"User Type Classification","text":"<p>Snowflake differentiates between user types based on their intended purpose:</p> User Type Purpose Current Authentication Support Human users (<code>TYPE=PERSON</code>) Interactive users accessing Snowflake Basic authentication supported Service users (<code>TYPE=SERVICE</code>) Applications and services (like Qualytics) Key-Pair authentication recommended Legacy service (<code>TYPE=LEGACY_SERVICE</code>) Temporary transition type Basic authentication (being phased out)"},{"location":"add-datastores/snowflake/#migration-timeline","title":"Migration Timeline","text":"<p>Snowflake's migration plan includes:</p> <ol> <li>Current Phase: Basic authentication still supported for service accounts</li> <li>Transition Phase: <code>LEGACY_SERVICE</code> user type available for organizations needing additional migration time</li> <li>Future Phase: Basic authentication will be fully deprecated for service users</li> </ol>"},{"location":"add-datastores/snowflake/#recommended-actions","title":"Recommended Actions","text":"<p>To prepare for this transition:</p> <ul> <li>New connections: Use Key-Pair authentication when creating new Snowflake datastores</li> <li>Existing connections: Plan migration from Basic to Key-Pair authentication</li> <li>Service accounts: Ensure proper user type classification (<code>TYPE=SERVICE</code>)</li> </ul>"},{"location":"add-datastores/snowflake/#additional-resources","title":"Additional Resources","text":"<p>For detailed information on the migration plan and implementation:</p> <ul> <li>Snowflake Security MFA Rollout (User Types &amp; Deprecation)</li> <li>Snowflake Key-Pair Authentication Guide</li> </ul> <p>Migration Recommendation</p> <p>While Basic authentication is currently supported, migrating to Key-Pair authentication ensures your Snowflake connections remain secure and future-proof as Snowflake implements their deprecation timeline.</p>"},{"location":"add-datastores/snowflake/#add-a-source-datastore","title":"Add a Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Snowflake is an example of a source datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the JDBC datastore enables the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore. (e.g., The specified name will appear on the datastore cards.) 2. Toggle Button Toggle ON to  create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select Snowflake from the dropdown list."},{"location":"add-datastores/snowflake/#option-i-create-a-source-datastore-with-a-new-connection","title":"Option I: Create a Source Datastore with a New Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Snowflake connector from the dropdown list and add the connection details.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Account (Required) Define the account identifier to be used for accessing the Snowflake. 2. Role (Required) Specify the user role that grants appropriate access and permissions. 3. Warehouse (Required) Provide the warehouse name that will be used for computing resources. 4 Authentication You can choose between Basic authentication or Keypair authentication for validating and securing the connection to your Snowflake instance.  Basic Authentication: This method uses a username and password combination for authentication. It is a straightforward method where the user's credentials are directly used to access Snowflake. <ul><li>Type: Select the authentication type from the dropdown menu.</li><li>User: Enter the username that Qualytics will use to connect to Snowflake.</li><li>Password: Enter the password associated with the specified user account.</li></ul> Keypair Authentication: This method uses a combination of a private key and a corresponding public key for authentication. This is a more secure method compared to basic authentication, as it involves asymmetric cryptography. <ul> <li>Type: Select \"Keypair\" from the dropdown menu.</li><li>User: Enter the username that Qualytics will use to connect to Snowflake.</li> <li>Private Key: Upload the private key file that will be used for authentication. This key is part of a public-private key pair used to securely authenticate the user.</li> <li>Private Key Password (Optional): Enter the password associated with the private key, if any. </li> </ul> 5. Database (Required) Specify the database name to be accessed. 6. Schema (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 8. Initiate Cataloging (Optional) Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p>"},{"location":"add-datastores/snowflake/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, Teams and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection  button to check and verify the source data connection. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Info</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/snowflake/#add-enrichment-datastore-connection","title":"Add Enrichment Datastore Connection","text":"<p>Once you have successfully tested and verified your source datastore connection, you have the option to add the enrichment datastore (recommended). The enrichment datastore is used to store the analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1 Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2 Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3 Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/snowflake/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window - Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>Once HashiCorp Vault is set up, use the ${key} format in the Connection form to reference a Vault secret.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 3: The configuration form, requesting credential details after selecting the enrichment datastore connector.</p> <p></p> REF. FIELDS ACTIONS 1. Account (Required) Define the account identifier to be used for accessing the Snowflake. 2. Role (Required) Specify the user role that grants appropriate access and permissions. 3. Warehouse (Required) Provide the warehouse name that will be used for computing resources. 4. Authentication You can choose between Basic authentication or Keypair authentication for validating and securing the connection to your Snowflake instance.  Basic Authentication: This method uses a username and password combination for authentication. It is a straightforward method where the user's credentials are directly used to access Snowflake. <ul><li>Type: Select the authentication type from the dropdown menu.</li><li>User: Enter the username that Qualytics will use to connect to Snowflake.</li><li>Password: Enter the password associated with the specified user account.</li></ul> Keypair Authentication: This method uses a combination of a private key and a corresponding public key for authentication. This is a more secure method compared to basic authentication, as it involves asymmetric cryptography. <ul> <li>Type: Select \"Keypair\" from the dropdown menu.</li><li>User: Enter the username that Qualytics will use to connect to Snowflake.</li> <li>Private Key: Upload the private key file that will be used for authentication. This key is part of a public-private key pair used to securely authenticate the user.</li> <li>Private Key Password (Optional): Enter the password associated with the private key, if any. </li> </ul> 5. Database (Required) Specify the database name to be accessed. 6. Schema (Required) Define the schema within the database that should be used. 7. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. <p>Step 4: Click on the Test Connection button to verify the enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the enrichment datastore has been successfully verified.</p> <p></p> <p>Step 5: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 6: Close the Success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/snowflake/#option-ii-use-an-existing-datastore","title":"Option II: Use an Existing Datastore","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2:  A modal window - Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files for metadata. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore. Example - All users are assigned to the Public team, which means that this enrichment datastore is accessible to all users.</p> </li> <li> <p>Host: This is the host domain of the Snowflake instance.</p> </li> <li> <p>Database: Refers to the specific database within the Snowflake environment. This database is a logical grouping of schemas. Each database belongs to a single Snowflake account.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/snowflake/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides detailed examples of API payloads to guide you through the process of creating and managing datastores using Qualytics API. Each example includes endpoint details, sample payloads, and instructions on how to replace placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/snowflake/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides sample payloads for creating a Snowflake datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create a Source Datastore with a new ConnectionCreate a Source Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"snowflake_database\",\n    \"schema\": \"snowflake_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"snowflake\",\n        \"host\": \"snowflake_host\",\n        \"username\": \"snowflake_username\",\n        \"password\": \"snowflake_password\",\n        \"passphrase\": \"key_passphrase\",\n        \"parameters\": {\n            \"role\": \"snowflake_read_role\",\n            \"warehouse\": \"qualytics_wh\",\n            \"authentication_type\": \"KEYPAIR\"\n        }\n    }\n}\n</code></pre> <p>Note</p> <p>If the <code>authentication_type</code> parameter is removed, <code>BASIC</code> authentication will be used by default.</p> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"snowflake_database\",\n    \"schema\": \"snowflake_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/snowflake/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":"<p>This section provides sample payloads for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint: <code>/api/datastores (post)</code></p> Create an Enrichment Datastore with a new ConnectionCreate an Enrichment Datastore with an existing Connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"snowflake_database\",\n    \"schema\": \"snowflake_schema\",\n    \"enrich_only\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"snowflake\",\n        \"host\": \"snowflake_host\",\n        \"username\": \"snowflake_username\",\n        \"password\": \"snowflake_password\",\n        \"parameters\": {\n            \"role\": \"snowflake_readwrite_role\",\n            \"warehouse\": \"qualytics_wh\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"snowflake_database\",\n    \"schema\": \"snowflake_schema\",\n    \"enrich_only\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/snowflake/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Use the provided endpoint to link an enrichment datastore to a source datastore:</p> <p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/synapse/","title":"Synapse","text":"<p>Adding and configuring a Synapse connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on adding Synapse as a source datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Synapse environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/synapse/#add-the-source-datastore","title":"Add the Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Synapse is an example of such a datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the Synapse datastore allows the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name Specify the name of the datastore (e.g., The specified name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection 3. Connector Select Synapse from the dropdown list."},{"location":"add-datastores/synapse/#option-i-create-a-datastore-with-a-new-connection","title":"Option I: Create a Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Synapse connector from the dropdown list and add connection details such as Secret Management, host, port, username, etc.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get Hostname from your Synapse account and add it to this field. 2. Port Specify the Port number. 3. User Enter the User ID to connect. 4. Password Enter the Password to connect to the database. 5. Database Specify the database name. 6. Schema Define the schema within the database that should be used. 7. Teams Select one or more teams from the dropdown to associate with the source datastore. 8. Initial Cataloging Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/synapse/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Use an existing connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to check and verify the source data connection. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.  </p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/synapse/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>After successfully testing and verifying your source datastore connection, you have the option to add an enrichment datastore (recommended). This datastore is used to store analyzed results, including any anomalies and additional metadata in tables. This setup provides comprehensive visibility into your data quality, enabling you to manage and improve it effectively.</p> <p>Warning</p> <p>Qualytics does not support the Synapse connector as an enrichment datastore, but you can point to a different enrichment datastore.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/synapse/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for Add New Connection Toggle ON to create a new enrichment datastore from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p>If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 5: Close the Success dialogue and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/synapse/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example - Marked as Public means that this datastore is accessible to all the users.  </li> <li>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment.  </li> <li>Database: Refers to the specific database within the enrichment datastore environment where the data is stored.  </li> <li>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/synapse/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"add-datastores/synapse/#creating-a-datastore","title":"Creating a Datastore","text":"<p>This section provides a sample payload for creating a datastore. Replace the placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/synapse/#endpoint-post","title":"Endpoint (Post)","text":"<p><code>/api/datastores</code> (post)</p> Creating a datastore with a new connectionCreating a datastore with an existing connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"synapse_database\",\n        \"schema\": \"synapse_schema\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"synapse\",\n            \"host\": \"synapse_host\",\n            \"port\": \"synapse_port\",\n            \"username\": \"synapse_username\",\n            \"password\": \"synapse_password\"\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"synapse_database\",\n        \"schema\": \"synapse_schema\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/synapse/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":""},{"location":"add-datastores/synapse/#endpoint-post_1","title":"Endpoint (Post)","text":"<p><code>/api/datastores</code> (post)</p> <p>This section provides a sample payload for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> Creating an enrichment datastore with a new connectionCreating an enrichment datastore with an existing connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"synapse_database\",\n        \"schema\": \"synapse_schema\",\n        \"enrich_only\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"synapse\",\n            \"host\": \"synapse_host\",\n            \"port\": \"synapse_port\",\n            \"username\": \"synapse_username\",\n            \"password\": \"synapse_password\"\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"synapse_database\",\n        \"schema\": \"synapse_schema\",\n        \"enrich_only\": true,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/synapse/#linking-datastore-to-an-enrichment-datastore-through-api","title":"Linking Datastore to an Enrichment Datastore through API","text":""},{"location":"add-datastores/synapse/#endpoint-patch","title":"Endpoint (Patch)","text":"<p><code>/api/datastores/{datastore-id}/enrichment/{enrichment-id}</code> (patch)</p>"},{"location":"add-datastores/teradata/","title":"Teradata","text":"<p>Adding and configuring a Teradata connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.  </p> <p>This documentation provides a step-by-step guide on adding Teradata as a source datastore in Qualytics. It covers the entire process from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Teradata environment is properly connected with Qualytics, unlocking the platform's potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/teradata/#add-the-source-datastore","title":"Add the Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Teradata is an example of such a datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the Teradata datastore allows the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window- Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name Specify the name of the datastore (e.g., the specified name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection 3. Connector Select Teradata from the dropdown list."},{"location":"add-datastores/teradata/#option-i-create-a-datastore-with-a-new-connection","title":"Option I: Create a Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Teradata connector from the dropdown list and add connection details such as Secret Management, host, port, username, etc.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get the Hostname from your Teradata account and add it to this field. 2. Port Specify the Port number. 3. User Enter the User ID to connect. 4. Password Enter the password to connect to the database. 5. Database Specify the database name. 6. Teams Select one or more teams from the dropdown to associate with this source datastore. 7. Initial Cataloging Check the checkbox to automatically perform a catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/teradata/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Use an existing connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit details such as Database, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to check and verify the source data connection. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/teradata/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>After successfully testing and verifying your source datastore connection, you have the option to add an enrichment datastore (recommended). This datastore is used to store analyzed results, including any anomalies and additional metadata in tables. This setup provides comprehensive visibility into your data quality, enabling you to manage and improve it effectively.</p> <p>Warning</p> <p>Qualytics does not support the Teradata connector as an enrichment datastore, but you can point to a different enrichment datastore.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window- Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1 Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2 Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3 Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/teradata/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Note</p> <p>Qualytics does not support Teradata as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using BigQuery as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4:  Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 5: Close the success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/teradata/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Note</p> <p>Qualytics does not support Teradata as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using BigQuery as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li> <p>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. For example, marked as Public means that this datastore is accessible to all the users.</p> </li> <li> <p>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment.</p> </li> <li> <p>Database: This refers to the specific database within the enrichment datastore environment where the data is stored.</p> </li> <li> <p>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</p> </li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/teradata/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"add-datastores/teradata/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides a sample payload for creating a datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint (Post): <code>/api/datastores (post)</code></p> Creating a source datastore with a new connectionCreating a datastore with an existing connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"schema\": \"schema_name\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"host\": \"teradata_host\",\n        \"port\": \"teradata_port\",\n        \"username\": \"teradata_user\",\n        \"password\": \"teradata_password\",\n        \"type\": \"teradata\"\n        }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"schema\": \"schema_name\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection_id\n}\n</code></pre>"},{"location":"add-datastores/teradata/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Endpoint Patch:</p> <p><code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/timescale-db/","title":"TimescaleDB","text":"<p>Adding and configuring a TimescaleDB connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on adding TimescaleDB as a source datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their TimescaleDB environment is properly connected with Qualytics, unlocking the platform\u2019s potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/timescale-db/#add-the-source-datastore","title":"Add the Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. TimescaleDB is an example of such a datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the TimescaleDB datastore allows the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore. (e.g., The specified name will appear on the datastore cards.) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection. 3. Connector Select TimescaleDB from the dropdown list."},{"location":"add-datastores/timescale-db/#option-i-create-a-datastore-with-a-new-connection","title":"Option I: Create a Datastore with a new Connection","text":"<p>If the toggle for Add New Connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the TimescaleDB connector from the dropdown list and add connection details such as Secrets Management, host, port, username, database, and schema.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get Hostname from your TimescaleDB account and add it to this field. 2. Port Specify the Port number. 3. User Enter the User ID to connect. 4. Password Enter the password to connect to the database. 5. Database Specify the database name. 6. Schema Define the schema within the database that should be used. 7. Teams Select one or more teams from the dropdown to associate with this source datastore. 8. Initial cataloging Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/timescale-db/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add New Connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Step 2: Click on the Test Connection button to check and verify the source data connection. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next  button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/timescale-db/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>After successfully testing and verifying your source datastore connection, you have the option to add an enrichment datastore (recommended). This datastore is used to store analyzed results, including any anomalies and additional metadata in tables. This setup provides full visibility into your data quality, enabling you to manage and improve it effectively.</p> <p>Warning</p> <p>Qualytics does not support the TimescaleDB connector as an enrichment datastore, but you can point to a different enrichment datastore.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1 Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2 Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3 Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/timescale-db/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add New Connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Enter a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment datastore from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Note</p> <p>Qualytics does not support TimescaleDB as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using Microsoft SQL Server as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 5: Close the Success dialogue and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p>"},{"location":"add-datastores/timescale-db/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p>Note</p> <p>Qualytics does not support Timescale as an enrichment datastore. Instead, you can select a different enrichment datastore for this purpose. For demonstration purposes, we are using Bank Enrichment as the enrichment datastore. You can use any other JDBC or DFS datastore of your choice for the enrichment datastore configuration.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example - Marked as Public means that this datastore is accessible to all the users.</li> <li>Host: This is the server address where the enrichment datastore instance is hosted. It is the endpoint used to connect to the enrichment datastore environment.</li> <li>Database: Refers to the specific database within the enrichment datastore environment where the data is stored.</li> <li>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal window will display and a success flash message stating that your data has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/timescale-db/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"add-datastores/timescale-db/#creating-a-source-datastore","title":"Creating a Source Datastore","text":"<p>This section provides a sample payload for creating a TimescaleDB datastore. Replace the placeholder values with actual data relevant to your setup.</p> <p>Endpoint (Post): <code>/api/datastores (post)</code></p> Creating a source datastore with a new connectionCreating a source datastore with an existing connection <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"timescale_database\",\n    \"schema\": \"timescale_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection\": {\n        \"name\": \"your_connection_name\",\n        \"type\": \"timescale\",\n        \"host\": \"timescale_host\",\n        \"port\": \"timescale_port\",\n        \"username\": \"timescale_username\",\n        \"password\": \"timescale_password\"\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"your_datastore_name\",\n    \"teams\": [\"Public\"],\n    \"database\": \"timescale_database\",\n    \"schema\": \"timescale_schema\",\n    \"enrich_only\": false,\n    \"trigger_catalog\": true,\n    \"connection_id\": connection-id\n}\n</code></pre>"},{"location":"add-datastores/timescale-db/#link-an-enrichment-datastore-to-a-source-datastore","title":"Link an Enrichment Datastore to a Source Datastore","text":"<p>Endpoint Details: <code>/api/datastores/{datastore-id}/enrichment/{enrichment-id} (patch)</code></p>"},{"location":"add-datastores/trino/","title":"Trino","text":"<p>Adding and configuring a Trino connection within Qualytics empowers the platform to build a symbolic link with your schema to perform operations like data discovery, visualization, reporting, cataloging, profiling, scanning, anomaly surveillance, and more.</p> <p>This documentation provides a step-by-step guide on adding Trino as a source datastore in Qualytics. It covers the entire process, from initial connection setup to testing and finalizing the configuration.</p> <p>By following these instructions, enterprises can ensure their Trino environment is properly connected with Qualytics, unlocking the platform\u2019s potential to help you proactively manage your full data quality lifecycle.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"add-datastores/trino/#add-the-source-datastore","title":"Add the Source Datastore","text":"<p>A source datastore is a storage location used to connect to and access data from external sources. Trino is an example of such a datastore, specifically a type of JDBC datastore that supports connectivity through the JDBC API. Configuring the Trino datastore allows the Qualytics platform to access and perform operations on the data, thereby generating valuable insights.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELD ACTIONS 1. Name Specify the name of the datastore (e.g., the specified name will appear on the datastore cards) 2. Toggle Button Toggle ON to create a new source datastore from scratch, or toggle OFF to reuse credentials from an existing connection 3. Connector Select Trino from the dropdown list."},{"location":"add-datastores/trino/#option-i-create-a-datastore-with-a-new-connection","title":"Option I: Create a Datastore with a new Connection","text":"<p>If the toggle for Add new connection is turned on, then this will prompt you to add and configure the source datastore from scratch without using existing connection details.</p> <p>Step 1: Select the Trino connector from the dropdown list and add connection details such as Secret Management, host, port, username, etc.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.</p> REF. FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form will expand, requesting credential details before establishing the connection.</p> <p></p> REF. FIELDS ACTIONS 1. Host Get Hostname from your Trino account and add it to this field. 2. Port Specify the Port number. 3. User Enter the User ID to connect. 4. Password Enter the Password to connect to the database. 5. Catalog Add a Catalog to fetch data structures and metadata from Trino. 6. Schema Define the schema within the database that should be used. 7. Teams Select one or more teams from the dropdown to associate with this source datastore. 8. Initial Cataloging Tick the checkbox to automatically perform catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p>"},{"location":"add-datastores/trino/#option-ii-use-an-existing-connection","title":"Option II: Use an Existing Connection","text":"<p>If the toggle for Add new connection is turned off, then this will prompt you to configure the source datastore using the existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p></p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Teams, and Initiate Cataloging.</p> <p>Step 2: Click on the Test Connection button to check and verify the source datastore connection. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Note</p> <p>Clicking on the Finish button will create the source datastore and bypass the enrichment datastore configuration step.</p> <p>Tip</p> <p>It is recommended to click on the Next button, which will take you to the enrichment datastore configuration page.</p>"},{"location":"add-datastores/trino/#add-enrichment-datastore","title":"Add Enrichment Datastore","text":"<p>After successfully testing and verifying your source datastore connection, you have the option to add an enrichment datastore (recommended). This datastore is used to store analyzed results, including any anomalies and additional metadata in tables. This setup provides comprehensive visibility into your data quality, helping you manage and improve it effectively.</p> <p>Step 1: Whether you have added a source datastore by creating a new datastore connection or using an existing connection, click on the Next button to start adding the Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window - Link Enrichment Datastore will appear, providing you with the options to configure an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix (Required) Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list."},{"location":"add-datastores/trino/#option-i-create-an-enrichment-datastore-with-a-new-connection","title":"Option I: Create an Enrichment Datastore with a new Connection","text":"<p>If the toggle Add new connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using an existing enrichment datastore and its connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for add new connection Toggle ON to create a new enrichment datastore from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p></p> <p>Step 3: Click on the Test Connection button to verify the selected enrichment datastore connection. If the connection is verified, a flash message will indicate that the connection with the datastore has been successfully verified.</p> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Step 5: Close the success dialog and the page will automatically redirect you to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/trino/#option-ii-use-an-existing-connection_1","title":"Option II: Use an Existing Connection","text":"<p>If the Use enrichment datastore option is selected from the caret button, you will be prompted to configure the datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. <p>Step 3: After selecting an existing enrichment datastore connection, you will view the following details related to the selected enrichment:</p> <ul> <li>Teams: The team associated with managing the enrichment datastore is based on the role of public or private. Example - Marked as Public means that this datastore is accessible to all the users.  </li> <li>Host: This is the server address where the Trino instance is hosted. It is the endpoint used to connect to the Trino environment.  </li> <li>Database: Refers to the specific database within the Trino environment where the data is stored.  </li> <li>Schema: The schema used in the enrichment datastore. The schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.</li> </ul> <p></p> <p>Step 4: Click on the Finish button to complete the configuration process for the existing enrichment datastore.</p> <p></p> <p>When the configuration process is finished, a modal will display a success message indicating that your datastore has been successfully added.</p> <p>Close the success message and you will be automatically redirected to the Source Datastore Details page where you can perform data operations on your configured source datastore.</p> <p></p>"},{"location":"add-datastores/trino/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"add-datastores/trino/#creating-a-datastore","title":"Creating a Datastore","text":"<p>This section provides a sample payload for creating a datastore. Replace the placeholder values with actual data relevant to your setup.</p>"},{"location":"add-datastores/trino/#endpoint-post","title":"Endpoint (Post)","text":"<p><code>/api/datastores</code> (post)</p> Creating a datastore with a new connectionCreating a datastore with an existing connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"trino_database\",\n        \"schema\": \"trino_schema\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"trino\",\n            \"host\": \"trino_host\",\n            \"port\": \"trino_port\",\n            \"username\": \"trino_username\",\n            \"password\": \"trino_password\",\n            \"parameters\":{\n                \"ssl_truststore\":\"truststore.jks\"\n            }\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"trino_database\",\n        \"enrich_only\": false,\n        \"trigger_catalog\": true,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/trino/#creating-an-enrichment-datastore","title":"Creating an Enrichment Datastore","text":""},{"location":"add-datastores/trino/#endpoint-post_1","title":"Endpoint (Post)","text":"<p><code>/api/datastores</code> (post)</p> <p>This section provides a sample payload for creating an enrichment datastore. Replace the placeholder values with actual data relevant to your setup.</p> Creating an enrichment datastore with a new connectionCreating an enrichment datastore with an existing connection <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"trino_database\",\n        \"schema\": \"trino_schema\",\n        \"enrich_only\": true,\n        \"connection\": {\n            \"name\": \"your_connection_name\",\n            \"type\": \"trino\",\n            \"host\": \"trino_host\",\n            \"port\": \"trino_port\",\n            \"username\": \"trino_username\",\n            \"password\": \"trino_password\",\n            \"parameters\":{\n                \"ssl_truststore\":\"truststore.jks\"\n            }\n        }\n    }\n</code></pre> <pre><code>    {\n        \"name\": \"your_datastore_name\",\n        \"teams\": [\"Public\"],\n        \"database\": \"trino_database\",\n        \"schema\": \"trino_schema\",\n        \"enrich_only\": true,\n        \"connection_id\": connection-id\n    }\n</code></pre>"},{"location":"add-datastores/trino/#linking-datastore-to-an-enrichment-datastore-through-api","title":"Linking Datastore to an Enrichment Datastore through API","text":""},{"location":"add-datastores/trino/#endpoint-patch","title":"Endpoint (Patch)","text":"<p><code>/api/datastores/{datastore-id}/enrichment/{enrichment-id}</code> (patch)</p>"},{"location":"anomalies/acknowledge-anomalies/","title":"Acknowledge Anomalies","text":"<p>By acknowledging anomalies, you indicate that they have been reviewed or recognized. This can be done either individually or in bulk, depending on your workflow. Acknowledging anomalies helps you keep track of issues that have been addressed, even if further action is still required.</p> <p>Warning</p> <p>Once an anomaly is acknowledged, it remains acknowledged and never reverts to the active state.</p>"},{"location":"anomalies/acknowledge-anomalies/#acknowledge-specific-anomaly","title":"Acknowledge Specific Anomaly","text":"<p>You can acknowledge individual anomalies either directly or through the action menu, giving you precise control over each anomaly's status.</p> <p>Step 1: Log in to your Qualytics account and select the datastore from the left menu on which you want to manage your anomalies.</p> <p></p> <p>Step 2: Click on the \u201cAnomalies\u201d from the Navigation Tab.</p> <p></p> <p>1. Acknowledge Directly</p> <p>Step 1: Locate the active anomaly you want to acknowledge.</p> <p></p> <p>Step 2: Click on the vertical ellipsis (\u22ee) located on the right side of the anomaly and select \u201cAcknowledge\u201d from the dropdown menu.</p> <p></p> <p>After clicking on the Acknowledge button your anomaly is successfully moved to the acknowledged state and a confirmation message appears on the screen.</p> <p>2. Acknowledge via Action Menu</p> <p>Step 1: Click on the active anomaly from the list of available anomalies that you want to acknowledge.</p> <p></p> <p>Step 2: You will be directed to the anomaly details page. Click on the Acknowledge button located at the top-right corner of the interface.</p> <p></p> <p>After clicking on the Acknowledge button your anomaly is successfully moved to the acknowledged state and a confirmation message appears on the screen.</p>"},{"location":"anomalies/acknowledge-anomalies/#acknowledge-anomalies-in-bulk","title":"Acknowledge Anomalies in Bulk","text":"<p>By acknowledging anomalies in bulk, you can quickly mark multiple anomalies as reviewed at once, saving time and ensuring that all relevant issues are addressed simultaneously.</p> <p>Step 1: Hover over the active anomalies and click on the checkbox to select multiple anomalies.</p> <p></p> <p>When multiple anomalies are selected, an action toolbar appears, displaying the total number of selected anomalies along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click on the vertical ellipsis (\u22ee) and choose \"Acknowledge\" from the dropdown menu to acknowledge the selected anomalies.</p> <p></p> <p>A modal window titled \u201cAcknowledge Anomalies\u201d will appear, confirming that this action acknowledges the anomalies as a legitimate data quality concern.</p> <p>You also have the option to leave a comment in the provided field to provide additional context or details.</p> <p></p> <p>Step 3: Click on the Acknowledge button to acknowledge the anomalies.</p> <p></p> <p>After clicking on the Acknowledge button your anomalies are successfully moved to the acknowledged state and a confirmation message appears on the screen.</p>"},{"location":"anomalies/anomalies/","title":"Anomalies","text":"<p>Anomalies in Qualytics represent data points that deviate from expected patterns or violate defined quality rules, often highlighting issues such as missing values, structural inconsistencies, or incorrect data. These anomalies are detected during scan operations through system-inferred or user-authored checks. </p>"},{"location":"anomalies/anomalies/#anomaly-types","title":"Anomaly Types","text":"<p>Qualytics classifies anomalies into two types: Record Anomalies and Shape Anomalies. Record anomalies flag rows with data issues like missing or invalid values, while shape anomalies detect structural problems such as missing columns or schema changes. Together, they ensure thorough data quality coverage at both the value and structure levels.</p> <p>Note</p> <p>For more information, please refer to the Anomaly Types Documentation.</p>"},{"location":"anomalies/anomalies/#anomaly-detection-process","title":"Anomaly Detection Process","text":"<p>The anomaly detection process in Qualytics ensures data quality by identifying deviations from expected patterns through a structured workflow. It starts with configuring datastores, cataloging metadata, and profiling data to understand its structure. Users then apply quality checks\u2014either authored or inferred\u2014during Scan operations. Any failures are flagged as anomalies, enabling timely detection and resolution of data issues to maintain overall data integrity.</p> <p>Note</p> <p>For more information, please refer to the Anomaly Detection Process Documentation.</p>"},{"location":"anomalies/anomaly-detection/","title":"Anomaly Detection Process","text":"<p>The anomaly detection process in Qualytics ensures data quality by identifying deviations from expected patterns through a structured workflow. It starts with configuring datastores, cataloging metadata, and profiling data to understand its structure. Users then apply quality checks\u2014either authored or inferred\u2014during Scan operations. Any failures are flagged as anomalies, enabling timely detection and resolution of data issues to maintain overall data integrity.</p> <p>Let\u2019s get started \ud83d\ude80</p> <p>1. Create a Datastore and Connection</p> <p>By setting up a datastore and establishing a connection to your data source (database or file system), you create a robust foundation for effective data management and analysis in Qualytics. This setup enables you to access, manipulate, and utilize your data efficiently, paving the way for advanced data quality checks, profiling, scanning, anomaly surveillance, and other analytics tasks.</p> <p>Note</p> <p>For more information, please refer to the Configuring Source Datastores documentation.</p> <p>2. Catalog Operation</p> <p>The Catalog operation involves systematically collecting data structures along with their corresponding metadata. This process also includes a thorough analysis of the existing metadata within the datastore. This ensures a solid foundation for the subsequent Profile and Scan operations.</p> <p>Note</p> <p>For more information, please refer to the Catalog Operation Documentation.</p> <p>3. Profile Operation</p> <p>The Profile operation enables training of the collected data structures and their associated metadata values. This is crucial for gathering comprehensive aggregated statistics on the selected data, providing deeper insights, and preparing the data for quality assessment.</p> <p>Note</p> <p>For more information, please refer to the documentation Profile Operation.</p> <p>4. Create Authored Checks</p> <p>Authored Checks are manually created data quality checks in Qualytics, defined by users either through the user interface (UI) or via API. These checks encapsulate a specific data quality check, along with additional context such as associated notifications, tags, filters, and tolerances. Authored checks can range from simple, template-based checks to more complex rules implemented through SQL or user-defined functions (UDFs) in Scala. By allowing users to define precise criteria for data quality, authored checks enable detailed monitoring and validation of data within the datastore, ensuring that it meets the specified standards and requirements.</p> <p>Note</p> <p>For more information, please refer to the documentation Authored Checks. </p> <p>5. Scan Operation</p> <p>The Scan operation asserts rigorous quality checks to identify any anomalies within the data. This step ensures data integrity and reliability by recording the analyzed data in your configured enrichment datastore, facilitating continuous data quality improvement.</p> <p>Note</p> <p>For more information, please refer to the documentation Scan Operation. </p> <p>6. Anomaly Analysis</p> <p>An Anomaly is a data record or column that fails a data quality check during a Scan Operation. These anomalies are identified through both Inferred and Authored Checks and are grouped together to highlight data quality issues. This process ensures that any deviations from expected data quality standards are promptly identified and addressed.</p> <p>Note</p> <p>For more information, please refer to the documentation Anomalies Overview.</p>"},{"location":"anomalies/anomaly-fingerprints/","title":"Anomaly Fingerprints","text":"<p>Anomaly fingerprints are unique identifiers generated for each detected anomaly to help the system recognize and manage duplicates effectively. By comparing these fingerprints, Qualytics can determine whether a newly detected anomaly matches a previously identified one. This mechanism reduces redundancy, ensures consistency in anomaly tracking, and simplifies decision-making during data quality operations.</p> <p>Info</p> <p>Fingerprinting works only when Incremental Row tracking is enabled.</p>"},{"location":"anomalies/anomaly-fingerprints/#duplicate-handling-configuration","title":"Duplicate Handling Configuration","text":"<p>Once anomalies are fingerprinted, Qualytics can use these unique identifiers to determine whether a newly detected anomaly matches any existing one. This fingerprint-based recognition powers the duplicate handling configuration during scan setup.</p> <p>When configuring a scan operation, you can define how the system should respond to anomalies that share fingerprints with previously detected ones:</p> <ul> <li> <p>Duplicate Status: You can instruct the system to automatically mark newly detected anomalies as \u201cDuplicate\u201d if their fingerprints match those of any open anomalies. These duplicates are then archived, ensuring focus remains on the original issue without creating redundant records.</p> </li> <li> <p>Re-opening Option: If a new anomaly matches an archived one, you can configure the system to automatically re-open the earlier anomaly. This ensures that reoccurring issues are not overlooked simply because they were resolved or dismissed in a prior scan.</p> </li> </ul>"},{"location":"anomalies/anomaly-fingerprints/#fingerprinting-criteria","title":"Fingerprinting Criteria","text":"<p>To determine whether anomalies are identical, Qualytics generates unique fingerprints based on specific criteria. These criteria differ depending on the type of anomaly being evaluated. This approach ensures that anomalies are only considered duplicates when they are truly the same in both nature and context.</p>"},{"location":"anomalies/anomaly-fingerprints/#record-anomalies","title":"Record Anomalies","text":"<p>For record-level anomalies, fingerprinting is based on the specific check that identified the issue and the complete source data of the anomalous record. This ensures that every unique row is evaluated precisely:</p> <ul> <li> <p>Identifying check: The check responsible for detecting the anomaly (e.g., a null value or out-of-range check).</p> </li> <li> <p>Source record data: All field values within the affected row.</p> </li> </ul> <p>Note</p> <p>Identical records across multiple scans will generate the same fingerprint and therefore be flagged as duplicates.</p>"},{"location":"anomalies/anomaly-fingerprints/#shape-anomalies","title":"Shape Anomalies","text":"<p>For shape anomalies\u2014which refer to patterns or distributions of data rather than individual records, the fingerprint is derived from a broader set of attributes:</p> <ul> <li> <p>Identifying check(s): The rule(s) that triggered the anomaly at the dataset level.</p> </li> <li> <p>Failure percentage: The proportion of records that failed the check(s) within the scanned batch.</p> </li> <li> <p>Maximum incremental identifier: The highest value of a designated incremental field (e.g., timestamp, ID) in the scanned dataset.</p> </li> </ul> <p>Tip</p> <p>Shape anomalies can only be fingerprinted if the data asset includes an incremental identifier. This field anchors the fingerprint to a specific range of data, ensuring accurate comparisons across different scans.</p> <p>This fingerprinting mechanism ensures consistent anomaly tracking by minimizing false duplicates and keeping historical issues relevant when they reoccur.</p>"},{"location":"anomalies/anomaly-fingerprints/#use-case-handling-daily-truncate-and-reload-tables","title":"Use Case: Handling Daily Truncate-and-Reload Tables","text":""},{"location":"anomalies/anomaly-fingerprints/#scenario","title":"Scenario","text":"<p>Many data pipelines use staging tables that follow a truncate-and-reload pattern daily. These tables present a unique challenge:</p> <ul> <li>No last update timestamp for incremental strategy  </li> <li>Table is completely truncated and reloaded each day  </li> <li>Same data anomalies appear repeatedly across scans  </li> <li>Standard incremental detection cannot identify \"already seen\" records  </li> </ul>"},{"location":"anomalies/anomaly-fingerprints/#problem","title":"Problem","text":"<p>Without fingerprinting, each daily scan treats truncated-and-reloaded data as entirely new:</p> <ul> <li>Day 1: Scan identifies 127 anomalies \u2192 Team acknowledges all 127  </li> <li>Day 2: Table truncated, data reloaded \u2192 Same 127 anomalies flagged as \"new\"  </li> <li>Day 3: Process repeats \u2192 Team faces anomaly fatigue from duplicate alerts  </li> </ul> <p>The lack of a persistent identifier means Qualytics cannot distinguish between truly new anomalies and recurring issues from reloaded data.</p>"},{"location":"anomalies/anomaly-fingerprints/#solution","title":"Solution","text":"<p>To handle recurring anomalies in truncate-and-reload tables, configure your scan to use fingerprint-based duplicate handling.</p> <p>Follow the steps in the scan operation configuration to reach the correct settings. Then, under Step 8 \u2192 Scan Settings, open the anomaly options section and enable both duplicate-handling options:</p> <ul> <li>Archive Duplicate Anomalies: When the same 127 anomalies appear again after the table reload, Qualytics recognizes their fingerprints and automatically marks them as duplicates rather than new anomalies.  </li> <li>Reactivate Recurring Anomalies: If an anomaly was previously archived or resolved but reappears in subsequent scans, Qualytics reactivates the original anomaly record, maintaining full historical context.  </li> </ul>"},{"location":"anomalies/anomaly-fingerprints/#benefits","title":"Benefits","text":"<ul> <li>Eliminates daily re-acknowledgment of the same known issues  </li> <li>Maintains clean anomaly counts reflecting only truly new problems  </li> <li>Preserves audit trail through anomaly reactivation  </li> <li>Reduces alert fatigue while ensuring genuine recurrences are tracked  </li> </ul>"},{"location":"anomalies/anomaly-fingerprints/#configuration","title":"Configuration","text":"<p>Enable these settings in Scan Settings of your Scan Operation:  </p> <ul> <li>Archive Duplicate Anomalies </li> <li>Reactivate Recurring Anomalies </li> </ul> <p>Set an appropriate Anomaly Rollup Threshold based on your data volume and tolerance for grouped anomalies.</p>"},{"location":"anomalies/anomaly-insights/","title":"Anomaly Insights","text":"<p>Anomaly Insights provides key insights into a specific data anomaly, including its status, anomalous record count, failed checks, and weight. It also shows when the anomaly was detected, the triggering scan, and the related datastore, table, and location. This view helps users quickly understand the scope and source of the anomaly for easier investigation and resolution.</p> <p>Let\u2019s get started \ud83d\ude80</p> <p>Step 1: Click on the anomaly that you want to see the details of.</p> <p></p> <p>You will be navigated to the details section, where you can view the Summary, Failed Checks, Source Records and Activity information.</p> <p></p>"},{"location":"anomalies/anomaly-insights/#description","title":"Description","text":"<p>The Description section displays a detailed, business-friendly explanation of the anomaly. When an anomaly is detected during a scan operation, the system automatically generates a description based on the quality check that failed, including relevant context such as filter conditions, thresholds, and the nature of the data quality issue.</p> <p>Users with Editor permission or higher can edit the description to add additional context, clarify the business impact, or document investigation findings. This makes it easier for team members to understand and address data quality issues.</p>"},{"location":"anomalies/anomaly-insights/#editing-the-description","title":"Editing the Description","text":"<p>To edit an anomaly's description:</p> <p>Step 1: Open the Anomaly Details modal or navigate to the Anomaly Overview page.</p> <p>Step 2: Locate the Description section. If the anomaly is in an editable state (not archived), you will see a pencil icon (\u270f\ufe0f) next to the \"Description\" label.</p> <p>Step 3: Click the pencil icon to enter edit mode. The description text will become editable in a text area.</p> <p>Step 4: Make your changes to the description text.</p> <p>Step 5: Click Save to persist your changes, or click Cancel to discard them and revert to the original description.</p> <p>Note</p> <ul> <li>Only users with the Editor role (or higher) on the respective datastore can edit the description</li> <li>Archived anomalies cannot be edited\u2014restore the anomaly first if you need to modify the description</li> <li>All changes to the description are tracked in the Activity section, maintaining a complete audit trail of modifications</li> </ul>"},{"location":"anomalies/anomaly-insights/#summary-section","title":"Summary Section","text":"<p>The Summary section provides a quick overview of the anomaly's key attributes. It includes the anomaly's status, total anomalous records, failed checks, weight, detection time, scan information, and the corresponding datastore and table. This section helps users quickly understand where the anomaly occurred and its potential impact.</p> <p></p> No. Field Description 1 Status and Type Shows the current state and category of the anomaly. In this case, the anomaly is Active and of type Shape, indicating it relates to the structure or distribution of the data. 2 Anomalous Records Indicates the total number of records affected by the anomaly. Here, 102 records were identified as anomalous. 3 Failed Check Displays the number of data quality checks that were violated and triggered this anomaly. In this instance, 1 check failed. 4 Weight Represents the significance or impact of the anomaly. A higher weight value implies a more critical issue. This anomaly has a weight of 8. 5 Detected Shows how long ago the anomaly was first detected. When you hover over the time the anomaly was detected, a pop-up appears displaying the complete date and time. 6 Scan Indicates the scan operation that detected the anomaly. Scan ID #21379 is shown here, and it was an incremental scan. When you click on the expand icon, you will be directed to the Scan Results page where you can view the specific scan that detected the anomaly. 7 Source Datastore Identifies the dataset that contains the anomaly. This anomaly occurred in the \"Qualytics Databricks POC\" datastore. Clicking the expand icon opens a detailed view and navigates to the dataset\u2019s page for more information about the source datastore. 8 Table Points to the specific table involved in the anomaly. The affected table is raw_order. Clicking on the expand icon navigates to the table\u2019s page, providing more in-depth information about the table structure and contents. 9 Location Displays the full path of the table in the datastore. This helps users trace the exact location of the anomaly within the data pipeline. You can click on the copy icon to copy the full location path of the table where the anomaly was detected. 10 Tags Highlights the severity or categorization of the anomaly. The tag High indicates a high-priority issue. You can add or remove tags from the anomaly by clicking on the tag badge. <p></p>"},{"location":"anomalies/anomaly-insights/#failed-checks","title":"Failed Checks","text":"<p>The Failed Checks section lists the data quality checks that were violated and subsequently triggered the anomaly. Each listed item displays the check ID, type of violation, and a summarized description of the failure condition.</p> <p></p> <p>Click on a failed check to view the corresponding quality check information.</p> <p></p> <p>A right-side panel will open, allowing you to view the details without navigating to a different page.</p> <p></p>"},{"location":"anomalies/anomaly-insights/#source-records","title":"Source Records","text":"<p>The Source Records section displays all the data and fields related to the detected anomaly from the dataset. It is an Enrichment Datastore that is used to store the analyzed results, including any anomalies and additional metadata in files; therefore, it is recommended to add/link an enrichment datastore with your connected source datastore.</p> <p></p> <p>For more information on Source Records, please refer to the Source Records section in the documentation.</p>"},{"location":"anomalies/anomaly-insights/#activity-section","title":"Activity Section","text":"<p>The Activity section provides a complete timeline of actions and events related to the anomaly. It helps users track how the anomaly has been handled and by whom, ensuring better collaboration and accountability.</p> <p></p> <p>Users can leave comments to discuss the issue, add context, or communicate decisions. All comments are timestamped and attributed to the respective user.</p> <p>Note</p> <p>Users can\u2019t add, edit, or delete comments in the Activity section when an anomaly is archived (Duplicated, Invalid, or Resolved). Restore the anomaly to make updates, then revert its status if needed.</p> <p></p>"},{"location":"anomalies/anomaly-status/","title":"Anomaly Status","text":"<p>Anomaly status provides a structured way to track the lifecycle of data quality issues\u2014from detection to resolution. Each anomaly is assigned a status that indicates its current state, helping teams prioritize actions and maintain oversight. These statuses are divided into two main categories: Open, for anomalies that still need attention, and Archived, for those that have been resolved, dismissed, or categorized for reference.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"anomalies/anomaly-status/#open-anomalies","title":"Open Anomalies","text":"<p>Open anomalies are data quality issues that have been detected but not yet resolved or archived. This category is divided into three sub-statuses that help track the progress and handling of each anomaly.</p> <p></p> <p>1 Active: By clicking on the Active button, the user can see anomalies that are currently unresolved and have not been acknowledged, archived, or resolved. It may require immediate attention.</p> <p>2 Acknowledged: By clicking on the Acknowledged button, the user can see anomalies that has been reviewed and marked as acknowledged, though it may still need further action.</p> <p>3 All: By clicking on the All button, the user can view all open anomalies, including those marked as Active and Acknowledged, providing a complete view of ongoing issues.</p>"},{"location":"anomalies/anomaly-status/#archived-anomalies","title":"Archived Anomalies","text":"<p>Archived anomalies are issues that have already been reviewed and moved out of the active monitoring flow. These anomalies are categorized based on how they were resolved or classified, helping maintain a clear historical record without cluttering ongoing monitoring efforts.</p> <p></p> <p>1 Resolved: This indicates that the anomaly was a legitimate data quality concern and has been addressed.</p> <p>2 Duplicate: This indicates that the anomaly is a duplicate of an existing record and has already been addressed.</p> <p>Info</p> <p>The main purpose of marking an anomaly as Duplicate is to support fingerprinting. If an anomaly is set as Duplicate without referencing the original anomaly, Qualytics cannot determine which one is the true original, which breaks fingerprinting.</p> <p>Recommended approach: Set the anomaly as Discarded instead and include the original anomaly ID or a meaningful comment. This keeps the fingerprinting logic accurate.</p> <p>For more information refer to the Anomaly Fingerprint Documentation</p> <p>3 Invalid: This indicates that the anomaly is not a legitimate data quality concern and does not require further action.</p> <p>4 Discarded: This indicates that the anomaly is no longer being reviewed or considered relevant. It helps remove outdated or unnecessary anomalies from the active list without marking them as invalid or resolved.</p> <p>5 All: Displays all archived anomalies, including those marked as Resolved, Duplicate, and Invalid, giving a comprehensive view of all past issues.</p> <p>Note</p> <p>For more information, refer to the Archived Anomalies Documentation.</p>"},{"location":"anomalies/anomaly-types/","title":"Anomaly Types","text":"<p>Anomalies in Qualytics are classified into two primary types, Record Anomalies and Shape Anomalies, each targeting different aspects of data integrity. Record anomalies flag individual rows that fail specific quality checks, such as missing or invalid values. Shape anomalies, on the other hand, detect structural issues in the dataset, like missing columns or schema mismatches. Together, these types provide a comprehensive approach to identifying both value-level and schema-level data quality issues.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"anomalies/anomaly-types/#record-anomaly","title":"Record Anomaly","text":"<p>A record anomaly identifies a single record (row) as anomalous and provides specific details regarding why it is considered anomalous. The simplest form of a record anomaly is a row that lacks an expected value for a field.</p>"},{"location":"anomalies/anomaly-types/#example-use-case","title":"Example Use Case","text":"<p>Scenario</p> <p>We have an Employee dataset used for payroll.</p> <p>Rules:</p> <ul> <li>Every employee must have a Salary greater than 40,000.  </li> <li>The dataset must contain these four columns: <code>id</code>, <code>name</code>, <code>age</code>, <code>salary</code>.  </li> <li>The <code>name</code> must follow the <code>\"First Last\"</code> format.</li> </ul> <p>Rule Checked: Salary &gt; 40,000</p>"},{"location":"anomalies/anomaly-types/#input-table","title":"Input Table","text":"id name age salary 1 John Doe 28 50,000 2 Jane Smith 35 75,000 3 Bob Johnson 22 30,000"},{"location":"anomalies/anomaly-types/#detection-result-record-anomaly","title":"Detection Result (Record Anomaly)","text":"id name age salary anomaly_reason 3 Bob Johnson 22 30,000 Salary is less than the required 40,000"},{"location":"anomalies/anomaly-types/#why-this-is-a-record-anomaly","title":"Why this is a Record Anomaly:","text":"<p>The table structure is correct. Only one row\u2019s value violates the rule.</p>"},{"location":"anomalies/anomaly-types/#shape-anomaly","title":"Shape Anomaly","text":"<p>A shape anomaly identifies an anomalous structure within the analyzed data. The simplest shape anomaly is a dataset that doesn't match the expected schema because it lacks one or more fields. Some shape anomalies only apply to a subset of the data being analyzed and can therefore produce a count of the number of rows that reflect the anomalous concern. Where that is possible, the shape anomaly's anomalous_record_count is populated.</p> <p>Note</p> <p>Sometimes, shape anomalies only affect a subset of the dataset. This means that only certain rows exhibit the structural issue, rather than the entire dataset.</p>"},{"location":"anomalies/anomaly-types/#example-use-case_1","title":"Example Use Case","text":"<p>Scenario</p> <p>We have a Sales Orders dataset.</p> <p>Rules:</p> <ul> <li>Required columns: <code>order_id</code>, <code>customer_id</code>, <code>order_date</code>, <code>total_amount</code>.  </li> <li><code>order_date</code> must be in YYYY-MM-DD format.</li> </ul>"},{"location":"anomalies/anomaly-types/#input-table_1","title":"Input Table","text":"order_id customer_id order_date 101 C001 2025-08-10 102 C002 08/11/2025 103 C003 2025-08-12"},{"location":"anomalies/anomaly-types/#detection-result-shape-anomalies","title":"Detection Result (Shape Anomalies)","text":"order_id customer_id order_date total_amount anomaly_reason 101 C001 2025-08-10 \u2013 Missing total_amount column 102 C002 08/11/2025 \u2013 Missing total_amount column; Date format incorrect 103 C003 2025-08-12 \u2013 Missing total_amount column"},{"location":"anomalies/anomaly-types/#why-this-is-a-shape-anomaly","title":"Why this is a Shape Anomaly:","text":"<ul> <li>A required column (<code>total_amount</code>) is completely missing from the structure.  </li> <li>A field format (<code>order_date</code> in row 102) does not match the required YYYY-MM-DD pattern.  </li> <li>The problem is with the shape/structure of the dataset, not just a wrong value.</li> </ul> <p>Note</p> <p>When a shape anomaly affects only a portion of the dataset, Qualytics can count the number of rows that have the structural problem. This count is stored in the anomalous_record_count field, providing a clear measure of how widespread the issue is within the dataset. Example: Imagine a dataset that is supposed to have columns for id, name, age, and salary. If some rows are missing the salary column, this would be flagged as a shape anomaly. If this issue only affects 50 out of 1,000 rows, the anomalous_record_count would be 50, indicating that 50 rows have a structural issue. </p>"},{"location":"anomalies/archive-anomalies/","title":"Archive Anomalies","text":"<p>By archiving anomalies, you move them to an inactive state, while still keeping them available for future reference or analysis. Archiving helps keep your active anomaly list clean without permanently deleting the records.</p>"},{"location":"anomalies/archive-anomalies/#archive-specific-anomalies","title":"Archive Specific Anomalies","text":"<p>You can archive individual anomalies either directly or through the action menu.</p>"},{"location":"anomalies/archive-anomalies/#1-archive-directly","title":"1. Archive Directly","text":"<p>Step 1: Locate the anomaly (whether Active or Acknowledged) you want to archive.</p> <p></p> <p>Step 2: Click on the vertical ellipsis (\u22ee) located on the right side of the anomaly and select \u201cArchive\u201d from the dropdown menu.</p> <p></p> <p>Step 3: A modal window titled \u201cArchive Anomaly\u201d will appear, providing you with the following archive options:</p> <ul> <li>Resolved: Choose this option if the anomaly was a legitimate data quality concern and has been addressed. This helps maintain a record of resolved issues while ensuring that they are no longer active.  </li> <li>Invalid: Select this option if the anomaly is not a legitimate data quality concern and does not require further action. Archiving anomalies as invalid helps differentiate between real issues and those that can be dismissed, improving overall data quality management.</li> <li>Duplicate: Choose this option if the anomaly is a duplicate of another existing data quality concern. This helps avoid redundant tracking and ensures that related issues are managed under a single, consolidated anomaly.</li> <li>Discarded: Choose this option if the anomaly is no longer being reviewed or considered relevant. This helps remove outdated or unnecessary anomalies from the active list without marking them as invalid or resolved.</li> </ul> <p></p> <p>You also have the option to leave a comment in the provided field to provide additional context or details.</p> <p></p> <p>Step 4: Once you've made your selection, click the Archive button to proceed.</p> <p></p> <p>After clicking on the Archive button, your anomaly is moved to the archive and a confirmation message appears on the screen.</p>"},{"location":"anomalies/archive-anomalies/#2-archive-via-action-menu","title":"2. Archive via Action Menu","text":"<p>Step 1: Click on the anomaly from the list of available (whether Active or Acknowledged) anomalies that you want to archive.</p> <p></p> <p>Step 2: You will be directed to the anomaly details page. Click on the Archive button located at the top right corner of the interface.</p> <p></p> <p>Step 3: A modal window titled \u201cArchive Anomaly\u201d will appear, providing you with the following archive options:</p> <ul> <li>Resolved: Choose this option if the anomaly was a legitimate data quality concern and has been addressed. This helps maintain a record of resolved issues while ensuring that they are no longer active.  </li> <li>Invalid: Select this option if the anomaly is not a legitimate data quality concern and does not require further action. Archiving anomalies as invalid helps differentiate between real issues and those that can be dismissed, improving overall data quality management.</li> <li>Duplicate: Choose this option if the anomaly is a duplicate of another existing data quality concern. This helps avoid redundant tracking and ensures that related issues are managed under a single, consolidated anomaly.</li> <li>Discarded: Choose this option if the anomaly is no longer being reviewed or considered relevant. This helps remove outdated or unnecessary anomalies from the active list without marking them as invalid or resolved.</li> </ul> <p></p> <p>You also have the option to leave a comment in the provided field to provide additional context or details.</p> <p></p> <p>Step 4: Once you've made your selection, click the Archive button to proceed.</p> <p></p> <p>After clicking on the Archive button, your anomaly is moved to the archive and a confirmation message appears on the screen.</p>"},{"location":"anomalies/archive-anomalies/#archive-anomalies-in-bulk","title":"Archive Anomalies in Bulk","text":"<p>To handle multiple anomalies efficiently, you can archive them in bulk, allowing you to quickly move large volumes of anomalies into the archived state.</p> <p>Step 1: Hover over the anomaly (whether Active or Acknowledged) and click on the checkbox to select multiple anomalies.</p> <p></p> <p>When multiple anomalies are selected, an action toolbar appears, displaying the total number of selected anomalies along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click on the vertical ellipsis (\u22ee) and choose \"Archive\" from the dropdown menu to archive the selected anomalies.</p> <p></p> <p>Step 3: A modal window titled \u201cArchive Anomalies\u201d will appear, providing you with the following archive options:</p> <ul> <li>Resolved: Choose this option if the anomaly was a legitimate data quality concern and has been addressed. This helps maintain a record of resolved issues while ensuring that they are no longer active.  </li> <li>Invalid: Select this option if the anomaly is not a legitimate data quality concern and does not require further action. Archiving anomalies as invalid helps differentiate between real issues and those that can be dismissed, improving overall data quality management.</li> <li>Duplicate: Choose this option if the anomaly is a duplicate of another existing data quality concern. This helps avoid redundant tracking and ensures that related issues are managed under a single, consolidated anomaly.</li> <li>Discarded: Choose this option if the anomaly is no longer being reviewed or considered relevant. This helps remove outdated or unnecessary anomalies from the active list without marking them as invalid or resolved.</li> </ul> <p></p> <p>You also have the option to leave a comment in the provided field to provide additional context or details.</p> <p></p> <p>Step 4: Once you've made your selection, click on the Archive button to proceed.</p> <p></p> <p>After clicking on the Archive button, your anomalies are moved to the archive and a confirmation message appears on the screen.</p>"},{"location":"anomalies/delete-anomalies/","title":"Delete Anomalies","text":"<p>Deleting anomalies allows you to permanently remove records that are no longer relevant or were logged in error. This can be done individually or for multiple anomalies at once, ensuring that your anomaly records remain clean and up to date.</p> <p>Note</p> <p>You can only delete archived anomalies, not active or acknowledged anomalies. If you want to delete an active or acknowledged anomaly, you must first move it to the archive, and then you can delete it. </p> <p>Warning</p> <p>Deleting an anomaly is a one-time action. It cannot be restored after deletion.</p>"},{"location":"anomalies/delete-anomalies/#delete-specific-anomaly","title":"Delete Specific Anomaly","text":"<p>You can delete individual anomalies using one of two methods:</p>"},{"location":"anomalies/delete-anomalies/#1-delete-directly","title":"1. Delete Directly","text":"<p>Step 1: Click on Archived from the navigation bar in the Anomalies section to view all archived anomalies.</p> <p></p> <p>Step 2: Locate the anomaly that you want to delete and click on the Delete icon located on the right side of the anomaly.</p> <p></p> <p>Step 3: A confirmation modal window will appear, click on the Delete button to permanently remove the anomaly from the system.</p> <p></p> <p>After clicking on the Delete button, your anomaly is successfully deleted and a confirmation message appears on the screen.</p>"},{"location":"anomalies/delete-anomalies/#2-delete-via-action-menu","title":"2. Delete via Action Menu","text":"<p>Step 1: Click on the archived anomaly from the list of archived anomalies that you want to delete.</p> <p></p> <p>Step 2: You will be directed to the anomaly details page. Click on the Settings icon located at the top right corner of the page and select \u201cDelete\u201d from the dropdown menu.</p> <p></p> <p>Step 3: A confirmation modal window will appear, click on the Delete button to permanently remove the anomaly from the system.</p> <p></p> <p>After clicking on the Delete button, your anomaly is successfully deleted and a confirmation message appears on the screen.</p>"},{"location":"anomalies/delete-anomalies/#delete-anomalies-in-bulk","title":"Delete Anomalies in Bulk","text":"<p>For more efficient management, you can delete multiple anomalies at once using the bulk delete option, allowing for faster cleanup of unwanted records.</p> <p>Step 1: Hover over the archived anomalies and click on the checkbox to select anomalies in bulk.</p> <p></p> <p>When multiple anomalies are selected, an action toolbar appears, displaying the total number of anomalies chosen along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click on the vertical ellipsis (\u22ee) and choose \"Delete\" from the dropdown menu to delete the selected anomalies.</p> <p></p> <p>Step 3: A confirmation modal window will appear, click on the Delete button to permanently remove the selected anomalies from the system.</p> <p></p> <p>After clicking on the Delete button, your anomalies are successfully deleted and a confirmation message appears on the screen.</p>"},{"location":"anomalies/edit-anomalies/","title":"Edit Anomalies","text":"<p>Anomalies support two editable properties: description and tags. The description allows you to add business context or investigation findings, while tags help you categorize and organize anomalies for downstream workflows.</p> <p>Note</p> <ul> <li>Individual anomalies can have both their description and tags edited</li> <li>When editing multiple anomalies in bulk, only tags can be modified</li> <li>Only users with the Editor role (or higher) on the respective datastore can make edits to non-archived anomalies</li> </ul>"},{"location":"anomalies/edit-anomalies/#edit-description","title":"Edit Description","text":"<p>The anomaly description provides a detailed explanation of the data quality issue. You can edit it to add additional context, clarify the business impact, or document investigation findings.</p> <p>For detailed instructions on editing the description, see the Description section in the Anomaly Insights documentation.</p>"},{"location":"anomalies/edit-anomalies/#edit-tags-bulk-edit","title":"Edit Tags (Bulk Edit)","text":"<p>Step 1: Hover over the anomaly (whether Active or Acknowledged) and click on the checkbox.</p> <p></p> <p>You can edit multiple anomalies by selecting the checkboxes next to each anomaly to choose multiple anomalies at once.</p> <p></p> <p>When multiple anomalies are selected, an action toolbar appears, displaying the total number of selected anomalies along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click on the vertical ellipsis (\u22ee) and choose \"Edit\" from the dropdown menu to edit the selected anomalies.</p> <p></p> <p>A modal window titled \u201cBulk Edit Anomalies\u201d will appear. Here you can only modify the \u201ctags\u201d of the selected anomalies.</p> <p></p> <p>Step 3: Turn on the toggle and assign tags to the selected anomalies.</p> <p></p> <p>Step 4: Once you have assigned the tags, click on the \u201cSave\u201d button.</p> <p>After clicking the Save button, the selected anomalies will be updated with the assigned tags.</p> <p></p>"},{"location":"anomalies/filter-and-sort/","title":"Filter and Sort","text":"<p>Filter and Sort options allow you to organize your anomalies by various criteria, such as Weight, Anomalous Records, and Created Date. You can also apply filters to refine your list of anomalies based on Timeframe, Type, and Rule etc.</p>"},{"location":"anomalies/filter-and-sort/#sort","title":"Sort","text":"<p>You can sort your anomalies by Anomalous Records, Created Date, and Weight to easily organize and prioritize them according to your needs.</p> <p></p> No. Sort By Option Description 1 Anomalous Records Sorts anomalies based on the number of anomalous records identified. 2 Created Date Sorts anomalies according to the date they were detected. 3 Weight Sorts anomalies by their assigned weight or importance level. <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.</p> <p></p>"},{"location":"anomalies/filter-and-sort/#filter","title":"Filter","text":"<p>Info</p> <p>Users can search across all filter inputs using typos, partial terms, or abbreviations. The system intelligently matches relevant results, making it easier to find what they need without exact inputs.</p> <p></p> <p>You can filter your anomalies based on values like Timeframe, Type, Rule, and Tags, etc.</p> <p></p> No. Field Description 1 Timeframe Filtering anomalies detected within specific time ranges (e.g., anomalies detected in the last week or year). 2 Type Filter anomalies based on anomaly type (Record or Shape). 3 Rule Filter anomalies based on specific rules applied to the anomaly. By clicking on the caret down button next to the Rule field, the available rule types will be dynamically populated based on the rule types present in the results. The rules displayed are based on the current dataset and provide more granular control over filtering. Each rule type will show a counter next to it, displaying the total number of occurrences for that rule in the dataset. For example, the rule type Between is displayed with a total of 7 occurrences. 4 Table Filters anomalies based on the table where they occurred. 5 Field Filters anomalies based on the column in the table where the issue was found. 6 Check Filters anomalies based on the check that generated them. <p></p> No. Filter Description 7 Tags Tag Filter displays only the tags associated with the currently visible items, along with their color icon, name, type, and the number of matching records. Selecting one or more tags refines the list based on your selection. If no matching items are found, a No options found message is displayed."},{"location":"anomalies/restore-anomalies/","title":"Restore Anomalies","text":"<p>By restoring archived anomalies, you can bring them back into the acknowledged state for further investigation or review. These anomalies will not return to the active state once they have been acknowledged.</p> <p>Step 1: Click on the anomaly that you want to restore from the list of archived anomalies.</p> <p></p> <p>Step 2: You will be directed to the anomaly details page. Click on the Settings icon located at the top right corner of the page and select \u201cRestore\u201d from the drop down menu.</p> <p></p> <p>After clicking on the Restore button, the selected anomaly is now restored in an acknowledged state and a confirmation message appears on the screen.</p>"},{"location":"anomalies/source-record/","title":"Source Records","text":"<p>The Source Records page provides a detailed view of all records from your dataset that have failed data quality checks and been identified as anomalies. It serves as the primary interface for reviewing anomalous data at the row level, with visual highlights indicating the specific fields that triggered the anomalies. All displayed records are sourced from the linked Enrichment Datastore, which stores the results of data quality scans along with relevant metadata.</p> <p>If the Anomaly Type is Shape, you will find the highlighted column(s) having anomalies in the source record.</p> <p></p> <p>If the Anomaly Type is Record, you will find the highlighted row(s) in the source record indicating failed checks. </p> <p></p> <p>Note</p> <p>In anomaly detection, source records are displayed as part of the Anomaly Details. For a Record anomaly, the specific record is highlighted. For a Shape anomaly, 10 samples from the underlying anomalous records are highlighted.</p>"},{"location":"anomalies/source-record/#source-record-visualization","title":"Source Record Visualization","text":"<p>Users can view source records with selectable display limits of 10, 100, 1,000, or 10,000 records for comprehensive dataset analysis. The interface includes sticky headers that remain visible when scrolling through large datasets, making navigation easier during data review.</p> <p></p>"},{"location":"anomalies/source-record/#download-source-record","title":"Download Source Record","text":"<p>User can download the source records that were captured for an anomaly during the scan run. The download always includes only the records that were selected to be stored in the scan settings at the time the scan was configured.</p> <p>For example, if the user selected 100 as the maximum number of source records per anomaly, then 100 anomalous records will be shown on the Source Records page and the same 100 records will be available for download. If the user selected 10, only those 10 records will be displayed and downloadable. If the user needs to download more source records, the limit must be increased in the scan settings and the scan must be re-run, as records that were not captured during the scan cannot be downloaded later.</p> <p></p>"},{"location":"anomalies/source-record/#comparison-source-records","title":"Comparison Source Records","text":"<p>Anomalies identified by the Data Diff rule type, configured with Row Identifiers, are displayed with a detailed source record comparison. This visualization highlights differences between rows, making it easier to identify specific discrepancies.</p> <p></p>"},{"location":"checks/after-date-check/","title":"After Date Time","text":"<p>Use the <code>afterDateTime</code> rule when you need to ensure that a timestamp value occurs strictly after a defined cutoff date and time.</p> <p>This rule is commonly used for data freshness validation, cutover enforcement, and post-migration correctness.</p>"},{"location":"checks/after-date-check/#what-is-after-date-time","title":"What is After Date Time?","text":"<p>Think of After Date Time as a gatekeeper for time-based data.</p> <p>Just like a security guard checks that people enter a building after opening time, this rule ensures that every value in a timestamp field occurs after a specific date and time.</p> <p>If any record shows up before or exactly at the cutoff, it gets flagged as an anomaly.</p>"},{"location":"checks/after-date-check/#add-after-date-time-check","title":"Add After Date Time Check","text":"<p>Use the After Date Time Check to validate timestamp fields against a fixed cutoff and detect records that violate expected time boundaries.</p>"},{"location":"checks/after-date-check/#what-does-after-date-time-do","title":"What Does After Date Time Do?","text":"<p>After Date Time helps you answer questions like:</p> <ul> <li>\u201cAre all records created after the system go-live time?\u201d</li> <li>\u201cDid any old data sneak in after migration?\u201d</li> <li>\u201cAre late or backdated records affecting reports?\u201d</li> <li>\u201cIs this dataset truly fresh and recent?\u201d</li> </ul> <p>In simple terms: It guarantees that your data is newer than a specific moment in time.</p>"},{"location":"checks/after-date-check/#how-does-after-date-time-work","title":"How Does After Date Time Work?","text":""},{"location":"checks/after-date-check/#step-1-select-the-timestamp-field","title":"Step 1: Select the Timestamp Field","text":"<p>You choose a single date or timestamp field, such as:</p> <ul> <li><code>created_at</code></li> <li><code>order_date</code></li> <li><code>event_time</code></li> <li><code>processed_at</code></li> </ul>"},{"location":"checks/after-date-check/#step-2-define-the-cutoff-date-time","title":"Step 2: Define the Cutoff Date &amp; Time","text":"<p>You specify the exact date and time that acts as the lower boundary.</p> <p>Example: 2025-12-01 06:15:00 </p> <p>Only records after this moment are valid.</p>"},{"location":"checks/after-date-check/#step-3-rule-evaluation","title":"Step 3: Rule Evaluation","text":"<p>For each record, Qualytics checks:</p> <ul> <li>Is the field value greater than the cutoff timestamp?</li> </ul>"},{"location":"checks/after-date-check/#step-4-get-your-results","title":"Step 4: Get Your Results","text":"<ul> <li>Pass \u2013 All timestamps are after the cutoff  </li> <li>Anomalies Found \u2013 One or more records violate the rule and are flagged</li> </ul>"},{"location":"checks/after-date-check/#why-should-you-use-after-date-time","title":"Why Should You Use After Date Time?","text":""},{"location":"checks/after-date-check/#1-enforce-system-cutovers","title":"1. Enforce System Cutovers","text":"<p>During migrations or system upgrades, old timestamps can corrupt reports. This rule ensures only post-cutover data exists.</p>"},{"location":"checks/after-date-check/#2-validate-data-freshness","title":"2. Validate Data Freshness","text":"<p>Helps confirm that pipelines are producing new data, not reprocessing old records.</p>"},{"location":"checks/after-date-check/#3-prevent-reporting-errors","title":"3. Prevent Reporting Errors","text":"<p>Backdated entries can:</p> <ul> <li>Inflate historical metrics</li> <li>Break daily dashboards</li> <li>Trigger incorrect alerts</li> </ul> <p>After Date Time stops this at the source.</p>"},{"location":"checks/after-date-check/#4-strengthen-compliance-audits","title":"4. Strengthen Compliance &amp; Audits","text":"<p>Regulated environments often require strict timestamp validation. This rule provides a clear, auditable boundary.</p>"},{"location":"checks/after-date-check/#real-life-example-enforcing-a-login-cutover-after-system-migration","title":"Real-Life Example: Enforcing a Login Cutover After System Migration","text":""},{"location":"checks/after-date-check/#the-situation","title":"The Situation","text":"<p>BrightCart, an e-commerce company, migrated its user authentication system to a new identity provider on December 1, 2025 at 06:15 UTC.</p> <p>From this point onward:</p> <ul> <li>All user login activity must be recorded by the new system</li> <li>Any login timestamp before this cutoff is considered invalid</li> <li>Old timestamps indicate legacy data, cache issues, or failed sync jobs</li> </ul> <p>This login data is critical because it feeds:</p> <ul> <li>Account security audits</li> <li>User activity tracking</li> <li>Fraud detection</li> <li>Session-based personalization</li> </ul>"},{"location":"checks/after-date-check/#the-problem-they-faced","title":"The Problem They Faced","text":"<p>A few days after the migration, the security team noticed something unusual:</p> <ul> <li>Some users appeared inactive for years</li> <li>Fraud detection rules started misfiring</li> <li>Recently active customers were flagged as \u201cdormant\u201d</li> </ul> <p>On investigation, they found that:</p> <ul> <li>The <code>LAST_LOGIN_TS</code> field still contained old timestamps</li> <li>Many records had values like <code>2022-03-01</code></li> <li>These values should not exist after a 2025 cutover</li> </ul> <p>Manual checking wasn\u2019t scalable \u2014 the table contained millions of rows.</p>"},{"location":"checks/after-date-check/#the-solution-after-date-time","title":"The Solution: After Date Time","text":"<p>The data team configured an After Date Time check to enforce a hard time boundary.</p>"},{"location":"checks/after-date-check/#what-they-configured","title":"What they configured:","text":"<ul> <li>Table: <code>ECOMMERCE_ORDERS</code></li> <li>Field: <code>LAST_LOGIN_TS</code></li> <li>Cutoff: <code>2025-12-01 06:15 UTC</code></li> <li>Coverage: 100% (every record must comply)</li> </ul> <p>Rule intent: Every user record must have a <code>LAST_LOGIN_TS</code> that occurred after the authentication system migration.</p>"},{"location":"checks/after-date-check/#sample-records-what-the-data-looked-like","title":"Sample Records (What the Data Looked Like)","text":"LAST_LOGIN_TS CUSTOMER_EMAIL CUSTOMER_ID 2025-12-03 09:42 customer043@example.com C013 2022-03-01 10:00 customer041@example.com C011 2022-03-01 10:00 customer040@example.com C010 2025-12-05 18:21 customer031@example.com C001"},{"location":"checks/after-date-check/#what-after-date-time-discovered","title":"What After Date Time Discovered","text":"<p>When the check ran, Qualytics flagged 110 anomalous records.</p> <p>All failed records shared the same issue:</p> <ul> <li>Their <code>LAST_LOGIN_TS</code> occurred before the cutover</li> <li>Some timestamps were copied from legacy systems</li> <li>Others were default or cached values</li> </ul> <p>ANOMALIES DETECTED</p> <ul> <li>Rule Applied: After Date Time  </li> <li>Field Checked: LAST_LOGIN_TS  </li> <li>Cutoff: 2025-12-01 06:15 UTC  </li> <li>Anomalous Records: 110  </li> </ul>"},{"location":"checks/after-date-check/#anomaly-output-source-records-view","title":"Anomaly Output (Source Records View)","text":"LAST_LOGIN_TS CUSTOMER_EMAIL CUSTOMER_ID 2022-03-01T10:00:00.000Z customer043@example.com C013 2022-03-01T10:00:00.000Z customer041@example.com C011 2022-03-01T10:00:00.000Z customer040@example.com C010 2022-03-01T10:00:00.000Z customer031@example.com C001 2022-03-01T10:00:00.000Z customer028@example.com C028 2022-03-01T10:00:00.000Z customer026@example.com C026 2022-03-01T10:00:00.000Z customer021@example.com 2022-03-01T10:00:00.000Z customer014@example.com C014 2022-03-01T10:00:00.000Z customer012@example.com C012 2022-03-01T10:00:00.000Z customer009@example.com C009 <p>Why they failed:</p> <ul> <li>The rule requires timestamps to be strictly later than the cutoff</li> <li>Any value before or equal to the cutoff violates post-migration expectations</li> </ul>"},{"location":"checks/after-date-check/#why-this-was-a-serious-issue","title":"Why This Was a Serious Issue","text":"<p>Without this check:</p> <ul> <li>Active users looked inactive</li> <li>Security reports were inaccurate</li> <li>Fraud systems operated on false signals</li> <li>Business decisions were based on stale behavior data</li> </ul>"},{"location":"checks/after-date-check/#the-outcome","title":"The Outcome","text":""},{"location":"checks/after-date-check/#immediate-fix","title":"Immediate Fix","text":"<ul> <li>Identified the service populating <code>LAST_LOGIN_TS</code></li> <li>Corrected the data pipeline to read from the new auth system</li> <li>Backfilled affected records with correct timestamps</li> </ul>"},{"location":"checks/after-date-check/#long-term-protection","title":"Long-Term Protection","text":"<ul> <li>Any future regression is detected automatically</li> <li>Cutover boundaries are continuously enforced</li> <li>Teams trust that login activity reflects real user behavior</li> </ul>"},{"location":"checks/after-date-check/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>After Date Time enforces strict time boundaries</li> <li>It guarantees post-cutover data correctness</li> <li>It prevents legacy or backdated records from polluting datasets</li> <li>It runs automatically once configured</li> </ul>"},{"location":"checks/after-date-check/#when-should-you-use-after-date-time","title":"When Should You Use After Date Time?","text":"<p>Use After Date Time whenever you need to:</p> <ul> <li>Enforce system go-live or migration cutoffs</li> <li>Validate ingestion or processing timestamps</li> <li>Detect stale or replayed data</li> <li>Protect dashboards from historical pollution</li> </ul>"},{"location":"checks/after-date-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code>"},{"location":"checks/after-date-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/after-date-check/#specific-properties","title":"Specific Properties","text":"<p>Specify a particular date and time to act as the threshold for the rule.</p> Name Description Date The timestamp used as the lower boundary. Values in the selected field should be after this timestamp."},{"location":"checks/after-date-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/after-date-check/#example","title":"Example","text":"<p>Objective: Ensure that all O_ORDERDATE entries in the ORDERS table are later than 10:30 AM on December 31st, 1991.</p> <p>Sample Data</p> O_ORDERKEY O_ORDERDATE 1 1991-12-31 10:30:00 2 1992-01-02 09:15:00 3 1991-12-14 10:25:00 Payload example <pre><code>{\n    \"description\": \"Ensure that all O_ORDERDATE entries in the ORDERS table are later than 10:30 AM on December 31st, 1991.\",\n    \"coverage\": 1,\n    \"properties\":  {\n        \"datetime\": \"1991-12-31 10:30:00\"\n    },\n    \"tags\": [],\n    \"fields\": [\"O_ORDERDATE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"afterDateTime\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>O_ORDERKEY</code> 1 and 3 do not satisfy the rule because their <code>O_ORDERDATE</code> values are not after 1991-12-31 10:30:00.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_ORDERDATE]\nB --&gt; C{Is O_ORDERDATE &gt; '1991-12-31 10:30:00'?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s)\nselect\n    o_orderkey\n    , o_orderdate\nfrom orders \nwhere\n    o_orderdate &lt;= '1991-12-31 10:30:00'\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>O_ORDERDATE</code> value of <code>1991-12-14 10:30:00</code> is not later than 1991-12-31 10:30:00</p> <p>Shape Anomaly</p> <p>In <code>O_ORDERDATE</code>, 66.667% of 3 filtered records (2) are not later than 1991-12-31 10:30:00</p>"},{"location":"checks/aggregation-comparison-check/","title":"Aggregation Comparison","text":""},{"location":"checks/aggregation-comparison-check/#definition","title":"Definition","text":"<p>Verifies that the specified comparison operator evaluates true when applied to two aggregation expressions.</p>"},{"location":"checks/aggregation-comparison-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>Aggregation Comparison</code> is a rule that allows for the dynamic analysis of aggregations across different datasets. It empowers users to establish data integrity by ensuring that aggregate values meet expected comparisons, whether they are totals, averages, counts, or any other aggregated metric.</p> <p>By setting a comparison between aggregates from potentially different tables or even source datastores, this rule confirms that relationships between data points adhere to business logic or historical data patterns. This is particularly useful when trying to validate interrelated financial reports, summary metrics, or when monitoring the consistency of data ingestion over time.</p>"},{"location":"checks/aggregation-comparison-check/#field-scope","title":"Field Scope","text":"<p>Calculated: The rule automatically identifies the fields involved, without requiring explicit field selection.</p>"},{"location":"checks/aggregation-comparison-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/aggregation-comparison-check/#specific-properties","title":"Specific Properties","text":"<p>Facilitates the comparison between a <code>target</code> aggregate metric and a <code>reference</code> aggregate metric across different datasets.</p> Name Description Target Aggregation Specifies the aggregation expression to evaluate Comparison Select the comparison operator (e.g., greater than, less than, etc.) Datastore Identifies the source datastore for the reference aggregation Table/File Specifies the table or file for the reference aggregation Reference Aggregation Defines the reference aggregation expression to compare against Reference Filter Applies a filter to the reference aggregation if necessary <p>Details</p> <p>It's important to understand that each aggregation must result in a single row. Also, similar to Spark expressions, the aggregation expressions must be written in a valid format for DataFrames.</p> <p>Examples</p> <p>Simple Aggregations <pre><code>SUM(O_TOTALPRICE)\n</code></pre></p> <p>Combining with SparkSQL Functions <pre><code>ROUND(SUM(O_TOTALPRICE))\n</code></pre></p> <p>Complex Aggregations <pre><code>ROUND(SUM(L_EXTENDEDPRICE * (1 - L_DISCOUNT) * (1 + L_TAX)))\n</code></pre></p> <p>Aggregation Expressions <pre><code>COUNT(CATEGORY) * MAX(VALUE) - FIRST(VALUE)\n</code></pre></p> <p>Here are some common aggregate functions used in SparkSQL:</p> <ul> <li><code>SUM</code>: Calculates the sum of all values in a column.</li> <li><code>AVG</code>: Calculates the average of all values in a column.</li> <li><code>MAX</code>: Returns the maximum value in a column.</li> <li><code>MIN</code>: Returns the minimum value in a column.</li> <li><code>COUNT</code>: Counts the number of rows in a column.</li> </ul> <p>For a detailed list of valid SparkSQL aggregation functions, refer to the Apache Spark SQL documentation.</p> Payload example <pre><code>{\n    \"description\": \"The aggregation \\\"SUM(`TARGET_FIELD`)\\\" must be less than a reference value\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"ref_datastore_id\": ref_datastore_id,\n        \"expression\": \"SUM(`TARGET_FIELD`)\",\n        \"comparison\": \"lt\",\n        \"ref_container_id\": ref_container_id,\n        \"ref_expression\": \"MAX(`REFERENCE_FIELD`)\"\n    },\n    \"fields\": fields,\n    \"status\": \"Active\",\n    \"rule\": \"aggregationComparison\",\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n}\n</code></pre>"},{"location":"checks/aggregation-comparison-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/aggregation-comparison-check/#example","title":"Example","text":"<p>Objective: Ensure that the aggregated sum of <code>total_price</code> from the <code>ORDERS</code> table matches the aggregated and rounded sum of <code>calculated_price</code> from the <code>LINEITEM</code> table.</p> <p>Info</p> <p>The <code>calculated_price</code> in this example is represented by the sum of each product's extended price, adjusted for discount and tax.</p> <p>Sample Data</p> <p>Aggregated data from ORDERS (Target)</p> TOTAL_PRICE 5000000 <p>Aggregated data from LINEITEM (Reference)</p> CALCULATED_PRICE 4999800 Inputs <ul> <li>Target Aggregation: ROUND(SUM(O_TOTALPRICE))</li> <li>Comparison: eq (Equal To), lt (Less Than), lte (Less Than or Equal to), gte (Greater Than or Equal To), gt (Greater Than)</li> <li>Reference Aggregation: ROUND(SUM(L_EXTENDEDPRICE * (1 - L_DISCOUNT) * (1 + L_TAX)))</li> </ul> Payload example <pre><code>{\n    \"description\": \"Ensure that the aggregated sum of total_price from the ORDERS table matches the aggregated and sum of l_totalprice from the LINEITEM table\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"comparison\": \"eq\",\n        \"expression\": f\"SUM(O_TOTALPRICE)\",\n        \"ref_container_id\": ref_container_id,\n        \"ref_datastore_id\": ref_datastore_id,\n        \"ref_expression\": f\"SUM(L_TOTALPRICE)\",\n        \"ref_filter\": \"1=1\",\n    },\n    \"tags\": [],\n    \"fields\": [\"O_TOTALPRICE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"aggregationComparison\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the aggregated <code>TOTAL_PRICE</code> from the <code>ORDERS</code> table is 5000000, while the aggregated and rounded <code>CALCULATED_PRICE</code> from the <code>LINEITEM</code> table is 4999800. The difference between these totals indicates a potential anomaly, suggesting issues in data calculation or recording methods.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Aggregated Values]\nB --&gt; C{Do Aggregated Totals Match?}\nC --&gt;|Yes| D[End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query related to the rule using TPC-H tables.\nwith orders_agg as (\n    select \n        round(sum(o_totalprice)) as total_order_price\n    from \n        orders\n),\nlineitem_agg as (\n    select \n        round(sum(l_extendedprice * (1 - l_discount) * (1 + l_tax))) as calculated_price\n    from \n        lineitem\n),\ncomparison as (\n    select\n        o.total_order_price,\n        l.calculated_price\n    from\n        orders_agg o\n        cross join lineitem_agg l\n)\nselect * from comparison\nwhere comparison.total_order_price != comparison.calculated_price;\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p><code>ROUND(SUM(O_TOTALPRICE))</code> is not equal to <code>ROUND(SUM(L_EXTENDEDPRICE * (1 - L_DISCOUNT) * (1 + L_TAX)))</code>.</p>"},{"location":"checks/any-not-null-check/","title":"Any Not Null","text":"<p>Use the Any Not Null rule when business logic requires at least one field in a group to contain a value. This rule is ideal for optional-but-required data scenarios where multiple fields exist, but at least one must be populated.</p>"},{"location":"checks/any-not-null-check/#what-is-any-not-null","title":"What is Any Not Null?","text":"<p>Think of Any Not Null as a \u201cminimum information required\u201d check for your data.</p> <p>It makes sure that at least one important field in a record is filled. If all selected fields are empty, that record is considered invalid and gets flagged.</p> <p>In simple terms: At least one of these fields must have a value.</p> <p>This rule is especially useful when multiple optional fields exist, but having none of them makes the record unusable.</p>"},{"location":"checks/any-not-null-check/#add-any-not-null-check","title":"Add Any Not Null Check","text":"<p>Use the Any Not Null check when you want to ensure that records are not completely blank across a group of related fields.</p> <p>This helps detect:</p> <ul> <li>Incomplete records  </li> <li>Broken data ingestion pipelines  </li> <li>UI or API issues where optional fields are skipped entirely  </li> </ul>"},{"location":"checks/any-not-null-check/#what-does-any-not-null-do","title":"What Does Any Not Null Do?","text":"<p>It answers questions like:</p> <ul> <li>\u201cDid this record capture any meaningful information at all?\u201d</li> <li>\u201cAre users submitting forms without filling any contact details?\u201d</li> <li>\u201cIs my system creating placeholder rows with no real data?\u201d</li> </ul> <p>In short: It prevents empty or useless records from silently entering your system.</p>"},{"location":"checks/any-not-null-check/#how-does-any-not-null-work","title":"How Does Any Not Null Work?","text":""},{"location":"checks/any-not-null-check/#step-1-select-multiple-fields","title":"Step 1: Select Multiple Fields","text":"<p>You choose a set of related fields, such as:</p> <ul> <li>Email  </li> <li>Phone number  </li> <li>Username  </li> </ul>"},{"location":"checks/any-not-null-check/#step-2-rule-evaluation","title":"Step 2: Rule Evaluation","text":"<p>For each record:</p> <ul> <li>If at least one field has a value \u2192 \u2705 Pass  </li> <li>If all selected fields are NULL \u2192 \ud83d\udea8 Anomaly  </li> </ul>"},{"location":"checks/any-not-null-check/#step-3-anomaly-reporting","title":"Step 3: Anomaly Reporting","text":"<p>Any record that fails the rule is flagged and appears in the anomaly results.</p>"},{"location":"checks/any-not-null-check/#why-should-you-use-any-not-null","title":"Why Should You Use Any Not Null?","text":""},{"location":"checks/any-not-null-check/#1-stop-empty-records-early","title":"1. Stop Empty Records Early","text":"<p>Empty rows can:</p> <ul> <li>Break downstream analytics  </li> <li>Inflate row counts  </li> <li>Cause confusion during audits  </li> </ul> <p>This rule blocks them immediately.</p>"},{"location":"checks/any-not-null-check/#2-improve-data-quality-at-the-source","title":"2. Improve Data Quality at the Source","text":"<p>If data is missing here, it\u2019s usually a form, API, or ingestion issue. Any Not Null helps you catch it where it starts.</p>"},{"location":"checks/any-not-null-check/#3-protect-reporting-automation","title":"3. Protect Reporting &amp; Automation","text":"<p>Automations, alerts, and reports rely on at least one usable field. This check ensures records are worth processing.</p>"},{"location":"checks/any-not-null-check/#real-life-example-orders-missing-required-context-after-system-update","title":"Real-Life Example: Orders Missing Required Context After System Update","text":""},{"location":"checks/any-not-null-check/#the-situation","title":"The Situation","text":"<p>SunriseMart is an online retail company that processes thousands of customer orders every day. Each order is stored in the <code>ORDERS</code> table and is used by multiple teams:</p> <ul> <li>Order fulfillment</li> <li>Customer support</li> <li>Sales and revenue reporting</li> </ul> <p>For every order, SunriseMart expects at least one of the following fields to be present:</p> <ul> <li><code>O_COMMENT</code> \u2013 customer or system notes</li> <li><code>O_ORDERSTATUS</code> \u2013 order state such as Pending, Shipped, or Cancelled</li> </ul> <p>Individually, these fields are optional \u2014 but having both missing makes the order unusable.</p>"},{"location":"checks/any-not-null-check/#the-problem-they-faced","title":"The Problem They Faced","text":"<p>After deploying a backend update, the operations team noticed something unusual:</p> <ul> <li>Some orders were appearing in reports</li> <li>But fulfillment teams could not process them</li> <li>Customer support could not identify their status</li> </ul> <p>On investigation, they discovered that:</p> <ul> <li>A background job was creating order records</li> <li>The job populated technical fields like <code>O_ORDERKEY</code>, timestamps, and metadata</li> <li>But failed to populate both <code>O_COMMENT</code> and <code>O_ORDERSTATUS</code></li> </ul> <p>This issue went unnoticed at first because:</p> <ul> <li>The table contained millions of rows</li> <li>The problematic records were mixed with valid ones</li> <li>Manually checking each record was not feasible</li> </ul>"},{"location":"checks/any-not-null-check/#why-manual-checking-didnt-work","title":"Why Manual Checking Didn\u2019t Work","text":"<p>Without an automated rule, the team had to:</p> <ul> <li>Write manual SQL queries</li> <li>Scan large result sets</li> <li>Re-run checks repeatedly as new data arrived</li> </ul> <p>This approach was:</p> <ul> <li>Time-consuming</li> <li>Easy to miss edge cases</li> <li>Not scalable as data volume increased</li> </ul> <p>By the time an issue was found, downstream systems had already consumed the bad data.</p>"},{"location":"checks/any-not-null-check/#the-solution-any-not-null","title":"The Solution: Any Not Null","text":"<p>To solve this, the data team implemented an Any Not Null check on:</p> <ul> <li><code>O_COMMENT</code></li> <li><code>O_ORDERSTATUS</code></li> </ul> <p>The rule enforces a simple requirement:</p> <p>At least one of these fields must contain a value for every order record.</p>"},{"location":"checks/any-not-null-check/#what-the-check-detected","title":"What the Check Detected","text":"<p>When the check ran, it immediately flagged anomalous records where:</p> <ul> <li><code>O_COMMENT</code> = NULL  </li> <li><code>O_ORDERSTATUS</code> = NULL  </li> </ul> <p>Example anomalous record:</p> O_COMMENT O_ORDERSTATUS O_ORDERKEY NULL NULL 1034599 <p>These records appeared under Failed Checks with a clear violation message:</p> <p>There is no value set for any of <code>O_COMMENT</code> and <code>O_ORDERSTATUS</code></p> <p></p>"},{"location":"checks/any-not-null-check/#what-this-confirmed","title":"What This Confirmed","text":"<p>The Any Not Null check confirmed that:</p> <ul> <li>Orders were being created without any meaningful context</li> <li>The issue originated from the ingestion layer</li> <li>The problem was systematic, not a one-off error</li> </ul>"},{"location":"checks/any-not-null-check/#the-outcome","title":"The Outcome","text":"<p>Immediate Benefits</p> <ul> <li>Invalid order records were detected automatically</li> <li>No manual scanning or ad-hoc queries were required</li> <li>Engineers quickly identified and fixed the faulty job</li> </ul> <p>Long-Term Benefits</p> <ul> <li>Every order now contains at least one usable field</li> <li>Fulfillment and support workflows work reliably</li> <li>Data quality issues are caught early instead of downstream</li> <li>Trust in reporting and analytics was restored</li> </ul>"},{"location":"checks/any-not-null-check/#key-takeaway","title":"Key Takeaway","text":"<p>Any Not Null acts as a safety net that prevents contextless records from silently entering the system, replacing slow and unreliable manual validation with automated enforcement.</p>"},{"location":"checks/any-not-null-check/#field-scope","title":"Field Scope","text":"<p>Multiple: The rule evaluates multiple specified fields.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/any-not-null-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/any-not-null-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/any-not-null-check/#example","title":"Example","text":"<p>Objective: Ensure that for every record in the ORDERS table, at least one of the fields (O_COMMENT, O_ORDERSTATUS) isn't null.</p> <p>Sample Data</p> O_ORDERKEY O_COMMENT O_ORDERSTATUS 1 NULL NULL 2 Good product NULL 3 NULL Shipped Payload example <pre><code>{\n    \"description\": \"Ensure that for every record in the ORDERS table, at least one of the fields (O_COMMENT, O_ORDERSTATUS) isn't null\",\n    \"coverage\": 1,\n    \"properties\": {},\n    \"tags\": [],\n    \"fields\": [\"O_ORDERSTATUS\",\"O_COMMENT\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"anyNotNull\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"_PARITY = 'odd'\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>O_ORDERKEY</code> 1 does not satisfy the rule because both <code>O_COMMENT</code> and <code>O_ORDERSTATUS</code> do not hold a value.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_COMMENT and O_ORDERSTATUS]\nB --&gt; C{Is Either Field Not Null?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    o_orderkey\n    , o_comment\n    , o_orderstatus\nfrom orders \nwhere\n    o_comment is null\n    and o_orderstatus is null\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>There is no value set for any of <code>O_COMMENT, O_ORDERSTATUS</code></p> <p>Shape Anomaly</p> <p>In <code>O_COMMENT, O_ORDERSTATUS</code>, 33.333% of 3 filtered records (1) have no value set for any of <code>O_COMMENT, O_ORDERSTATUS</code></p>"},{"location":"checks/apply-check-template-for-quality-checks/","title":"Apply Check Template for Quality Checks","text":"<p>You can export check templates to make quality checks easier and more consistent. Using a set template lets you quickly verify that your data meets specific standards, reducing mistakes and improving data quality. Exporting these templates simplifies the process, making finding and fixing errors more efficient, and ensuring your quality checks are applied across different projects or systems without starting from scratch.</p> <p>Let\u2019s get started \ud83d\ude80</p> <p>Step 1:  Log in to your Qualytics account and click the \u201cLibrary\u201d button on the left side panel of the interface.</p> <p></p> <p>Here you can view the list of all the customer data validation templates. </p> <p></p> <p>Step 2: Locate the template, click on the vertical ellipsis (three dots) next to it, and select \u201cAdd Check\u201d from the dropdown menu to create a Quality Check based on this template</p> <p>For demonstration purposes, we have selected the \u201cAfter Date Time\u201d template.</p> <p></p> <p>A modal window titled \u201cAuthored Check Template\u201d will appear, displaying all the details of the Quality Check Template.</p> <p></p> <p>Step 3: Enter the following details: </p> <p>1. Associate with a Check Template: </p> <ul> <li> <p>If you toggle ON the \"Associate with a Check Template\" option, the check will be linked to a specific template.</p> </li> <li> <p>If you toggle OFF the \"Associate with a Check Template\" option, the check will not be linked to any template, which allows you full control to modify the properties independently.</p> </li> </ul> <p>Since we are applying a check template to create quality checks, it's important to keep the toggle on to ensure the template is applied as a quality check.</p> <p></p> <p>2. Template: Choose a Template from the dropdown menu that you want to associate with the quality check. The check will inherit properties from the selected template.</p> <ul> <li> <p>Locked: If the template is locked, it will automatically sync with any future updates made to the template. However, you won't be able to modify the check's properties directly, except for specific fields like Datastore, Table, and Fields, which can still be updated while maintaining synchronization with the template.</p> </li> <li> <p>Unlocked: If the template is unlocked, you are free to modify the check's properties as needed. However, any future updates to the template will no longer affect this check, as it will no longer be synced with the template.</p> </li> </ul> <p></p> <p>3. Datastore: Select the Datastore, Table and Field where you want to apply the check template. This ensures that the template is linked to the correct data source, allowing the quality checks to be performed on the specified datastore.</p> <p>For demonstration purposes, we have selected the \u201cAnalytics-DBT POC\u201d datastore, with the \u201cLINEITEM\u201d table and the \u201cL_COMMITDATE\u201d field.</p> <p></p> <p>Step 4: After completing all the check details, click on the \"Validate\" button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct. It ensures that the check will work as expected by running it against the data without committing any changes.</p> <p></p> <p>If the validation is successful, a green message will appear saying \"Validation Successful\". </p> <p></p> <p>If the validation fails, a red message will appear saying \"Failed Validation\". This typically occurs when the check logic or parameters do not match the data properly.</p> <p></p> <p>Step 5: Once you have a successful validation, click the \"Save\" button. </p> <p>Info</p> <p>You can create as many Quality checks as you want for a specific template.</p> <p></p> <p>After clicking on the \u201cSave\u201d button your check is successfully created and a success flash message will appear saying \u201cCheck successfully created\u201d.</p> <p></p>"},{"location":"checks/authored-check/","title":"Authored Check","text":"<p>Authored checks are manually created by users within the Qualytics platform or API. You can author many types of checks, ranging from simple templates for common checks to complex rules using Spark SQL and User-Defined Functions (UDF) in Scala.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"checks/authored-check/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and select the datastore from the left menu.</p> <p></p> <p>Step 2: Click on the \"Checks\" from the Navigation Tab.</p> <p></p> <p>Step 3: In the top-right corner, click on the \"Add\" button, then select \"Check\" from the dropdown menu.</p> <p></p> <p>A modal window titled \u201cAuthored Check Details\u201d will appear, providing you the options to add the authored check details.</p> <p></p> <p>Step 4: Enter the following details to add the authored check:</p> <p>1. Associate with a Check Template:</p> <ul> <li> <p>If you toggle ON the \"Associate with a Check Template\" option, the check will be linked to a specific template.</p> </li> <li> <p>If you toggle OFF the \"Associate with a Check Template\" option, the check will not be linked to any template, which allows you full control to modify the properties independently.</p> </li> </ul> <p></p> <p>2. Rule Type (Required): Select a Rule from the dropdown menu, such as checking for non-null values, matching patterns, comparing numerical values, or verifying date-time constraints. Each rule type defines the specific validation logic to be applied.</p> <p>For demonstration purposes we have selected the After Date Time rule type.</p> <p></p> <p>For more details about the available rule types, refer to the Rule Types Overview documentation.</p> <p>Note</p> <p>Different rule types have different sets of fields and options appearing when selected. </p> <p>3. File (Required): Select a file from the dropdown menu on which the check will be performed.</p> <p></p> <p>4. Field (Required): Select a field from the dropdown menu on which the check will be performed.</p> <p></p> <p>5. Filter Clause: Specify a valid Spark SQL WHERE expression to filter the data on which the check will be applied.</p> <p>The filter clause defines the conditions under which the check will be applied. It typically includes a WHERE statement that specifies which rows or data points should be included in the check.</p> <p></p> <p>6. Custom Anomaly Messages: Enable this option to use a source record field as the anomaly message instead of the default system-generated message.</p> <p>Select a source field from the dropdown. The value in that field will be displayed as the anomaly description for failed records.</p> <p>Note</p> <p>If the selected field is empty for a failed record, the default system-generated message will be used.</p> <p></p> <p>7. Date (Required): Enter the reference date for the rule. For the After Date Time rule, records in the selected field must have a timestamp later than this specified date.</p> <p></p> <p>Note</p> <p>Spark SQL expressions used in calculated fields are editable, enabling greater flexibility in configuration.</p> <p>8. Coverage: Adjust the Coverage setting to specify the percentage of records that must comply with the check.</p> <p>Note</p> <p>The Coverage setting applies to most rule types and allows you to specify the percentage of records that must meet the validation criteria.</p> <p></p> <p>9. Description (Required): Enter a detailed description of the check template, including its purpose, applicable data, and relevant information to ensure clarity for users. If you're unsure of what to include, click on the \"\ud83d\udca1\" lightbulb icon to apply a suggested description based on the rule type.</p> <p>Example: The Date of Birth must be a timestamp later than &lt; date_time &gt;.</p> <p>This description specifies that the Date of Birth field must have a timestamp later than the specified &lt; date_time &gt;.</p> <p></p> <p>10. Tag: Assign relevant tags to your check to facilitate easier searching and filtering based on categories like \"data quality,\" \"financial reports,\" or \"critical checks.\"</p> <p></p> <p>11. Additional Metadata: Add key-value pairs as additional metadata to enrich your check. Click the plus icon (+) next to this section to open the metadata input form, where you can add key-value pairs.</p> <p></p> <p>Enter the desired key-value pairs (e.g., DataSourceType: SQL Database and PriorityLevel: High). After entering the necessary metadata, click \"Confirm\" to save the custom metadata.</p> <p></p> <p>Step 4: After completing all the check details, click on the \"Validate\" button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct. It ensures that the check will work as expected by running it against the data without committing any changes.</p> <p></p> <p>If the validation is successful, a green message will appear saying \"Validation Successful\".</p> <p></p> <p>If the validation fails, a red message will appear saying \"Failed Validation\". This typically occurs when the check logic or parameters do not match the data properly.</p> <p></p> <p>Step 5: Once you have a successful validation, click the \"Save\" button.</p> <p></p> <p>After clicking on the \u201cSave\u201d button your check is successfully created and a success flash message will appear saying \u201cCheck successfully created\u201d.</p> <p></p>"},{"location":"checks/authored-check/#author-a-check-via-api","title":"Author a Check via API","text":"<p>Users are able to author and interact with Checks through the API by passing JSON Payloads. Please refer to the API documentation on details: <code>qualytics.io/api/docs</code></p> <p></p>"},{"location":"checks/before-date-time-check/","title":"Before Date Time","text":""},{"location":"checks/before-date-time-check/#definition","title":"Definition","text":"<p>Asserts that the field is a timestamp earlier than a specific date and time.</p>"},{"location":"checks/before-date-time-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code>"},{"location":"checks/before-date-time-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/before-date-time-check/#specific-properties","title":"Specific Properties","text":"<p>Specify a particular date and time to act as the threshold for the rule.</p> Name Description Date The timestamp used as the upper boundary. Values in the selected field should be before this timestamp."},{"location":"checks/before-date-time-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/before-date-time-check/#example","title":"Example","text":"<p>Objective: Ensure that all L_SHIPDATE entries in the LINEITEM table are earlier than 3:00 PM on December 1st, 1998.</p> <p>Sample Data</p> L_ORDERKEY L_SHIPDATE 1 1998-12-01 15:30:00 2 1998-11-02 12:45:00 3 1998-08-01 10:20:00 Payload example <pre><code>{\n    \"description\": \"Make sure datetime values are earlier than 3:00 PM, Dec 01, 1998\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"datetime\":\"1998-12-01T15:00:00Z\"\n    },\n    \"tags\": [],\n    \"fields\": [\"L_SHIPDATE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"beforeDateTime\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>L_ORDERKEY</code> 1 does not satisfy the rule because its <code>L_SHIPDATE</code> value is not before 1998-12-01 15:00:00.</p> FlowchartsSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_SHIPDATE]\nB --&gt; C{Is L_SHIPDATE &lt; '1998-12-01 15:00:00'?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey\n    , l_shipdate\nfrom lineitem \nwhere\n    l_shipdate &gt;= '1998-12-01 15:00:00'\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>L_SHIPDATE</code> value of <code>1998-12-01 15:30:00</code> is not earlier than 1998-12-01 15:00:00.</p> <p>Shape Anomaly</p> <p>In <code>L_SHIPDATE</code>, 33.33% of 3 filtered records (1) are not earlier than 1998-12-01 15:00:00.</p>"},{"location":"checks/between-check/","title":"Between","text":"<p>The Between rule is used to ensure that numeric values stay within an expected and acceptable range. It is commonly used to catch outliers, invalid measurements, and data entry or pipeline errors before they impact reports or downstream systems.</p>"},{"location":"checks/between-check/#what-is-between","title":"What is Between?","text":"<p>Think of Between as a \u201csafety fence\u201d for numeric values in your data.</p> <p>Just like speed limits on a road prevent unsafe driving, the Between check ensures that numbers stay within boundaries that make sense for your business. If a value goes below the minimum or above the maximum, it gets flagged immediately.</p> <p>In simple terms: This number must fall within this range.</p>"},{"location":"checks/between-check/#add-between-check","title":"Add Between Check","text":"<p>Use the Between check when you want to validate that a numeric field stays within an expected minimum and maximum value.</p> <p>This helps you detect:</p> <ul> <li>Unexpected spikes or drops  </li> <li>Data entry mistakes  </li> <li>Sensor or system malfunctions  </li> <li>Broken transformations or unit conversion issues  </li> </ul>"},{"location":"checks/between-check/#what-does-between-do","title":"What Does Between Do?","text":"<p>Between helps you answer questions like:</p> <ul> <li>\u201cIs this quantity unreasonably low or high?\u201d  </li> <li>\u201cDid a system send an invalid numeric value?\u201d  </li> <li>\u201cAre measurements staying within realistic limits?\u201d  </li> <li>\u201cDid a unit conversion break and inflate values?\u201d  </li> </ul> <p>In short: It ensures your numbers stay realistic and trustworthy.</p>"},{"location":"checks/between-check/#how-does-between-work","title":"How Does Between Work?","text":"<p>Let\u2019s break it down step by step.</p>"},{"location":"checks/between-check/#step-1-choose-a-numeric-field","title":"Step 1: Choose a Numeric Field","text":"<p>You select a single numeric field, such as:</p> <ul> <li>Quantity  </li> <li>Price  </li> <li>Temperature  </li> <li>Duration  </li> <li>Weight  </li> <li>Usage count  </li> </ul>"},{"location":"checks/between-check/#step-2-define-the-acceptable-range","title":"Step 2: Define the Acceptable Range","text":"<p>You specify:</p> <ul> <li>Minimum value  </li> <li>Maximum value  </li> <li>Whether each boundary is inclusive or exclusive  </li> </ul> <p>Example:</p> <ul> <li>Min = 5 (inclusive)  </li> <li>Max = 20 (inclusive)  </li> </ul> <p>This means: \ud83d\udc49 5 \u2264 value \u2264 20</p>"},{"location":"checks/between-check/#step-3-validation-happens-automatically","title":"Step 3: Validation Happens Automatically","text":"<p>For each record, the Between check evaluates:</p> <ul> <li>Is the value less than the minimum?  </li> <li>Is the value greater than the maximum?  </li> </ul> <p>If yes \u2192 Anomaly detected</p>"},{"location":"checks/between-check/#step-4-review-the-results","title":"Step 4: Review the Results","text":"<p>The output shows:</p> <ul> <li>Pass \u2013 Value is within the allowed range  </li> <li>Anomaly Found \u2013 Value falls outside the range  </li> </ul>"},{"location":"checks/between-check/#why-should-you-use-between","title":"Why Should You Use Between?","text":""},{"location":"checks/between-check/#1-catch-invalid-values-early","title":"1. Catch Invalid Values Early","text":"<p>A quantity of <code>-3</code>, <code>0</code>, or <code>10,000</code> may technically be a number\u2014but it may not make sense for your business.</p> <p>Between catches these issues before they reach dashboards or customers.</p>"},{"location":"checks/between-check/#2-prevent-bad-decisions","title":"2. Prevent Bad Decisions","text":"<p>Out-of-range values can:</p> <ul> <li>Skew averages  </li> <li>Break charts  </li> <li>Trigger false alerts  </li> <li>Lead to wrong operational decisions  </li> </ul> <p>Between protects your analytics from bad inputs.</p>"},{"location":"checks/between-check/#3-save-manual-validation-time","title":"3. Save Manual Validation Time","text":"<p>Instead of scanning reports for suspicious numbers, Between continuously validates every new record automatically.</p>"},{"location":"checks/between-check/#4-increase-confidence-in-data-quality","title":"4. Increase Confidence in Data Quality","text":"<p>When stakeholders see numbers within expected bounds, trust in your data increases.</p>"},{"location":"checks/between-check/#real-life-example-insurance-location-risk-validation-between-check","title":"Real-Life Example: Insurance Location Risk Validation (Between Check)","text":""},{"location":"checks/between-check/#the-situation","title":"The Situation","text":"<p>An insurance company maintains a property insurance portfolio in the <code>FL_INSURANCE</code> table. This data is used by actuarial and catastrophe-risk models to calculate premiums and exposure.</p> <p>One critical numeric field is <code>POINT_GRANULARITY</code>, which defines the resolution used for location-based risk calculations.</p> <p>Based on business and modeling requirements:</p> <ul> <li>Minimum allowed value: <code>1.000</code> </li> <li>Maximum allowed value: <code>5.000</code> </li> </ul> <p>Any value outside this range makes the risk calculation unreliable.</p>"},{"location":"checks/between-check/#the-problem","title":"The Problem","text":"<p>During a routine data quality scan on the Insurance Portfolio \u2013 Staging datastore, analysts noticed unexpected behavior in downstream risk models.</p> <p>There were:</p> <ul> <li>No missing records  </li> <li>No schema mismatches  </li> <li>No pipeline failures  </li> </ul> <p>However, some policies showed unexpected risk scores, even though the data appeared complete.</p>"},{"location":"checks/between-check/#the-solution-between-check","title":"The Solution: Between Check","text":"<p>To validate numeric correctness, the data team configured a Between check on the <code>POINT_GRANULARITY</code> field with the following rules:</p> <ul> <li>Min: <code>1.000</code> (inclusive)  </li> <li>Max: <code>5.000</code> (inclusive)</li> </ul> <p>This ensured that every insurance record used a valid granularity level for risk computation.</p>"},{"location":"checks/between-check/#what-the-between-check-discovered","title":"What the Between Check Discovered","text":"<p>OUT-OF-RANGE VALUES DETECTED</p> <ul> <li>Field: <code>POINT_GRANULARITY</code></li> <li>Rule Applied: Between</li> <li>Violation: 0.049% of filtered records (36,645 total)</li> <li>Allowed Range: 1.000 to 5.000</li> </ul>"},{"location":"checks/between-check/#source-records-anomalous-values","title":"Source Records (Anomalous Values)","text":"POINT_GRANULARITY EQ_SITE_DEDUCTIBLE TIV_2011 HU_SITE_LIMIT 6 26405 191651 369075 6 72628 580170 -80130 9 92381 165469 753709 6 17849 939037 365831 7 17753 196937 676446 6 35655 957449 -75944 8 57319 856692 923312 7 56104 78318 770455 6 21986 152933 -19570 10 71621 -12793 -32706 <p>These records were structurally valid but numerically invalid according to business rules.</p>"},{"location":"checks/between-check/#anomaly-interpretation","title":"Anomaly Interpretation","text":"<ul> <li>The records existed in the table and passed schema validation  </li> <li>Only the numeric constraint was violated  </li> <li>Because a range rule failed, the anomaly was classified as a Shape anomaly </li> <li>This indicated a data quality issue, not missing or duplicate data  </li> </ul>"},{"location":"checks/between-check/#root-cause-identified","title":"Root Cause Identified","text":"<p>Further investigation showed:</p> <ul> <li>An upstream ingestion change defaulted <code>POINT_GRANULARITY</code> to higher values</li> <li>No validation existed at the source system</li> <li>The issue silently propagated into staging analytics</li> </ul>"},{"location":"checks/between-check/#the-outcome","title":"The Outcome","text":"<p>Immediate Results - Invalid values were corrected - Risk models were recalculated using valid granularity levels</p> <p>Long-Term Protection - The Between check now runs automatically on each scan - Any future out-of-range values are flagged immediately - Incorrect risk calculations are prevented before reporting or pricing</p>"},{"location":"checks/between-check/#key-takeaway","title":"\ud83d\udd0d Key Takeaway","text":"<p>Between checks protect numeric fields that must stay within strict business limits.</p> <p>In this case, the Between check caught values that were technically valid numbers\u2014but business-invalid, preventing incorrect insurance risk assessments and downstream decisions.</p>"},{"location":"checks/between-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/between-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/between-check/#specific-properties","title":"Specific Properties","text":"<p>Specify both minimum and maximum boundaries, and determine if these boundaries should be inclusive.</p> Name Explanation Max The upper boundary of the range. Inclusive (Max) If true, the upper boundary is considered a valid value within the range. Otherwise, it's exclusive. Min The lower boundary of the range. Inclusive (Min) If true, the lower boundary is considered a valid value within the range. Otherwise, it's exclusive."},{"location":"checks/between-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/between-check/#example","title":"Example","text":"<p>Objective: Ensure that all L_QUANTITY entries in the LINEITEM table are between 5 and 20 (inclusive).</p> <p>Sample Data</p> L_ORDERKEY L_QUANTITY 1 4 2 15 3 21 Payload example <pre><code>{\n    \"description\": \"Ensure that all L_QUANTITY entries in the LINEITEM table are between 5 and 20 (inclusive)\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"min\":5,\n        \"inclusive_min\":true,\n        \"max\":20,\n        \"inclusive_max\":true\n    },\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"between\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 1 and 3 do not satisfy the rule because their <code>L_QUANTITY</code> values are not between 5 and 20 inclusive.</p> FlowchartsSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_QUANTITY]\nB --&gt; C{Is 5 &lt;= L_QUANTITY &lt;= 20?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey\n    , l_quantity\nfrom lineitem \nwhere\n    l_quantity &lt; 5\n    or l_quantity &gt; 20\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The value for <code>L_QUANTITY</code> of 4 is not between 5.000 and 20.000.</p> <p>Shape Anomaly</p> <p>In <code>L_QUANTITY</code>, 66.67% of 3 filtered records (2) are not between 5.000 and 20.000.</p>"},{"location":"checks/between-times-check/","title":"Between Times","text":"<p>Use the <code>betweenTimes</code> rule when you need to validate that date or timestamp values fall within an expected time window. This check is ideal for catching early, late, or invalid records caused by ingestion delays, timezone issues, or system errors.</p>"},{"location":"checks/between-times-check/#what-is-between-times","title":"What is Between Times?","text":"<p>Think of Between Times as a time window guardrail for your data \u23f1\ufe0f.</p> <p>Just like a store that only accepts deliveries between 9 AM and 6 PM, this check makes sure your data shows up when it\u2019s supposed to\u2014not before, not after.</p> <p>If a record arrives too early, too late, or completely outside the allowed window, Between Times flags it immediately.</p> <p>In simple terms: Between Times makes sure your data arrives on time\u2014and only on time.</p>"},{"location":"checks/between-times-check/#add-between-times-check","title":"Add Between Times Check","text":"<p>Use the Between Times check to validate timestamps, enforce business hours, and detect out-of-range records before they impact reporting or downstream systems.</p>"},{"location":"checks/between-times-check/#what-does-between-times-do","title":"What Does Between Times Do?","text":"<p>Between Times helps you answer questions like:</p> <ul> <li>\u201cDid these records arrive during valid business hours?\u201d</li> <li>\u201cAre any timestamps outside the expected date range?\u201d</li> <li>\u201cDid old or future-dated records sneak into my dataset?\u201d</li> </ul> <p>In simple terms: It ensures every date or timestamp stays within an allowed start and end time.</p>"},{"location":"checks/between-times-check/#how-does-between-times-work","title":"How Does Between Times Work?","text":""},{"location":"checks/between-times-check/#step-1-choose-the-time-field","title":"Step 1: Choose the Time Field","text":"<p>Select the date or timestamp field you want to validate (for example, <code>created_at</code> or <code>event_time</code>).</p>"},{"location":"checks/between-times-check/#step-2-define-the-allowed-time-window","title":"Step 2: Define the Allowed Time Window","text":"<p>Set:</p> <ul> <li>Start Time</li> <li>End Time</li> </ul> <p>This could be a date range, business hours, or a strict ingestion window.</p>"},{"location":"checks/between-times-check/#step-3-the-validation-happens","title":"Step 3: The Validation Happens","text":"<p>Between Times checks every record:</p> <ul> <li> <p>Inside the window \u2192 \u2705 Pass  </p> </li> <li> <p>Outside the window \u2192 \u274c Flagged as anomalous</p> </li> </ul>"},{"location":"checks/between-times-check/#step-4-review-the-results","title":"Step 4: Review the Results","text":"<p>If violations exist, you\u2019ll see exactly:</p> <ul> <li>Which records failed</li> <li>By how much they were early or late</li> <li>Which field caused the anomaly</li> </ul>"},{"location":"checks/between-times-check/#why-should-you-use-between-times","title":"Why Should You Use Between Times?","text":""},{"location":"checks/between-times-check/#1-catch-timing-issues-early","title":"1. Catch Timing Issues Early","text":"<p>Late or early data can silently corrupt reports. This check surfaces issues immediately.</p>"},{"location":"checks/between-times-check/#2-prevent-invalid-historical-or-future-data","title":"2. Prevent Invalid Historical or Future Data","text":"<p>Old test records or future-dated entries won\u2019t sneak into production analytics.</p>"},{"location":"checks/between-times-check/#3-improve-trust-in-time-based-reports","title":"3. Improve Trust in Time-Based Reports","text":"<p>Dashboards, SLAs, and audits depend on correct timestamps. Between Times keeps them reliable.</p>"},{"location":"checks/between-times-check/#4-reduce-downstream-failures","title":"4. Reduce Downstream Failures","text":"<p>Many pipelines break when timestamps are out of range. This check stops bad data upstream.</p>"},{"location":"checks/between-times-check/#real-life-example-city-infrastructure-dataset","title":"Real-Life Example: City Infrastructure Dataset","text":""},{"location":"checks/between-times-check/#the-situation","title":"The Situation","text":"<p>A city data team manages a building permits dataset. Each permit record includes a <code>CREATED_DATE</code> field that should fall between:</p> <ul> <li>January 1, 1995</li> <li>June 2, 2019</li> </ul> <p>This dataset feeds:</p> <ul> <li>Urban planning reports</li> <li>Historical trend analysis</li> <li>Compliance dashboards</li> </ul>"},{"location":"checks/between-times-check/#the-problem-they-faced","title":"The Problem They Faced","text":"<p>Over time, the team noticed strange trends in reports:</p> <ul> <li>Some permits appeared decades too early</li> <li>Others showed dates far in the future</li> <li>KPIs based on yearly counts were off</li> </ul> <p>They suspected bad timestamps but had no automated way to catch them.</p>"},{"location":"checks/between-times-check/#the-solution-between-times","title":"The Solution: Between Times","text":"<p>They configured a Between Times check on the <code>CREATED_DATE</code> field with the allowed date range.</p> <p>Rule Setup:</p> <ul> <li>Field: <code>CREATED_DATE</code></li> <li>Start Time: <code>1995-01-01</code></li> <li>End Time: <code>2019-06-02</code></li> </ul>"},{"location":"checks/between-times-check/#what-between-times-discovered","title":"What Between Times Discovered","text":"<p>VIOLATION DETECTED</p> <ul> <li>Field: <code>CREATED_DATE</code></li> <li>Rule Applied: Between Times</li> <li>21.98% of records were outside the allowed time window</li> </ul> <p>Example of Flagged Records:</p> CREATED_DATE CT2010 XCoord CB2010 1990-03-05T16:36:27.448Z 112 959857 2034 1993-03-28T09:24:21.483Z 112 959604 1020 1993-08-19T01:27:27.406Z 70 959403 2009 2020-04-08T13:08:12.275Z 239 942672 1006 1992-06-28T02:11:50.123Z 207 947120 3000 2020-11-04T16:30:36.819Z 177 null null 2020-06-04T14:36:40.406Z 177 958051 2002 1991-03-20T21:22:40.939Z 187 948974 1015 1992-09-20T18:47:49.845Z 169 951372 1012 2019-10-16T12:39:31.870Z 169 953637 1002 <p></p>"},{"location":"checks/between-times-check/#summary","title":"\ud83d\udd0d Summary","text":"<ul> <li>These records fall outside the valid historical window</li> <li>They likely came from:</li> <li>Legacy system imports</li> <li>Incorrect timezone conversions</li> <li>Manual data entry errors</li> <li>Without this check, they were silently skewing reports</li> </ul>"},{"location":"checks/between-times-check/#the-outcome","title":"The Outcome","text":"<p>Immediate Benefits: - Invalid records were identified in minutes - Analysts excluded bad data from reports - Data quality issues were traced to the source system</p> <p>Long-Term Benefits: - Automated monitoring for timestamp validity - Accurate historical reporting - Increased trust in time-based analytics - Fewer surprises during audits</p>"},{"location":"checks/between-times-check/#key-takeaways","title":"Key Takeaways","text":"<p>Between Times acts like a time gate for your data.</p> <ul> <li>It ensures records arrive only within expected time windows</li> <li>It catches early, late, and impossible timestamps</li> <li>It runs automatically once configured</li> <li>It protects reports, dashboards, and downstream systems</li> </ul>"},{"location":"checks/between-times-check/#when-should-you-use-between-times","title":"When Should You Use Between Times?","text":"<p>Use Between Times whenever you need to validate when data occurs:</p> <ul> <li>Event ingestion timestamps</li> <li>Record creation dates</li> <li>Business hours enforcement</li> <li>Historical data boundaries</li> <li>SLA and compliance monitoring</li> </ul>"},{"location":"checks/between-times-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code>"},{"location":"checks/between-times-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/between-times-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the range of dates or times that values in the selected field should fall between.</p> Name Description Min The timestamp used as the lower boundary. Values in the selected field should be after this timestamp. Max The timestamp used as the upper boundary. Values in the selected field should be before this timestamp."},{"location":"checks/between-times-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/between-times-check/#example","title":"Example","text":"<p>Objective: Ensure that all O_ORDERDATE entries in the ORDERS table are between 10:30 AM on January 1st, 1991 and 3:00 PM on December 31st, 1991.</p> <p>Sample Data</p> O_ORDERKEY O_ORDERDATE 1 1990-12-31 10:30:00 2 1991-06-02 09:15:00 3 1992-01-01 01:25:00 Payload example <pre><code>{\n    \"description\": \"Ensure that all O_ORDERDATE entries in the ORDERS table are between 10:30 AM on January 1st, 1991 and 3:00 PM on December 31st, 1991\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"min_time\":\"1991-01-01T10:30:00Z\",\n        \"max_time\":\"1991-12-31T15:00:00Z\"\n    },\n    \"tags\": [],\n    \"fields\": [\"O_ORDERDATE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"betweenTimes\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"_PARITY = 'odd'\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>O_ORDERKEY</code> 1 and 3 do not satisfy the rule because their <code>O_ORDERDATE</code> values are not between 1991-01-01 10:30:00 and 1991-12-31 15:00:00.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_ORDERDATE]\nB --&gt; C{Is '1991-01-01 10:30:00' &lt;= O_ORDERDATE &lt;= '1991-12-31 15:00:00'?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    o_orderkey\n    , o_orderdate\nfrom orders \nwhere\n    o_orderdate &lt; '1991-01-01 10:30:00'\n    or o_orderdate &gt; '1991-12-31 15:00:00'\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The value for <code>O_ORDERDATE</code> of <code>1990-12-31 10:30:00</code> is not between 1991-01-01 10:30:00 and 1991-12-31 15:00:00.</p> <p>Shape Anomaly</p> <p>In <code>O_ORDERDATE</code>, 66.667% of 3 filtered records (2) are not between 1991-01-01 10:30:00 and 1991-12-31 15:00:00.</p>"},{"location":"checks/checks-template/","title":"Check Templates","text":"<p>Check Templates empower users to efficiently create, manage, and apply standardized checks across various datastores, acting as blueprints that ensure consistency and data integrity across different datasets and processes.</p> <p>Check Templates streamline the validation process by enabling check management independently of specific data assets such as datastores, containers, or fields. These templates reduce manual intervention, minimize errors, and provide a reusable framework that can be applied across multiple datasets, ensuring all relevant data adheres to defined criteria. This not only saves time but also enhances the reliability of data quality checks within an organization.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"checks/checks-template/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the \"Library\" button on the left-side panel.</p> <p></p> <p>Step 2: Click on the \"Add Check Template\" button in the top-right corner.</p> <p></p> <p>A modal titled \"Check Template Details\" will appear, providing you with the options to add the check template details.</p> <p></p>"},{"location":"checks/checks-template/#create-a-check-template","title":"Create a Check Template","text":"<p>Step 3: Enter the following details to add the check template:</p> <ul> <li>Rule Type (Required) </li> <li>Filter Clause</li> <li>Template Options</li> <li>Description (Required) </li> <li>Tags </li> <li>Additional Metadata </li> </ul>"},{"location":"checks/checks-template/#rule-type","title":"Rule Type","text":"<p>Select a Rule Type from the dropdown menu for data validation, such as checking for non-null values, matching patterns, comparing numerical values, or verifying datetime constraints. Each rule type defines the specific validation logic to be applied.</p> <p>For more details about the available rule types, refer to the \"Check Rule Types\" section.</p> <p>Note</p> <p>Different rule types have different sets of fields and options appearing when selected.</p> <p></p>"},{"location":"checks/checks-template/#filter-clause","title":"Filter Clause","text":"<p>Specify a valid Spark SQL <code>WHERE</code> expression to filter the data on which the check will be applied.</p> <p>The filter clause defines the conditions under which the check will be applied. It typically includes a <code>WHERE</code> statement that specifies which rows or data points should be included in the check.</p> <p>Example: A filter clause might be used to apply the check only to rows where a certain column meets a specific condition, such as <code>WHERE status = 'active'</code>.</p> <p></p>"},{"location":"checks/checks-template/#using-each","title":"Using each()","text":"<p>The <code>each()</code> keyword allows you to create multiple checks from a single Check Template \u2014 one for each value in a provided list.</p> <p>This makes it easier to apply the same check logic across multiple dimensions (such as regions, countries, or product categories) without manually creating separate checks.</p> <p>Syntax:</p> <pre><code>&lt;field_name&gt; = each('&lt;VALUE_1&gt;', '&lt;VALUE_2&gt;', '&lt;VALUE_3&gt;', ...)\n</code></pre> <p>Example:</p> <p>A data team wants to ensure that daily order counts are never zero for each country where the business operates. Instead of creating a separate check for every country, they use each() in a single template.</p> <pre><code>country_code = each('BRA', 'USA', 'ESP', 'CHN')\n</code></pre> <p>When creating checks from this template, Qualytics automatically generates four separate checks one per country\u2014each validating order data independently.</p> <p>Behavior:</p> <ul> <li>The <code>each()</code> keyword can only be used when creating checks from a template.</li> <li>It is not supported when updating an existing check's filter.</li> <li>If you try to create a check with <code>each()</code> when permutations already exist for the check, an error will appear. To add or remove permutation values, update the template's filter instead.</li> <li>You can include multiple <code>each()</code> clauses in the same filter. Each clause is expanded independently, generating all possible permutations.</li> </ul> <p>Note</p> <p>Permutation logic runs when checks are created from a template or when the template's filter is updated. Editing an existing check's filter does not trigger permutations.</p> <p>Checks Created by each()</p> <p>Example with Multiple each() Clauses:</p> <p>A marketplace team wants to verify transaction volume across regions and business segments.</p> <pre><code>region = each('EU', 'US') AND category = each('Retail', 'Wholesale')\n</code></pre> <p>This produces 4 checks (2 \u00d7 2 permutations):</p> <ul> <li><code>region = 'EU' AND category = 'Retail'</code></li> <li><code>region = 'EU' AND category = 'Wholesale'</code></li> <li><code>region = 'US' AND category = 'Retail'</code></li> <li><code>region = 'US' AND category = 'Wholesale'</code></li> </ul> <p>Each check runs independently, making it easy to pinpoint which segment or region has an issue.</p> <p>Managing Permutations</p> <p>Once checks with permutations exist, add or remove values by updating the template's filter. The system will create or delete checks accordingly based on whether the template is locked.</p> <p>Locked vs. Unlocked Templates</p> Template State Behavior Locked Removing a value from an <code>each()</code> array deletes the corresponding check. Adding a new value creates a new check. Unlocked Adding a new value creates a new check. Removing a value does not delete existing checks. <p>Example</p> <p>Initial filter when creating checks: <pre><code>country_code = each('ITA', 'MEX', 'ESP')\n</code></pre></p> <p>Result:</p> <ul> <li> <p>The system generates three checks \u2014 one each for Italy, Mexico, and Spain.</p> </li> <li> <p>Adding 'BRA' creates a new check for Brazil.</p> </li> <li> <p>If the template is locked and 'ESP' is removed, the Spain check is deleted.</p> </li> <li> <p>If the template is unlocked, existing checks remain even after removing values.</p> </li> </ul> <p>Note</p> <ul> <li><code>each()</code> can be combined with other standard filter operators (<code>=</code>, <code>&gt;</code>, <code>&lt;</code>, <code>IN</code>, etc.).</li> <li>Multiple <code>each()</code> clauses can be used within the same filter to support complex filtering logic.</li> <li>This feature applies only to checks created from templates \u2014 not to standalone checks without a template.</li> </ul>"},{"location":"checks/checks-template/#coverage","title":"Coverage","text":"<p>Adjust the Coverage setting to specify the percentage of records that must comply with the check.</p> <p>Note</p> <p>The Coverage setting applies to most rule types and allows you to specify the percentage of records that must meet the validation criteria.</p> <p></p>"},{"location":"checks/checks-template/#template-options","title":"Template Options","text":"<p>Check or uncheck the \"Template Locked\" option to determine whether all checks created from this template will have their properties automatically synced to any changes made to the template.</p> <p>For more information about the template state, jump to the \"Template State\" section below.  </p> <p></p>"},{"location":"checks/checks-template/#description","title":"Description","text":"<p>Enter a detailed description of the check template, including its purpose, applicable data, and relevant information to ensure clarity for users. If you're unsure of what to include, click on the \"\ud83d\udca1\" lightbulb icon to apply a suggested description based on the rule type.</p> <p>Example: \"The &lt; field &gt; must exist in <code>bank_transactions_*.csv.Total_Transaction_Amount</code> (Bank Dataset - Staging)\".</p> <p>This description clarifies that the specified field must be present in a particular file (<code>bank_transactions_*.csv</code>) and column (<code>Total_Transaction_Amount</code>) within the Bank Dataset.</p> <p></p>"},{"location":"checks/checks-template/#tags","title":"Tags","text":"<p>Assign relevant tags to your check template to facilitate easier searching and filtering based on categories like \"data quality\", \"financial reports\", or \"critical checks\".</p> <p></p>"},{"location":"checks/checks-template/#additional-metadata","title":"Additional Metadata","text":"<p>Add key-value pairs as additional metadata to enrich your check. Click the plus icon (+) next to this section to open the metadata input form, where you can add key-value pairs.</p> <p></p> <p>Enter the desired key-value pairs (e.g., DataSourceType: SQL Database and PriorityLevel: High). After entering the necessary metadata, click \"Confirm\" to save the custom metadata.  </p> <p></p> <p>Step 4: Once you have entered all the required fields, click the \u201cSave\u201d button to finalize the template.</p> <p>Warning</p> <p>Once a template is saved, the selected rule type becomes locked and cannot be changed.</p> <p></p> <p>After clicking the Save button, a success notification appears on the screen confirming that the check template was created successfully.</p> <p>After saving the check template, you can now Apply a Check Template to create Quality Checks, which will enforce the validation rules defined in the template across your datastores. This ensures consistent data quality and compliance with the criteria you\u2019ve established.</p> <p>Once a check template is created, you can view its details by clicking on it, where three tabs are displayed at the top: Overview, Checks, and Anomalies.</p>"},{"location":"checks/checks-template/#template-details","title":"Template Details","text":""},{"location":"checks/checks-template/#overview-tab","title":"Overview Tab","text":"<p>The Overview tab gives a complete view of a check template, showing its key details, configuration, and recent activities. In the Summary section, users can also use redirect buttons to quickly navigate to related tabs like Checks and Active Anomalies. Information is divided into three sections: Summary, Activity, and Definition.</p> <p></p>"},{"location":"checks/checks-template/#summary","title":"Summary","text":"<p>The Summary section provides a quick overview of the check template's key details, including its name, type, priority, coverage, associated checks, active anomalies, description, and tags \u2014 helping users quickly understand its purpose and current status.</p> <p></p> REF. Field Description 1 Template The rule type of the check template, indicating its purpose (e.g., \u201cAfter Date Time\u201d). 2 Type Indicates whether the template is Locked (cannot be edited) or Unlocked (editable). 3 Weight A numeric value representing the importance or priority level of the template in scoring or decision-making. 4 Coverage The percentage of relevant dataset elements to which this template is applied. 5 Checks The total number of checks currently using this template. You can click on the (\u2197) button to navigate directly to the Checks tab. 6 Active Anomalies The count of unresolved anomalies detected by checks associated with this template. You can click on the (\u2197) button to navigate directly to the Anomalies tab. 7 Description A short statement explaining the logic or purpose of the check template. 8 Tags Used to categorize and filter templates (e.g., \u201cSandbox\u201d). Users can change the tags by clicking the tag badge. <p></p>"},{"location":"checks/checks-template/#definition","title":"Definition","text":"<p>The Definition section displays the configuration details of a check template. It outlines the target conditions, specific properties, and any additional metadata associated with the template, providing clarity on how and where it is applied.</p> <p></p> REF. Field Description 1 Target Defines the filter condition applied to the dataset. If no filter is specified, the check template applies to all data in the target scope. 2 Properties Displays configuration details specific to the check type. Content varies based on the selected check: <ul><li> Field Count checks: Shows \"Number of Fields\". </li><li> Metric checks: Shows \"Comparison\", \"Min Value\", and \"Max Value\".</li> 3 Metadata Displays any custom metadata properties linked to the template. If none are defined, this section remains empty. <p></p>"},{"location":"checks/checks-template/#activity","title":"Activity","text":"<p>The Activity section provides a chronological log of all actions and updates related to this template. It tracks key events such as creation, modifications, and other relevant activities, along with timestamps to show when they occurred.</p> <p></p> <p>You can hover over a timestamp to view the full date and last modified time.</p> <p></p>"},{"location":"checks/checks-template/#checks-tab","title":"Checks Tab","text":"<p>The Checks tab provides a comprehensive view of all checks linked to the chosen datastore, container, or field, along with their source details such as computed table and field information. By clicking options such as Active, Important, Favorite, Draft, Archived (Invalid and Discarded), or All, users can instantly view checks based on their status. This categorization helps in organizing, reviewing, and managing checks more effectively for consistent data quality oversight.</p> <p></p> <p>Alternatively, users can navigate to the Checks tab directly from the Overview tab by clicking the redirect button in the Checks section of the Summary panel.</p> <p></p>"},{"location":"checks/checks-template/#anomalies-tab","title":"Anomalies Tab","text":"<p>The Anomalies tab displays all anomalies detected for the selected check template, along with details such as source datastore, computed table, field, rule, and the number of anomalous records. Users can view anomalies based on their status: Open, Active, Acknowledged, Archived, or All and sort them based on specific parameters.</p> <p></p> <p>Alternatively, users can navigate to the Anomalies tab directly from the Overview tab by clicking the redirect button in the Active Anomalies section of the Summary panel.</p> <p></p>"},{"location":"checks/checks-template/#multiple-checks-creation","title":"Multiple Checks Creation","text":"<p>Users can create multiple checks at once by selecting a template and adding multiple targets. Each target will generate its own check.</p> <p>Step 1: Click on the Add button located in the top right corner and select Multiple Checks from the dropdown.</p> <p></p> <p>A Bulk Add Quality Checks modal window will appear. Fill in the details:</p> No. Field Description 1. Datastore Select the datastore where the check should be applied. 2. File/Table Choose the file/table within the selected datastore. 3. Field Select the field to apply the check on. 4. Filter Clause Specify a valid Spark SQL <code>WHERE</code> expression to filter the data on which the check will be applied. <p></p> <p>Step 2: Click on the Add Target button to create another check. You can keep adding targets to create as many checks as you need within the same template.</p> <p></p>"},{"location":"checks/checks-template/#template-state","title":"Template State","text":"<p>Any changes to a template may or may not impact its related checks, depending on whether the template state is locked or unlocked. Managing the template state allows you to control whether updates automatically apply to all related checks or let them function independently.</p> <p>Unlocked</p> <ul> <li>Quality Checks can evolve independently of the template. Subsequent updates to an unlocked Check Template do not affect its related quality checks.</li> </ul> <p>Locked</p> <ul> <li>Quality Checks from a locked Check Template will inherit changes made to the template. Subsequent updates to a locked Check Template do affect its related quality checks.</li> </ul> <p>Info</p> <p>Tags will be synced independently of unlocked and locked Check Templates, while Description and Additional Metadata will not be synced. This behavior is general for Check Templates.</p> Template State <pre><code>graph TD\nA[Start] --&gt;|Is `Template Locked` enabled?| B{Yes/No}\nB --&gt;|No| E[The quality check can evolve independently]\nB --&gt;|Yes| C[They remain synchronized with the template]\nC --&gt; D[End]\nE --&gt; D[End]</code></pre>"},{"location":"checks/contains-credit-card-check/","title":"Contains Credit Card","text":"<p>Use the <code>containsCreditCard</code> rule when you need to ensure that sensitive payment fields actually contain valid credit card numbers.  </p>"},{"location":"checks/contains-credit-card-check/#what-is-contains-credit-card","title":"What is Contains Credit Card?","text":"<p>Think of Contains Credit Card as a sanity check for payment data.</p> <p>Just like a cashier won\u2019t accept a payment without a card number, this rule ensures that a selected field contains a valid credit card number \u2014 not empty values, placeholders, or incorrect data.</p> <p>In simple terms: It verifies that credit card fields truly contain credit card numbers.</p>"},{"location":"checks/contains-credit-card-check/#add-contains-credit-card-check","title":"Add Contains Credit Card Check","text":"<p>Use the Contains Credit Card check to validate payment-related fields and detect records where the credit card number is missing or invalid.</p>"},{"location":"checks/contains-credit-card-check/#what-does-contains-credit-card-do","title":"What Does Contains Credit Card Do?","text":"<p>It helps you answer questions like:</p> <ul> <li>\u201cDo all completed orders actually have a credit card number?\u201d</li> <li>\u201cAre payment records missing sensitive fields?\u201d</li> <li>\u201cDid upstream systems send incomplete payment data?\u201d</li> </ul> <p>In short: It flags records where a credit card number is expected but missing or invalid.</p>"},{"location":"checks/contains-credit-card-check/#how-does-contains-credit-card-work","title":"How Does Contains Credit Card Work?","text":""},{"location":"checks/contains-credit-card-check/#step-1-select-the-field","title":"Step 1: Select the Field","text":"<p>Choose a single field that should contain a credit card number (e.g., <code>CARD_NUMBER</code>).</p>"},{"location":"checks/contains-credit-card-check/#step-2-pattern-validation","title":"Step 2: Pattern Validation","text":"<p>Qualytics checks whether each value matches known credit card formats (continuous digits or hyphen-separated patterns).</p>"},{"location":"checks/contains-credit-card-check/#step-3-anomaly-detection","title":"Step 3: Anomaly Detection","text":"<p>If a record does not contain a valid credit card number, it is marked anomalous.</p>"},{"location":"checks/contains-credit-card-check/#step-4-review-results","title":"Step 4: Review Results","text":"<p>You can review exactly which records failed and why.</p>"},{"location":"checks/contains-credit-card-check/#real-life-example-e-commerce-orders","title":"Real-Life Example: E-commerce Orders","text":""},{"location":"checks/contains-credit-card-check/#the-situation","title":"The Situation","text":"<p>An e-commerce company stores order data in the <code>ECOMMERCE_ORDERS</code> table.</p> <p>Each successful order is expected to include a credit card number in the <code>CARD_NUMBER</code> field.</p> <p>This data is critical for:</p> <ul> <li>Payment reconciliation</li> <li>Fraud investigation</li> <li>Compliance audits</li> </ul>"},{"location":"checks/contains-credit-card-check/#the-problem","title":"The Problem","text":"<p>Before this check was configured, the data team had to manually review order records to verify whether credit card numbers were present.</p> <p>This manual process:</p> <ul> <li>Took significant time during each review</li> <li>Did not scale as order volume increased</li> <li>Still risked missing incomplete or invalid payment records</li> </ul>"},{"location":"checks/contains-credit-card-check/#the-solution","title":"The Solution","text":"<p>The team implemented the Contains Credit Card check on the <code>CARD_NUMBER</code> field.</p> <p>With this check in place, Qualytics automatically validates every record during each scan and flags any order where the credit card number is missing or invalid\u2014removing the need for manual review.</p>"},{"location":"checks/contains-credit-card-check/#what-the-check-detected","title":"What the Check Detected","text":"<p>During the scan, Qualytics identified 26 anomalous records where the <code>CARD_NUMBER</code> field did not contain a valid credit card number, even though the orders were marked as completed.</p>"},{"location":"checks/contains-credit-card-check/#output","title":"Output","text":"CARD_NUMBER CUSTOMER_EMAIL PRODUCT_ID QUANTITY ORDER_DATE Missing customer056@example.com P115 1 2022-02-01 Missing customer052@example.com P111 -1 2022-02-01 Missing customer048@example.com P107 3 2022-01-01 Missing customer036@example.com P115 1 2022-02-01 Missing customer028@example.com P107 3 2022-02-01 Missing customer020@example.com P119 5 2022-02-01 Missing customer016@example.com P115 1 2022-02-01 Missing customer012@example.com P111 2 2022-02-01 Missing customer008@example.com P107 3 2022-02-01 Missing customer004@example.com P103 4 2022-02-01 <p>Anomaly Detected</p> <ul> <li>Rule Applied: Contains Credit Card  </li> <li>Field: CARD_NUMBER  </li> <li>Anomalous Records: 26  </li> <li>Violation: Credit card number not found  </li> </ul> <p>Violation Message Example: In <code>CARD_NUMBER</code>, 26.00% of filtered records (26) do not contain credit card numbers.</p>"},{"location":"checks/contains-credit-card-check/#why-this-matters","title":"Why This Matters","text":"<p>Without this check:</p> <ul> <li>Payments cannot be reliably audited</li> <li>Fraud investigations lack critical data</li> <li>Compliance requirements may fail</li> <li>Downstream billing systems may break</li> </ul>"},{"location":"checks/contains-credit-card-check/#the-outcome","title":"The Outcome","text":"<p>After enabling the Contains Credit Card check:</p> <ul> <li>The team identified faulty upstream ingestion logic</li> <li>Missing card numbers were traced to a failed payment gateway response</li> <li>Incomplete payment records were blocked</li> <li>Data quality and compliance confidence improved</li> </ul>"},{"location":"checks/contains-credit-card-check/#when-should-you-use-contains-credit-card","title":"When Should You Use Contains Credit Card?","text":"<p>Use this rule when you:</p> <ul> <li>Validate payment or transaction tables</li> <li>Enforce PCI or financial compliance</li> <li>Monitor upstream payment integrations</li> <li>Prevent incomplete payment records</li> </ul>"},{"location":"checks/contains-credit-card-check/#key-takeaway","title":"Key Takeaway","text":"<p>Contains Credit Card acts as a guardrail for payment data. If a system says \u201cpayment received\u201d, this rule ensures the data proves it.</p>"},{"location":"checks/contains-credit-card-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/contains-credit-card-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/contains-credit-card-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/contains-credit-card-check/#example","title":"Example","text":"<p>Objective: Ensure that every O_PAYMENT_DETAILS in the ORDERS table contains a credit card number to confirm the payment method used for each order.</p> <p>Sample Data</p> O_ORDERKEY O_PAYMENT_DETAILS 1 {\"date\": \"2023-09-25\", \"amount\": 250.50, \"credit_card\": \"5105105105105100\"} 2 {\"date\": \"2023-09-25\", \"amount\": 150.75, \"credit_card\": \"ABC12345XYZ\"} 3 {\"date\": \"2023-09-25\", \"amount\": 200.00, \"credit_card\": \"4111-1111-1111-1111\"} Payload example <pre><code>{\n    \"description\": \"Ensure that every O_PAYMENT_DETAILS in the ORDERS table contains a credit card number to confirm the payment method used for each order\",\n    \"coverage\": 1,\n    \"properties\": {},\n    \"tags\": [],\n    \"fields\": [\"C_CCN_JSON\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"containsCreditCard\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>O_ORDERKEY</code> 2 violates the rule as the <code>O_PAYMENT_DETAILS</code> does not contain a credit card number, indicating an incomplete order record.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_PAYMENT_DETAILS]\nB --&gt; C{Contains Credit Card Number?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query to identify order records that don't contain a credit card number in the payment details.\nselect\n    o_orderkey,\n    o_payment_details\nfrom orders\nwhere\n    not (regexp_like(o_payment_details, '[0-9]{16}'))\n    or not (regexp_like(o_payment_details, '\\d{4}-\\d{4}-\\d{4}-\\d{4}'))\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>O_PAYMENT_DETAILS</code> value of <code>{\"date\": \"2023-09-25\", \"amount\": 150.75, \"credit_card\": \"ABC12345XYZ\"}</code> does not contain a credit card number.</p> <p>Shape Anomaly</p> <p>In <code>O_PAYMENT_DETAILS</code>, 33.33% of 3 order records (1) do not contain a credit card number.</p>"},{"location":"checks/contains-email-check/","title":"Contains Email","text":"<p>Think of Contains Email as a basic hygiene check for contact data.</p> <p>Any time your system stores customer or user information, email addresses are often the primary identifier\u2014used for communication, login, billing, and notifications. This check ensures that the field you expect to contain an email address actually does.</p>"},{"location":"checks/contains-email-check/#add-contains-email-check","title":"Add Contains Email Check","text":"<p>Use the Contains Email check to validate that a specific field includes an email address in the expected format and to detect records where email data is missing, broken, or malformed.</p>"},{"location":"checks/contains-email-check/#what-does-contains-email-do","title":"What Does Contains Email Do?","text":"<p>Contains Email helps you catch issues like:</p> <ul> <li>Missing <code>@</code> or domain name  </li> <li>Truncated or partially stored email values  </li> <li>Corrupted email data during ingestion or transformation  </li> <li>Non-email text accidentally stored in an email field  </li> </ul> <p>In simple terms: It makes sure your email fields actually contain emails.</p>"},{"location":"checks/contains-email-check/#how-does-contains-email-work","title":"How Does Contains Email Work?","text":""},{"location":"checks/contains-email-check/#step-1-choose-the-field","title":"Step 1: Choose the Field","text":"<p>You select one field that is expected to contain an email address (for example: <code>email</code>, <code>contact_details</code>, or a JSON column containing email data).</p>"},{"location":"checks/contains-email-check/#step-2-the-validation-happens","title":"Step 2: The Validation Happens","text":"<p>Qualytics scans each record and checks whether the field value matches a valid email pattern.</p>"},{"location":"checks/contains-email-check/#step-3-get-your-results","title":"Step 3: Get Your Results","text":"<ul> <li>Pass \u2013 The field contains a valid email address  </li> <li>Anomaly Found \u2013 The value does not contain a valid email  </li> </ul>"},{"location":"checks/contains-email-check/#why-should-you-use-contains-email","title":"Why Should You Use Contains Email?","text":""},{"location":"checks/contains-email-check/#1-prevent-broken-communication","title":"1. Prevent Broken Communication","text":"<p>If emails are malformed, automated emails fail silently\u2014customers never receive confirmations, alerts, or updates.</p>"},{"location":"checks/contains-email-check/#2-improve-data-quality-upstream","title":"2. Improve Data Quality Upstream","text":"<p>Catching bad emails early prevents downstream issues in CRM systems, marketing tools, and analytics.</p>"},{"location":"checks/contains-email-check/#3-avoid-operational-errors","title":"3. Avoid Operational Errors","text":"<p>Invalid email data can cause:</p> <ul> <li>Failed notifications</li> <li>Login issues</li> <li>Customer support escalations</li> <li>Compliance gaps</li> </ul> <p>Contains Email acts as a first-line validation check.</p>"},{"location":"checks/contains-email-check/#real-life-example-customer-contact-data-review","title":"Real-Life Example: Customer Contact Data Review","text":"<p>Let me walk you through a complete, real-world scenario:</p>"},{"location":"checks/contains-email-check/#the-situation","title":"The Situation","text":"<p>BrightCart is an online retail platform that collects customer contact details during account creation and checkout. Email addresses are critical because they are used for:</p> <ul> <li>Order confirmations  </li> <li>Shipping notifications  </li> <li>Password resets  </li> <li>Customer support communication  </li> </ul> <p>Before sharing this data with marketing and support teams, the data team reviews it to ensure email information is correct.</p>"},{"location":"checks/contains-email-check/#the-problem-they-had","title":"The Problem They Had","text":"<p>Initially, the team relied on manual review to validate email data.</p> <p>One week, customer support noticed an increase in complaints like:</p> <ul> <li>\u201cI didn\u2019t receive my order confirmation\u201d</li> <li>\u201cPassword reset email never arrived\u201d</li> </ul> <p>When the data team investigated, they found that some customer records did not contain valid email addresses\u2014but these issues had gone unnoticed during manual review.</p> <p>After digging deeper, they discovered:</p> <ul> <li>The dataset had grown to tens of thousands of records  </li> <li>Analysts were reviewing only a small sample due to time constraints  </li> <li>Subtle issues (like missing domains or incomplete emails) were easy to miss  </li> <li>Manual checks were inconsistent and error-prone  </li> </ul>"},{"location":"checks/contains-email-check/#the-solution-contains-email","title":"The Solution: Contains Email","text":"<p>To avoid relying on visual inspection, the team introduced the Contains Email check to automatically validate email-related fields during every scan.</p> <p>Instead of checking a few records by hand, the rule evaluated every record consistently.</p>"},{"location":"checks/contains-email-check/#what-the-system-discovered","title":"What the System Discovered","text":"<p>ALERT GENERATED:</p> <p>EMAIL VALIDATION FAILED</p> <ul> <li>Rule Applied: Contains Email  </li> <li>Anomalous Records: 100% of evaluated records  </li> </ul> <p>Technical Output (from Qualytics):</p> <p>When reviewing the flagged records, the team noticed values like:</p> C_NAME C_MKTSEGMENT C_PHONE CP_CUSTKEY Customer#000000002 AUTOMOBILE 23-768-687-3665 2 Customer#000000002 AUTOMOBILE 23-768-687-3665 2 Customer#000000002 AUTOMOBILE 23-768-687-3665 2 Customer#000000002 AUTOMOBILE 23-768-687-3665 2 Customer#000000002 AUTOMOBILE 23-768-687-3665 2 Customer#000000002 AUTOMOBILE 23-768-687-3665 2 Customer#000000002 AUTOMOBILE 23-768-687-3665 2 Customer#000000002 AUTOMOBILE 23-768-687-3665 2 Customer#000000002 AUTOMOBILE 23-768-687-3665 2 Customer#000000002 AUTOMOBILE 23-768-687-3665 2 <p></p>"},{"location":"checks/contains-email-check/#summary","title":"Summary","text":"<ul> <li>Manual review did not scale as data volume increased  </li> <li>Incorrect values slipped through unnoticed  </li> <li>The automated check immediately surfaced the issue across the entire dataset  </li> <li>The problem was identified in minutes instead of days  </li> </ul>"},{"location":"checks/contains-email-check/#the-outcome","title":"The Outcome","text":"<p>Immediate Benefits:</p> <ul> <li>The team corrected the rule configuration  </li> <li>Email validation was applied to the correct data  </li> <li>False assumptions from manual review were eliminated  </li> </ul> <p>Long-Term Benefits:</p> <ul> <li>No more time-consuming manual inspections  </li> <li>Consistent validation across all records  </li> <li>Fewer customer communication failures  </li> <li>Greater trust in data shared across teams  </li> </ul>"},{"location":"checks/contains-email-check/#key-takeaway","title":"Key Takeaway","text":"<p>Manual data review works only at small scale. Automated checks like Contains Email ensure that every record is validated, every time\u2014without relying on human attention or sampling.</p>"},{"location":"checks/contains-email-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/contains-email-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/contains-email-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/contains-email-check/#example","title":"Example","text":"<p>Objective: Ensure that the <code>C_CONTACT_DETAILS</code> field contains at least one valid email address for every record in the CUSTOMER dataset.</p> <p>Sample Data</p> C_CUSTKEY C_CONTACT_DETAILS 1 {\"name\": \"John Doe\", \"email\": \"john.doe@example.com\"} 2 {\"name\": \"Amy Lu\", \"email\": \"amy.lu@\"} 3 {\"name\": \"Jane Smith\", \"email\": \"jane.smith@domain.org\"} Payload example <pre><code>{\n    \"description\": \"Ensure that all C_CONTACT_DETAILS entries in the CUSTOMER table contain valid email addresses\",\n    \"coverage\": 1,\n    \"properties\": {},\n    \"tags\": [],\n    \"fields\": [\"C_CONTACT_DETAILS\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"containsEmail\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>C_CUSTKEY</code> 2 does not satisfy the rule because its <code>C_CONTACT_DETAILS</code> value does not follow a typical email format.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve C_CONTACT_DETAILS]\nB --&gt; C{Does C_CONTACT_DETAILS contain an email address?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    c_custkey,\n    c_contact_details\nfrom customer \nwhere\n    not regexp_like(c_contact_details, '^[a-zA-Z0-9._%-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}$')\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>C_CONTACT_DETAILS</code> value of <code>{\"name\": \"Amy Lu\", \"email\": \"amy.lu@\"}</code> does not contain an email address.</p> <p>Shape Anomaly</p> <p>In <code>C_CONTACT_DETAILS</code>, 33.333% of 3 filtered records (1) do not contain email addresses.</p>"},{"location":"checks/contains-social-security-number-check/","title":"Contains Social Security Number","text":"<p>Use the Contains Social Security Number rule when a field is expected to store a valid Social Security Number (SSN).</p>"},{"location":"checks/contains-social-security-number-check/#what-is-contains-social-security-number","title":"What Is Contains Social Security Number?","text":"<p>Think of Contains Social Security Number as a data integrity guard for sensitive identity fields.</p> <p>It checks whether a field actually contains a valid SSN, instead of missing, malformed, or incomplete values that may look correct but fail compliance or downstream usage.</p> <p>In simple terms: It ensures SSN data is present, usable, and consistently formatted.</p>"},{"location":"checks/contains-social-security-number-check/#add-contains-social-security-number-check","title":"Add Contains Social Security Number Check","text":"<p>Use this check to validate that a specific field contains a valid SSN and to detect records where the SSN is missing or incorrectly stored.</p>"},{"location":"checks/contains-social-security-number-check/#what-does-this-check-do","title":"What Does This Check Do?","text":"<p>The rule scans a single field and confirms:</p> <ul> <li>An SSN value exists  </li> <li>The SSN follows a recognizable pattern (for example: <code>XXX-XX-XXXX</code>)  </li> <li>The value is not null, truncated, or malformed  </li> </ul> <p>If a record fails any of these conditions, it is flagged as an anomaly.</p>"},{"location":"checks/contains-social-security-number-check/#how-does-contains-social-security-number-work","title":"How Does Contains Social Security Number Work?","text":""},{"location":"checks/contains-social-security-number-check/#step-1-select-the-field","title":"Step 1: Select the Field","text":"<p>Choose the field that is expected to contain an SSN (for example: <code>C_SSN</code>).</p>"},{"location":"checks/contains-social-security-number-check/#step-2-validate-the-content","title":"Step 2: Validate the Content","text":"<p>Qualytics verifies whether the field value matches a valid SSN format.</p>"},{"location":"checks/contains-social-security-number-check/#step-3-review-the-results","title":"Step 3: Review the Results","text":"<ul> <li>Pass \u2013 Field contains a valid SSN  </li> <li>Fail \u2013 SSN is missing, malformed, or not detected  </li> </ul>"},{"location":"checks/contains-social-security-number-check/#why-should-you-use-this-check","title":"Why Should You Use This Check?","text":""},{"location":"checks/contains-social-security-number-check/#1-prevent-compliance-gaps","title":"1. Prevent Compliance Gaps","text":"<p>Missing or invalid SSNs can lead to regulatory violations, audit failures, and processing issues.</p>"},{"location":"checks/contains-social-security-number-check/#2-catch-issues-early","title":"2. Catch Issues Early","text":"<p>Instead of discovering problems during reporting or audits, this check flags them as soon as data is scanned.</p>"},{"location":"checks/contains-social-security-number-check/#3-protect-sensitive-workflows","title":"3. Protect Sensitive Workflows","text":"<p>Payroll, identity verification, and compliance systems depend on accurate SSN data. This rule helps keep those workflows reliable.</p>"},{"location":"checks/contains-social-security-number-check/#real-life-example-customer-identity-data-validation","title":"Real-Life Example: Customer Identity Data Validation","text":""},{"location":"checks/contains-social-security-number-check/#the-situation","title":"The Situation","text":"<p>BrightMart is a retail company that maintains a central customer database containing identity information required for compliance and verification.</p> <p>Each customer record is expected to include a valid Social Security Number (SSN).  </p> <p>This data is used for:</p> <ul> <li>Customer identity verification  </li> <li>Regulatory and compliance reporting  </li> <li>Internal audits and analytics  </li> </ul>"},{"location":"checks/contains-social-security-number-check/#the-problem-they-had","title":"The Problem They Had","text":"<p>As BrightMart\u2019s customer base grew, the data team relied on manual reviews to verify SSN values.</p> <p>During a routine compliance review, the team noticed that some customer records could not be validated, even though SSN data appeared to be present. This raised an important question:</p> <p>Why are some records failing verification when they seem correct at first glance?</p> <p>After investigating, the team discovered:</p> <ul> <li>Some SSN values were missing or empty </li> <li>Some values were incorrectly structured </li> <li>Manually reviewing thousands of records took significant time </li> <li>Human review made it easy to miss subtle issues </li> </ul> <p>These problems had gone unnoticed for weeks, creating a serious compliance risk and delaying reporting.</p>"},{"location":"checks/contains-social-security-number-check/#the-solution-contains-social-security-number","title":"The Solution: Contains Social Security Number","text":"<p>To eliminate manual checks, the team implemented the Contains Social Security Number rule type.</p> <p>Instead of reviewing records one by one, the rule automatically scanned all customer records and validated whether each one actually contained a valid SSN.</p>"},{"location":"checks/contains-social-security-number-check/#what-the-check-reviewed","title":"What the Check Reviewed","text":"<p>After running the check, Qualytics produced the following Source Records view. Records that failed SSN validation were automatically highlighted, making them easy to identify.</p> C_SSN_JSON _CHECKSUM C_CUSTKEY { \"ssn\": \"966-15-3666\" } 00a7b86dc440f5799515d58dafdb5be 43661 { \"ssn\": \"938-35-4653\" } 0097c6fd693f6b42207cf0cfa38f53d 93248 { \"ssn\": \"980-04-5020\" } 009abea2080b1713b168bc998c52f0f 83639 { \"ssn\": \"918-58-8770\" } 0076beb855ab8742a5b0ef7fc1442b 131926 { \"ssn\": \"933-20-6278\" } 006b9646dcba176709eb7a322e61e2b7 21691 { \"ssn\": \"989-09-1445\" } 00650747d878eeca4d468a880de38b2 11860 { \"ssn\": \"963-62-7104\" } 0061f5779e53186143b077a516e5a0c4 137432 { \"ssn\": \"961-06-0718\" } 004bd16c4a27e11bed1d9e9d13d94c9 126126 { \"ssn\": \"913-38-6794\" } 004881ebe3826fc16cfd124968f2a5bb 12223 { \"ssn\": \"796-00-1624\" } 001d507e80c4e4d2ce4aba05590f8313 80971 <p></p>"},{"location":"checks/contains-social-security-number-check/#what-contains-social-security-number-discovered","title":"What Contains Social Security Number Discovered","text":"<p>ALERT GENERATED:</p> <p>SSN VALIDATION FAILED</p> <ul> <li>Rule Applied: Contains Social Security Number  </li> <li>Anomalous Records: 50  </li> <li>Violation Rate: 0.335% of 14,941 records  </li> </ul> <p>Although the failure rate was small, each failed record represented a potential compliance issue.</p>"},{"location":"checks/contains-social-security-number-check/#summary","title":"\ud83d\udd0d Summary","text":"<ul> <li>Most customer records contained valid SSNs  </li> <li>A small but critical subset did not meet SSN requirements  </li> <li>The failed records were clearly marked in red, eliminating guesswork  </li> </ul>"},{"location":"checks/contains-social-security-number-check/#the-outcome","title":"The Outcome","text":"<p>Immediate Benefits</p> <ul> <li>Manual SSN reviews were eliminated  </li> <li>Invalid records were identified instantly  </li> <li>Review time dropped from hours to minutes  </li> </ul> <p>Long-Term Benefits</p> <ul> <li>Continuous monitoring of SSN data quality  </li> <li>Reduced audit and compliance risk  </li> <li>Higher confidence in customer identity data  </li> </ul>"},{"location":"checks/contains-social-security-number-check/#why-this-matters","title":"Why This Matters","text":"<p>Just like missing orders can impact revenue, missing or invalid SSNs can impact compliance.</p> <p>The Contains Social Security Number check acts as an automated safeguard\u2014ensuring sensitive identity data is always complete, valid, and audit-ready, without relying on slow and error-prone manual reviews.</p>"},{"location":"checks/contains-social-security-number-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/contains-social-security-number-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/contains-social-security-number-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/contains-social-security-number-check/#example","title":"Example","text":"<p>Objective: Ensure that all C_CONTACT_DETAILS entries in the CUSTOMER table contain valid social security numbers.</p> <p>Sample Data</p> C_CUSTKEY C_CONTACT_DETAILS 1 {\"name\": \"John Doe\", \"ssn\": \"234567890\"} 2 {\"name\": \"Amy Lu\", \"ssn\": \"666-12-3456\"} 3 {\"name\": \"Jane Smith\", \"ssn\": \"429-14-2216\"} Payload example <pre><code>{\n    \"description\": \"Ensure that all C_CONTACT_DETAILS entries in the CUSTOMER table contain valid social security numbers\",\n    \"coverage\": 1,\n    \"properties\": {},\n    \"tags\": [],\n    \"fields\": [\"C_CONTACT_DETAILS\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"containsSocialSecurityNumber\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>C_CUSTKEY</code> 2 does not satisfy the rule because its <code>C_CONTACT_DETAILS</code> value does not contain the typical social security number format.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve C_CONTACT_DETAILS]\nB --&gt; C{Does C_CONTACT_DETAILS contain a valid SSN format?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    c_custkey,\n    c_contact_details\nfrom customer \nwhere\n    not regexp_like(ssn, '^[0-9]{3}-[0-9]{2}-[0-9]{4}$')\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>C_CONTACT_DETAILS</code> value of <code>{\"name\": \"Amy Lu\", \"ssn\": \"666-12-3456\"}</code> does not contain a social security number.</p> <p>Shape Anomaly</p> <p>In <code>C_CONTACT_DETAILS</code>, 33.333% of 3 filtered records (1) do not contain social security numbers.</p>"},{"location":"checks/contains-url/","title":"Contains URL","text":""},{"location":"checks/contains-url/#definition","title":"Definition","text":"<p>Asserts that the values contain valid URLs.</p>"},{"location":"checks/contains-url/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/contains-url/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/contains-url/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/contains-url/#example","title":"Example","text":"<p>Objective: Ensure that all S_DETAILS entries in the SUPPLIER table contain valid URLs.</p> <p>Sample Data</p> S_SUPPKEY S_DETAILS 1 {\"name\": \"Tech Parts\", \"website\": \"www.techparts.com\"} 2 {\"name\": \"Hardwarepro\", \"website\": \"https://www.hardwarepro.com\"} 3 {\"name\": \"Smith's Tools\", \"website\": \"ftp:server:8080\"} Payload example <pre><code>{\n    \"description\": \"Ensure that all S_DETAILS entries in the SUPPLIER table contain valid URLs\",\n    \"coverage\": 1,\n    \"properties\": {},\n    \"tags\": [],\n    \"fields\": [\"S_DETAILS\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"containsUrl\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>S_SUPPKEY</code> 1 and 3 do not satisfy the rule because their <code>S_DETAILS</code> values do not contain a valid URL pattern.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve S_DETAILS]\nB --&gt; C{Does S_DETAILS contain a valid URL?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    s_suppkey,\n    s_details\nfrom supplier \nwhere\n    not regexp_like(url, '^https?://.+')\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>S_DETAILS</code> value of <code>{\"name\": \"Tech Parts\", \"website\": \"www.techparts.com\"}</code> does not contain a valid URL.</p> <p>Shape Anomaly</p> <p>In <code>S_DETAILS</code>, 66.667% of 3 filtered records (2) do not contain a valid URL.</p>"},{"location":"checks/data-diff-check/","title":"Data Diff","text":"<p>Recommended Check</p> <p>Qualytics recommends using the <code>dataDiff</code> rule type instead of the <code>isReplicaOf</code>.</p> <p>The <code>isReplicaOf</code> check is being deprecated and will no longer be maintained, while <code>dataDiff</code> provides the same functionality with enhanced performance and additional capabilities.</p>"},{"location":"checks/data-diff-check/#what-is-data-diff","title":"What is Data Diff?","text":"<p>Think of Data Diff as a \"spot the difference\" game for your business data. </p> <p>Just like when you compare two pictures side-by-side to find what's changed, Data Diff compares two sets of information to make sure they match perfectly. It's like having a super-careful assistant who checks that when you copy something important, nothing gets lost, changed, or added by mistake.</p>"},{"location":"checks/data-diff-check/#add-data-diff-check","title":"Add Data Diff Check","text":"<p>Use the Data Diff Check to compare two tables, detect anomalies, and run a scan to identify mismatched or missing records for accurate data validation.</p>"},{"location":"checks/data-diff-check/#what-does-data-diff-do","title":"What Does Data Diff Do?","text":"<p>Data Diff helps you answer questions like:</p> <ul> <li>\"Did all my customer orders copy correctly to the backup system?\"</li> <li>\"Is the sales report showing the same numbers as the original database?\"</li> <li>\"When we moved data from System A to System B, did everything transfer properly?\"</li> </ul> <p>In simple terms: It makes sure Data Set A is an exact match of Data Set B.</p>"},{"location":"checks/data-diff-check/#how-does-data-diff-work","title":"How Does Data Diff Work?","text":"<p>Let's break it down into simple steps:</p>"},{"location":"checks/data-diff-check/#step-1-choose-what-to-compare","title":"Step 1: Choose What to Compare","text":"<p>You pick two sets of data:</p> <ul> <li>The Original (your main source of truth)</li> <li>The Copy (backup, report, or transferred data)</li> </ul>"},{"location":"checks/data-diff-check/#step-2-pick-what-matters","title":"Step 2: Pick What Matters","text":"<p>You decide which information is important to check. For example:</p> <ul> <li>Customer names</li> <li>Order amounts</li> <li>Product IDs</li> <li>Dates</li> </ul>"},{"location":"checks/data-diff-check/#step-3-the-comparison-happens","title":"Step 3: The Comparison Happens","text":"<p>Data Diff automatically looks at both sets:</p> <ul> <li>Is everything from the original in the copy?</li> <li>Is there anything extra in the copy that shouldn't be there?</li> <li>Do all the values match exactly?</li> </ul>"},{"location":"checks/data-diff-check/#step-4-get-your-results","title":"Step 4: Get Your Results","text":"<p>The Data Diff report shows:</p> <ul> <li>Pass \u2013 Target and reference datasets match; no action needed.</li> <li>Anomalies Found \u2013 Differences detected; view the report to see which rows or fields differ.</li> </ul>"},{"location":"checks/data-diff-check/#why-should-you-use-data-diff","title":"Why Should You Use Data Diff?","text":""},{"location":"checks/data-diff-check/#1-catch-mistakes-before-they-cause-problems","title":"1. Catch Mistakes Before They Cause Problems","text":"<p>Imagine your finance team creates a quarterly report from last night's data backup. If some transactions didn't copy over, your report would be wrong. Data Diff catches this immediately.</p>"},{"location":"checks/data-diff-check/#2-save-time-and-reduce-stress","title":"2. Save Time and Reduce Stress","text":"<p>Instead of manually checking thousands of rows in spreadsheets, Data Diff does it automatically in seconds.</p>"},{"location":"checks/data-diff-check/#3-build-trust-in-your-data","title":"3. Build Trust in Your Data","text":"<p>When you present numbers to leadership or clients, you can confidently say, \"This data has been verified.\"</p>"},{"location":"checks/data-diff-check/#4-protect-your-business","title":"4. Protect Your Business","text":"<p>Wrong data can lead to:</p> <ul> <li>Incorrect invoices</li> <li>Bad business decisions</li> <li>Compliance issues</li> <li>Customer complaints</li> </ul> <p>Data Diff acts as your safety net.</p>"},{"location":"checks/data-diff-check/#real-life-example-online-retail-store","title":"Real-Life Example: Online Retail Store","text":"<p>Let me walk you through a complete, real-world scenario:</p>"},{"location":"checks/data-diff-check/#the-situation","title":"The Situation","text":"<p>Sunshine Electronics is an online store that sells gadgets. Every night at midnight, their system creates a backup copy of all the day's orders. This backup is used for:</p> <ul> <li>Creating daily sales reports</li> <li>Feeding data to their accounting system</li> <li>Analyzing customer trends</li> </ul>"},{"location":"checks/data-diff-check/#the-problem-they-had","title":"The Problem They Had","text":"<p>One morning, the Sales Manager noticed the daily report showed 1,247 orders, but the warehouse had shipped 1,250 packages. Where did 3 orders go?</p> <p>After investigating, they discovered:</p> <ul> <li>The backup system had a glitch</li> <li>Some orders placed between 11:58 PM and midnight weren't copied over</li> <li>This had been happening for weeks</li> <li>They had been under-reporting revenue and had incorrect inventory counts</li> </ul>"},{"location":"checks/data-diff-check/#the-solution-data-diff","title":"The Solution: Data Diff","text":"<p>They set up Data Diff to automatically compare their main orders database with the backup every morning.</p> <p>Here's what they compared:</p> <p>Original Orders Database:</p> Order ID Customer Name Product Amount Date 10001 Sarah Johnson Laptop $899 Jan 15, 2025 10002 Mike Chen Headphones $149 Jan 15, 2025 10003 Emily Davis Tablet $399 Jan 15, 2025 ... ... ... ... ... 10248 David Lee Phone Case $19 Jan 15, 2025 10249 Anna Brown USB Cable $12 Jan 15, 2025 10250 Tom Wilson Mouse $29 Jan 15, 2025 <p>Backup Orders Database:</p> Order ID Customer Name Product Amount Date 10001 Sarah Johnson Laptop $899 Jan 15, 2025 10002 Mike Chen Headphones $149 Jan 15, 2025 10003 Emily Davis Tablet $399 Jan 15, 2025 ... ... ... ... ... 10248 Missing Missing Missing Missing 10249 Missing Missing Missing Missing 10250 Missing Missing Missing Missing"},{"location":"checks/data-diff-check/#what-data-diff-discovered","title":"What Data Diff Discovered","text":"<p>ALERT GENERATED:</p> <p>DIFFERENCE DETECTED!</p> <ul> <li>Fields Affected: amount, order_id, product, order_date, customer_name</li> <li>Rule Applied: Data Diff</li> <li>Anomalous Records: 3</li> </ul> <p>Technical Output (from Qualytics):</p> <p>After running the Data Diff check, the system identified mismatched records between the Original Orders Database (Left) and the Backup Orders Database (Right).</p> Row Status order_id amount (Left \u2192 Right) order_date (Left \u2192 Right) customer_name (Left \u2192 Right) product (Left \u2192 Right) removed 10248 19.00 \u2192 missing 2025-01-15 \u2192 missing David Lee \u2192 missing Phone Case \u2192 missing removed 10249 12.00 \u2192 missing 2025-01-15 \u2192 missing Anna Brown \u2192 missing USB Cable \u2192 missing removed 10250 29.00 \u2192 missing 2025-01-15 \u2192 missing Tom Wilson \u2192 missing Mouse \u2192 missing <p></p>"},{"location":"checks/data-diff-check/#summary","title":"\ud83d\udd0d Summary","text":"<ul> <li>These three records exist in the Original Orders Database but are missing from the Backup Orders Database.  </li> <li>The \u201cremoved\u201d status means Data Diff detected entries that weren\u2019t found in the reference (right) table.  </li> <li>This confirms that some orders failed to copy during the backup process.</li> </ul>"},{"location":"checks/data-diff-check/#the-outcome","title":"The Outcome","text":"<p>Immediate Benefits:</p> <ul> <li>They fixed the backup system timing issue</li> <li>They recovered the missing orders data</li> <li>They corrected their sales reports</li> </ul> <p>Long-term Benefits:</p> <ul> <li>Now they get an automatic email every morning confirming data matches</li> <li>If there's ever a mismatch, they know within hours instead of weeks</li> <li>They prevented thousands of dollars in unreported revenue</li> <li>Their inventory tracking became accurate again</li> </ul>"},{"location":"checks/data-diff-check/#another-quick-example-healthcare-clinic","title":"Another Quick Example: Healthcare Clinic","text":"<p>City Health Clinic transfers patient appointment data from their scheduling system to their billing system every hour.</p> <p>They use Data Diff to check:</p> <ul> <li>Patient Name</li> <li>Appointment Date</li> <li>Doctor Assigned</li> <li>Service Type</li> <li>Insurance Information</li> </ul>"},{"location":"checks/data-diff-check/#before-correction-data-diff-caught-this","title":"\ud83d\udccb Before Correction (Data Diff Caught This)","text":"Field Scheduling System Billing System Patient Robert Martinez Robert Martinez Doctor Dr. Smith Dr. Smith Insurance Plan BlueCross Plan A BlueCross Plan B <p>The Insurance Plan code changed during transfer. Without Data Diff, the clinic would have billed the wrong insurer.</p>"},{"location":"checks/data-diff-check/#after-correction-fixed-data","title":"\u2705 After Correction (Fixed Data)","text":"Field Scheduling System Billing System Patient Robert Martinez Robert Martinez Doctor Dr. Smith Dr. Smith Insurance Plan BlueCross Plan A BlueCross Plan A <p>Info</p> <p>Data Diff caught the mismatch and the billing team corrected it before submitting the claim \u2014 avoiding claim rejection, payment delays, and extra work.</p>"},{"location":"checks/data-diff-check/#anomalies-detected-output-table","title":"\ud83e\udde9 Anomalies Detected \u2013 Output Table","text":"<p>The Data Diff check found a mismatch between the scheduling_system and billing_system datasets for one record. The issue was detected in the insurance_plan field for the patient Robert Martinez.</p> Row Status Patient Field Left (Scheduling System) Right (Billing System) Changed Robert Martinez insurance_plan BlueCross Plan A BlueCross Plan B <p></p>"},{"location":"checks/data-diff-check/#key-takeaways","title":"Key Takeaways","text":"<p>Data Diff is like having a careful proofreader who checks that when you copy important information, nothing goes wrong.</p> <p>It works automatically- you set it up once, and it keeps watching your data 24/7.</p> <p>It catches problems early- before they affect your reports, decisions, or customers.</p> <p>It gives you peace of mind- you can trust that your backup, reports, and transferred data are accurate.</p>"},{"location":"checks/data-diff-check/#when-should-you-use-data-diff","title":"When Should You Use Data Diff?","text":"<p>Use Data Diff whenever you:</p> <ul> <li>Copy data from one place to another</li> <li>Create backups of important information</li> <li>Generate reports from multiple sources</li> <li>Transfer data between different systems</li> <li>Move data to the cloud</li> <li>Export data to partners or vendors</li> </ul>"},{"location":"checks/data-diff-check/#field-scope","title":"Field Scope","text":"<p>Multi: The rule evaluates multiple specified fields.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/data-diff-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/data-diff-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the datastore and table/file where the reference data for the targeted fields is located for comparison.</p> Name Description Row Identifiers The list of fields defining the compound key to identify rows in the comparison analysis. Datastore The source datastore where the reference data for the targeted field(s) is located. Table/file The table, view or file in the source datastore that should serve as the reference for comparison. Comparators Specifies how variations are handled, allowing for slight deviations within a defined margin of error. <p>Info</p> <p>The <code>DataDiff</code> rule supports editing of <code>Row Identifiers</code> and <code>Passthrough Fields</code>, allowing for more tailored configuration.</p> <p>Details</p> <p>This optional input allows row comparison analysis by defining a list of fields as row identifiers, it enables a more detailed comparison between tables/files, where each row compound key is used to identify its presence or absence in the reference table/file compared to the target table/file.  Qualytics can inform if the row exists or not and distinguish which field values differ in each row present in the reference table/file, helping to determine if it is a data diff.</p> <p>Info</p> <p>Anomalies produced by a <code>DataDiff</code> quality check making use of <code>Row Identifiers</code> have their source records presented in a different visualization.  See more at: Comparison Source Records</p> <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"checks/data-diff-check/#row-identifiers","title":"Row Identifiers","text":""},{"location":"checks/data-diff-check/#comparators","title":"Comparators","text":""},{"location":"checks/data-diff-check/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"checks/data-diff-check/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"checks/data-diff-check/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"checks/data-diff-check/#duration","title":"Duration","text":"<p>Duration comparators support time-based comparisons, allowing for flexibility in how duration differences are managed. This flexibility is crucial for datasets where time measurements are essential but can vary slightly.</p>"},{"location":"checks/data-diff-check/#unit","title":"Unit","text":"<p>The unit of time you select determines how granular the comparison is:</p> <ul> <li>Millis: Measures time in milliseconds, ideal for high-precision needs.</li> <li>Seconds: Suitable for most general purposes where precision is important but doesn't need to be to the millisecond.</li> <li>Days: Best for longer durations.</li> </ul>"},{"location":"checks/data-diff-check/#value","title":"Value","text":"<p>Value sets the maximum acceptable difference in time to consider two values as equal. It serves to define the margin of error, accommodating small discrepancies that naturally occur over time.</p> Illustration using Duration Comparator Unit Value A Value B Difference Threshold Are equal? Millis 500 ms 520 ms 20 ms 25 ms True Seconds 30 sec 31 sec 1 sec 2 sec True Days 5 days 7 days 2 days 1 day False Millis 1000 ms 1040 ms 40 ms 25 ms False Seconds 45 sec 48 sec 3 sec 2 sec False"},{"location":"checks/data-diff-check/#string","title":"String","text":"<p>String comparators facilitate comparisons of textual data by allowing variations in spacing. This capability is essential for ensuring data consistency, particularly where minor text inconsistencies may occur.</p>"},{"location":"checks/data-diff-check/#ignore-whitespace","title":"Ignore Whitespace","text":"<p>When enabled, this setting allows the comparator to ignore differences in whitespace. This means sequences of whitespace are collapsed into a single space, and any leading or trailing spaces are removed. This can be particularly useful in environments where data entry may vary in formatting but where those differences are not relevant to the data's integrity.</p> Illustration <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined string comparison to <code>ignore whitespace</code> as <code>True</code>.</p> Value A Value B Are equal? Has whitespace? <code>Leonidas</code> <code>Leonidas</code> True No <code>Beth</code> <code>Beth</code> True Yes <code>Ana</code> <code>Anna</code> False Yes <code>Joe</code> <code>Joel</code> False No"},{"location":"checks/data-diff-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/data-diff-check/#example","title":"Example","text":"<p>Scenario: Consider that the fields N_NATIONKEY and N_NATIONNAME in the NATION table need to be compared with a backup database for data validation purposes. The data engineering team wants to ensure that both fields in the backup accurately match the original.</p> <p>Objective: Ensure that N_NATIONKEY and N_NATIONNAME from the NATION table match the data in the NATION_BACKUP table.</p> <p>Sample Data from NATION</p> N_NATIONKEY N_NATIONNAME 1 Australia 2 United States 3 Uruguay <p>Reference Sample Data from NATION_BACKUP</p> N_NATIONKEY N_NATIONNAME 1 Australia 2 USA 3 Uruguay Payload example <pre><code>{\n    \"description\": \"Ensure that N_NATIONKEY and N_NATIONNAME from the NATION table match the data in the NATION_BACKUP table\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"ref_container_id\": {ref_container_id},\n        \"ref_datastore_id\": {ref_datastore_id}\n    },\n    \"tags\": [],\n    \"fields\": [\"N_NATIONKEY\", \"N_NATIONNAME\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"dataDiff\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>The datasets representing the fields <code>N_NATIONKEY</code> and <code>N_NATIONNAME</code> in the original and the reference data are not completely identical, indicating a possible discrepancy in the data or an unintended change.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Original Data]\nB --&gt; C[Retrieve Reference Data]\nC --&gt; D{Do datasets match for both fields?}\nD --&gt;|Yes| E[End]\nD --&gt;|No| F[Mark as Anomalous]\nF --&gt; E</code></pre> <pre><code>-- An illustrative SQL query comparing original to reference data for both fields.\nselect\n    orig.n_nationkey as original_key,\n    orig.n_nationname as original_name,\n    ref.n_nationkey as reference_key,\n    ref.n_nationname as reference_name\nfrom nation as orig\nleft join nation_backup as ref on orig.n_nationkey = ref.n_nationkey\nwhere\n    orig.n_nationname &lt;&gt; ref.n_nationname\nor\n    orig.n_nationkey &lt;&gt; ref.n_nationkey\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>There is 1 record that differs between <code>NATION_BACKUP</code> (3 records) and <code>NATION</code> (3 records) in <code>&lt;datastore_name&gt;</code></p>"},{"location":"checks/distinct-count-check/","title":"Distinct Count","text":""},{"location":"checks/distinct-count-check/#definition","title":"Definition","text":"<p>Asserts on the approximate count distinct of the given column.</p>"},{"location":"checks/distinct-count-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/distinct-count-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/distinct-count-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the distinct count expectation for the values in the field.</p> Name Description Comparison Specifies how the distinct count should be compared against the value. Value The value to compare the distinct count against. <p>Details</p> <p>Comparison is a required property that accepts the following values:</p> Comparison Description <code>eq</code> Equal To - Assert distinct count equals the value <code>gt</code> Greater Than - Assert distinct count is greater than the value <code>gte</code> Greater Than Or Equal To - Assert distinct count is \u2265 value <code>lt</code> Less Than - Assert distinct count is less than the value <code>lte</code> Less Than Or Equal To - Assert distinct count is \u2264 value"},{"location":"checks/distinct-count-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/distinct-count-check/#example","title":"Example","text":"<p>Objective: Ensure that there are exactly 3 distinct O_ORDERSTATUS in the ORDERS table: 'O' (Open), 'F' (Finished), and 'P' (In Progress).</p> <p>Sample Data</p> O_ORDERKEY O_ORDERSTATUS 1 O 2 F ... ... 20 X 21 O Payload example <pre><code>{\n    \"description\": \"Ensure that there are exactly 3 distinct O_ORDERSTATUS in the ORDERS table: 'O' (Open), 'F' (Finished), and 'P' (In Progress)\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"comparison\": \"eq\",\n        \"value\": 3\n    },\n    \"tags\": [],\n    \"fields\": [\"O_ORDERSTATUS\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"distinctCount\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the rule is violated because the <code>O_ORDERSTATUS</code> contains 4 distinct values, which is not equal to 3. The expected values were 'O' (Open), 'F' (Finished), and 'P' (In Progress), but an unexpected value 'X' was found.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve all O_ORDERSTATUS entries and count distinct values]\nB --&gt; C{Does distinct count satisfy comparison condition?}\nC --&gt;|No| D[Mark as Anomalous]\nC --&gt;|Yes| E[End]</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\n-- Using comparison = 'eq' (equal to)\nselect\n    count(distinct o_orderstatus)\nfrom orders\nhaving count(distinct o_orderstatus) &lt;&gt; 3\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly (eq)</p> <p>The distinct count of values in <code>O_ORDERSTATUS</code> is 4 which is not equal to 3.</p> <p>Shape Anomaly (gte)</p> <p>The distinct count of values in <code>O_ORDERSTATUS</code> is 2 which is not greater than or equal to 3.</p> <p>Shape Anomaly (lte)</p> <p>The distinct count of values in <code>O_ORDERSTATUS</code> is 5 which is not less than or equal to 3.</p>"},{"location":"checks/entity-resolution/","title":"Entity Resolution","text":""},{"location":"checks/entity-resolution/#definition","title":"Definition","text":"<p>Asserts that every distinct entity is appropriately represented once and only once</p>"},{"location":"checks/entity-resolution/#in-depth-overview","title":"In-Depth Overview","text":"<p>This check performs automated entity name clustering to identify entities with similar names that likely represent the same entity. It then assigns each cluster a unique entity identifier and asserts that every row with the same  entity identifier shares the same value for the designated <code>distinction field</code></p>"},{"location":"checks/entity-resolution/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/entity-resolution/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/entity-resolution/#specific-properties","title":"Specific Properties","text":"Name Description Distinction Field The field that must hold a distinct value for every distinct entity Pair Substrings Considers entities a match if one entity is part of the other Pair Homophones Considers entities a match if they sound alike, even if spelled differently Spelling Similarity The minimum similarity required for clustering two entity names"},{"location":"checks/entity-resolution/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/entity-resolution/#example","title":"Example","text":"<p>Objective: If you have a <code>businesses</code> table with an <code>id</code> field and a <code>name</code> field, this check can be configured to resolve <code>name</code> and use <code>id</code> as the <code>distinction field</code>.  During each scan, similar names will be grouped and assigned the same <code>entity identifier</code> and any rows that share the same <code>entity identifier</code> but have different values for <code>id</code> will be identified as anomalies.</p> <p>Sample Data</p> BUSINESS_ID BUSINESS_NAME 1 ACME Boxing 2 Frank's Flowers 3 ACME Boxes Payload example <pre><code>{\n    \"description\": \"Ensure a `businesses` table with an `BUSINESS_ID` field and a `BUSINESS_NAME` field shares the same `entity identifier`\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"distinct_field_name\":\"BUSINESS_ID\",\n        \"pair_substrings\":true,\n        \"pair_homophones\":true,\n        \"spelling_similarity_threshold\":0.6\n    },\n    \"tags\": [],\n    \"fields\": [\"BUSINESS_NAME\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"entityResolution\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>BUSINESS_ID</code> 1 and 3 do not satisfy the rule because their <code>BUSINESS_NAME</code> values will be marked as similar yet they do not share the same <code>BUSINESS_ID</code> </p> Flowchart <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Original Data]\nB --&gt; C{Which entities are similar?}\nC --&gt; D[Assign each record an entity identifier]\nD --&gt; E[Cluster records by entity identifier]\nE --&gt; F{Do records with same&lt;br/&gt;entity identifier share the&lt;br/&gt;same distinction field value?}\nF --&gt;|Yes| I[End]\nF --&gt;|No| H[Mark as Anomalous]\nH --&gt; I</code></pre>"},{"location":"checks/equal-to-check/","title":"Equal To","text":""},{"location":"checks/equal-to-check/#definition","title":"Definition","text":"<p>Asserts that all of the selected fields' equal a value.</p>"},{"location":"checks/equal-to-check/#field-scope","title":"Field Scope","text":"<p>Multi: The rule evaluates multiple specified fields.</p> <p>Accepted Types</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/equal-to-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/equal-to-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the field to compare for equality with the selected field.</p> Name Description Value Specifies the value a field should be equal to. Comparators Specifies how variations are handled, allowing for slight deviations within a defined margin of error. <p>Details</p> <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"checks/equal-to-check/#comparators","title":"Comparators","text":""},{"location":"checks/equal-to-check/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"checks/equal-to-check/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"checks/equal-to-check/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"checks/equal-to-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/equal-to-check/#example","title":"Example","text":"<p>Objective: Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table is equal to a value of 10.</p> <p>Sample Data</p> L_ORDERKEY L_LINENUMBER L_QUANTITY 1 1 10 2 2 5 3 3 10 4 4 8 Payload example <pre><code>{\n    \"description\": \"Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table is equal to a value of 10\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\":\"10\",\n        \"inclusive\":true\n    },\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"equalTo\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 2 and 4 do not satisfy the rule because their <code>L_QUANTITY</code> values are below the specified minimum value of 10.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_QUANTITY]\nB --&gt; C{Is L_QUANTITY = 10?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_linenumber,\n    l_quantity\nfrom lineitem \nwhere\n    l_quantity &lt; 10;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>Not all of the fields equal are equal to the value of <code>10</code></p> <p>Shape Anomaly</p> <p>In <code>L_QUANTITY</code>, 2 of 4 filtered records (4) are not equal to the value of <code>10</code></p>"},{"location":"checks/equal-to-field-check/","title":"Equal To Field","text":""},{"location":"checks/equal-to-field-check/#definition","title":"Definition","text":"<p>Asserts that a field is equal to another field.</p>"},{"location":"checks/equal-to-field-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code>"},{"location":"checks/equal-to-field-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/equal-to-field-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the field to compare for equality with the selected field.</p> Name Description Field to compare The field name whose values should match those of the selected field. Comparators Specifies how variations are handled, allowing for slight deviations within a defined margin of error. <p>Details</p> <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"checks/equal-to-field-check/#comparators","title":"Comparators","text":""},{"location":"checks/equal-to-field-check/#string","title":"String","text":"<p>String comparators facilitate comparisons of textual data by allowing variations in spacing. This capability is essential for ensuring data consistency, particularly where minor text inconsistencies may occur.</p>"},{"location":"checks/equal-to-field-check/#ignore-whitespace","title":"Ignore Whitespace","text":"<p>When enabled, this setting allows the comparator to ignore differences in whitespace. This means sequences of whitespace are collapsed into a single space, and any leading or trailing spaces are removed. This can be particularly useful in environments where data entry may vary in formatting but where those differences are not relevant to the data's integrity.</p> Illustration <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined string comparison to <code>ignore whitespace</code> as <code>True</code>.</p> Value A Value B Are equal? Has whitespace? <code>Leonidas</code> <code>Leonidas</code> True No <code>Beth</code> <code>Beth</code> True Yes <code>Ana</code> <code>Anna</code> False Yes <code>Joe</code> <code>Joel</code> False No"},{"location":"checks/equal-to-field-check/#duration","title":"Duration","text":"<p>Duration comparators support time-based comparisons, allowing for flexibility in how duration differences are managed. This flexibility is crucial for datasets where time measurements are essential but can vary slightly.</p>"},{"location":"checks/equal-to-field-check/#unit","title":"Unit","text":"<p>The unit of time you select determines how granular the comparison is:</p> <ul> <li>Millis: Measures time in milliseconds, ideal for high-precision needs.</li> <li>Seconds: Suitable for most general purposes where precision is important but doesn't need to be to the millisecond.</li> <li>Days: Best for longer durations.</li> </ul>"},{"location":"checks/equal-to-field-check/#value","title":"Value","text":"<p>Value sets the maximum acceptable difference in time to consider two values as equal. It serves to define the margin of error, accommodating small discrepancies that naturally occur over time.</p> Illustration using Duration Comparator Unit Value A Value B Difference Threshold Are equal? Millis 500 ms 520 ms 20 ms 25 ms True Seconds 30 sec 31 sec 1 sec 2 sec True Days 5 days 7 days 2 days 1 day False Millis 1000 ms 1040 ms 40 ms 25 ms False Seconds 45 sec 48 sec 3 sec 2 sec False"},{"location":"checks/equal-to-field-check/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"checks/equal-to-field-check/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"checks/equal-to-field-check/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"checks/equal-to-field-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/equal-to-field-check/#example","title":"Example","text":"<p>Scenario: An e-commerce platform sells digital products. The shipping date (when the digital product link is sent) should always be the same as the delivery date (when the customer acknowledges receiving the product).</p> <p>Objective: Ensure that the O_SHIPDATE in the ORDERS table matches its delivery date O_DELIVERYDATE.</p> <p>Sample Data</p> O_ORDERKEY O_SHIPDATE O_DELIVERYDATE 1 1998-01-04 1998-01-04 2 1998-01-14 1998-01-15 3 1998-01-12 1998-01-12 Payload example <pre><code>{\n    \"description\": \"Ensure that the O_SHIPDATE in the ORDERS table matches its delivery date O_DELIVERYDATE\",\n    \"coverage\": 1,\n    \"properties\": {\"field_name\":\"O_DELIVERYDATE\", \"inclusive\":false},\n    \"tags\": [],\n    \"fields\": [\"O_SHIPDATE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"equalToField\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>O_ORDERKEY</code> 2 does not satisfy the rule because its <code>O_SHIPDATE</code> of 1998-01-14 does not match the <code>O_DELIVERYDATE</code> of 1998-01-15.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_SHIPDATE and O_DELIVERYDATE]\nB --&gt; C{Is O_SHIPDATE = O_DELIVERYDATE?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    o_orderkey,\n    o_shipdate,\n    o_deliverydate\nfrom orders \nwhere\n    o_shipdate != o_deliverydate\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>O_SHIPDATE</code> value of 1998-01-14 is not equal to the value of O_DELIVERYDATE which is 1998-01-15.</p> <p>Shape Anomaly</p> <p>In <code>O_SHIPDATE</code>, 33.333% of the filtered fields are not equal.</p>"},{"location":"checks/exists-in-check/","title":"Exists In","text":""},{"location":"checks/exists-in-check/#definition","title":"Definition","text":"<p>Asserts that values assigned to a field exist as values in another field.</p>"},{"location":"checks/exists-in-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>ExistsIn</code> rule allows you to cross-validate data between different sources, whether it\u2019s object storage systems or databases.</p> <p>Traditionally, databases might utilize foreign key constraints (if available) to enforce data integrity between related tables. The <code>ExistsIn</code> rule extends this concept in two powerful ways:</p> <ol> <li>Cross-System Integrity: it allows for integrity checks to span across different databases or even entirely separate systems. This is particularly advantageous in scenarios where data sources are fragmented across diverse platforms.</li> <li>Flexible Data Formats: Beyond just databases, this rule can validate values against various data formats, such as ensuring values in a file align with those in a table.</li> </ol> <p>These enhancements enable businesses to maintain data integrity even in complex, multi-system environments.</p>"},{"location":"checks/exists-in-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/exists-in-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/exists-in-check/#specific-properties","title":"Specific Properties","text":"<p>Define the datastore, table/file, and field where the rule should look for matching values.</p> Name Description Datastore The source datastore where the profile of the reference field is located. Table/file The profile (e.g. table, view or file) containing the reference field. Field The field name whose values should match those of the selected field."},{"location":"checks/exists-in-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/exists-in-check/#example","title":"Example","text":"<p>Objective: Ensure that all NATION_NAME entries in the NATION table match entries under the COUNTRY_NAME column in an external lookup file listing official country names.</p> <p>Sample Data</p> N_NATIONKEY N_NATIONNAME 1 Algeria 2 Argentina 3 Atlantida Payload example <pre><code>{\n    \"description\": \"Ensure that all NATION_NAME entries in the NATION table match entries under the COUNTRY_NAME column in an external lookup file listing official country names\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"field_name\":\"COUNTRY_NAME\",\n        \"ref_container_id\": {ref_container_id},\n        \"ref_datastore_id\": {ref_datastore_id}\n    },\n    \"tags\": [],\n    \"fields\": [\"NATION_NAME\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"existsIn\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Lookup File Sample</p> COUNTRY_NAME Algeria Argentina Brazil Canada ... Zimbabwe <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>N_NATIONKEY</code> 3 does not satisfy the rule because the <code>N_NATIONNAME</code> \"Atlantida\" does not match any <code>COUNTRY_NAME</code> in the official country names lookup file.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve COUNTRY_NAME]\nB --&gt; C[Retrieve N_NATIONNAME]\nC --&gt; D{Does N_NATIONNAME exists in COUNTRY_NAME?}\nD --&gt;|Yes| E[Move to Next Record/End]\nD --&gt;|No| F[Mark as Anomalous]\nF --&gt; E</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    n_nationkey\n    , n_nationname\nfrom nation \nwhere\n    n_nationname not in ('Algeria', 'Argentina', ... /* other valid countries */)\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>N_NATIONNAME</code> value of <code>'Atlantida'</code> does not exist in <code>COUNTRY_NAME</code>.</p> <p>Shape Anomaly</p> <p>In <code>N_NATIONNAME</code>, 33.333% of 3 filtered records (1) do not match any <code>COUNTRY_NAME</code>.</p>"},{"location":"checks/expected-schema-check/","title":"Expected Schema","text":""},{"location":"checks/expected-schema-check/#definition","title":"Definition","text":"<p>Asserts that all of the selected fields must be present in the datastore.</p>"},{"location":"checks/expected-schema-check/#behavior","title":"Behavior","text":"<p>The expected schema is the first check to be tested during a scan operation. If it fails, the scan operation will result as <code>Failure</code> with the following message:</p> <p><code>&lt;container-name&gt;</code>: Aborted because schema check anomalies were identified.</p>"},{"location":"checks/expected-schema-check/#general-properties","title":"General Properties","text":"Details Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions"},{"location":"checks/expected-schema-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the fields that must be present in the schema, and determine if a schema change caused by additional fields should fail or pass the assertion.</p> Name Description Fields List of fields that must be presented in the schema. Allow other fields If true, then new fields are allowed to be presented in the schema. Otherwise, the assertion will be stricter."},{"location":"checks/expected-schema-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/expected-schema-check/#example","title":"Example","text":"<p>Objective: Ensure that expected fields such as L_ORDERKEY, L_PARTKEY, and L_SUPPKEY are always present in the LINEITEM table.</p> <p>Sample Data</p> Valid FIELD_NAME FIELD_TYPE L_ORDERKEY NUMBER L_PARTKEY NUMBER L_SUPPKEY NUMBER L_LINENUMBER NUMBER L_QUANTITY NUMBER L_EXTENDEDPRICE NUMBER ... ... Invalid <p>L_SUPPKEY is missing from the schema</p> FIELD_NAME FIELD_TYPE L_ORDERKEY NUMBER L_PARTKEY NUMBER L_LINENUMBER NUMBER L_QUANTITY NUMBER L_EXTENDEDPRICE NUMBER ... ... Payload example <pre><code>{\n    \"description\": \"Ensure that expected fields such as L_ORDERKEY, L_PARTKEY, and L_SUPPKEY are always present in the LINEITEM table\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"allow_other_fields\":false,\n        \"list\":[\"L_ORDERKEY\",\"L_PARTKEY\",\"L_SUPPKEY\"]\n    },\n    \"tags\": [],\n    \"fields\": null,\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"expectedSchema\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>Among the presented sample schemas, the second one is missing one of the expected schema. Only the first schema has the correct expected schema.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B{Check for Field Presence}\nB -.-&gt;|Field is missing| C[Mark as Shape Anomaly]\nB -.-&gt;|All fields present| D[End]</code></pre> <pre><code>-- An illustrative SQL query to check the existence of columns.\nselect \n    column_name \nfrom \n    information_schema.columns \nwhere \n    table_name = 'LINEITEM' and \n    column_name in ('L_ORDERKEY', 'L_PARTKEY', 'L_SUPPKEY');\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>The required fields (<code>L_SUPPKEY</code>) are not present.</p>"},{"location":"checks/expected-values-check/","title":"Expected Values","text":""},{"location":"checks/expected-values-check/#definition","title":"Definition","text":"<p>Asserts that values are contained within a list of expected values.</p> <p>Info</p> <p>Visual warnings for spacing issues are available across all forms where values are entered. When a value includes leading or trailing spaces, the system automatically highlights it inside a warning-colored chip and displays a tooltip message indicating the issue in real time \u2014 for example:</p> <p>\u201cThe following value has leading or trailing spaces: \u2018ship \u2019\u201d</p> <p></p>"},{"location":"checks/expected-values-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/expected-values-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/expected-values-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the list of expected values for the data in the field.</p> Name Description List A predefined set of values against which the data is validated."},{"location":"checks/expected-values-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/expected-values-check/#example","title":"Example","text":"<p>Objective: Ensure that all O_ORDERSTATUS entries in the ORDERS table only contain expected order statuses: \"O\", \"F\", and \"P\".</p> <p>Sample Data</p> O_ORDERKEY O_ORDERSTATUS 1 F 2 O 3 P 4 X Payload example <pre><code>{\n    \"description\": \"Ensure that all O_ORDERSTATUS entries in the ORDERS table only contain expected order statuses: \"O\", \"F\", and \"P\"\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"list\":[\"O\",\"F\",\"P\"]\n    },\n    \"tags\": [],\n    \"fields\": [\"O_ORDERSTATUS\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"expectedValues\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>O_ORDERKEY</code> 4 does not satisfy the rule because the <code>O_ORDERSTATUS</code> \"X\" is not on the list of expected order statuses (\"O\", \"F\", \"P\").</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_ORDERSTATUS]\nB --&gt; C{Is O_ORDERSTATUS in 'O', 'F', 'P'?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    o_orderkey\n    , o_orderstatus\nfrom orders \nwhere\n    o_orderstatus not in ('O', 'F', 'P')\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>O_ORDERSTATUS</code> value of <code>'X'</code> does not appear in the list of expected values</p> <p>Shape Anomaly</p> <p>In <code>O_ORDERSTATUS</code>, 25.000% of 4 filtered records (1) do not appear in the list of expected values</p>"},{"location":"checks/export-check-templates/","title":"Export Check Templates","text":"<p>You can export check templates to easily share or reuse your quality check settings across different systems or projects. This saves time by eliminating the need to recreate the same checks repeatedly and ensures that your quality standards are consistently applied. Exporting templates helps maintain accuracy and efficiency in managing data quality across various environments.</p> <p>Let\u2019s get started \ud83d\ude80</p> <p>Step 1:  Log in to your Qualytics account and click the \u201cLibrary\u201d button on the left side panel of the interface.</p> <p></p> <p>Step 2: Click on the \u201cExport Check Template\u201d button located in the top right corner.</p> <p></p> <p>Step 3: A modal window titled \u201cExport Check Templates\u201d will appear, where you have to select the enrichment store to which the check templates will be exported.</p> <p></p> <p>Step 4:  Once you have selected the enrichment store, click on the \u201cExport\u201d button</p> <p></p> <p>After clicking \u201cExport,\u201d the process starts, and a message will confirm that the metadata will be available in your Enrichment Datastore shortly.</p> <p></p>"},{"location":"checks/export-check-templates/#review-exported-check-templates","title":"Review Exported Check Templates","text":"<p>Step 1: Once the checks have been exported, navigate to the \u201cEnrichment Datastores\u201d located on the left menu.</p> <p></p> <p>Step 2: In the \u201cEnrichment Datastores\u201d section, select the datastore where you exported the checks templates. The exported check templates will now be visible in the selected datastore.</p> <p>When you export check templates, you can reuse them for other datastores, share them with teams, or save them as a backup. Once exported, the templates can be imported and customized to fit different datasets, making them versatile and easy to adapt.</p> <p></p> <p>You also have the option to download them as a CSV file, allowing you to share or store them for future use.</p> <p></p>"},{"location":"checks/field-count-check/","title":"Field Count","text":""},{"location":"checks/field-count-check/#definition","title":"Definition","text":"<p>Asserts that there must be exactly a specified number of fields.</p>"},{"location":"checks/field-count-check/#general-properties","title":"General Properties","text":"Details Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions"},{"location":"checks/field-count-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the exact number of fields expected in the profile.</p> Name Description Number of Fields The exact number of fields that should be present in the profile."},{"location":"checks/field-count-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/field-count-check/#example","title":"Example","text":"<p>Objective: Ensure that the ORDERS profile contains exactly 9 fields.</p> <p>Sample Profile</p> Valid FIELD_NAME FIELD_TYPE O_ORDERKEY STRING O_CUSTKEY STRING O_ORDERSTATUS STRING O_TOTALPRICE FLOAT O_ORDERDATE DATE O_ORDERPRIORITY STRING O_CLERK STRING O_SHIPPRIORITY STRING O_COMMENT STRING Invalid <p>count (8) less than expected (9)</p> FIELD_NAME FIELD_TYPE O_ORDERKEY STRING O_CUSTKEY STRING O_ORDERSTATUS STRING O_TOTALPRICE FLOAT O_ORDERDATE DATE O_ORDERPRIORITY STRING O_CLERK STRING O_COMMENT STRING <p>count (10) greater than expected (9)</p> FIELD_NAME FIELD_TYPE O_ORDERKEY STRING O_CUSTKEY STRING O_ORDERSTATUS STRING O_TOTALPRICE FLOAT O_ORDERDATE DATE O_ORDERPRIORITY STRING O_CLERK STRING O_COMMENT STRING EXTRA_FIELD UNKNOWN Payload example <pre><code>{\n    \"description\": \"Ensure that the ORDERS profile contains exactly 9 fields\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\": 9\n    },\n    \"tags\": [],\n    \"fields\": null,\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"fieldCount\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>Among the presented sample profiles, the second one is missing a field, while the third one contains an extra field. Only the first profile has the correct number of fields, which is 9.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Profile Fields]\nB --&gt; C{Does the profile have exactly 9 fields?}\nC --&gt;|Yes| D[End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query related to the rule.\nselect\n    table_name, count(column_name) as number_of_fields\nfrom information_schema.columns \nwhere table_name = 'orders'\ngroup by table_name\nhaving count(column_name) &lt;&gt; 9\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>In <code>ORDERS</code>, the field count is not <code>9</code>.</p>"},{"location":"checks/freshness-check/","title":"Freshness Check","text":"<p>A Freshness Check ensures data stays up-to-date by monitoring its last update time. It prevents stale data from impacting reports and dashboards while detecting outdated information early. By setting a maximum age threshold, it helps identify pipeline failures and ensures accurate, real-time analytics for reliable business insights.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"checks/freshness-check/#importance-of-freshness-check","title":"Importance of Freshness Check","text":"<p>A Freshness Check ensures data is always up-to-date for real-time analytics. It prevents stale data from affecting reports and dashboards while detecting pipeline failures early.</p> <ul> <li> <p>Keeps data up-to-date for real-time analytics.</p> </li> <li> <p>Prevents stale data from affecting reports and dashboards.</p> </li> <li> <p>Detects data pipeline failures early.</p> </li> </ul>"},{"location":"checks/freshness-check/#how-it-works","title":"How It Works","text":"<p>A Freshness Check monitors when a dataset (table, file, or view) was last updated. If the data is older than the allowed limit, the system triggers an alert.</p>"},{"location":"checks/freshness-check/#process-flow","title":"Process Flow","text":"<ol> <li> <p>Data Update: System records the last update timestamp.</p> </li> <li> <p>Threshold Definition: A Maximum Age (e.g., 1 hour, 24 hours) is set.</p> </li> <li> <p>Scan: The system checks if the data is within the allowed time.</p> </li> <li> <p>Result Evaluation:</p> </li> <li> <p>Pass \u2192 Data is updated within the allowed time.</p> </li> <li> <p>Fail \u2192 Data is older than the limit, triggering an alert.  </p> </li> </ol>"},{"location":"checks/freshness-check/#how-freshness-is-measured-across-platforms","title":"How Freshness is Measured Across Platforms","text":"<p>Qualytics performs volumetric and freshness measurements differently depending on the data platform and container type (table, view, or computed).</p>"},{"location":"checks/freshness-check/#snowflake","title":"Snowflake","text":""},{"location":"checks/freshness-check/#tables","title":"Tables","text":"<p>Freshness and volumetrics are retrieved directly from the <code>INFORMATION_SCHEMA</code>. (System-level metadata is used \u2014 not the Incremental Field.)</p>"},{"location":"checks/freshness-check/#views-and-computed-objects","title":"Views and Computed Objects","text":"<ul> <li> <p>Volumetrics: <code>SELECT COUNT(*) FROM &lt;object&gt;</code></p> </li> <li> <p>Freshness: <code>SELECT MAX(&lt;incremental_identifier&gt;) FROM &lt;object&gt;</code> (Here, the Incremental Field defined in Table Settings determines freshness.)</p> </li> </ul>"},{"location":"checks/freshness-check/#oracle","title":"Oracle","text":""},{"location":"checks/freshness-check/#tables_1","title":"Tables","text":"<ul> <li> <p>Volumetrics: Retrieved from the <code>information_schema</code></p> </li> <li> <p>Freshness: Determined using <code>SELECT MAX(&lt;incremental_identifier&gt;)</code></p> </li> </ul>"},{"location":"checks/freshness-check/#views-and-computed-objects_1","title":"Views and Computed Objects","text":"<ul> <li> <p>Volumetrics: <code>SELECT COUNT(*) FROM &lt;object&gt;</code></p> </li> <li> <p>Freshness: <code>SELECT MAX(&lt;incremental_identifier&gt;) FROM &lt;object&gt;</code></p> </li> </ul>"},{"location":"checks/freshness-check/#s3","title":"S3","text":"<ul> <li> <p>Freshness: Based on the file modification time.</p> </li> <li> <p>Volumetrics: Depends on file format.</p> </li> <li>CSV, Excel \u2192 file load + row count.</li> <li>Delta or structured formats \u2192 row counts extracted from metadata.</li> </ul>"},{"location":"checks/freshness-check/#bigquery","title":"BigQuery","text":"<p>Same behavior as Snowflake:</p> <ul> <li>Tables: Retrieved from <code>INFORMATION_SCHEMA</code>.</li> <li>Views/Computed: <code>SELECT COUNT(*)</code> and <code>SELECT MAX(&lt;incremental_identifier&gt;)</code>.</li> </ul> <p>Note</p> <ul> <li>For regular tables, freshness is evaluated from <code>INFORMATION_SCHEMA</code> \u2014 the configured Incremental Field is not used.</li> <li>For views or computed tables, freshness is derived from the Incremental Field defined in Table Settings.</li> </ul>"},{"location":"checks/freshness-check/#configuring-freshness-check","title":"Configuring Freshness Check","text":"<p>Step 1: Log in into your Qualytics account and select the datastore from the left menu on which you want to add a volumetric check.</p> <p></p> <p>Step 2: Click the Add button and select Checks.</p> <p></p> <p>Step 3: A modal window appears. Enter the required details to configure the Freshness Check.</p> <p></p> <p>Step 4: Enter the details to configure the volumetric check:</p> No. Field Description 1. Rule Type Select the Freshness Rule type from the dropdown. 2. Table Select the table for the rule to apply. 3. Unit Select time unit (Hours, Minutes, Days) for freshness measurement. 4. Maximum Age Set the time limit for data freshness. If exceeded, the check fails. 5. Description Enter a description for the check. 6. Tag Add tags for categorizing the check. 7. Additional Metadata Add custom metadata for additional details. <p></p> <p>Step 5: After completing all the check details, click on the \"Validate\" button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct. It ensures that the check will work as expected by running it against the data without committing any changes.</p> <p></p> <p>If the validation is successful, a green message will appear saying \"Validation Successful\".</p> <p></p> <p>Step 6: Once you have a successful validation, click the \"Save\" button.</p> <p></p> <p>After clicking on the \u201cSave\u201d button your check is successfully created and a success flash message will appear saying \u201cCheck successfully created\u201d.</p> <p></p>"},{"location":"checks/freshness-check/#example","title":"Example","text":"<p>A company needs hourly updates on sales data to ensure real-time reports. A Freshness Check is set up with a 1-hour threshold.</p>"},{"location":"checks/freshness-check/#before-running-the-check-data-is-fresh","title":"Before Running the Check (Data is Fresh)","text":"No. Order ID Customer Amount ($) Last Updated 01 12345 John Doe 49.99 10:30 AM 02 12346 Jane Smith 89.50 10:35 AM <ul> <li> <p>Current Time: <code>11:00 AM</code></p> </li> <li> <p>Threshold: 1 Hour</p> </li> <li> <p>Pass (Data is fresh)</p> </li> </ul>"},{"location":"checks/freshness-check/#when-freshness-check-fails-data-is-stale","title":"When Freshness Check Fails (Data is Stale)","text":"No. Order ID Customer Amount($) Last Updated 01 12345 John Doe 49.99 09:30 AM 02 12346 Jane Smith 89.50 09:35 AM <ul> <li> <p>Current Time: <code>11:00 AM.</code></p> </li> <li> <p>Threshold: 1 Hour.</p> </li> <li> <p>Fail (Data is older than 1 hour).</p> </li> </ul>"},{"location":"checks/greater-than-check/","title":"Greater Than","text":""},{"location":"checks/greater-than-check/#definition","title":"Definition","text":"<p>Asserts that the field is a number greater than (or equal to) a value.</p>"},{"location":"checks/greater-than-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/greater-than-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/greater-than-check/#specific-properties","title":"Specific Properties","text":"<p>Allows specifying a numeric value that acts as the threshold.</p> Name Description Value The number to use as the base comparison. Inclusive If true, the comparison will also allow values equal to the threshold. Otherwise, it's exclusive. Comparators Specifies how variations are handled, allowing for slight deviations within a defined margin of error. <p>Details</p> <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"checks/greater-than-check/#comparators","title":"Comparators","text":""},{"location":"checks/greater-than-check/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"checks/greater-than-check/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"checks/greater-than-check/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"checks/greater-than-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/greater-than-check/#example","title":"Example","text":"<p>Objective: Ensure that all L_QUANTITY entries in the LINEITEM table are greater than 10.</p> <p>Sample Data</p> L_ORDERKEY L_QUANTITY 1 9 2 15 3 5 Payload example <pre><code>{\n    \"description\": \"Ensure that all L_QUANTITY entries in the LINEITEM table are greater than 10\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"inclusive\": true,\n        \"value\": 10\n    },\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"greaterThan\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 1 and 3 do not satisfy the rule because their <code>L_QUANTITY</code> values are not greater than 10.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_QUANTITY]\nB --&gt; C{Is L_QUANTITY &gt; 10?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_quantity\nfrom lineitem \nwhere\n    l_quantity &lt;= 10;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>L_QUANTITY</code> value of <code>5</code> is not greater than the value of 10.</p> <p>Shape Anomaly</p> <p>In <code>L_QUANTITY</code>, 66.667% of 3 filtered records (2) are not greater than 10.</p>"},{"location":"checks/greater-than-field-check/","title":"Greater Than Field","text":""},{"location":"checks/greater-than-field-check/#definition","title":"Definition","text":"<p>Asserts that the field is greater than another field.</p>"},{"location":"checks/greater-than-field-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code>"},{"location":"checks/greater-than-field-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/greater-than-field-check/#specific-properties","title":"Specific Properties","text":"<p>Allows specifying another field against which the value comparison will be performed.</p> Name Description Field to compare Specifies the name of the field against which the value will be compared. Inclusive If true, the comparison will also allow values equal to the value of the other field. Otherwise, it's exclusive. Comparators Specifies how variations are handled, allowing for slight deviations within a defined margin of error. <p>Details</p> <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"checks/greater-than-field-check/#comparators","title":"Comparators","text":""},{"location":"checks/greater-than-field-check/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"checks/greater-than-field-check/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"checks/greater-than-field-check/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"checks/greater-than-field-check/#duration","title":"Duration","text":"<p>Duration comparators support time-based comparisons, allowing for flexibility in how duration differences are managed. This flexibility is crucial for datasets where time measurements are essential but can vary slightly.</p>"},{"location":"checks/greater-than-field-check/#unit","title":"Unit","text":"<p>The unit of time you select determines how granular the comparison is:</p> <ul> <li>Millis: Measures time in milliseconds, ideal for high-precision needs.</li> <li>Seconds: Suitable for most general purposes where precision is important but doesn't need to be to the millisecond.</li> <li>Days: Best for longer durations.</li> </ul>"},{"location":"checks/greater-than-field-check/#value","title":"Value","text":"<p>Value sets the maximum acceptable difference in time to consider two values as equal. It serves to define the margin of error, accommodating small discrepancies that naturally occur over time.</p> Illustration using Duration Comparator Unit Value A Value B Difference Threshold Are equal? Millis 500 ms 520 ms 20 ms 25 ms True Seconds 30 sec 31 sec 1 sec 2 sec True Days 5 days 7 days 2 days 1 day False Millis 1000 ms 1040 ms 40 ms 25 ms False Seconds 45 sec 48 sec 3 sec 2 sec False"},{"location":"checks/greater-than-field-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/greater-than-field-check/#example","title":"Example","text":"<p>Objective: Ensure that all O_TOTALPRICE entries in the ORDERS table are greater than their respective O_DISCOUNT.</p> <p>Sample Data</p> O_ORDERKEY O_TOTALPRICE O_DISCOUNT 1 100 105 2 500 10 3 120 121 Payload example <pre><code>{\n    \"description\": \"Ensure that all O_TOTALPRICE entries in the ORDERS table are greater than their respective O_DISCOUNT\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"field_name\": \"O_DISCOUNT\",\n        \"inclusive\": true\n    },\n    \"tags\": [],\n    \"fields\": [\"O_TOTALPRICE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"greaterThanField\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>O_ORDERKEY</code> 1 and 3 do not satisfy the rule because their <code>O_TOTALPRICE</code> values are not greater than their respective <code>O_DISCOUNT</code> values.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_TOTALPRICE and O_DISCOUNT]\nB --&gt; C{Is O_TOTALPRICE &gt; O_DISCOUNT?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    o_orderkey,\n    o_totalprice,\n    o_discount\nfrom orders \nwhere\n    o_totalprice &lt;= o_discount;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>O_TOTALPRICE</code> value of <code>100</code> is not greater than the value of <code>O_DISCOUNT</code>.</p> <p>Shape Anomaly</p> <p>In <code>O_TOTALPRICE</code>, 66.667% of 3 filtered records (2) are not greater than <code>O_DISCOUNT</code>.</p>"},{"location":"checks/inferred-check/","title":"Inferred Check","text":"<p>Qualytics automatically generates inferred checks during a Profile operation. These checks typically cover 80-90% of the rules needed by users. They are created and maintained through profiling, which involves statistical analysis and machine learning methods.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"checks/inferred-check/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and select the datastore from the left menu.</p> <p></p> <p>Step 2: Click on the \"Checks\" from the Navigation Tab.</p> <p></p> <p>Step 3: In the top-right corner, click on the \"Run\" button, then select \"Profile\" from the dropdown menu. This action will initiate the profiling process that generates inferred checks.</p> <p></p> <p>Note</p> <p>Inferred checks will be automatically updated with the next Profiling run. Manually updating an inferred check will take it out of the automatic update workflow.</p> <p>To understand how Inferred checks work, you can follow the steps from the documentation Profile Operation.</p> <p>After the profiling run is complete, inferred checks will be automatically created based on the analysis of your data.</p> <p></p> <p>1. Check Summary: Provides a summary of the schema validation check, including its unique identifier, type, status, and associated warnings or information. It serves as a quick reference for users to assess the check's current state and access relevant actions.</p> <p>For demonstration purposes, the applied rule type is Expected Schema.</p> <p></p> <p>Check ID: A unique identifier assigned to this particular check.</p> <p></p> <p>Check Type: Indicates the nature of the validation being performed on the check.</p> <p></p> <p>Warnings: Indicates the presence of active anomalies detected in the dataset. Clicking on this icon opens a dropdown menu with the following options:</p> <ul> <li> <p>View: Displays detailed information about the detected anomalies.</p> </li> <li> <p>Acknowledge: Marks the anomaly as reviewed or acknowledged.</p> </li> <li> <p>Archive: Moves the anomaly record to the archive for future reference.</p> </li> </ul> <p></p> <p>Open Details: Provides additional details or guidance about the check. Clicking this icon typically displays more context or documentation related to schema validation.</p> <p></p> <p>Check Actions: Opens a dropdown menu with more actions related to managing or modifying the check.</p> <p></p> <p>2. Target: Specifies the dataset and file that the inferred check will be applied to. This section ensures that the validation rules are correctly assigned to the intended source datastore. Users can select a different file if needed by clicking the dropdown.</p> <p></p> <p>3. Fields List: Displays the fields that are expected to be present in the dataset.</p> <p></p> No. Fields Description 1 Date of Birth Stores date and time values, ensuring precise representation of birth dates. 2 Created Date Holds the record\u2019s creation date as a text value rather than a structured date format. <p>4. Allow Other Fields (Checkbox):</p> <ul> <li>If checked, the validation process allows additional fields beyond those explicitly listed.</li> <li>If unchecked, any unexpected field in the dataset will trigger an error.</li> </ul> <p></p> <p>5. Description: Enter a detailed description of the check template, including its purpose, applicable data, and relevant information to ensure clarity for users. If you're unsure of what to include, click on the \"\ud83d\udca1\" lightbulb icon to apply a suggested description based on the rule type.</p> <p></p> <p>6. Tags: Tags help categorize and manage checks efficiently. The tag \"test partition scan\" in this case suggests that this check is related to a specific test or partition scan process.</p> <p></p> <p>7. Additional Metadata: Add key-value pairs as additional metadata to enrich your check. Click the plus icon (+) next to this section to open the metadata input form, where you can add key-value pairs.</p> <p></p> <p>Enter the desired key-value pairs. After entering the necessary metadata, click \"Confirm\" to save the custom metadata.</p> <p></p> <p>Note</p> <p>The Target field is non-editable. It automatically reflects the selected dataset and cannot be modified manually.</p> <p>Step 4: After completing all the check details, click on the \"Validate\" button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct. It ensures that the check will work as expected by running it against the data without committing any changes.</p> <p></p> <p>If the validation is successful, a green message will appear saying \"Validation Successful\".</p> <p></p> <p>Step 5: Once you have a successful validation, click the \"Update\" button.</p> <p></p> <p>After clicking on the \u201cUpdate\u201d button, your check is successfully updated and a success flash message will appear saying \u201cCheck successfully updated\u201d.</p> <p></p>"},{"location":"checks/inferred-check/#inference-engine","title":"Inference Engine","text":"<ol> <li> <p>After metadata is generated by a Profile Operation, Inference Engine is initiated to kick off Inductive and Unsupervised learning methods.  </p> </li> <li> <p>Available data is partitioned into a training set and a testing set.  </p> </li> <li> <p>The engine applies numerous machine learning models and techniques to the training data in an effort to discover well-fitting data quality constraints.</p> </li> <li> <p>Those inferred constraints are then filtered by testing them against the held out testing set and only those that assert true above a certain threshold are converted and exposed to users as Inferred Checks.</p> </li> </ol>"},{"location":"checks/is-address/","title":"Is Address","text":""},{"location":"checks/is-address/#definition","title":"Definition","text":"<p>Asserts that the values contain the specified required elements of an address.</p>"},{"location":"checks/is-address/#in-depth-overview","title":"In-Depth Overview","text":"<p>This check leverages machine learning powered by the libpostal library to support multilingual street address parsing/normalization that can handle addresses all over the world. The underlying statistical NLP model was trained using data from OpenAddress and OpenStreetMap, a total of about 1.2 billion records of data from over 230 countries, in 100+ languages. The international address parser uses Conditional Random Fields, which can infer a globally optimal tag sequence instead of making local decisions at each word, and it achieves 99.45% full-parse accuracy on held-out addresses (i.e. addresses from the training set that were purposefully removed so we could evaluate the parser on addresses it hasn\u2019t seen before).</p>"},{"location":"checks/is-address/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/is-address/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/is-address/#specific-properties","title":"Specific Properties","text":"Name Description Required Labels The labels that must be identifiable in the value of each record <p>Info</p> <p>The address parser can technically use any string labels that are defined in the training data, but these are the ones currently supported:</p> <ul> <li>road: Street name(s)</li> <li>city: Any human settlement including cities, towns, villages, hamlets, localities, etc</li> <li>state: First-level administrative division. Scotland, Northern Ireland, Wales, and England in the UK are mapped to \"state\" as well (convention used in OSM, GeoPlanet, etc.)</li> <li>country: Sovereign nations and their dependent territories, anything with an ISO-3166 code</li> <li>postcode: Postal codes used for mail sorting</li> </ul> <p>This check allows the user to define any combination of these labels as required elements of the value held in each record. Any value these does not contain every required element will be identified as anomalous.</p>"},{"location":"checks/is-address/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/is-address/#example","title":"Example","text":"<p>Objective: Ensure that all values in O_MAILING_ADDRESS include the labels \"road\", \"city\", \"state\", and \"postcode\"</p> <p>Sample Data</p> O_ORDERKEY O_MAILING_ADDRESS 1 One-hundred twenty E 96th St, new york NY 14925 2 Quatre vingt douze R. de l'\u00c9glise, 75196 cedex 04 3 781 Franklin Ave Crown Heights Brooklyn NYC NY 11216 USA Payload example <pre><code>{\n    \"description\": \"Ensure that all values in O_MAILING_ADDRESS include the labels \"road\", \"city\", \"state\", and \"postcode\"\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"required_labels\": [\"road\",\"city\",\"state\",\"country\",\"postcode\"]\n        },\n    \"tags\": [],\n    \"fields\": [\"O_MAILING_ADDRESS\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"isAddress\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>O_ORDERKEY</code> 2 does not satisfy the rule because the <code>O_MAILING_ADDRESS</code> value includes only a road and postcode which violates the business logic that city and state also be present.</p> Flowchart <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_MAILING_ADDRESS]\nB --&gt; C[Infer address labels using ML]\nC --&gt; D{Are all required labels present?}\nD --&gt;|Yes| E[Move to Next Record/End]\nD --&gt;|No| F[Mark as Anomalous]\nF --&gt; E</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>O_MAILING_ADDRESS</code> value of <code>Quatre vingt douze R. de l'\u00c9glise, 75196 cedex 04</code> does not adhere to the required format.</p> <p>Shape Anomaly</p> <p>In <code>O_MAILING_ADDRESS</code>, 33.33% of 3 filtered records (1) do not adhere to the required format.</p>"},{"location":"checks/is-credit-card-check/","title":"Is Credit Card","text":""},{"location":"checks/is-credit-card-check/#definition","title":"Definition","text":"<p>Asserts that the values are credit card numbers.</p>"},{"location":"checks/is-credit-card-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/is-credit-card-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/is-credit-card-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/is-credit-card-check/#example","title":"Example","text":"<p>Objective: Ensure that all C_CREDIT_CARD entries in the CUSTOMER table are valid credit card numbers.</p> <p>Sample Data</p> C_CUSTKEY C_CREDIT_CARD 1 5105105105105100 2 ABC12345XYZ 3 4111111111111111 Payload example <pre><code>{\n    \"description\": \"Ensure that all C_CREDIT_CARD entries in the CUSTOMER table are valid credit card numbers\",\n    \"coverage\": 1,\n    \"properties\": {},\n    \"tags\": [],\n    \"fields\": [\"C_CREDIT_CARD\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"isCreditCard\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>C_CUSTKEY</code> 2 does not satisfy the rule because its <code>C_CREDIT_CARD</code> value is not a valid credit card number.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve C_CREDIT_CARD]\nB --&gt; C{Is C_CREDIT_CARD valid?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    c_custkey\n    , c_credit_card\nfrom customer \nwhere\n    not regexp_like(replace(c_credit_card, '-', ''), '^[0-9]{16}$')\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>C_CREDIT_CARD</code> value of <code>ABC12345XYZ</code> is not a valid credit card number.</p> <p>Shape Anomaly</p> <p>In <code>C_CREDIT_CARD</code>, 33.33% of 3 filtered records (1) are not valid credit card numbers.</p>"},{"location":"checks/is-replica-of-check/","title":"Is Replica Of (Is sunsetting)","text":"<p>Deprecation Notice</p> <p>The <code>isReplicaOf</code> check is being deprecated and will no longer be maintained. We strongly recommend using the Data Diff check, which offers the same functionality with improved performance and additional features.</p> <p>Our recommendation:</p> <ul> <li>Consider using <code>Data Diff</code> for new implementations</li> <li><code>dataDiff</code> provides enhanced performance and additional capabilities</li> <li>Both checks will continue to coexist in the system</li> </ul> <p>If you have questions about this change, please contact our support team</p>"},{"location":"checks/is-replica-of-check/#definition","title":"Definition","text":"<p>Asserts that the dataset created by the targeted field(s) is replicated by the referred field(s).</p>"},{"location":"checks/is-replica-of-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>IsReplicaOf</code> rule ensures that data integrity is maintained when data is replicated from one source to another. This involves checking not only the data values themselves but also ensuring that the structure and relationships are preserved.</p> <p>In a distributed data ecosystem, replication often occurs to maintain high availability, create backups, or feed data into analytical systems. However, discrepancies might arise due to various reasons such as network glitches, software bugs, or human errors. The <code>IsReplicaOf</code> rule serves as a safeguard against these issues by:</p> <ol> <li>Preserving Data Structure: Ensuring that the structure of the replicated data matches the original.</li> <li>Checking Data Values: Ensuring that every piece of data in the source exists in the replica.</li> </ol>"},{"location":"checks/is-replica-of-check/#field-scope","title":"Field Scope","text":"<p>Multi: The rule evaluates multiple specified fields.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/is-replica-of-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/is-replica-of-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the datastore and table/file where the replica of the targeted fields is located for comparison.</p> Name Description Row Identifiers The list of fields defining the compound key to identify rows in the comparison analysis. Datastore The source datastore where the replica of the targeted field(s) is located. Table/file The table, view or file in the source datastore that should serve as the replica. Comparators Specifies how variations are handled, allowing for slight deviations within a defined margin of error. <p>Info</p> <p>The <code>IsReplicaOf</code> rule supports editing of <code>Row Identifiers</code> and <code>Passthrough Fields</code>, allowing for more tailored configuration.</p> <p>Details</p> <p>This optional input allows row comparison analysis by defining a list of fields as row identifiers, it enables a more detailed comparison between tables/files, where each row compound key is used to identify its presence or absence in the reference table/file compared to the target table/file.  Qualytics can inform if the row exists or not and distinguish which field values differ in each row present in the reference table/file, helping to determine if it is a replica.</p> <p>Info</p> <p>Anomalies produced by a <code>IsReplicaOf</code> quality check making use of <code>Row Identifiers</code> have their source records presented in a different visualization.  See more at: Comparison Source Records</p> <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"checks/is-replica-of-check/#row-identifiers","title":"Row Identifiers","text":""},{"location":"checks/is-replica-of-check/#comparators","title":"Comparators","text":""},{"location":"checks/is-replica-of-check/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"checks/is-replica-of-check/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"checks/is-replica-of-check/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"checks/is-replica-of-check/#duration","title":"Duration","text":"<p>Duration comparators support time-based comparisons, allowing for flexibility in how duration differences are managed. This flexibility is crucial for datasets where time measurements are essential but can vary slightly.</p>"},{"location":"checks/is-replica-of-check/#unit","title":"Unit","text":"<p>The unit of time you select determines how granular the comparison is:</p> <ul> <li>Millis: Measures time in milliseconds, ideal for high-precision needs.</li> <li>Seconds: Suitable for most general purposes where precision is important but doesn't need to be to the millisecond.</li> <li>Days: Best for longer durations.</li> </ul>"},{"location":"checks/is-replica-of-check/#value","title":"Value","text":"<p>Value sets the maximum acceptable difference in time to consider two values as equal. It serves to define the margin of error, accommodating small discrepancies that naturally occur over time.</p> Illustration using Duration Comparator Unit Value A Value B Difference Threshold Are equal? Millis 500 ms 520 ms 20 ms 25 ms True Seconds 30 sec 31 sec 1 sec 2 sec True Days 5 days 7 days 2 days 1 day False Millis 1000 ms 1040 ms 40 ms 25 ms False Seconds 45 sec 48 sec 3 sec 2 sec False"},{"location":"checks/is-replica-of-check/#string","title":"String","text":"<p>String comparators facilitate comparisons of textual data by allowing variations in spacing. This capability is essential for ensuring data consistency, particularly where minor text inconsistencies may occur.</p>"},{"location":"checks/is-replica-of-check/#ignore-whitespace","title":"Ignore Whitespace","text":"<p>When enabled, this setting allows the comparator to ignore differences in whitespace. This means sequences of whitespace are collapsed into a single space, and any leading or trailing spaces are removed. This can be particularly useful in environments where data entry may vary in formatting but where those differences are not relevant to the data's integrity.</p> Illustration <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined string comparison to <code>ignore whitespace</code> as <code>True</code>.</p> Value A Value B Are equal? Has whitespace? <code>Leonidas</code> <code>Leonidas</code> True No <code>Beth</code> <code>Beth</code> True Yes <code>Ana</code> <code>Anna</code> False Yes <code>Joe</code> <code>Joel</code> False No"},{"location":"checks/is-replica-of-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/is-replica-of-check/#example","title":"Example","text":"<p>Scenario: Consider that the fields N_NATIONKEY and N_NATIONNAME in the NATION table are being replicated to a backup database for disaster recovery purposes. The data engineering team wants to ensure that both fields in the replica in the backup accurately reflect the original.</p> <p>Objective: Ensure that N_NATIONKEY and N_NATIONNAME from the NATION table are replicas in the NATION_BACKUP table.</p> <p>Sample Data from NATION</p> N_NATIONKEY N_NATIONNAME 1 Australia 2 United States 3 Uruguay <p>Replica Sample Data from NATION_BACKUP</p> N_NATIONKEY N_NATIONNAME 1 Australia 2 USA 3 Uruguay Payload example <pre><code>{\n    \"description\": \"Ensure that N_NATIONKEY and N_NATIONNAME from the NATION table are replicas in the NATION_BACKUP table\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"ref_container_id\": {ref_container_id},\n        \"ref_datastore_id\": {ref_datastore_id}\n    },\n    \"tags\": [],\n    \"fields\": [\"N_NATIONKEY\", \"N_NATIONNAME\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"isReplicaOf\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>The datasets representing the fields <code>N_NATIONKEY</code> and <code>N_NATIONNAME</code> in the original and the replica are not completely identical, indicating a possible discrepancy in the replication process or an unintended change.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Original Data]\nB --&gt; C[Retrieve Replica Data]\nC --&gt; D{Do datasets match for both fields?}\nD --&gt;|Yes| E[End]\nD --&gt;|No| F[Mark as Anomalous]\nF --&gt; E</code></pre> <pre><code>-- An illustrative SQL query comparing original to replica for both fields.\nselect\n    orig.n_nationkey as original_key,\n    orig.n_nationname as original_name,\n    replica.n_nationkey as replica_key,\n    replica.n_nationname as replica_name\nfrom nation as orig\nleft join nation_backup as replica on orig.n_nationkey = replica.n_nationkey\nwhere\n    orig.n_nationname &lt;&gt; replica.n_nationname\nor\n    orig.n_nationkey &lt;&gt; replica.n_nationkey\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>There is 1 record that differs between <code>NATION_BACKUP</code> (3 records) and <code>NATION</code> (3 records) in <code>&lt;datastore_name&gt;</code></p>"},{"location":"checks/is-type-check/","title":"Is Type","text":""},{"location":"checks/is-type-check/#definition","title":"Definition","text":"<p>Asserts that the data is of a specific type.</p>"},{"location":"checks/is-type-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/is-type-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/is-type-check/#specific-properties","title":"Specific Properties","text":"<p>Specify the expected type for the data in the field.</p> Name Description Field Type The type that values in the selected field should conform to."},{"location":"checks/is-type-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/is-type-check/#example","title":"Example","text":"<p>Objective: Ensure that all L_QUANTITY entries in the LINEITEM table are of Integral type.</p> <p>Sample Data</p> L_ORDERKEY L_QUANTITY 1 \"10\" 2 \"15.5\" 3 \"Ten\" Payload example <pre><code>{\n    \"description\": \"Ensure that all L_QUANTITY entries in the LINEITEM table are of Integral type\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"field_type\":\"Integral\"\n    },\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"isType\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 2 and 3 do not satisfy the rule because their <code>L_QUANTITY</code> values are not of Integral type.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_QUANTITY]\nB --&gt; C{Is L_QUANTITY of Integral type?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_quantity\nfrom lineitem \nwhere\n    typeof(l_quantity) != 'INTEGER'\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>L_QUANTITY</code> value of <code>Ten</code> is not a valid Integral.</p> <p>Shape Anomaly</p> <p>In <code>L_QUANTITY</code>, 66.667% of 3 filtered records (2) are not a valid Integral.</p>"},{"location":"checks/less-than-check/","title":"Less Than","text":""},{"location":"checks/less-than-check/#definition","title":"Definition","text":"<p>Asserts that the field is a number less than (or equal to) a value.</p>"},{"location":"checks/less-than-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/less-than-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/less-than-check/#specific-properties","title":"Specific Properties","text":"<p>Allows specifying a numeric value that acts as the threshold.</p> Name Description Value The number to use as the base comparison. Inclusive If true, the comparison will also allow values equal to the threshold. Otherwise, it's exclusive. Comparators Specifies how variations are handled, allowing for slight deviations within a defined margin of error. <p>Details</p> <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"checks/less-than-check/#comparators","title":"Comparators","text":""},{"location":"checks/less-than-check/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"checks/less-than-check/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"checks/less-than-check/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"checks/less-than-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/less-than-check/#example","title":"Example","text":"<p>Objective: Ensure that all L_PRICE entries in the LINEITEM table are less than 20.</p> <p>Sample Data</p> L_ORDERKEY L_PRICE 1 18 2 25 3 23 Payload example <pre><code>{\n    \"description\": \"Ensure that all L_PRICE entries in the LINEITEM table are less than 20\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"inclusive\": true,\n        \"value\": 20\n    },\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"lessThan\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 2 and 3 do not satisfy the rule because their <code>L_PRICE</code> values are not less than 20.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_PRICE]\nB --&gt; C{Is L_PRICE &lt; 20?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_price\nfrom lineitem \nwhere\n    l_price &gt;= 20;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>L_PRICE</code> value of <code>23</code> is not less than the value of 20.</p> <p>Shape Anomaly</p> <p>In <code>L_PRICE</code>, 66.667% of 3 filtered records (2) are not less than 20.</p>"},{"location":"checks/less-than-field-check/","title":"Less Than Field","text":""},{"location":"checks/less-than-field-check/#definition","title":"Definition","text":"<p>Asserts that the field is less than another field.</p>"},{"location":"checks/less-than-field-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code>"},{"location":"checks/less-than-field-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/less-than-field-check/#specific-properties","title":"Specific Properties","text":"<p>Allows specifying another field against which the value comparison will be performed.</p> Name Description Field to compare Specifies the name of the field against which the value will be compared. Inclusive If true, the comparison will also allow values equal to the value of the other field. Otherwise, it's exclusive. Comparators Specifies how variations are handled, allowing for slight deviations within a defined margin of error. <p>Details</p> <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"checks/less-than-field-check/#comparators","title":"Comparators","text":""},{"location":"checks/less-than-field-check/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"checks/less-than-field-check/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"checks/less-than-field-check/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"checks/less-than-field-check/#duration","title":"Duration","text":"<p>Duration comparators support time-based comparisons, allowing for flexibility in how duration differences are managed. This flexibility is crucial for datasets where time measurements are essential but can vary slightly.</p>"},{"location":"checks/less-than-field-check/#unit","title":"Unit","text":"<p>The unit of time you select determines how granular the comparison is:</p> <ul> <li>Millis: Measures time in milliseconds, ideal for high-precision needs.</li> <li>Seconds: Suitable for most general purposes where precision is important but doesn't need to be to the millisecond.</li> <li>Days: Best for longer durations.</li> </ul>"},{"location":"checks/less-than-field-check/#value","title":"Value","text":"<p>Value sets the maximum acceptable difference in time to consider two values as equal. It serves to define the margin of error, accommodating small discrepancies that naturally occur over time.</p> Illustration using Duration Comparator Unit Value A Value B Difference Threshold Are equal? Millis 500 ms 520 ms 20 ms 25 ms True Seconds 30 sec 31 sec 1 sec 2 sec True Days 5 days 7 days 2 days 1 day False Millis 1000 ms 1040 ms 40 ms 25 ms False Seconds 45 sec 48 sec 3 sec 2 sec False"},{"location":"checks/less-than-field-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/less-than-field-check/#example","title":"Example","text":"<p>Objective: Ensure that all O_DISCOUNT entries in the ORDERS table are less than their respective O_TOTALPRICE.</p> <p>Sample Data</p> O_ORDERKEY O_TOTALPRICE O_DISCOUNT 1 105 100 2 500 10 3 121 125 Payload example <pre><code>{\n    \"description\": \"Ensure that all O_DISCOUNT entries in the ORDERS table are less than their respective O_TOTALPRICE\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"field_name\": \"O_TOTALPRICE\",\n        \"inclusive\":true\n    },\n    \"tags\": [],\n    \"fields\": [\"O_DISCOUNT\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"lessThanField\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>O_ORDERKEY</code> 3 does not satisfy the rule because its <code>O_DISCOUNT</code> value is not less than its respective <code>O_TOTALPRICE</code> value.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_TOTALPRICE and O_DISCOUNT]\nB --&gt; C{Is O_DISCOUNT &lt; O_TOTALPRICE?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    o_orderkey,\n    o_totalprice,\n    o_discount\nfrom orders \nwhere\n    o_discount &gt;= o_totalprice;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>O_DISCOUNT</code> value of <code>125</code> is not less than the value of <code>O_TOTALPRICE</code>.</p> <p>Shape Anomaly</p> <p>In <code>O_DISCOUNT</code>, 33.333% of 3 filtered records (1) is not less than <code>O_TOTALPRICE</code>.</p>"},{"location":"checks/matches-pattern-check/","title":"Matches Pattern","text":""},{"location":"checks/matches-pattern-check/#definition","title":"Definition","text":"<p>Asserts that a field must match a pattern.</p>"},{"location":"checks/matches-pattern-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>Patterns, typically expressed as regular expressions, allow for the enforcement of custom structural norms for data fields. For complex patterns, regular expressions offer a powerful tool to ensure conformity to the expected format.</p>"},{"location":"checks/matches-pattern-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/matches-pattern-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/matches-pattern-check/#specific-properties","title":"Specific Properties","text":"<p>Allows specifying a pattern against which the field will be checked.</p> Name Description Pattern Specifies the regular expression pattern the field must match."},{"location":"checks/matches-pattern-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/matches-pattern-check/#example","title":"Example","text":"<p>Objective: Ensure that all P_SERIAL entries in the PART table match the pattern for product serial numbers: <code>TPCH-XXXX-####</code>, where <code>XXXX</code> are uppercase alphabetic characters and <code>####</code> are numbers.</p> <p>Sample Data</p> P_PARTKEY P_SERIAL 1 TPCH-ABCD-1234 2 TPCH-1234-ABCD 3 TPCH-WXYZ-9876 Payload example <pre><code>{\n    \"description\": \"Ensure that all P_SERIAL entries in the PART table match the pattern for product serial numbers: `TPCH-XXXX-####`, where `XXXX` are uppercase alphabetic characters and `####` are numbers\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"pattern\":\"^tpch-[a-z]{4}-[0-9]{4}$\"\n    },\n    \"tags\": [],\n    \"fields\": [\"P_SERIAL\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"matchesPattern\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>P_PARTKEY</code> 2 does not satisfy the rule because its <code>P_SERIAL</code> does not match the required pattern.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve P_SERIAL]\nB --&gt; C{Does P_SERIAL match TPCH-XXXX-#### format?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    p_partkey,\n    p_serial\nfrom part \nwhere\n    not regexp_like(p_serial, '^tpch-[a-z]{4}-[0-9]{4}$')\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>P_SERIAL</code> value of <code>TPCH-1234-ABCD</code> does not match the pattern <code>TPCH-XXXX-####</code>.</p> <p>Shape Anomaly</p> <p>In <code>P_SERIAL</code>, 33.333% of 3 filtered records (1) do not match the pattern <code>TPCH-XXXX-####</code>.</p>"},{"location":"checks/max-length-check/","title":"Max Length","text":""},{"location":"checks/max-length-check/#definition","title":"Definition","text":"<p>Asserts that a string has a maximum length.</p>"},{"location":"checks/max-length-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/max-length-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/max-length-check/#specific-properties","title":"Specific Properties","text":"<p>Determines the maximum acceptable length of the string.</p> Name Description Length Specifies the maximum number of characters a string in the field should have."},{"location":"checks/max-length-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/max-length-check/#example","title":"Example","text":"<p>Objective: Ensure that P_DESCRIPTION in the PART table do not exceed 50 characters in length.</p> <p>Sample Data</p> P_PARTKEY P_DESCRIPTION 1 Standard industrial widget 2 A product description that clearly goes way beyond the specified fifty characters limit. 3 Basic office equipment Payload example <pre><code>{\n    \"description\": \"Ensure that P_DESCRIPTION in the PART table do not exceed 50 characters in length\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\": 3\n    },\n    \"tags\": [],\n    \"fields\": [\"C_BLOOD_GROUP\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"maxLength\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>P_PARTKEY</code> 2 does not satisfy the rule because its <code>P_DESCRIPTION</code> exceeds 50 characters in length.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve P_DESCRIPTION]\nB --&gt; C{Is P_DESCRIPTION length &lt;= 50 characters?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    p_partkey,\n    p_description\nfrom part \nwhere\n    length(p_description) &gt; 50;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>P_DESCRIPTION</code> length of <code>A product description that clearly goes way beyond the specified fifty characters limit.</code> is greater than the max length of 50.</p> <p>Shape Anomaly</p> <p>In <code>P_DESCRIPTION</code>, 33.333% of 3 filtered records (1) have a length greater than 50.</p>"},{"location":"checks/max-partition-size-check/","title":"Max Partition Size","text":""},{"location":"checks/max-partition-size-check/#definition","title":"Definition","text":"<p>Asserts the maximum number of records that should be loaded from each file or table partition.</p>"},{"location":"checks/max-partition-size-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>Managing the volume of data in each partition is critical when dealing with partitioned datasets. This is especially pertinent when system limitations or data processing capabilities are considered, ensuring that no partition exceeds the system's ability to handle data efficiently.</p> <p>The Max Partition Size rule is designed to set an upper limit on the number of records each partition can contain.</p>"},{"location":"checks/max-partition-size-check/#general-properties","title":"General Properties","text":"Details Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions"},{"location":"checks/max-partition-size-check/#specific-properties","title":"Specific Properties","text":"<p>Specifies the maximum allowable record count for each data partition</p> Name Description Maximum partition size The maximum number of records that can be loaded from each partition."},{"location":"checks/max-partition-size-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/max-partition-size-check/#example","title":"Example","text":"<p>Objective: Ensure that no partition of the LINEITEM table contains more than 10,000 records to prevent data processing bottlenecks.</p> <p>Sample Data for Partition P3</p> Row Number L_ITEM 1 Data 2 Data ... ... 10,050 Data Payload example <pre><code>{\n    \"description\": \"Ensure that no partition of the LINEITEM table contains more than 10,000 records to prevent data processing bottlenecks\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\":10000\n    },\n    \"tags\": [],\n    \"fields\": null,\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"maxPartitionSize\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>In the sample data above, the rule is violated because partition P3 contains 10,050 records, which exceeds the set maximum of 10,000 records.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Number of Records for Each Partition]\nB --&gt; C{Does Partition have &lt;= 10,000 records?}\nC --&gt;|Yes| D[Move to Next Partition/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s). \nselect\n    subset_name, -- or any column indicating the partition or subset\n    count(*)\nfrom lineitem \ngroup by subset_name\nhaving count(*) &gt; 10000;\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>In <code>LINEITEM</code>, more than 10,000 records were loaded.</p>"},{"location":"checks/max-value-check/","title":"Max Value","text":""},{"location":"checks/max-value-check/#definition","title":"Definition","text":"<p>Asserts that a field has a maximum value.</p>"},{"location":"checks/max-value-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/max-value-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/max-value-check/#specific-properties","title":"Specific Properties","text":"<p>Determines the maximum allowable value for the field.</p> Name Description Value Specifies the maximum value a field should have."},{"location":"checks/max-value-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/max-value-check/#example","title":"Example","text":"<p>Objective: Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table does not exceed a value of 50.</p> <p>Sample Data</p> L_ORDERKEY L_LINENUMBER L_QUANTITY 1 1 40 1 2 55 2 1 20 3 1 60 Payload example <pre><code>{\n    \"description\": \"Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table does not exceed a value of 50\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\": 50\n    },\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"maxValue\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 1 and 3 do not satisfy the rule because their <code>L_QUANTITY</code> values exceed the specified maximum value of 50.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_QUANTITY]\nB --&gt; C{Is L_QUANTITY &lt;= 50?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_linenumber,\n    l_quantity\nfrom lineitem \nwhere\n    l_quantity &gt; 50;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>L_QUANTITY</code> value of <code>55</code> is greater than the max value of <code>50</code>.</p> <p>Shape Anomaly</p> <p>In <code>L_QUANTITY</code>, 50.000% of 4 filtered records (2) are greater than the max value of <code>50</code>.</p>"},{"location":"checks/metric-check/","title":"Metric","text":""},{"location":"checks/metric-check/#definition","title":"Definition","text":"<p>Records the value of the selected field during each scan operation and asserts limits based upon an expected change or absolute range (inclusive).</p>"},{"location":"checks/metric-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>Metric</code> rule is designed to monitor the values of a selected field over time. It is particularly useful in a time-series context where values are expected to evolve within certain bounds or limits. This rule allows for tracking absolute values or changes, ensuring they remain within predefined thresholds.</p>"},{"location":"checks/metric-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/metric-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/metric-check/#specific-properties","title":"Specific Properties","text":"<p>Determines the evaluation method and allowable limits for field value comparisons over time.</p> Name Description Comparison Specifies the type of comparison: Absolute Change, Absolute Value, or Percentage Change. Min Value Indicates the minimum allowable increase in value. Use a negative value to represent an allowable decrease. Max Value Indicates the maximum allowable increase in value. <p>Details</p>"},{"location":"checks/metric-check/#comparison-options","title":"Comparison Options","text":"<p>Absolute Change</p> <p>The <code>Absolute Change</code> comparison works by comparing the change in a numeric field's value to a pre-set limit (Min / Max). If the field's value changes by more than this specified limit since the last relevant scan, an anomaly is identified.</p> Illustration <p>Any record with a value change smaller than 30 or greater than 70 compared to the last scan should be flagged as anomalous</p> <p>Thresholds: Min Change = 30, Max Change = 70</p> Scan Previous Value Current Value Absolute Change Anomaly Detected #1 - 100 - No #2 100 150 50 No #3 150 220 70 No #4 220 300 80 Yes <p>Absolute Value</p> <p>The <code>Absolute Value</code> comparison works by comparing the change in a numeric field's value to a pre-set limit <code>between</code> Min and Max values. If the field's value changes by more than this specified range since the last relevant scan, an anomaly is identified.</p> Illustration <p>The value of the record in each scan should be within 100 and 300 to be considered normal</p> <p>Thresholds: Min Value = 100, Max Value = 300</p> Scan Current Value Anomaly Detected #1 150 No #2 90 Yes #3 250 No #4 310 Yes <p>Percentage Change</p> <p>The <code>Percentage Change</code> comparison operates by tracking changes in a numeric field's value relative to its previous value. If the change exceeds the predefined percentage (%) limit since the last relevant scan, an anomaly is generated.</p> Illustration <p>An anomaly is identified if the record's value decreases by more than 20% or increases by more than 50% compared to the last scan.</p> <p>Thresholds: Min Percentage Change = -20%, Max Percentage Change = 50%</p> <p>Percentage Change Formula: ( (current_value - previous_value) / previous_value ) * 100</p> Scan Previous Value Current Value Percentage Change Anomaly Detected 1 - 100 - No 2 100 150 50% No 3 150 120 -20% No 4 120 65 -45.83% Yes 5 65 110 69.23% Yes"},{"location":"checks/metric-check/#thresholds","title":"Thresholds","text":"<p>At least the Min or Max value must be specified, and including both is optional. These values determine the acceptable range or limit of change in the field's value.</p> <p>Min Value</p> <ul> <li>Represents the minimum allowable increase in the field's value.</li> <li>A negative Min Value signifies an allowable decrease, determining the minimum value the field can drop to be considered valid.</li> </ul> <p>Max Value</p> <ul> <li>Indicates the maximum allowable increase in the field\u2019s value, setting an upper limit for the value's acceptable growth or change.</li> </ul>"},{"location":"checks/metric-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/metric-check/#example","title":"Example","text":"<p>Objective: Ensure that the total price in the ORDERS table does not fluctuate beyond a predefined percentage limit between scans.</p> <p>Thresholds: Min Percentage Change = -30%, Max Percentage Change = 30%</p> <p>Sample Scan History</p> Scan O_ORDERKEY Previous O_TOTALPRICE Current O_TOTALPRICE Percentage Change Anomaly Detected #1 1 - 100 - No #2 1 100 110 10% No #3 1 110 200 81.8% Yes #4 1 200 105 -47.5% Yes Payload example <pre><code>{\n    \"description\": \"Ensure that the total price in the ORDERS table does not fluctuate beyond a predefined percentage limit between scans\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"comparison\":\"Percentage Change\",\n        \"min\":-0.3,\n        \"max\":0.3\n    },\n    \"tags\": [],\n    \"fields\": [\"O_TOTALPRICE \"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"metric\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample scan history above, anomalies are identified in scans #3 and #4. The <code>O_TOTALPRICE</code> values in these scans fall outside the declared percentage change limits of -30% and 30%, indicating that something unusual might be happening and further investigation is needed.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_TOTALPRICE]\nB --&gt; C{Is Percentage Change in O_TOTALPRICE within -30% and 30%?}\nC --&gt;|Yes| D[End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s)\nselect \n    o_orderkey,\n    o_totalprice,\n    lag(o_totalprice) over (order by o_orderkey) as previous_o_totalprice\nfrom\n    orders\nhaving\n    abs((o_totalprice - previous_o_totalprice) / previous_o_totalprice) * 100 &gt; 30\n    or\n    abs((o_totalprice - previous_o_totalprice) / previous_o_totalprice) * 100 &lt; -30;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly (Percentage Change)</p> <p>The percentage change of <code>O_TOTALPRICE</code> from '110' to '200' falls outside the declared limits</p> <p>Record Anomaly (Absolute Change)</p> <p>using hypothetical numbers</p> <p>The absolute change of <code>O_TOTALPRICE</code> from '150' to '300' falls outside the declared limits</p> <p>Record Anomaly (Absolute Value)</p> <p>using hypothetical numbers</p> <p>The value for <code>O_TOTALPRICE</code> of '50' is not between the declared limits</p>"},{"location":"checks/min-length-check/","title":"Min Length","text":""},{"location":"checks/min-length-check/#definition","title":"Definition","text":"<p>Asserts that a string has a minimum length.</p>"},{"location":"checks/min-length-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>String</code>"},{"location":"checks/min-length-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/min-length-check/#specific-properties","title":"Specific Properties","text":"<p>Determines the minimum allowable length for the field.</p> Name Description Value Specifies the minimum length that the string field should have."},{"location":"checks/min-length-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/min-length-check/#example","title":"Example","text":"<p>Objective: Ensure that all C_COMMENT entries in the CUSTOMER table have a minimum length of 5 characters.</p> <p>Sample Data</p> C_CUSTKEY C_COMMENT 1 Ok 2 Excellent customer service, very satisfied! 3 Nice staff Payload example <pre><code>{\n    \"description\": \"Ensure that all C_COMMENT entries in the CUSTOMER table have a minimum length of 5 characters\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\": 5\n    },\n    \"tags\": [],\n    \"fields\": [\"C_COMMENT\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"minLength\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>C_CUSTKEY</code> 1 does not satisfy the rule because the length of its <code>C_COMMENT</code> values is below the required minimum length of 5 characters.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve C_COMMENT]\nB --&gt; C{Is C_COMMENT length &gt;= 5?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    c_custkey,\n    c_comment\nfrom customer \nwhere\n    length(c_comment) &lt; 5;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>C_COMMENT</code> length of <code>Ok</code> is less than the min length of 5.</p> <p>Shape Anomaly</p> <p>In <code>C_COMMENT</code>, 33.333% of 3 filtered records (1) have a length less than 5.</p>"},{"location":"checks/min-partition-size-check/","title":"Min Partition Size","text":""},{"location":"checks/min-partition-size-check/#definition","title":"Definition","text":"<p>Asserts the minimum number of records that should be loaded from each file or table partition.</p>"},{"location":"checks/min-partition-size-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>When working with large datasets that are often partitioned for better performance and scalability, ensuring a certain minimum number of records from each partition becomes crucial. This could be to ensure that each partition is well-represented in the analysis, to maintain data consistency or even to verify that data ingestion or migration processes are functioning properly.</p> <p>The Min Partition Size rule allows users to set a threshold ensuring that each partition has loaded at least the specified minimum number of records.</p>"},{"location":"checks/min-partition-size-check/#general-properties","title":"General Properties","text":"Details Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions"},{"location":"checks/min-partition-size-check/#specific-properties","title":"Specific Properties","text":"<p>Sets the required minimum record count for each data partition</p> Name Description Minimum partition size Specifies the minimum number of records that should be loaded from each partition"},{"location":"checks/min-partition-size-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/min-partition-size-check/#example","title":"Example","text":"<p>Objective: Ensure that each partition of the LINEITEM table has at least 1000 records.</p> <p>Sample Data for Partition P3</p> Row Number L_ITEM 1 Data 2 Data ... ... 900 Data Payload example <pre><code>{\n    \"description\": \"Ensure that each partition of the LINEITEM table has at least 1000 records\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\": 1000\n    },\n    \"tags\": [],\n    \"fields\": null,\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"minPartitionSize\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>The sample data above does not satisfy the rule because it contains only 900 records, which is less than the required minimum of 1000 records.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Number of Records for Each Partition]\nB --&gt; C{Does Partition have &gt;= 1000 records?}\nC --&gt;|Yes| D[Move to Next Partition/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s). \nselect\n    subset_name, -- or any column indicating the partition or subset\n    count(*)\nfrom lineitem \ngroup by subset_name\nhaving count(*) &lt; 1000;\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>In <code>LINEITEM</code>, fewer than 900 records were loaded.</p>"},{"location":"checks/min-value-check/","title":"Min Value","text":""},{"location":"checks/min-value-check/#definition","title":"Definition","text":"<p>Asserts that a field has a minimum value.</p>"},{"location":"checks/min-value-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/min-value-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/min-value-check/#specific-properties","title":"Specific Properties","text":"<p>Determines the minimum allowable value for the field.</p> Name Description Value Specifies the minimum value a field should have."},{"location":"checks/min-value-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/min-value-check/#example","title":"Example","text":"<p>Objective: Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table is not below a value of 10.</p> <p>Sample Data</p> L_ORDERKEY L_LINENUMBER L_QUANTITY 1 1 40 1 2 5 2 1 20 3 1 8 Payload example <pre><code>{\n    \"description\": \"Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table is not below a value of 10\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\": 10\n    },\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"minValue\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 1 and 3 do not satisfy the rule because their <code>L_QUANTITY</code> values are below the specified minimum value of 10.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_QUANTITY]\nB --&gt; C{Is L_QUANTITY &gt;= 10?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_linenumber,\n    l_quantity\nfrom lineitem \nwhere\n    l_quantity &lt; 10;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>L_QUANTITY</code> value of <code>5</code> is less than the min value of <code>10</code>.</p> <p>Shape Anomaly</p> <p>In <code>L_QUANTITY</code>, 50.000% of 4 filtered records (2) are less than the min value of <code>10</code>.</p>"},{"location":"checks/not-exists-in-check/","title":"Not Exists In","text":""},{"location":"checks/not-exists-in-check/#definition","title":"Definition","text":"<p>Asserts that values assigned to this field do not exist as values in another field.</p>"},{"location":"checks/not-exists-in-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>Not ExistsIn</code> rule allows you to ensure data exclusivity between different sources, whether it\u2019s object storage systems or databases.</p> <p>While databases might utilize unique constraints to maintain data distinctiveness between related tables, the <code>Not ExistsIn</code> rule extends this capability in two significant ways:</p> <ol> <li>Cross-System Exclusivity: it enables checks to ensure data does not overlap across different databases or even entirely separate systems. This can be essential in scenarios where data should be partitioned or isolated across platforms.</li> <li>Flexible Data Formats: Not just limited to databases, this rule can validate values against various data formats, such as ensuring values in a file do not coincide with those in a table.</li> </ol> <p>These functionalities enable businesses to maintain data exclusivity even in intricate, multi-system settings.</p>"},{"location":"checks/not-exists-in-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/not-exists-in-check/#specific-properties","title":"Specific Properties","text":"<p>Define the datastore, table/file, and field where the rule should look for non-matching values.</p> Name Description Datastore The source datastore where the profile of the reference field is located. Table/file The profile (e.g. table, view or file) containing the reference field. Field The field name whose values should not match those of the selected field."},{"location":"checks/not-exists-in-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/not-exists-in-check/#example","title":"Example","text":"<p>Scenario: A shipping company needs to ensure that all NATION_NAME entries in the NATION table aren't listed in an external unsupported regions file, which lists countries they don't ship to.</p> <p>Sample Data</p> N_NATIONKEY N_NATIONNAME 1 Antarctica 2 Argentina 3 Atlantida <p>Unsupported Regions File Sample</p> UNSUPPORTED_REGION Antarctica Mars ... Payload example <pre><code>{\n    \"description\": \"A shipping company needs to ensure that all NATION_NAME entries in the NATION table aren't listed in an external unsupported regions file, which lists countries they don't ship to\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"field_name\":\"UNSUPPORTED_REGION\",\n        \"ref_container_id\": {ref_container_id},\n        \"ref_datastore_id\": {ref_datastore_id}\n    },\n    \"tags\": [],\n    \"fields\": [\"NATION_NAME\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"notExistsIn\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>N_NATIONKEY</code> 1 does not satisfy the rule because the <code>N_NATIONNAME</code> \"Antarctica\" is listed as an <code>UNSUPPORTED_REGION</code> in the unsupported regions file, indicating the company doesn't ship there.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve UNSUPPORTED_REGION]\nB --&gt; C[Retrieve N_NATIONNAME]\nC --&gt; D{Is N_NATIONNAME listed in UNSUPPORTED_REGION?}\nD --&gt;|No| E[Move to Next Record/End]\nD --&gt;|Yes| F[Mark as Anomalous]\nF --&gt; E</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    n_nationkey\n    , n_nationname\nfrom nation \nwhere\n    n_nationname in ('Antarctica', 'Mars', ... /* other unsupported regions */)\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>N_NATIONNAME</code> value of '<code>Antarctica</code>' is an <code>UNSUPPORTED_REGION</code>.</p> <p>Shape Anomaly</p> <p>In <code>N_NATIONNAME</code>, 33.333% of 3 filtered records (1) do exist in <code>UNSUPPORTED_REGION</code>.</p>"},{"location":"checks/not-future-check/","title":"Not Future","text":""},{"location":"checks/not-future-check/#definition","title":"Definition","text":"<p>Asserts that the field's value is not in the future.</p>"},{"location":"checks/not-future-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Fields</p> Field <code>Date</code> <code>Timestamp</code>"},{"location":"checks/not-future-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/not-future-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/not-future-check/#example","title":"Example","text":"<p>Objective: Ensure that the delivery dates (O_DELIVERYDATE) in the ORDERS table are not set in the future.</p> <p>Sample Data</p> O_ORDERKEY O_DELIVERYDATE 1 2023-09-20 2 2023-10-25 (Future Date) 3 2023-10-10 Payload example <pre><code>{\n    \"description\": \"Ensure that the delivery dates (O_DELIVERYDATE) in the ORDERS table are not set in the future\",\n    \"coverage\": 1,\n    \"properties\": null,\n    \"tags\": [],\n    \"fields\": [\"O_DELIVERYDATE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"notFuture\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>O_ORDERKEY</code> 2 does not satisfy the rule because its <code>O_DELIVERYDATE</code> is set in the future.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_DELIVERYDATE]\nB --&gt; C{Is O_DELIVERYDATE &lt;= Current Date?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    o_orderkey,\n    o_deliverydate\nfrom orders \nwhere\n    o_deliverydate &gt; current_date;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The value for <code>O_DELIVERYDATE</code> of <code>2023-10-25</code> is in the future.</p> <p>Shape Anomaly</p> <p>In <code>O_DELIVERYDATE</code>, 33.333% of 3 filtered records (1) are future times.</p>"},{"location":"checks/not-negative-check/","title":"Not Negative","text":""},{"location":"checks/not-negative-check/#definition","title":"Definition","text":"<p>Asserts that this is a non-negative number.</p>"},{"location":"checks/not-negative-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Fields</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/not-negative-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/not-negative-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/not-negative-check/#example","title":"Example","text":"<p>Objective: Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table is a non-negative number.</p> <p>Sample Data</p> L_ORDERKEY L_LINENUMBER L_QUANTITY 1 1 40 2 2 -5 3 1 20 Payload example <pre><code>{\n    \"description\": \"Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table is a non-negative number\",\n    \"coverage\": 1,\n    \"properties\": null,\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"notNegative\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entry with <code>L_ORDERKEY</code> 2 does not satisfy the rule because its <code>L_QUANTITY</code> value is a negative number.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_QUANTITY]\nB --&gt; C{Is L_QUANTITY &gt;= 0?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_linenumber,\n    l_quantity\nfrom lineitem \nwhere\n    l_quantity &lt; 0;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The value for <code>L_QUANTITY</code> of <code>-5</code> is a negative number.</p> <p>Shape Anomaly</p> <p>In <code>L_QUANTITY</code>, 33.333% of 3 filtered records (1) are negative numbers.</p>"},{"location":"checks/not-null-check/","title":"Not Null","text":""},{"location":"checks/not-null-check/#definition","title":"Definition","text":"<p>Asserts that none of the selected fields' values are explicitly set to nothing.</p>"},{"location":"checks/not-null-check/#field-scope","title":"Field Scope","text":"<p>Multi: The rule evaluates multiple specified fields.</p> <p>Accepted Fields</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/not-null-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/not-null-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/not-null-check/#example","title":"Example","text":"<p>Objective: Ensure that every record in the CUSTOMER table has an assigned value for the C_NAME and C_ADDRESS fields.</p> <p>Sample Data</p> C_CUSTKEY C_NAME C_ADDRESS 1 Alice 123 Oak St 2 Bob NULL 3 Charlie 789 Maple Ave 4 NULL 456 Pine Rd Payload example <pre><code>{\n    \"description\": \"Ensure that every record in the CUSTOMER table has an assigned value for the C_NAME and C_ADDRESS fields\",\n    \"coverage\": 1,\n    \"properties\": null,\n    \"tags\": [],\n    \"fields\": [\"C_ADDRESS\",\"C_NAME\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"notNull\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>C_CUSTKEY</code> 2 and 4 do not satisfy the rule because they have <code>NULL</code> values in the <code>C_NAME</code> or <code>C_ADDRESS</code> fields.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve C_NAME and C_ADDRESS]\nB --&gt; C{Are C_NAME and C_ADDRESS non-null?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    c_custkey,\n    c_name,\n    c_address\nfrom customer \nwhere\n    c_name is null or c_address is null;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>There is no assigned value for <code>C_NAME</code>.</p> <p>Shape Anomaly</p> <p>In <code>C_NAME</code> and <code>C_ADDRESS</code>, 50.000% of 4 filtered records (2) are not assigned values.</p>"},{"location":"checks/overview-of-a-check/","title":"Checks Overview","text":"<p>Checks in Qualytics are rules applied to data that ensure quality by validating accuracy, consistency, and integrity. Each check includes a data quality rule, along with filters, tags, tolerances, and notifications, allowing efficient management of data across tables and fields.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"checks/overview-of-a-check/#checks-types","title":"Checks Types","text":"<p>In Qualytics, you will come across two types of checks:</p>"},{"location":"checks/overview-of-a-check/#inferred-checks","title":"Inferred Checks","text":"<p>Qualytics automatically generates inferred checks during a Profile operation. These checks typically cover 80-90% of the rules needed by users. They are created and maintained through profiling, which involves statistical analysis and machine learning methods.</p> <p>For more details on Inferred Checks, please refer to the Inferred Check documentation.</p>"},{"location":"checks/overview-of-a-check/#authored-checks","title":"Authored Checks","text":"<p>Authored checks are manually created by users within the Qualytics platform or API. You can author many types of checks, ranging from simple templates for common checks to complex rules using Spark SQL and User-Defined Functions (UDF) in Scala.</p> <p>For more details on Authored Checks, please refer to the Authored Checks documentation.</p>"},{"location":"checks/overview-of-a-check/#editing-computed-assets-from-checks","title":"Editing Computed Assets from Checks","text":"<p>Qualytics allows users to edit computed tables and computed fields directly from check interfaces. This feature helps users quickly update computed logic while working on data quality checks\u2014without leaving the check context.</p>"},{"location":"checks/overview-of-a-check/#what-you-can-do","title":"What You Can Do","text":""},{"location":"checks/overview-of-a-check/#edit-computed-assets-from-a-check","title":"Edit Computed Assets from a Check","text":"<p>From a Check Details screen, users can:</p> <ul> <li>Edit the computed table used by the check</li> <li>Edit the computed field used by the check</li> </ul> <p>The edit action opens the same Edit Computed dialog available from the container page, but directly within the check context. This allows users to quickly adjust logic when a check fails or needs refinement.</p> <p></p>"},{"location":"checks/overview-of-a-check/#keyboard-shortcut","title":"Keyboard Shortcut","text":""},{"location":"checks/overview-of-a-check/#e-edit-computed-asset","title":"<code>E</code> \u2014 Edit Computed Asset","text":"<p>When focused on a computed asset within a check:</p> <ul> <li>Press <code>E</code> to open Edit Computed</li> <li>This opens the Edit Computed Table or Edit Computed Field dialog</li> </ul> <p></p> <p>Note</p> <p>On this screen, the \u201cE\u201d keyboard shortcut is used to open Edit Computed.</p>"},{"location":"checks/overview-of-a-check/#dependency-protection","title":"Dependency Protection","text":"<p>Qualytics automatically protects dependencies when computed assets are edited.</p>"},{"location":"checks/overview-of-a-check/#when-a-dependency-is-affected","title":"When a Dependency Is Affected","text":"<p>If an edit to a computed table or field would impact an existing check\u2014for example:</p> <ul> <li>Renaming or removing a field used by the check</li> <li>Modifying a query that drops a referenced column</li> </ul> <p>Qualytics will display a warning message before the change is applied.</p> <p>The warning clearly lists:</p> <ul> <li>Fields that will be removed</li> <li>Checks and anomalies that depend on those fields</li> </ul>"},{"location":"checks/overview-of-a-check/#proceeding-with-the-change","title":"Proceeding with the Change","text":"<p>If you choose Proceed Anyway:</p> <ul> <li>The affected check(s) will be deleted</li> <li>Related anomalies will also be removed</li> <li>Qualytics will automatically redirect you to the most appropriate page (for example, the container page)</li> </ul> <p>This behavior ensures system consistency and prevents broken references.</p>"},{"location":"checks/overview-of-a-check/#important-considerations","title":"Important Considerations","text":"<ul> <li>There are no strict limitations on editing computed assets</li> <li>Changes may impact dependent checks</li> <li>Always review dependency warnings before proceeding</li> <li>To preserve an existing check, avoid removing or renaming fields it depends on</li> </ul>"},{"location":"checks/overview-of-a-check/#view-manage-checks","title":"View &amp; Manage Checks","text":"<p>Checks tab in Qualytics provides users with an interface to view and manage various checks associated with their data. These checks are accessible through two different methods, as discussed below.</p>"},{"location":"checks/overview-of-a-check/#method-1-datastore-specific-checks","title":"Method 1: Datastore-Specific Checks","text":"<p>Step 1: Log in to your Qualytics account and select the datastore from the left menu.</p> <p></p> <p>Step 2: Click the \"Checks\" from the navigation tab.</p> <p></p> <p>You will see a list of all the checks that have been applied to the selected datastore.</p> <p></p> <p>You can switch between different types of checks to view them categorically (such as All, Active, Draft, and Archived).</p> <p></p>"},{"location":"checks/overview-of-a-check/#method-2-explore-section","title":"Method 2: Explore Section","text":"<p>Step 1: Log in to your Qualytics account and click the Explore button on the left side panel of the interface.</p> <p></p> <p>Step 2: Click the \"Checks\" from the navigation tab.</p> <p></p> <p>You'll see a list of all the checks that have been applied to various tables and fields across different source datastores.</p> <p></p>"},{"location":"checks/overview-of-a-check/#check-templates","title":"Check Templates","text":"<p>Check Templates empower users to efficiently create, manage, and apply standardized checks across various datastores, acting as blueprints that ensure consistency and data integrity across different datasets and processes.</p> <p>Check templates streamline the validation process by enabling check management independently of specific data assets such as datastores, containers, or fields. These templates reduce manual intervention, minimize errors, and provide a reusable framework that can be applied across multiple datasets, ensuring all relevant data adheres to defined criteria. This not only saves time but also enhances the reliability of data quality checks within an organization.</p> <p>For more details about check templates, please refer to the Check Templates documentation.</p>"},{"location":"checks/overview-of-a-check/#apply-check-template-for-quality-checks","title":"Apply Check Template for Quality Checks","text":"<p>You can export check templates to make quality checks easier and more consistent. Using a set template lets you quickly verify that your data meets specific standards, reducing mistakes and improving data quality. Exporting these templates simplifies the process, making finding and fixing errors more efficient, and ensuring your quality checks are applied across different projects or systems without starting from scratch.</p> <p>For more details on how to apply check templates for quality checks, please refer to the Apply Check Template for Quality Checks documentation.</p>"},{"location":"checks/overview-of-a-check/#export-check-templates","title":"Export Check Templates","text":"<p>You can export check templates to easily share or reuse your quality check settings across different systems or projects. This saves time by eliminating the need to recreate the same checks repeatedly and ensures that your quality standards are consistently applied. Exporting templates helps maintain accuracy and efficiency in managing data quality across various environments.</p> <p>For more details about export check templates, please refer to the Export Check Templates documentation.</p>"},{"location":"checks/overview-of-a-check/#manage-checks-in-datastore","title":"Manage Checks in Datastore","text":"<p>Managing your checks within a datastore is important to maintain data integrity and ensure quality. You can categorize, create, update, archive, restore, delete, and clone checks, making it easier to apply validation rules across the datastores. The system allows for checks to be set as active, draft, or archived based on their current state of use. You can also define reusable templates for quality checks to streamline the creation of multiple checks with similar criteria. With options for important and favorite, users have full flexibility to manage data quality efficiently.</p> <p>For more details on how to manage checks in datastores, please refer to the Manage Checks in Datastore documentation.</p>"},{"location":"checks/overview-of-a-check/#check-rule-types","title":"Check Rule Types","text":"<p>In Qualytics, a variety of check rule types are provided to maintain data quality and integrity. These rules define specific criteria that data must meet, and checks apply these rules during the validation process.</p> <p>For more details about check rule types, please refer to the Rule Types Overview documentation.</p> Rule Type Description After Date Time Asserts that the field is a timestamp later than a specific date and time. Aggregation Comparison Verifies that the specified comparison operator evaluates true when applied to two aggregation expressions. Any Not Null Asserts that one of the fields must not be null. Before Date Time Asserts that the field is a timestamp earlier than a specific date and time. Between Asserts that values are equal to or between two numbers. Between Times Asserts that values are equal to or between two dates or times. Contains Credit Card Asserts that the values contain a credit card number. Contains Email Asserts that the values contain email addresses. Contains Social Security Number Asserts that the values contain social security numbers. Contains Url Asserts that the values contain valid URLs. Data Diff Asserts that the dataset created by the targeted field(s) has differences compared to the referred field(s). Distinct Count Asserts on the approximate count distinct of the given column. Entity Resolution Asserts that every distinct entity is appropriately represented once and only once. Equal To Asserts that all of the selected fields equal a value. Equal To Field Asserts that this field is equal to another field. Exists in Asserts if the rows of a compared table/field of a specific Datastore exists in the selected table/field. Expected Schema Asserts that all selected fields are present and that all declared data types match expectations. Expected Values Asserts that values are contained within a list of expected values. Field Count Asserts that there must be exactly a specified number of fields. Freshness Check Asserts that data was added or updated in the data asset after a declared time. Greater Than Asserts that the field is a number greater than (or equal to) a value. Greater Than Field Asserts that this field is greater than another field. Is Address Asserts that the values contain the specified required elements of an address. Is Credit Card Asserts that the values are credit card numbers. Is Replica Of (is sunsetting) Asserts that the dataset created by the targeted field(s) is replicated by the referred field(s). Is Type Asserts that the data is of a specific type. Less Than Asserts that the field is a number less than (or equal to) a value. Less Than Field Asserts that this field is less than another field. Matches Pattern Asserts that a field must match a pattern. Max Length Asserts that a string has a maximum length. Max Partition Size Asserts the maximum number of records that should be loaded from each file or table partition. Max Value Asserts that a field has a maximum value. Metric Records the value of the selected field during each scan operation and asserts that the value is within a specified range (inclusive). Min Length Asserts that a string has a minimum length. Min Partition Size Asserts the minimum number of records that should be loaded from each file or table partition. Min Value Asserts that a field has a minimum value. Not Exists In Asserts that values assigned to this field do not exist as values in another field. Not Future Asserts that the field's value is not in the future. Not Negative Asserts that this is a non-negative number. Not Null Asserts that the field's value is not explicitly set to nothing. Positive Asserts that this is a positive number. Predicted By Asserts that the actual value of a field falls within an expected predicted range. Required Values Asserts that all of the defined values must be present at least once within a field. Satisfies Expression Evaluates the given expression (any valid <code>Spark SQL</code>) for each record. Sum Asserts that the sum of a field is a specific amount. Time Distribution Size Asserts that the count of records for each interval of a timestamp is between two numbers. Unique Asserts that the field's value is unique. Volumetric Asserts that the data volume (rows or bytes) remains within dynamically inferred thresholds based on historical trends (daily, weekly, monthly)."},{"location":"checks/positive-check/","title":"Positive","text":""},{"location":"checks/positive-check/#definition","title":"Definition","text":"<p>Asserts that this is a positive number.</p>"},{"location":"checks/positive-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Fields</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/positive-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/positive-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/positive-check/#example","title":"Example","text":"<p>Objective: Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table is a positive number.</p> <p>Sample Data</p> L_ORDERKEY L_LINENUMBER L_QUANTITY 1 1 40 2 1 0 3 1 -5 4 1 20 Payload example <pre><code>{\n    \"description\": \"Ensure that the quantity of items (L_QUANTITY) in the LINEITEM table is a positive number\",\n    \"coverage\": 1,\n    \"properties\": null,\n    \"tags\": [],\n    \"fields\": [\"L_QUANTITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"positive\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 2 and 3 do not satisfy the rule because their <code>L_QUANTITY</code> values are not positive numbers.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_QUANTITY]\nB --&gt; C{Is L_QUANTITY Positive?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_linenumber,\n    l_quantity\nfrom lineitem \nwhere\n    l_quantity &lt;= 0;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The value for <code>L_QUANTITY</code> of <code>-5</code> is not a positive number.</p> <p>Shape Anomaly</p> <p>In <code>L_QUANTITY</code>, 50.000% of 4 filtered records (2) are not positive numbers.</p>"},{"location":"checks/predicted-by-check/","title":"Predicted By","text":""},{"location":"checks/predicted-by-check/#definition","title":"Definition","text":"<p>Asserts that the actual value of a field falls within an expected predicted range.</p>"},{"location":"checks/predicted-by-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>Predicted By</code> rule is used to verify whether the actual values of a specific field align with a set of expected values that are derived from a prediction expression. This expression could be a mathematical formula, statistical calculation, or any other valid predictive logic.</p>"},{"location":"checks/predicted-by-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Fields</p> Type <code>Integral</code> <code>Fractional</code> <code>Date</code> <code>Timestamp</code>"},{"location":"checks/predicted-by-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/predicted-by-check/#specific-properties","title":"Specific Properties","text":"<p>Determines if the actual value of a field falls within an expected predicted range.</p> Name Description Expression The prediction expression or formula for the field. Tolerance The allowed deviation from the predicted value. Note <p>The tolerance level must be defined to allow a permissible range of deviation from the predicted values.</p> <p>Here\u2019s a simple breakdown:</p> <ul> <li>An expression predicts what the value of a field should be.</li> <li>A tolerance value specifies how much deviation from the predicted value is acceptable.</li> <li>The actual value is then compared against the range defined by the predicted value \u00b1 tolerance.</li> </ul>"},{"location":"checks/predicted-by-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/predicted-by-check/#example","title":"Example","text":"<p>Objective: Ensure that the discount (L_DISCOUNT) in the LINEITEM table is calculated correctly based on the actual price (L_EXTENDEDPRICE). A correct discount should be approximately 8% less than the actual price, within a tolerance of \u00b12.</p> <p>Sample Data</p> L_ORDERKEY L_LINENUMBER L_EXTENDEDPRICE L_DISCOUNT 1 1 100 8 2 1 100 12 3 1 100 9 Inputs <ul> <li>Expression: L_EXTENDEDPRICE \u00d7 0.08</li> <li>Tolerance: 2</li> </ul> Payload example <pre><code>{\n    \"description\": \"Ensure that the discount (L_DISCOUNT) in the LINEITEM table is calculated correctly based on the actual price (L_EXTENDEDPRICE). A correct discount should be approximately 8% less than the actual price, within a tolerance of \u00b12\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"expression\": \"L_EXTENDEDPRICE \u00d7 0.08\",\n        \"tolerance\": 2\n    },\n    \"tags\": [],\n    \"fields\": [\"L_DISCOUNT\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"predictedBy\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>For the entry with <code>L_ORDERKEY</code> 2, the discount is 12, which is outside of the computed range. Based on an 8% expected discount with a tolerance of \u00b12, the discount should be between 6 and 10 (calculated from the actual price of 100). Therefore, this record is marked as anomalous.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_EXTENDEDPRICE and L_DISCOUNT]\nB --&gt; C{Is Discount within Predicted Range?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_linenumber,\n    l_extendedprice,\n    l_discount\nfrom lineitem \nwhere\n    l_discount not between l_extendedprice * 0.06 and l_extendedprice * 0.10;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The <code>L_DISCOUNT</code> value of '12' is not within the predicted range defined by L_EXTENDEDPRICE * 0.08 +/- 2.0</p> <p>Shape Anomaly</p> <p>In <code>L_DISCOUNT</code>, 33.333% of 3 filtered records (1) are not within the predicted range defined by L_EXTENDEDPRICE * 0.08 +/- 2.0</p>"},{"location":"checks/required-values-check/","title":"Required Values","text":""},{"location":"checks/required-values-check/#definition","title":"Definition","text":"<p>Asserts that all of the defined values must be present at least once within a field.</p> <p>Info</p> <p>Visual warnings for spacing issues are available across all forms where values are entered. When a value includes leading or trailing spaces, the system automatically highlights it inside a warning-colored chip and displays a tooltip message indicating the issue in real time \u2014 for example:</p> <p>\u201cThe following value has leading or trailing spaces: \u2018ship \u2019\u201d</p> <p></p>"},{"location":"checks/required-values-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/required-values-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/required-values-check/#specific-properties","title":"Specific Properties","text":"<p>Ensures that a specific set of values is present within a field.</p> Name Description Values Specifies the list of values that must exist in the field."},{"location":"checks/required-values-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/required-values-check/#example","title":"Example","text":"<p>Objective: Ensure that orders have priorities labeled as '1-URGENT', '2-HIGH', '3-MEDIUM', '4-LOW', and '5-NOT URGENT'.</p> <p>Sample Data</p> O_ORDERKEY O_ORDERPRIORITY 1 1-URGENT 2 2-HIGH 3 3-MEDIUM 4 3-MEDIUM Payload example <pre><code>{\n    \"description\": \"Ensure that orders have priorities labeled as '1-URGENT', '2-HIGH', '3-MEDIUM', '4-LOW', and '5-NOT URGENT'\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"list\":[\"1-URGENT\",\"2-HIGH\",\"3-MEDIUM\",\"4-LOW\",\"5-NOT URGENT\"]\n    },\n    \"tags\": [],\n    \"fields\": [\"O_ORDERPRIORITY\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"requiredValues\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the rule is violated because the values '4-LOW' and '5-NOT URGENT' are not present in the <code>O_ORDERPRIORITY</code> field of the ORDERS table.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B{Check if all specified values exist in the field}\nB --&gt;|Yes| C[End: No Anomalies]\nB --&gt;|No| D[Mark as Anomalous: Missing Values]\nD --&gt; C</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect distinct\n    o_orderpriority\nfrom orders\nwhere o_orderpriority in ('1-URGENT', '2-HIGH', '3-MEDIUM', '4-LOW', '5-NOT URGENT');\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>In <code>O_ORDERPRIORITY</code>, required values are missing in 40.000% of filtered records.</p>"},{"location":"checks/rule-types-overview/","title":"Rule Types Overview","text":"<p>In Qualytics, a variety of rule types are provided to maintain data quality and integrity.These rules define specific criteria that data must meet, and checks apply these rules during the validation process.</p> <p>Here\u2019s an overview of the rule types and their purposes:</p>"},{"location":"checks/rule-types-overview/#check-rule-types","title":"Check Rule Types","text":"Rule Type Description After Date Time Asserts that the field is a timestamp later than a specific date and time. Aggregation Comparison Verifies that the specified comparison operator evaluates true when applied to two aggregation expressions. Any Not Null Asserts that one of the fields must not be null. Before DateTime Asserts that the field is a timestamp earlier than a specific date and time. Between Asserts that values are equal to or between two numbers. Between Times Asserts that values are equal to or between two dates or times. Contains Credit Card Asserts that the values contain a credit card number. Contains Email Asserts that the values contain email addresses. Contains Social Security Number Asserts that the values contain social security numbers. Contains Url Asserts that the values contain valid URLs. Data Diff Asserts that the dataset created by the targeted field(s) has differences compared to the referred field(s). Distinct Count Asserts on the approximate count distinct of the given column. Entity Resolution Asserts that every distinct entity is appropriately represented once and only once Equal To Field Asserts that this field is equal to another field. Exists in Asserts if the rows of a compared table/field of a specific Datastore exists in the selected table/field. Expected Schema Asserts that all selected fields are present and that all declared data types match expectations. Expected Values Asserts that values are contained within a list of expected values. Field Count Asserts that there must be exactly a specified number of fields. Greater Than Asserts that the field is a number greater than (or equal to) a value. Greater Than Field Asserts that this field is greater than another field. Is Address Asserts that the values contain the specified required elements of an address. Is Credit Card Asserts that the values are credit card numbers. Is Replica Of (is sunsetting) Asserts that the dataset created by the targeted field(s) is replicated by the referred field(s). Is Type Asserts that the data is of a specific type. Less Than Asserts that the field is a number less than (or equal to) a value. Less Than Field Asserts that this field is less than another field. Matches Pattern Asserts that a field must match a pattern. Max Length Asserts that a string has a maximum length. Max Value Asserts that a field has a maximum value. Metric Records the value of the selected field during each scan operation and asserts that the value is within a specified range (inclusive). Min Length Asserts that a string has a minimum length. Min Partition Size Asserts the minimum number of records that should be loaded from each file or table partition. Min Value Asserts that a field has a minimum value. Not Exists In Asserts that values assigned to this field do not exist as values in another field. Not Future Asserts that the field's value is not in the future. Not Negative Asserts that this is a non-negative number. Not Null Asserts that the field's value is not explicitly set to nothing. Positive Asserts that this is a positive number. Predicted By Asserts that the actual value of a field falls within an expected predicted range. Required Values Asserts that all of the defined values must be present at least once within a field. Satisfies Expression Evaluates the given expression (any valid <code>Spark SQL</code>) for each record. Sum Asserts that the sum of a field is a specific amount. Time Distribution Size Asserts that the count of records for each interval of a timestamp is between two numbers. Unique Asserts that the field's value is unique. Volumetric Check Asserts that the volume of the data asset has not changed by more than an inclusive percentage amount for the prescribed moving daily average."},{"location":"checks/satisfies-expression-check/","title":"Satisfies Expression","text":""},{"location":"checks/satisfies-expression-check/#definition","title":"Definition","text":"<p>Evaluates the given expression (any valid Spark SQL) for each record.</p>"},{"location":"checks/satisfies-expression-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>Satisfies Expression</code> rule allows for a wide range of custom validations on the dataset. By defining a Spark SQL expression, you can create customized conditions that the data should meet.</p> <p>This rule will evaluate an expression against each record, marking those that do not satisfy the condition as anomalies. It provides the flexibility to create complex validation logic without being restricted to predefined rule structures.</p>"},{"location":"checks/satisfies-expression-check/#field-scope","title":"Field Scope","text":"<p>Calculated: The rule automatically identifies the fields involved, without requiring explicit field selection.</p>"},{"location":"checks/satisfies-expression-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/satisfies-expression-check/#specific-properties","title":"Specific Properties","text":"<p>Evaluates each record against a specified Spark SQL expression to ensure it meets custom validation conditions.</p> Name Description Expression Defines the Spark SQL expression that each record should meet. <p>Info</p> <p>Refers to the Filter Guide in the General Properties topic for examples of valid Spark SQL expressions.</p>"},{"location":"checks/satisfies-expression-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/satisfies-expression-check/#example-1-satisfies-expression-using-a-case-statement","title":"Example 1: Satisfies Expression Using a <code>CASE</code> Statement","text":"<p>Let's assume you want to ensure that for orders with a priority of '1-URGENT' or '2-HIGH', the <code>orderstatus</code> must be 'O' (for open), and for orders with a priority of '3-MEDIUM', the <code>orderstatus</code> must be either 'O' or 'P' (for pending).</p> <pre><code> CASE\n    WHEN o_orderpriority IN ('1-URGENT', '2-HIGH') AND o_orderstatus != 'O' THEN FALSE\n    WHEN o_orderpriority = '3-MEDIUM' AND o_orderstatus NOT IN ('O', 'P') THEN FALSE\n    ELSE TRUE\n END\n</code></pre>"},{"location":"checks/satisfies-expression-check/#example-2-satisfies-expression-using-a-relatively-complex-cte-statement","title":"Example 2: Satisfies Expression Using a <code>Relatively Complex CTE</code> Statement","text":"<p>Objective:: To ensure that the overall effect of discounts on item prices remains within acceptable limits, we validate whether the average discounted price of all items is greater than the maximum discount applied to any single item.</p> <p>Background:</p> <p>In pricing analysis, it\u2019s important to monitor how discounts affect the final prices of products. By comparing the average price after discounts with the maximum discount applied, we can assess whether the discounts are having an overly significant impact or if they are within a reasonable range.</p> <pre><code>CASE \n        WHEN (SELECT AVG(l_extendedprice * (1 - l_discount)) FROM lineitem) &gt; \n             (SELECT MAX(l_discount) FROM {{ _qualytics_self }}) \n        THEN TRUE \n        ELSE FALSE \nEND AS is_discount_within_limits\n</code></pre>"},{"location":"checks/satisfies-expression-check/#use-case","title":"Use Case","text":"<p>Objective: Ensure that the total tax applied to each item in the LINEITEM table is not more than 10% of the extended price.</p> <p>Sample Data</p> L_ORDERKEY L_LINENUMBER L_EXTENDEDPRICE L_TAX 1 1 10000 900 2 1 15000 2000 3 1 20000 1800 4 1 10000 1500 Inputs <ul> <li>Expression: L_TAX &lt;= L_EXTENDEDPRICE * 0.10</li> </ul> Payload example <pre><code>{\n    \"description\": \"Ensure that the total tax applied to each item in the LINEITEM table is not more than 10% of the extended price\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"expression\":\"L_TAX &lt;= L_EXTENDEDPRICE * 0.10\"\n        },\n    \"tags\": [],\n    \"fields\": [\"L_TAX\", \"L_EXTENDEDPRICE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"satisfiesExpression\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>L_ORDERKEY</code> 2 and 4 do not satisfy the rule because the <code>L_TAX</code> values are more than 10% of their respective <code>L_EXTENDEDPRICE</code> values.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_EXTENDEDPRICE and L_TAX]\nB --&gt; C{Is L_TAX &lt;= L_EXTENDEDPRICE * 0.10?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect\n    l_orderkey,\n    l_linenumber,\n    l_extendedprice,\n    l_tax\nfrom\n    lineitem \nwhere\n    l_tax &gt; l_extendedprice * 0.10;\n</code></pre> <p>Potential Violation Messages</p> <p>Record Anomaly</p> <p>The record does not satisfy the expression: <code>L_TAX &lt;= L_EXTENDEDPRICE * 0.10</code></p> <p>Shape Anomaly</p> <p>50.000% of 4 filtered records (2) do not satisfy the expression: <code>L_TAX &lt;= L_EXTENDEDPRICE * 0.10</code></p>"},{"location":"checks/sum-check/","title":"Sum","text":""},{"location":"checks/sum-check/#definition","title":"Definition","text":"<p>Asserts that the sum of a field is a specific amount.</p>"},{"location":"checks/sum-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Integral</code> <code>Fractional</code>"},{"location":"checks/sum-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/sum-check/#specific-properties","title":"Specific Properties","text":"<p>Ensures that the total sum of values in a specified field matches a defined amount.</p> Name Description Sum Specifies the expected sum of the values in the field."},{"location":"checks/sum-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/sum-check/#example","title":"Example","text":"<p>Objective: Ensure that the total discount value in the LINEITEM table does not exceed $2000.</p> <p>Sample Data</p> L_ORDERKEY L_LINENUMBER L_EXTENDEDPRICE L_DISCOUNT L_DISCOUNT_VALUE 1 1 10000 0.05 500 2 1 8000 0.10 800 3 1 7000 0.05 350 4 1 5000 0.10 500 Payload example <pre><code>{\n    \"description\": \"Ensure that the total discount value in the LINEITEM table does not exceed $2000\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"value\": \"2000\"\n    },\n    \"tags\": [],\n    \"fields\": [\"L_DISCOUNT_VALUE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"sum\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the total of the <code>L_DISCOUNT_VALUE</code> column is (500 + 800 + 350 + 500 = 2150), which exceeds the specified maximum total discount value of $2000.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve L_DISCOUNT_VALUE]\nB --&gt; C{Sum of L_DISCOUNT_VALUE &lt;= 2000?}\nC --&gt;|Yes| D[End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect \n    sum(l_discount_value) as total_discount_value\nfrom \n    lineitem \nhaving \n    sum(l_discount_value) &gt; 2000;\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>In <code>L_DISCOUNT_VALUE</code>, the sum of the 4 records is not 2000.000</p>"},{"location":"checks/time-distribution-size-check/","title":"Time Distribution Size","text":""},{"location":"checks/time-distribution-size-check/#definition","title":"Definition","text":"<p>Asserts that the count of records for each interval of a timestamp is between two numbers.</p>"},{"location":"checks/time-distribution-size-check/#in-depth-overview","title":"In-Depth Overview","text":"<p>The <code>Time Distribution Size</code> rule helps in identifying irregularities in the distribution of records over time intervals such as hours, days, or months.</p> <p>For instance, in a retail context, it could ensure that there\u2019s a consistent number of orders each month to meet business targets. A sudden drop in orders might highlight operational issues or shifts in market demand that require immediate attention.</p>"},{"location":"checks/time-distribution-size-check/#field-scope","title":"Field Scope","text":"<p>Single: The rule evaluates a single specified field.</p> <p>Accepted Types</p> Type <code>Timestamp</code> <code>Date</code>"},{"location":"checks/time-distribution-size-check/#specific-properties","title":"Specific Properties","text":"Name Description Interval Defines the time interval for segmentation. Min Count Specifies the minimum count of records in each segment. Max Count Specifies the maximum count of records in each segment."},{"location":"checks/time-distribution-size-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/time-distribution-size-check/#example","title":"Example","text":"<p>Objective: Ensure that the number of orders for each month is consistently between 5 and 10.</p> <p>Sample Data</p> O_ORDERKEY O_ORDERDATE 1 2023-01-01 2 2023-01-15 3 2023-01-20 4 2023-01-25 5 2023-02-01 6 2023-02-05 7 2023-02-10 8 2023-02-15 9 2023-02-20 10 2023-02-25 11 2023-02-28 Payload example <pre><code>{\n    \"description\": \"Ensure that the number of orders for each month is consistently between 5 and 10\",\n    \"coverage\": 1,\n    \"properties\": {\n        \"interval_name\": \"Monthly\",\n        \"min_size\": 5,\n        \"max_size\": 10\n    },\n    \"tags\": [],\n    \"fields\": [\"O_ORDERDATE\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"timeDistributionSize\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the January segment fails the rule because there are only 4 orders, which is below the specified minimum count of 5.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve O_ORDERDATE]\nB --&gt; C{Segment data by month}\nC --&gt; D{Is count of records in each segment between 5 and 10?}\nD --&gt;|Yes| E[End]\nD --&gt;|No| F[Mark as Anomalous]\nF --&gt; E</code></pre> <pre><code>-- An illustrative SQL query demonstrating the rule applied to example dataset(s).\nselect \n    extract(month from o_orderdate) as month,\n    count(*) as order_count\nfrom \n    orders \ngroup by \n    extract(month from o_orderdate)\nhaving \n    count(*) &lt; 5\n    or count(*) &gt; 10;\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>50.000% of the monthly segments of <code>O_ORDERDATE</code> have record counts not between 5 and 10.</p>"},{"location":"checks/unique-check/","title":"Unique","text":""},{"location":"checks/unique-check/#definition","title":"Definition","text":"<p>Asserts that every value held by a field appears only once. If multiple fields are specified, then every combination of values of the fields should appear only once.</p>"},{"location":"checks/unique-check/#field-scope","title":"Field Scope","text":"<p>Multi: The rule evaluates multiple specified fields.</p> <p>Accepted Types</p> Type <code>Date</code> <code>Timestamp</code> <code>Integral</code> <code>Fractional</code> <code>String</code> <code>Boolean</code>"},{"location":"checks/unique-check/#general-properties","title":"General Properties","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"checks/unique-check/#anomaly-types","title":"Anomaly Types","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"checks/unique-check/#example","title":"Example","text":"<p>Objective: Ensure that each combination of C_NAME and C_ADDRESS in the CUSTOMER table is unique.</p> <p>Sample Data</p> C_CUSTKEY C_NAME C_ADDRESS 1 Customer_A 123 Main St 2 Customer_B 456 Oak Ave 3 Customer_A 123 Main St 4 Customer_C 789 Elm St Payload example <pre><code>{\n    \"description\": \"Ensure that each combination of C_NAME and C_ADDRESS in the CUSTOMER table is unique\",\n    \"coverage\": 1,\n    \"properties\": null,\n    \"tags\": [],\n    \"fields\": [\"C_NAME\", \"C_ADDRESS\"],\n    \"additional_metadata\": {\"key 1\": \"value 1\", \"key 2\": \"value 2\"},\n    \"rule\": \"unique\",\n    \"container_id\": {container_id},\n    \"template_id\": {template_id},\n    \"filter\": \"1=1\"\n}\n</code></pre> <p>Anomaly Explanation</p> <p>In the sample data above, the entries with <code>C_CUSTKEY</code> 1 and 3 have the same <code>C_NAME</code> and <code>C_ADDRESS</code>, which violates the rule because this combination of keys should be unique.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve C_NAME and C_ADDRESS]\nB --&gt; C{Is the combination unique?}\nC --&gt;|Yes| D[Move to Next Record/End]\nC --&gt;|No| E[Mark as Anomalous]\nE --&gt; D</code></pre> <pre><code>-- An illustrative SQL query to find non-unique C_NAME and C_ADDRESS combinations.\nselect\n    c_custkey,\n    c_name,\n    c_address\nfrom customer \ngroup by c_name, c_address\nhaving count(*) &gt; 1;\n</code></pre> <p>Potential Violation Messages</p> <p>Shape Anomaly</p> <p>In <code>C_NAME</code> and <code>C_ADDRESS</code>, 25.000% of 4 filtered records (1) are not unique.</p>"},{"location":"checks/volumetric-check/","title":"Volumetric Check","text":"<p>Volumetric Check ensures data stability by monitoring dataset size fluctuations in rows or bytes. It detects anomalies by comparing current volumes against historical trends (daily, weekly, monthly). Users can configure rules for precise control, while automated threshold adjustments enhance accuracy over time.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"checks/volumetric-check/#configure-volumetric-check","title":"Configure Volumetric Check","text":"<p>Step 1: Login into your Qualytics account and select the datastore from the left menu on which you want to add a volumetric check.</p> <p></p> <p>Step 2: Click the Add button and select Checks.</p> <p></p> <p>Step 3: A modal window appears. Enter the required details to configure the Volumetric Check.</p> <p></p> <p>Step 4: Enter the details to configure the volumetric check:</p> No. Field Description 1. Rule Type Select the Volumetric Rule type from the dropdown. 2. Table Select the table for the rule to apply. <p></p> <p>3. Comparison:  Specifies the type of comparison: Absolute Change, Absolute Value, or Percentage Change:</p> <p></p> <p>Details</p> <p></p> <p>Details</p> <p></p> <p>Details</p> <p></p> <p>4. Measurement Period Days: Enter the number of days for measurement.</p> <p></p> <p>5. Threshold: At least the Min or Max value must be specified, and including both is optional. These values determine the acceptable range or limit of change in the field's value.</p> <p></p> <p>Min Value</p> <ul> <li> <p>Represents the minimum allowable increase in the field's value.</p> </li> <li> <p>A negative Min Value signifies an allowable decrease, determining the minimum value the field can drop to be considered valid.</p> </li> </ul> <p></p> <p>Max Value</p> <ul> <li>Indicates the maximum allowable increase in the field\u2019s value, setting an upper limit for the value's acceptable growth or change.</li> </ul> <p></p> No. Field Description 6. Description Enter a description for the check. 7. Tag Add tags for categorizing the check. 8. Additional Metadata Add custom metadata for additional details. <p></p> <p>Step 4: After completing all the check details, click on the \"Validate\" button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct. It ensures that the check will work as expected by running it against the data without committing any changes.</p> <p></p> <p>If the validation is successful, a green message will appear saying \"Validation Successful\".</p> <p></p> <p>Step 5: Once you have a successful validation, click the \"Save\" button.</p> <p></p> <p>After clicking on the \u201cSave\u201d button your check is successfully created and a success flash message will appear saying \u201cCheck successfully created\u201d.</p> <p></p>"},{"location":"checks/volumetric-check/#comparison-options","title":"Comparison Options","text":"<p>Absolute Value</p> <p>The <code>Absolute Value</code> comparison works by comparing the change in a numeric field's value to a pre-set limit <code>between</code> Min and Max values. If the field's value changes by more than this specified range since the last relevant scan, an anomaly is identified.</p> Illustration <p>The value of the record in each scan should be within 100 and 300 to be considered normal</p> <p>Thresholds: Min Value = 100, Max Value = 300</p> Scan Current Value Anomaly Detected #1 150 No #2 90 Yes #3 250 No #4 310 Yes"},{"location":"checks/volumetric-check/#comparison-options_1","title":"Comparison Options","text":"<p>Absolute Change</p> <p>The <code>Absolute Change</code> comparison works by comparing the change in a numeric field's value to a pre-set limit (Min / Max). If the field's value changes by more than this specified limit since the last relevant scan, an anomaly is identified.</p> Illustration <p>Any record with a value change smaller than 30 or greater than 70 compared to the last scan should be flagged as anomalous</p> <p>Thresholds: Min Change = 30, Max Change = 70</p> Scan Previous Value Current Value Absolute Change Anomaly Detected #1 - 100 - No #2 100 150 50 No #3 150 220 70 No #4 220 300 80 Yes"},{"location":"checks/volumetric-check/#comparison-options_2","title":"Comparison Options","text":"<p>Percentage Change</p> <p>The <code>Percentage Change</code> comparison operates by tracking changes in a numeric field's value relative to its previous value. If the change exceeds the predefined percentage (%) limit since the last relevant scan, an anomaly is generated.</p> Illustration <p>An anomaly is identified if the record's value decreases by more than 20% or increases by more than 50% compared to the last scan.</p> <p>Thresholds: Min Percentage Change = -20%, Max Percentage Change = 50%</p> <p>Percentage Change Formula: ( (current_value - previous_value) / previous_value ) * 100</p> Scan Previous Value Current Value Percentage Change Anomaly Detected 1 - 100 - No 2 100 150 50% No 3 150 120 -20% No 4 120 65 -45.83% Yes 5 65 110 69.23% Yes"},{"location":"checks/volumetric-check/#how-it-works","title":"How It Works","text":"<p>The system automatically infers and maintains volumetric checks based upon observed daily, weekly, and monthly averages. These checks enable proactive management of data volume trends, ensuring that any unexpected deviations are identified as anomalies for review.</p>"},{"location":"checks/volumetric-check/#automating-adaptive-volumetric-checks","title":"Automating Adaptive Volumetric Checks","text":"<p>The following Volumetric Checks are automatically inferred for data assets with automated volume measurements enabled:</p> <ul> <li> <p>Daily: the expected daily volume expressed as an absolute minimum and maximum threshold. The thresholds are calculated as standard deviations from the previous 7-day moving average.  </p> </li> <li> <p>Weekly: the expected weekly volume expressed as an absolute minimum and maximum threshold. The thresholds are calculated as standard deviations from the previous four weeks\u2019 weekly volume moving average.  </p> </li> <li> <p>Monthly: the expected 4-week volume expressed as an absolute minimum and maximum threshold. The thresholds are calculated as standard deviations from the previous sixteen weeks\u2019 4-week volume moving average.</p> </li> </ul>"},{"location":"checks/volumetric-check/#scan-assertion-and-anomaly-creation","title":"Scan Assertion and Anomaly Creation","text":"<p>Volumetric Checks are asserted during a Scan Operation just like all other check types and enrichment of volumetric check anomalies is fully supported. This enables full support for custom scheduling of volumetric checks and remediation workflows of volumetric anomalies.</p>"},{"location":"checks/volumetric-check/#adaptive-thresholds-and-manual-adjustments","title":"Adaptive Thresholds and Manual Adjustments","text":"<p>Each time data volume is measured for an asset, the system automatically updates the inferred Volumetric Checks.</p> <p>1.Automatic Threshold Adjustment:</p> <ul> <li> <p>The system sets initial thresholds at 2 standard deviations from the moving average.</p> </li> <li> <p>Over time, these thresholds adjust automatically using historical data trends to improve accuracy.</p> </li> </ul> <p>2.Continuous Learning:</p> <ul> <li>The system monitors past data and adapts thresholds to detect unusual data volume changes.</li> </ul> <p>3.Why It Matters:</p> <ul> <li> <p>Helps maintain data integrity by identifying unexpected volume changes.</p> </li> <li> <p>Ensures quick detection and response to potential data issues.</p> </li> </ul> <p></p>"},{"location":"cli/overview-of-qualytics-cli/","title":"Qualytics CLI","text":"<p>Qualytics CLI is a command-line tool designed to interact with the Qualytics API. With this tool, users can manage configurations, export and import checks, run operations and more.</p> <p>You can check more the latest version in Qualytics CLI</p>"},{"location":"cli/overview-of-qualytics-cli/#installation-and-upgrading","title":"Installation and Upgrading","text":"<p>You can install <code>Qualytics CLI</code> via pip: <pre><code>pip install qualytics-cli\n</code></pre></p> <p>You can upgrade the <code>Qualytics CLI</code> via pip:</p> <pre><code>pip install qualytics-cli --upgrade\n</code></pre>"},{"location":"cli/overview-of-qualytics-cli/#usage","title":"Usage","text":""},{"location":"cli/overview-of-qualytics-cli/#help","title":"Help","text":"<p>To view available commands and their usage:</p> <pre><code>qualytics --help\n</code></pre>"},{"location":"cli/overview-of-qualytics-cli/#initializing-configuration","title":"Initializing Configuration","text":"<p>To set up your Qualytics URL and token:</p> Bash ExamplePython Example <pre><code>    qualytics init \n        --url \"https://your-qualytics.qualytics.io/\" \n        --token \"YOUR_TOKEN_HERE\"\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n    CLI_TOKEN = \"&lt;your-token&gt;\"\n    AUDIENCE = \"https://your-qualytics.qualytics.io/\"\n\n    qualytics.init(AUDIENCE, CLI_TOKEN)\n</code></pre> <pre><code>    Configuration saved! \n</code></pre></p> <p>Options:</p> Option Type Description Default Required <code>--url</code> TEXT The URL of your Qualytics instance None Yes <code>--token</code> TEXT The personal access token for accessing Qualytics None Yes"},{"location":"cli/overview-of-qualytics-cli/#display-configuration","title":"Display Configuration","text":"<p>To view the currently saved configuration:</p> Bash ExamplePython Example <pre><code>qualytics show-config\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    qualytics.show_config()\n</code></pre> <pre><code>    Config file located in: /home/user/.qualytics/config.json\n    URL: (https://your-qualytics.qualytics.io/)\n    Token: &lt;your-token&gt;\n</code></pre></p>"},{"location":"cli/overview-of-qualytics-cli/#export-checks","title":"Export Checks","text":"<p>To export checks to a file:</p> Bash ExamplePython Example <pre><code>    qualytics checks export \n        --datastore DATASTORE_ID [--containers CONTAINER_IDS] \n        [--tags TAG_NAMES] \n        [--output LOCATION_TO_BE_EXPORTED]\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    DATASTORE_ID = 844\n    CONTAINERS = \"7504, 6657\"\n    qualytics.checks_export(\n        datastore=DATASTORE_ID, \n        containers=CONTAINERS, \n        tags=None, \n        output=\"/home/user/.qualytics/data_checks.json\"\n    )\n</code></pre> <pre><code>    Exporting quality checks... -------------------------------------- 100% 0:00:00\n    Total of Quality Checks = 27 \n    Total pages = 1 \n    Data exported to /home/user/.qualytics/data_checks.json\n</code></pre></p> <p>Options:</p> Option Type Description Default Required <code>--datastore</code> INTEGER Datastore ID None Yes <code>--containers</code> List of INTEGER Containers IDs None No <code>--tags</code> List of TEXT Tag names None No <code>--output</code> TEXT Output file path <code>$HOME/.qualytics/data_checks.json</code> No"},{"location":"cli/overview-of-qualytics-cli/#export-check-templates","title":"Export Check Templates","text":"<p>To export check templates:</p> Bash ExamplePython Example to enrichmentPython Example to local (output destination) <pre><code>qualytics checks export-templates \n    --enrichment_datastore_id 123 \n    [--check_templates \"1, 2, 3\" or \"[1,2,3]\"]\n    [--status `true` or `false`]\n    [--rules \"afterDateTime, aggregationComparison\" or \"[afterDateTime, aggregationComparison]\"]\n    [--tags \"tag1, tag2, tag3\" or \"[tag1, tag2, tag3]\"]\n    [--output \"/home/user/.qualytics/data_checks_template.json\"]\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    ENRICH_DATASTORE_ID = 597\n    CHECK_TEMPLATES = \"182716, 179514\"\n    qualytics.check_templates_export(\n        enrich_datastore_id=ENRICH_DATASTORE_ID,\n        check_templates=CHECK_TEMPLATES,\n        status=None,\n        rules=None,\n        tags=None\n    )\n</code></pre> <pre><code>    The check templates were exported to the table `_export_check_templates` to enrichment id: 597.\n</code></pre></p> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    ENRICH_DATASTORE_ID = 597\n    CHECK_TEMPLATES = \"182716, 179514\"\n    qualytics.check_templates_export(\n        enrich_datastore_id=None,\n        check_templates=CHECK_TEMPLATES,\n        status=None,\n        rules=None,\n        tags=None,\n        output=\"/home/user/.qualytics/data_checks_template.json\"\n    )\n</code></pre> <pre><code>    Exporting quality checks... -------------------------------------- 100% 0:00:01\n    Total of Check Templates = 123 \n    Total pages = 2 \n    Data exported to /home/user/.qualytics/data_checks_template.json\n</code></pre></p> <p>Options:</p> Option Type Description Default Required <code>--enrichment_datastore_id</code> INTEGER The ID of the enrichment datastore where check templates will be exported. Yes <code>--check_templates</code> TEXT IDs of specific check templates to export (comma-separated or array-like). No <code>--status</code> BOOL Check Template status send <code>true</code> if it's locked or <code>false</code> to unlocked. No No <code>--rules</code> TEXT Comma-separated list of check templates rule types or array-like format. Example: \"afterDateTime, aggregationComparison\" or \"[afterDateTime, aggregationComparison]\". No No <code>--tags</code> TEXT Comma-separated list of Tag names or array-like format. Example: \"tag1, tag2, tag3\" or \"[tag1, tag2, tag3]\". No No <code>--output</code> TEXT Output file path [example: <code>/home/user/.qualytics/data_checks_template.json</code>]. No No"},{"location":"cli/overview-of-qualytics-cli/#import-checks","title":"Import Checks","text":"<p>To import checks from a file:</p> Bash ExamplePython Example <pre><code>qualytics checks import \n    --datastore DATASTORE_ID_LIST \n    [--input LOCATION_FROM_THE_EXPORT]\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    TARGET_DATASTORE_ID = 1172\n    qualytics.checks_import(\n        datastore=TARGET_DATASTORE_ID, \n        input_file=\"/home/user/.qualytics/data_checks.json\"\n    )\n</code></pre> <pre><code>    Quality check id: 195646 for container: CUSTOMER created successfully\n    Quality check id: 195647 for container: CUSTOMER created successfully\n    Quality check id: 195648 for container: CUSTOMER created successfully\n    Quality check id: 195649 for container: CUSTOMER created successfully\n    Quality check id: 195650 for container: CUSTOMER created successfully\n    Quality check id: 195651 for container: CUSTOMER created successfully\n    Quality check id: 195652 for container: CUSTOMER created successfully\n    Quality check id: 195653 for container: CUSTOMER created successfully\n    Quality check id: 195654 for container: CUSTOMER created successfully\n</code></pre></p> <p>Options:</p> Option Type Description Default Required <code>--datastore</code> TEXT Datastore IDs to import checks into (comma-separated or array-like). None Yes <code>--input</code> TEXT Input file path HOME/.qualytics/data_checks.json No <p>Note: Errors during import will be logged in <code>$HOME/.qualytics/errors.log</code>.</p>"},{"location":"cli/overview-of-qualytics-cli/#run-a-catalog-operation-on-a-datastore","title":"Run a Catalog Operation on a Datastore","text":"<p>Allows you to trigger a catalog operation on any current datastore (datastore permission required by admin)</p> Bash ExamplePython Example <pre><code>    qualytics run catalog \n        --datastore \"DATSTORE_ID_LIST\" \n        --include \"INCLUDE_LIST\" \n        --prune \n        --recreate \n        --background\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    DATASTORE_ID = \"1172\"\n    qualytics.catalog_operation(\n        datastores=DATASTORE_ID, \n        include=None,\n        prune=None ,\n        recreate=None,\n        background=False\n    )\n</code></pre> <pre><code>    Started Catalog operation 29464 for datastore: 1172 \n    Waiting for operation to finish\n    Waiting for operation to finish\n    Waiting for operation to finish\n    Successfully Finished Catalog operation 29464for datastore: 1172 \n    Processing... ---------------------------------------- 100% 0:00:29\n</code></pre></p> <p>Options:</p> Option Type Description Required <code>--datastore</code> TEXT Comma-separated list of Datastore IDs or array-like format. Example: 1,2,3,4,5 or \"[1,2,3,4,5]\" Yes <code>--include</code> TEXT Comma-separated list of include types or array-like format. Example: \"table,view\" or \"[table,view]\" No <code>--prune</code> BOOL Prune the operation. Do not include if you want prune == false No <code>--recreate</code> BOOL Recreate the operation. Do not include if you want recreate == false No <code>--background</code> BOOL Starts the catalog but does not wait for the operation to finish No"},{"location":"cli/overview-of-qualytics-cli/#run-a-profile-operation-on-a-datastore","title":"Run a Profile Operation on a Datastore","text":"<p>Allows you to trigger a profile operation on any current datastore (datastore permission required by admin)</p> Bash ExamplePython Example <pre><code>qualytics run profile \n    --datastore \"DATSTORE_ID_LIST\" \n    --container_names \"CONTAINER_NAMES_LIST\" \n    --container_tags \"CONTAINER_TAGS_LIST\"\n    --infer_constraints \n    --max_records_analyzed_per_partition \"MAX_RECORDS_ANALYZED_PER_PARTITION\" \n    --max_count_testing_sample \"MAX_COUNT_TESTING_SAMPLE\"\n    --percent_testing_threshold \"PERCENT_TESTING_THRESHOLD\" \n    --high_correlation_threshold \"HIGH_CORRELATION_THRESHOLD\" \n    --greater_then_date \"GREATER_THAN_TIME\"\n    --greater_than_batch \"GREATER_THAN_BATCH\" \n    --histogram_max_distinct_values \"HISTOGRAM_MAX_DISTINCT_VALUES\" \n    --background\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    DATASTORE_ID = \"844\"\n    CONTAINER_NAMES = \"CUSTOMER, NATION\"\n    qualytics.profile_operation(\n        datastores=DATASTORE_ID,\n        container_names=CONTAINER_NAMES,\n        container_tags=None,\n        infer_constraints=True,\n        max_records_analyzed_per_partition=None,\n        max_count_testing_sample=None,\n        percent_testing_threshold=None,\n        high_correlation_threshold=None,\n        greater_than_time=None,\n        greater_than_batch=None,\n        histogram_max_distinct_values=None,\n        background=False\n    )\n</code></pre> <pre><code>    Successfully Started Profile 29466 for datastore: 844 \n    Waiting for operation to finish\n    Waiting for operation to finish\n    Waiting for operation to finish\n    Waiting for operation to finish\n    Waiting for operation to finish\n    Waiting for operation to finish\n    Successfully Finished Profile operation 29466 for datastore: 844 \n    Processing... ---------------------------------------- 100% 0:00:46\n</code></pre></p> <p>Options:</p> Option Type Description Required <code>--datastore</code> TEXT Comma-separated list of Datastore IDs or array-like format. Example: 1,2,3,4,5 or \"[1,2,3,4,5]\" Yes <code>--container_names</code> TEXT Comma-separated list of include types or array-like format. Example: \"container1,container2\" or \"[container1,container2]\" No <code>--container_tags</code> TEXT Comma-separated list of include types or array-like format. Example: \"tag1,tag2\" or \"[tag1,tag2]\" No <code>--infer_constraints</code> BOOL Infer quality checks in profile. Do not include if you want infer_constraints == false No <code>--max_records_analyzed_per_partition</code> INT Number of max records analyzed per partition No <code>--max_count_testing_sample</code> INT The number of records accumulated during profiling for validation of inferred checks. Capped at 100,000 No <code>--percent_testing_threshold</code> FLOAT Percent of testing threshold No <code>--high_correlation_threshold</code> FLOAT Number of Correlation Threshold No <code>--greater_than_time</code> DATETIME Only include rows where the incremental field's value is greater than this time. Use one of these formats %Y-%m-%dT%H:%M:%S or %Y-%m-%d %H:%M:%S No <code>--greater_than_batch</code> FLOAT Only include rows where the incremental field's value is greater than this number No <code>--histogram_max_distinct_values</code> INT Number of max distinct values of the histogram No <code>--background</code> BOOL Starts the catalog but does not wait for the operation to finish No"},{"location":"cli/overview-of-qualytics-cli/#run-a-scan-operation-on-a-datastore","title":"Run a Scan Operation on a Datastore","text":"<p>Allows you to trigger a scan operation on a datastore (datastore permission required by admin)</p> Bash ExamplePython Example <pre><code>qualytics run scan \n    --datastore \"DATSTORE_ID_LIST\"\n    --container_names \"CONTAINER_NAMES_LIST\" \n    --container_tags \"CONTAINER_TAGS_LIST\"\n    --incremental \n    --remediation \n    --max_records_analyzed_per_partition \"MAX_RECORDS_ANALYZED_PER_PARTITION\" \n    --enrichment_source_records_limit\n    --greater_then_date \"GREATER_THAN_TIME\" \n    --greater_than_batch \"GREATER_THAN_BATCH\" \n    --background\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    DATASTORE_ID = 1172\n    CONTAINER_NAMES = \"CUSTOMER, NATION\"\n    qualytics.scan_operation(\n        datastores=str(DATASTORE_ID),\n        container_names=None,\n        container_tags=None,\n        incremental=False,\n        remediation=\"none\",\n        enrichment_source_record_limit=10,\n        greater_than_batch=None,\n        greater_than_time=None,\n        max_records_analyzed_per_partition=10000,\n        background=False\n    )\n</code></pre> <pre><code>    Successfully Started Scan 29467 for datastore: 1172 \n    Waiting for operation to finish\n    Waiting for operation to finish\n    Waiting for operation to finish\n    Waiting for operation to finish\n    Waiting for operation to finish\n    Successfully Finished Scan operation 29467 for datastore: 1172 \n    Processing... ---------------------------------------- 100% 0:03:04\n</code></pre></p> <p>Options:</p> Option Type Description Required <code>--datastore</code> TEXT Comma-separated list of Datastore IDs or array-like format. Example: 1,2,3,4,5 or \"[1,2,3,4,5]\" Yes <code>--container_names</code> TEXT Comma-separated list of include types or array-like format. Example: \"container1,container2\" or \"[container1,container2]\" No <code>--container_tags</code> TEXT Comma-separated list of include types or array-like format. Example: \"tag1,tag2\" or \"[tag1,tag2]\" No <code>--incremental</code> BOOL Process only new or records updated since the last incremental scan No <code>--remediation</code> TEXT Replication strategy for source tables in the enrichment datastore. Either 'append', 'overwrite', or 'none' No <code>--max_records_analyzed_per_partition</code> INT Number of max records analyzed per partition. Value must be Greater than or equal to 0 No <code>--enrichment_source_record_limit</code> INT Limit of enrichment source records per . Value must be Greater than or equal to -1 No <code>--greater_than_date</code> DATETIME Only include rows where the incremental field's value is greater than this time. Use one of these formats %Y-%m-%dT%H:%M:%S or %Y-%m-%d %H:%M:%S No <code>--greater_than_batch</code> FLOAT Only include rows where the incremental field's value is greater than this number No <code>--background</code> BOOL Starts the catalog but does not wait for the operation to finish No <p>Note: Errors during any of the three operations will be logged in <code>$HOME/.qualytics/operation-error.log</code>.</p>"},{"location":"cli/overview-of-qualytics-cli/#check-operation-status","title":"Check Operation Status","text":"<p>To check the status of operations:</p> Bash ExamplePython Example <pre><code>qualytics operation check_status \n    --ids \"OPERATION_IDS\"\n</code></pre> <p><pre><code>    import qualytics.qualytics as qualytics\n\n    qualytics.operation_status(ids=\"29468\")\n</code></pre> <pre><code>    Operation: 29468 is still running \n    Processing... ---------------------------------------- 100% 0:00:00\n</code></pre></p> <p>Options:</p> Option Type Description Required <code>--ids</code> TEXT Comma-separated list of Operation IDs or array-like format. Example: 1,2,3,4,5 or \"[1,2,3,4,5]\" Yes"},{"location":"cli/python-installation/","title":"Installing Python on Windows","text":"<p>Installing Python on Windows is simple and requires a compatible system, administrative access, and an internet connection. It can be installed through the Microsoft Store or the official website, with options to configure settings for seamless use. Verifying the installation ensures Python is ready for development, and setting environment variables can help with advanced tasks.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"cli/python-installation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you meet the following requirements:</p> <ul> <li> <p>System Requirements: Windows 7 or later with sufficient disk space.</p> </li> <li> <p>Administrative Privileges: You need admin rights to install Python and make changes to system settings.</p> </li> <li> <p>Internet Connection: A stable internet connection is required for downloading the installer.</p> </li> </ul>"},{"location":"cli/python-installation/#method-1-install-python-from-the-microsoft-store","title":"Method 1: Install Python from the Microsoft Store","text":"<p>Step 1: Click the Windows icon in the bottom-left corner of the screen, type Microsoft Store in the search bar.</p> <p></p> <p>Step 2:  Click on the Microsoft Store app to open it.</p> <p></p> <p>A Microsoft Store window will open, displaying a home screen with featured apps, games, and promotions.</p> <p></p> <p>Step 3: Click the search bar, type Python, and press Enter to search.</p> <p></p> <p>Step 4: A list of available Python versions appears. Select the latest version published by the Python Software Foundation to open its installation page.</p> <p></p> <p>Step 5: Click on the Python version you wish to install.</p> <p>For demonstration purposes, we will install Python 3.12.</p> <p></p> <p>Step 6: Click the Get button to start the download and installation process.</p> <p></p> <p>Step 7: Once the download and installation are complete, click the Downloads button in the left panel of the Microsoft Store to view the downloaded application.</p> <p></p> <p>Step 8: Click the Open button next to the downloaded Python version.</p> <p> </p> <p>Step 9: A modal command prompt window will open. In the command prompt, type python --version and press Enter.</p> <p></p> <p>If the installed Python version appears, it confirms that Python has been successfully installed on your system.</p> <p></p>"},{"location":"cli/python-installation/#method-2-installing-python-from-the-official-website","title":"Method 2: Installing Python from the Official Website","text":"<p>Step 1: Open a web browser and navigate to the Downloads for Windows section of the official Python website.</p> <p></p> <p>In the Downloads section, you will see different Python versions listed under Stable Releases and Pre-releases. Each version includes multiple installer options, such as:</p> <ul> <li> <p>Windows installer (64-bit) \u2013 For 64-bit systems</p> </li> <li> <p>Windows installer (32-bit) \u2013 For 32-bit systems</p> </li> <li> <p>Windows installer (ARM64) \u2013 For ARM-based systems</p> </li> </ul> <p>Choose the appropriate installer based on your system requirements before proceeding with the download.</p> <p></p> <p>Step 2: Click the link to download the file. For demonstration purposes, we have selected the Download Windows installer (64-bit).</p> <p></p> <p>Step 3: Locate the downloaded Python installer on your system and click to open it.</p> <p></p> <p>Step 4: Once the Python installer opens, the installation window shows two checkboxes:</p> <p></p> <ul> <li>Admin privileges: Check the box labelled Admin Privileges parameter controls whether to install Python for the current or all system users. This option allows you to change the installation folder for Python.</li> </ul> <p></p> <ul> <li>Add Python to Path: Check the box labeled Add Python to PATH important for running Python from the command line.</li> </ul> <p></p> <p>Step 5: Click Install Now option for the recommended installation.</p> <p></p> <p>Once installation is complete, you\u2019ll see an option to Disable path length limit. Click this option if prompted, as it can prevent issues with long file paths during development.</p> <p></p> <p>Step 6: Click Close to exit the installer.</p> <p></p> <p>Step 7: Verify the installation by opening a Command Prompt and typing:</p> <p>Syntax</p> <p>python --version</p> <p></p> <p>If Python is installed correctly, it should display the installed version:</p> <p>Example</p> <p>C:\\Users\\user&gt;python --version Python 3.12.4</p> <p></p>"},{"location":"cli/python-installation/#setting-up-environment-variables","title":"Setting Up Environment Variables","text":"<p>If the Python installer does not include the Add Python to PATH checkbox or you have not selected that option, continue in this step. Otherwise, skip these steps.</p> <p>Step 1: Open File Explorer (Win +E) and navigate to where Python is installed. The default location is:</p> <p>Example</p> <p>C:\\Users\\win 10\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Python 3.12 </p> <p></p> <p>Step 2: Copy this path, then press Win + R on your keyboard, type sysdm.cpl, and press Enter.</p> <p></p> <p>Step 3: A modal window system properties will appear. Click on the Advanced Tab.</p> <p></p> <p>Step 4: Click on the Environment Variable button.</p> <p></p> <p>A modal window will appear. Under System Variables, select Path and click on Edit button.</p> <p></p> <p>Paste the copied Python installation path.</p> <p></p> <p>Step 5: Also, add the Scripts folder path:</p> <p>Example</p> <p>C:\\Users\\YourUsername\\AppData\\Local\\Programs\\Python\\Python3x\\Scripts </p> <p></p> <p>Step 6: Click OK to save the changes and restart your computer.</p> <p></p> <p>Step 7: Open the Command Prompt and type:</p> <p>Syntax</p> <p>python --version </p> <p></p> <p>If Python is installed correctly, it should display the installed version:</p> <p>Example</p> <p>C:\\Users\\user&gt;python --version Python 3.12.4 </p> <p></p>"},{"location":"components/anomaly-support/","title":"Index","text":"Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field Type Supported RecordFlag inconsistencies at the row level ShapeFlag inconsistencies in the overall patterns and distributions of a field"},{"location":"components/comparators/","title":"Index","text":"### Comparators  <p>The Comparators allow you to set margins of error, accommodating slight variations in data validation. This flexibility is crucial for maintaining data integrity, especially when working with different data types such as numeric values, durations, and strings. Here's an overview of how each type of comparator can be beneficial for you:</p>"},{"location":"components/comparators/duration/","title":"Duration","text":""},{"location":"components/comparators/duration/#duration","title":"Duration","text":"<p>Duration comparators support time-based comparisons, allowing for flexibility in how duration differences are managed. This flexibility is crucial for datasets where time measurements are essential but can vary slightly.</p>"},{"location":"components/comparators/duration/#unit","title":"Unit","text":"<p>The unit of time you select determines how granular the comparison is:</p> <ul> <li>Millis: Measures time in milliseconds, ideal for high-precision needs.</li> <li>Seconds: Suitable for most general purposes where precision is important but doesn't need to be to the millisecond.</li> <li>Days: Best for longer durations.</li> </ul>"},{"location":"components/comparators/duration/#value","title":"Value","text":"<p>Value sets the maximum acceptable difference in time to consider two values as equal. It serves to define the margin of error, accommodating small discrepancies that naturally occur over time.</p> Illustration using Duration Comparator Unit Value A Value B Difference Threshold Are equal? Millis 500 ms 520 ms 20 ms 25 ms True Seconds 30 sec 31 sec 1 sec 2 sec True Days 5 days 7 days 2 days 1 day False Millis 1000 ms 1040 ms 40 ms 25 ms False Seconds 45 sec 48 sec 3 sec 2 sec False"},{"location":"components/comparators/numeric/","title":"Numeric","text":""},{"location":"components/comparators/numeric/#numeric","title":"Numeric","text":"<p>Numeric comparators enable you to compare numbers with a specified margin, which can be a fixed absolute value or a percentage. This allows for minor numerical differences that are often acceptable in real-world data.</p>"},{"location":"components/comparators/numeric/#comparison-type","title":"Comparison Type","text":"<ul> <li>Absolute Value: Uses a fixed threshold for determining equality. It's ideal when you need consistent precision across measurements.</li> <li>Percentage Value: Uses a percentage of the original value as the threshold for equality comparisons. It's suitable for floating point numbers where precision varies.</li> </ul>"},{"location":"components/comparators/numeric/#threshold","title":"Threshold","text":"<p>The threshold is the value you set to define the margin of error:</p> <ul> <li>When using Absolute Value, the threshold represents the maximum allowable difference between two values for them to be considered equal.</li> <li>For Percentage Value, the threshold is the percentage that describes how much a value can deviate from a reference value and still be considered equal.</li> </ul> Illustration using Absolute Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 50.</p> Value A Value B Difference Are equal? 100 150 50 True 100 90 10 True 100 155 55 False 100 49 51 False Illustration using Percentage Value <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined Threshold of 10%.</p> <p>Percentage Change Formula: [ (<code>Value B</code> - <code>Value A</code>) / <code>Value A</code> ] * 100</p> Value A Value B Percentage Change Are equal? 120 132 10% True 150 135 10% True 200 180 10% True 160 150 6.25% True 180 200 11.11% False"},{"location":"components/comparators/string/","title":"String","text":""},{"location":"components/comparators/string/#string","title":"String","text":"<p>String comparators facilitate comparisons of textual data by allowing variations in spacing. This capability is essential for ensuring data consistency, particularly where minor text inconsistencies may occur.</p>"},{"location":"components/comparators/string/#ignore-whitespace","title":"Ignore Whitespace","text":"<p>When enabled, this setting allows the comparator to ignore differences in whitespace. This means sequences of whitespace are collapsed into a single space, and any leading or trailing spaces are removed. This can be particularly useful in environments where data entry may vary in formatting but where those differences are not relevant to the data's integrity.</p> Illustration <p>In this example, it compares <code>Value A</code> and <code>Value B</code> according to the defined string comparison to <code>ignore whitespace</code> as <code>True</code>.</p> Value A Value B Are equal? Has whitespace? <code>Leonidas</code> <code>Leonidas</code> True No <code>Beth</code> <code>Beth</code> True Yes <code>Ana</code> <code>Anna</code> False Yes <code>Joe</code> <code>Joel</code> False No"},{"location":"components/general-props/","title":"Index","text":"DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p> DetailsFilter Guide Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions <p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p> Details Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions Details Name Supported FilterAllows the targeting of specific data based on conditions Coverage CustomizationAllows adjusting the percentage of records that must meet the rule's conditions"},{"location":"components/general-props/filter-guide/","title":"Filter guide","text":"<p>The filter allows you to define a subset of data upon which the rule will operate.</p> <p>It requires a valid Spark SQL expression that determines the criteria rows in the DataFrame should meet. This means the expression specifies which rows the DataFrame should include based on those criteria. Since it's applied directly to the Spark DataFrame, traditional SQL constructs like WHERE clauses are not supported.</p> Examples <p>Direct Conditions</p> <p>Simply specify the condition you want to be met.</p> Correct usage\" collapsible=\"true <pre><code>O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>C_MKTSEGMENT = 'BUILDING'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_TOTALPRICE &gt; 1000\n</code></pre> <pre><code>WHERE C_MKTSEGMENT = 'BUILDING'\n</code></pre> <p>Combining Conditions</p> <p>Combine multiple conditions using logical operators like <code>AND</code> and <code>OR</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>(L_SHIPDATE = '1998-09-02' OR L_RECEIPTDATE = '1998-09-01') AND L_RETURNFLAG = 'R'\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>WHERE O_ORDERPRIORITY = '1-URGENT' AND O_ORDERSTATUS = 'O'\n</code></pre> <pre><code>O_TOTALPRICE &gt; 1000, O_ORDERSTATUS = 'O'\n</code></pre> <p>Utilizing Functions</p> <p>Leverage Spark SQL functions to refine and enhance your conditions.</p> Correct usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - INSTR('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>LEVENSHTEIN(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>RIGHT(\n    O_ORDERPRIORITY,\n    LENGTH(O_ORDERPRIORITY) - CHARINDEX('-', O_ORDERPRIORITY)\n) = 'URGENT'\n</code></pre> <pre><code>EDITDISTANCE(C_NAME, 'Supplier#000000001') &lt; 7\n</code></pre> <p>Using scan-time variables</p> <p>To refer to the current dataframe being analyzed, use the reserved dynamic variable <code>{{ _qualytics_self }}</code>.</p> Correct usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM {{ _qualytics_self }}\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> Incorrect usage\" collapsible=\"true <pre><code>O_ORDERSTATUS IN (\n    SELECT DISTINCT O_ORDERSTATUS\n    FROM ORDERS\n    WHERE O_TOTALPRICE &gt; 1000\n)\n</code></pre> <p>While subqueries can be useful, their application within filters in our context has limitations. For example, directly referencing other containers or the broader target container in such subqueries is not supported. Attempting to do so will result in an error.</p> <p>Important Note on <code>{{ _qualytics_self }}</code></p> <p>The <code>{{ _qualytics_self }}</code> keyword refers to the dataframe that's currently under examination. In the context of a full scan, this variable represents the entire target container. However, during incremental scans, it only reflects a subset of the target container, capturing just the incremental data. It's crucial to recognize that in such scenarios, using <code>{{ _qualytics_self }}</code> may not encompass all entries from the target container.</p>"},{"location":"components/general-props/typos/","title":"Typos","text":"<p>Info</p> <p>Users can search across all filter inputs using typos, partial terms, or abbreviations. The system intelligently matches relevant results, making it easier to find what they need without exact inputs.</p>"},{"location":"components/general-props/warning/","title":"Warning","text":"<p>Info</p> <p>Visual warnings for spacing issues are available across all forms where values are entered. When a value includes leading or trailing spaces, the system automatically highlights it inside a warning-colored chip and displays a tooltip message indicating the issue in real time \u2014 for example:</p> <p>\u201cThe following value has leading or trailing spaces: \u2018ship \u2019\u201d</p> <p></p>"},{"location":"connections/overview-of-a-connection/","title":"Connections Overview","text":"<p>In Qualytics, setting up datastore connections is simple and efficient. Enter the necessary details like datastore name, connector, and authentication credentials to connect your datastores. You can also enable Secrets Management for secure credential handling with HashiCorp Vault.</p> <p>Once verified, the Reuse Connection feature lets you use existing credentials for future datastores, saving time and ensuring consistency. Manage your connections easily by adding, editing, or deleting datastores as needed.</p>"},{"location":"connections/overview-of-a-connection/#setup-a-connection","title":"Setup a Connection","text":"<p>To configure a datastore connection in Qualytics, begin by entering the required details such as the datastore name, connector, and authentication credentials. Optionally, enable Secrets Management for secure credential handling. Once the connection is tested and confirmed, your datastore will be set up and ready for use.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore (e.g., the specified name will appear on the datastore cards.) 2. Toggle Button Toggle ON to create a new source datastore from scratch. 3. Connector (Required) Select a connector from the drop-down list. <p>For demonstration purposes, we have selected the BigQuery Connector.</p> <p>Step 3: Add connection details such as temp dataset ID, service account key, project ID, and dataset ID.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any connection property to reference a key from the configured Vault secret. Each time the connection is initiated, the corresponding secret value will be retrieved dynamically. </p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 4: The configuration form requests credential details before establishing a connection.</p> <p></p> <p>Note</p> <p>Different connectors have unique fields and parameters. For this demonstration, we have selected the BigQuery Connector, so the fields displayed are specific to the BigQuery configuration. </p> REF. FIELDS ACTIONS 1. Temp Dataset ID (Optional) Enter a temporary dataset ID for intermediate data storage during BigQuery operations. 2. Service Account Key (Required) Upload a JSON file that contains the credentials required for accessing BigQuery. 3. Project ID (Required) Enter the Project ID associated with BigQuery. 4. Dataset ID (Required) Enter the Dataset ID (schema name) associated with BigQuery. 5. Teams (Required) Select one or more teams from the dropdown to associate with this source datastore. 6. Initiate Cataloging (Optional) Tick the checkbox to automatically perform a catalog operation on the configured source datastore to gather data structures and corresponding metadata. <p>Step 5: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p> <p>Step 6: Once the connection is verified, click the Finish button to complete the process.</p> <p></p> <p>Step 7: A message will appear indicating that your datastore has been successfully added. Once the datastore is added, you can reuse the connection for future tasks without needing to re-enter the details.</p> <p></p>"},{"location":"connections/overview-of-a-connection/#reuse-a-connection","title":"Reuse a Connection","text":"<p>The Reuse Connection feature lets you use existing credentials to set up a new datastore, saving time and ensuring consistency in your Qualytics account. Simply toggle the option to reuse credentials instead of entering new ones.</p> <p>Step 1: Log in to your Qualytics account and click on the Add Source Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window - Add Datastore will appear, providing you with the options to connect a datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Specify the name of the datastore (e.g., the specified name will appear on the datastore cards). 2. Toggle Button Toggle OFF to reuse credentials from an existing connection. 3. Connector (Required) Select a connector from the drop-down list. <p>For demonstration purposes, we have selected the BigQuery Connector.</p> <p>Step 3: Add connection details such as temp dataset ID, service account key, project ID, and dataset ID.</p> <p></p> <p>Step 4: Click on the Test Connection button to verify the existing connection details. If connection details are verified, a success message will be displayed.</p> <p></p> <p>Step 5: Once the connection is verified, click the Finish button to complete the process.</p> <p></p> <p>A message will appear indicating that your datastore has been successfully added. </p> <p></p>"},{"location":"connections/overview-of-a-connection/#manage-connection","title":"Manage Connection","text":"<p>You can effectively manage your connections by editing, deleting, and adding datastores to maintain accuracy and efficiency.</p> <p>For more information on managing connections, refer to the Manage Connection section.</p>"},{"location":"connections/overview-of-a-connection/#conclusion","title":"Conclusion","text":"<p>Using Connections optimizes datastore management by enabling the reuse of connection parameters, making the process more streamlined and organized.</p>"},{"location":"container/actions-on-container/","title":"Actions on Container","text":"<p>Users can perform various operations on containers to manage datasets effectively. The actions are divided into three main sections: Settings, Add, and Run. Each section contains specific options to perform different tasks.</p> <p></p>"},{"location":"container/actions-on-container/#settings","title":"Settings","text":"<p>The Settings button allows users to configure the container. By clicking on the Settings button, users can access the following options:</p> <p></p> No Options Description 1. Settings Configure incremental strategy, partitioning fields, and exclude specific fields from analysis. 2. Score Score allowing you to adjust the decay period and factor weights for metrics like completeness, accuracy, and consistency.Note: To understand how each score metric works in detail (completeness, accuracy, consistency, decay period, and weights), please refer to the Quality Score Page.Score settings modified here apply only to this container and do not affect any other container in the datastore. 3. Observability Enables or disables tracking for data volume and freshness.Volume Tracking: Monitors daily volume metrics to identify trends and detect anomalies over time.Freshness Tracking: Records the last update timestamp to ensure data timeliness and detect pipeline delays. 4. Migrate Migrate authored quality checks from one container to another (even across datastores) to quickly reuse, standardize, and avoid recreating rules. 5. Export Export quality checks, field profiles, and anomalies to an enrichment datastore for further action or analysis. 6. Materialize Captures snapshots of data from a source datastore and exports it to an enrichment datastore for faster access and analysis. 7. Delete Delete the selected container from the system."},{"location":"container/actions-on-container/#add","title":"Add","text":"<p>The Add button allows users to add checks or computed fields. By clicking on the Add button, users can access the following options:</p> <p></p> No. Options Description 1. Checks Checks allow you to add new checks or validation rules for the container.Note: To learn how to add checks, refer to the Check Templates documentation. 2. Computed Field Allows you to add a computed field.Note: To learn how to create a computed field, refer to the Computed Field Guide."},{"location":"container/actions-on-container/#run","title":"Run","text":"<p>The Run button provides options to execute operations on datasets, such as profiling, scanning, and external scans. By clicking on the Run button, users can access the following options:</p> <p></p> No. Options Description 1. Profile Profile allows you to run a profiling operation to analyze the data structure, gather metadata, set thresholds, and define record limits for comprehensive dataset profiling.Note: For profile operation, please refer to the Profile Operation documentation. 2. Scan Scan allows you to perform data quality checks, configure scan strategies, and detect anomalies in the dataset.Note: For scan operation, please refer to the Scan Operation documentation. 3. External Scan External Scan allows you to upload a file and validate its data against predefined checks in the selected table.Note: For external scan, please refer to the  External Scan documentation."},{"location":"container/computed-join/","title":"Computed Join","text":"<p>A Computed Join Container allows you to combine data from two containers, which can be from the same source datastore or different source datastores (e.g., a database table vs. a file system container). You can choose the join type (Inner, Left, Right, or Full Outer) and apply transformations, filters, and custom queries to the joined result.</p> <p>This feature is useful when you want to:</p> <ul> <li> <p>Merge information from multiple source datastores into a single dataset.</p> </li> <li> <p>Perform cross-datastore analysis (e.g., JDBC tables with DFS files).</p> </li> <li> <p>Apply Spark SQL transformations and filters on top of the joined data.</p> </li> </ul> <p>Let's get started \ud83d\ude80</p> <p>Note</p> <p>Computed Join only supports base containers (physical tables, views, or files). Computed Tables and Computed Files cannot be used as join inputs.</p>"},{"location":"container/computed-join/#how-it-works","title":"How It Works","text":"<p>The Add Computed Join form consists of:</p> REF. FIELDS DESCRIPTION 1 Name The unique name for your computed join container. 2 Join Type Choose one of the following:\u2022 Inner Join: Keeps only rows with matching keys in both containers.\u2022 Left Join: Keeps all rows from the left container, matching rows from the right.\u2022 Right Join: Keeps all rows from the right container, matching rows from the left.\u2022 Full Outer Join: Keeps all rows from both containers. 3 Left Reference \u2022 Datastore: Source datastore where the computed join container will be created.\u2022 Container: The left container to join.\u2022 Field: The key (column) to join on.\u2022 Prefix: A label (e.g., <code>left</code>) applied to all columns from this container. 4 Right Reference \u2022 Datastore: Source datastore containing the second container.\u2022 Container: The right container to join.\u2022 Field: The key (column) to join on.\u2022 Prefix: A label (e.g., <code>right</code>) applied to all columns from this container. 5 Select Expression A list of columns to include in the result. Columns are automatically prefixed (e.g., <code>left_name</code>, <code>right_name</code>) to avoid conflicts. 6 Filter Clause (WHERE) Additional filters applied to the join result. <p></p>"},{"location":"container/computed-join/#example-use-case","title":"Example Use Case","text":"<p>Scenario</p> <p>We want to join:</p> <ul> <li>Left Container: customers</li> <li>Right Container: orders</li> <li>Join Key: customer_id</li> <li>Join Type: Left Join</li> <li>Prefixes: cust_ and order_</li> </ul>"},{"location":"container/computed-join/#input-tables","title":"Input Tables","text":""},{"location":"container/computed-join/#customers","title":"customers","text":"customer_id name city 1 Alice Berlin 2 Bob London 3 Charlie Paris"},{"location":"container/computed-join/#orders","title":"orders","text":"order_id customer_id product 101 1 Laptop 102 1 Mouse 103 2 Keyboard"},{"location":"container/computed-join/#joined-result-left-join","title":"Joined Result (Left Join)","text":"cust_customer_id cust_name cust_city order_order_id order_product 1 Alice Berlin 101 Laptop 1 Alice Berlin 102 Mouse 2 Bob London 103 Keyboard 3 Charlie Paris NULL NULL"},{"location":"container/computed-join/#visual-diagram","title":"Visual Diagram","text":"<pre><code>+------------+                 +--------+\n| customers  |    LEFT JOIN    | orders |\n+------------+ &lt;-------------&gt; +--------+\n    |                               |\n    +-- customer_id = customer_id --+\n</code></pre>"},{"location":"container/computed-join/#api-example","title":"API Example","text":""},{"location":"container/computed-join/#endpoint","title":"Endpoint","text":"<p>POST:<code>/api/containers</code> </p> <p>Expected response: <code>200 OK</code></p> Request Payload <pre><code>{\n    \"container_type\": \"computed_join\",\n    \"name\": \"customer_orders_join\",\n    \"select_clause\": \"cust_customer_id, cust_name, cust_city, order_order_id, order_product\",\n    \"where_clause\": null,\n    \"left_join_field_name\": \"customer_id\",\n    \"left_prefix\": \"cust\",\n    \"right_join_field_name\": \"customer_id\",\n    \"right_prefix\": \"order\",\n    \"join_type\": \"left\",\n    \"left_container_id\": 101,\n    \"right_container_id\": 202\n}\n</code></pre>"},{"location":"container/computed-join/#tips","title":"Tips","text":"<ul> <li>Always set prefixes to avoid column name collisions.</li> <li>Use Select Expression to choose only the columns you need.</li> <li>Apply a Filter Clause for better performance by reducing unnecessary data.</li> <li>Test the join type with sample data to verify expected behavior.</li> </ul>"},{"location":"container/computed-join/#limitations","title":"Limitations","text":""},{"location":"container/computed-join/#computed-tables-and-computed-files-cannot-be-used-as-inputs","title":"Computed Tables and Computed Files Cannot Be Used as Inputs","text":"<p>Computed Join only accepts base containers as inputs\u2014these are physical tables, views, or files that exist in your datastore catalog. You cannot select a Computed Table or Computed File as the left or right reference in a Computed Join.</p>"},{"location":"container/computed-join/#why-this-limitation-exists","title":"Why This Limitation Exists","text":"<p>Computed Join is built to work with containers that have specific characteristics:</p> Requirement Why It Matters Cataloged in the datastore The container must have a stable identifier that Qualytics can discover and reference. Predictable load behavior Base containers support partitioning, incremental identifiers, and sampling\u2014features that Computed Join relies on for efficient execution. Schema metadata available Qualytics needs to profile and understand the structure of the containers being joined. Deterministic execution The join must produce consistent results when re-run, which requires stable source objects. <p>Computed Tables and Computed Files are internal Qualytics assets. They are not exposed as queryable objects in your data warehouse or DFS, which means they cannot be discovered, profiled, or loaded in the same way as base containers.</p> <p>Additionally, allowing computed assets as join inputs would introduce complex dependency management:</p> <ul> <li>Dependency ordering: If Computed Join B depends on Computed Table A, which depends on base table X, Qualytics would need to manage this execution chain.</li> <li>Incremental processing: Determining the correct watermark for incremental loads across dependent assets is complex.</li> <li>Failure handling: Retries and error recovery become more complicated when multiple dependent assets are involved.</li> </ul>"},{"location":"container/computed-join/#workarounds","title":"Workarounds","text":"<p>If you need to join derived datasets, you have two main options:</p>"},{"location":"container/computed-join/#option-1-build-the-join-as-a-computed-table","title":"Option 1: Build the Join as a Computed Table","text":"<p>Instead of using Computed Join, define your join logic directly in a Computed Table using SQL:</p> <pre><code>WITH customers_filtered AS (\n    SELECT customer_id, name, city\n    FROM customers\n    WHERE status = 'active'\n),\norders_recent AS (\n    SELECT order_id, customer_id, product\n    FROM orders\n    WHERE order_date &gt;= '2024-01-01'\n)\nSELECT\n    c.customer_id,\n    c.name,\n    c.city,\n    o.order_id,\n    o.product\nFROM customers_filtered c\nLEFT JOIN orders_recent o ON c.customer_id = o.customer_id\n</code></pre> <p>This approach lets you combine filtering, transformations, and joins in a single query.</p>"},{"location":"container/computed-join/#option-2-materialize-intermediate-results-in-your-data-warehouse","title":"Option 2: Materialize Intermediate Results in Your Data Warehouse","text":"<p>If you need to reuse intermediate datasets across multiple contexts:</p> <ol> <li>Create the intermediate result as a view or table in your data warehouse (using your warehouse's SQL interface, dbt, or another ETL tool).</li> <li>Catalog that view or table in Qualytics as a base container.</li> <li>Use the cataloged container as an input to your Computed Join.</li> </ol> <p>This works because the materialized result becomes a first-class object in your datastore that Qualytics can discover, profile, and join like any other base container.</p>"},{"location":"container/container-attributes/","title":"Container Attributes","text":""},{"location":"container/container-attributes/#totals","title":"Totals","text":"<p>Note</p> <p>Totals are calculated from sampled data, not the full dataset. Values may differ from actual totals across all records.</p> <p>1 Quality Score: This represents the overall health of the data based on various checks. A higher score indicates better data quality and fewer issues detected.</p> <p>2 Sampling: Displays the percentage of data sampled during profiling. A 100% sampling rate means the entire dataset was analyzed for the quality report.</p> <p>3 Completeness: Indicates the percentage of records that are fully populated without missing or incomplete data. Lower percentages may suggest that some fields have missing values.</p> <p>4 Records Profiled: Shows the number or percentage of records that have been analyzed during the profiling process.</p> <p>5 Fields Profiled: This shows the number of fields or attributes within the dataset that have undergone data profiling, which helps identify potential data issues in specific columns.</p> <p>6 Active Checks: Represents the number of ongoing checks applied to the dataset. These checks monitor data quality, consistency, and correctness.</p> <p>7 Active Anomalies: Displays the total number of anomalies found during the data profiling process. Anomalies can indicate inconsistencies, outliers, or potential data quality issues that need resolution.  </p> <p></p>"},{"location":"container/container-attributes/#observability","title":"Observability","text":"<p>1 Volumetric Measurement </p> <p>Volumetric measurement allows users to track the size of data stored within the table over time. This helps in monitoring how the data grows or changes, making it easier to detect sudden spikes that may impact system performance. Users can visualize data volume trends and manage the table's efficiency. This helps in optimizing storage, adjusting resource allocation, and improving query performance based on the size and growth of the computed table.</p> <p></p> <p>2 Anomalies Measurement</p> <p>The Anomalies section helps users track any unusual data patterns or issues within the computed tables. It shows a visual representation of when anomalies occurred over a specific time period, making it easy to spot unusual activity. This allows users to quickly identify when something might have gone wrong and take action to fix it, ensuring the data stays accurate and reliable.</p> <p></p>"},{"location":"container/container-types/","title":"Container Types","text":"<p>There are two main types of containers in Qualytics:</p>"},{"location":"container/container-types/#jdbc-container","title":"JDBC Container","text":"<p>JDBC containers are virtual representations of database objects, making it easier to work with data stored in relational databases. These containers include tables, which organize data into rows and columns like a spreadsheet, views that provide customized displays of data from one or more tables, and other database objects such as indexes or stored procedures. Acting as a bridge between applications and databases, JDBC enables seamless interaction with these containers, allowing efficient data management and retrieval.</p>"},{"location":"container/container-types/#dfs-container","title":"DFS Container","text":"<p>DFS containers are used to represent files stored in distributed file systems, such as Hadoop or cloud storage. These files can include formats like CSV, JSON, or Parquet, which are commonly used for storing and organizing data. DFS containers make it easier for applications to work with these files by providing a structured way to access and process data in large-scale storage systems.</p>"},{"location":"container/data-preview/","title":"Data Preview","text":"<p>Data Preview in Qualytics makes it easy for users to view and understand their container data. It provides a clear snapshot of the data's structure and contents, showing up to 100 rows from the source. With options to filter specific data, refresh for the latest updates, and download records, it helps users focus on the most relevant information, troubleshoot issues, and analyze data effectively. The simple grid view ensures a smooth and efficient way to explore and work with your data.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"container/data-preview/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and select the source datastore (JDBC or DFS) from the left menu that contains the data you want to preview.</p> <p></p> <p>Step 2: Select Tables (if a JDBC datastore is connected) or File Patterns (if a DFS datastore is connected) from the Navigation tab at the top.</p> <p></p> <p>Step 3: You will view the full list of tables or files belonging to the selected source datastore. Select the specific table or file whose data you want to preview.</p> <p></p> <p>Alternatively, you can access the tables or files by clicking the drop-down arrow on the selected datastore. This will display the full list of tables or files associated with the selected source datastore. From there, select the specific table or file whose data you want to preview.</p> <p></p> <p>Step 4: After selecting the specific table or file, click on the Data Preview tab.</p> <p></p> <p>You will see a tabular view of the data, displaying the field names (columns) and their corresponding data values, allowing you to review the data's structure, types, and sample records.</p> <p></p>"},{"location":"container/data-preview/#ui-caching","title":"UI Caching","text":"<p>Upon initial access to the Data Preview section, the data may not be stored (cached) yet, which can cause longer loading times. How long it takes to load depends on the type of datastore being used (like DFS or JDBC) and whether the data warehouse is serverless. However, the next time you access the same data, it will load faster because it will be cached, meaning the data is stored temporarily for quicker access.</p>"},{"location":"container/data-preview/#filter-clause-and-refresh","title":"Filter Clause and Refresh","text":"<p>The Data Preview tab includes filter functionality that enables users to focus on specific fields by applying filter clauses. This refines the displayed rows based on specific criteria, enhancing data analysis and providing more targeted insights and includes a Refresh button to update the data view with the latest data.</p>"},{"location":"container/data-preview/#filter-clause","title":"Filter Clause","text":"<p>Use the Filter Clause to narrow down the displayed rows by applying specific criteria, allowing for focused and precise data analysis.</p> <p></p>"},{"location":"container/data-preview/#refresh","title":"Refresh","text":"<p>Click the Refresh button to update the data view with the latest information, ensuring accuracy and relevance.</p> <p></p>"},{"location":"container/data-preview/#select-specific-fields","title":"Select Specific Fields","text":"<p>Select specific fields to display, allowing you to focus on the most relevant data for analysis. Click on the Select Fields to Show dropdown and choose specific fields you want to review by checking or unchecking options.</p> <p></p>"},{"location":"container/data-preview/#download-records","title":"Download Records","text":"<p>The Download Records feature in Qualytics allows users to easily export all source records from the selected enrichment dataset. This functionality is essential for performing deeper analysis outside the platform or for sharing data with external tools and teams.</p> <p></p>"},{"location":"container/data-preview/#use-cases","title":"Use Cases","text":""},{"location":"container/data-preview/#debugging-checks","title":"Debugging Checks","text":"<p>One of the primary use cases of the Data Preview tab is for debugging checks. Users can efficiently inspect the first 100 rows of container data to identify any anomalies, inconsistencies, or errors, facilitating the debugging process and improving data quality.</p>"},{"location":"container/data-preview/#data-analysis","title":"Data Analysis","text":"<p>The Data Preview tab also serves as a valuable tool for data analysis tasks. Users can explore the dataset, apply filters to focus on specific subsets of data, and gain insights into patterns, trends, and correlations within the container data.</p>"},{"location":"container/data-preview/#examples","title":"Examples","text":""},{"location":"container/data-preview/#example-1-debugging-data-import","title":"Example 1: Debugging Data Import","text":"<p>Suppose a user encounters issues with importing data into a container. By utilizing the Data Preview tab, the user can quickly examine the first 100 rows of imported data, identify any formatting errors or missing values, and troubleshoot the data import process effectively.</p>"},{"location":"container/data-preview/#example-2-filtering-data-by-date-range","title":"Example 2: Filtering Data by Date Range","text":"<p>In another scenario, a user needs to analyze sales data within a specific date range. The user can leverage the filter support feature of the Data Preview tab to apply date range filters, displaying only the sales records that fall within the specified timeframe. This allows for targeted analysis and informed decision-making.</p>"},{"location":"container/field-profiles/","title":"Field Profiles","text":"<p>A Field Profile provides a detailed breakdown of a field\u2019s data after a profiling operation. It helps you understand the structure, quality, and distribution of values for each field inside a container.</p>"},{"location":"container/field-profiles/#what-field-profiles-are-used-for","title":"What Field Profiles Are Used For","text":"<p>Field Profiles help you:</p> <ul> <li>Validate the completeness, consistency, and quality of individual fields  </li> <li>Identify unexpected patterns such as outliers, skewed values, or sudden changes  </li> <li>Compare current field behavior with previous profiling runs  </li> <li>Support quality checks by exposing metrics like distinct values, min/max length, and statistical indicators  </li> </ul> <p>These insights make it easier to detect data issues early and understand how a field behaves over time.</p>"},{"location":"container/field-profiles/#how-field-profiles-are-generated","title":"How Field Profiles Are Generated","text":"<p>Field Profiles are automatically created when you run a Profile operation on a container:</p> <ol> <li>Qualytics scans the dataset and evaluates each field.</li> <li>It computes metrics such as declared type, distinct counts, distribution statistics, sampling, and completeness.</li> <li>The platform stores these results as the latest profile.</li> <li>Each time you re-profile the container, the Field Profile is updated, allowing you to compare current metrics with previous runs.</li> </ol> <p>This ensures your field-level insights remain current and can be tracked across multiple profiling sessions.</p>"},{"location":"container/field-profiles/#totals","title":"Totals","text":"<p>1 Quality Score: This provides a comprehensive assessment of the overall health of the data, factoring in multiple checks for accuracy, consistency, and completeness. A higher score, closer to 100, indicates optimal data quality with minimal issues or errors detected. A lower score may highlight areas that require attention and improvement.</p> <p>2 Sampling: This shows the percentage of data that was evaluated during profiling. A sampling rate of 100% indicates that the entire dataset was analyzed, ensuring a complete and accurate representation of the data\u2019s quality across all records, rather than just a partial sample.</p> <p>3 Completeness: This metric measures how fully the data is populated without missing or null values. A higher completeness percentage means that most fields contain the necessary information, while a lower percentage indicates data gaps that could negatively impact downstream processes or analysis.</p> <p>4 Active Checks: This refers to the number of ongoing quality checks being applied to the dataset. These checks monitor aspects such as format consistency, uniqueness, and logical correctness. Active checks help maintain data integrity and provide real-time alerts about potential issues that may arise.</p> <p>5 Active Anomalies: This tracks the number of anomalies or irregularities detected in the data. These could include outliers, duplicates, or inconsistencies that deviate from expected patterns. A count of zero indicates no anomalies, while a higher count suggests that further investigation is needed to resolve potential data quality issues.</p> <p></p>"},{"location":"container/field-profiles/#profile","title":"Profile","text":"<p>This provides detailed insights into the characteristics of the field, including its type, distinct values, and length. You can use this information to evaluate the data's uniqueness, length consistency, and complexity.</p> No. Profile Description 1 Declared Type Indicates whether the type is declared by the source or inferred. 2 Distinct Values Count of distinct values observed in the dataset. 3 Min Length Shortest length of the observed string values or lowest value for numerics. 4 Max Length Greatest length of the observed string values or highest value for numerics. 5 Mean Mathematical average of the observed numeric values. 6 Median The median of the observed numeric values. 7 Standard Deviation Measure of the amount of variation in observed numeric values. 8 Kurtosis Measure of the \u2018tailedness\u2019 of the distribution of observed numeric values. 9 Skewness Measure of the asymmetry of the distribution of observed numeric values. 10 Q1 The first quartile; the central point between the minimum and the median. 11 Q3 The third quartile; the central point between the median and the maximum. 12 Sum Total sum of all observed numeric values. <p></p>"},{"location":"container/field-profiles/#last-profile","title":"Last Profile","text":"<p>The Last Profile timestamp helps users understand how up-to-date the field is. When you hover over the time indicator shown on the right side of the Last Profile label (e.g., \"1 week ago\"), a tooltip displays the complete date and time the field was last profiled.</p> <p></p> <p>This visibility ensures better context for interpreting profile metrics like mean, completeness, and anomalies.</p>"},{"location":"container/field-profiles/#compare-profile","title":"Compare Profile","text":"<p>You can compare the current field profile with earlier versions to spot changes over time. Visual indicators highlight modified metrics, interactive charts show numeric trends across profile history, and special badges identify data drift or field type changes.</p> <p>By clicking on the dropdown under Compare With, you can select an earlier profile run (for example, 1 day ago or 5 days ago).</p> <p></p> <p>Once selected, the system highlights differences between profiles, marking metrics as Changed or Unchanged. It compares data quality (Sampling, Completeness) and statistical measures (mean, median, standard deviation, skewness, kurtosis, min, max, distinct values, etc.), making it easy to track shifts in data quality and distribution.</p> <p></p>"},{"location":"container/field-profiles/#view-metric-chart","title":"View Metric Chart","text":"<p>You can access detailed metric charts by clicking the View Metric Chart button. This will display variations across the last 10 profiles. By hovering over points on the chart, you can see additional details such as profile dates, measured values, and sampling percentages for deeper analysis.</p> <p></p>"},{"location":"container/overview/","title":"Containers Overview","text":"<p>Containers are fundamental entities representing structured data sets. These containers could manifest as tables in JDBC datastores or as files within DFS datastores. They play a pivotal role in data organization, profiling, and quality checks within the Qualytics application.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"container/overview/#container-types","title":"Container Types","text":"<p>There are two main types of containers in Qualytics:</p>"},{"location":"container/overview/#jdbc-container","title":"JDBC Container","text":"<p>JDBC containers are virtual representations of database objects, making it easier to work with data stored in relational databases.</p> <p>Note</p> <p>For more information, please refer to the jdbc container section.</p>"},{"location":"container/overview/#dfs-container","title":"DFS Container","text":"<p>DFS containers are used to represent files stored in distributed file systems, such as Hadoop or cloud storage.</p> <p>Note</p> <p>For more information, please refer to the DFS container section.</p>"},{"location":"container/overview/#computed-tables","title":"Computed Tables","text":"<p>Computed Tables allow you to create derived tables using SQL-like transformations on your source datastore containers.</p> <p>Note</p> <p>For more information, please refer to the Computed Tables Documentation.   </p>"},{"location":"container/overview/#computed-files","title":"Computed Files","text":"<p>Computed Files allow you to generate new file-based datasets derived from existing containers within your selected datastore.</p> <p>Note</p> <p>For more information, please refer to the Computed Files Documentation.    </p>"},{"location":"container/overview/#computed-joins","title":"Computed Joins","text":"<p>A Computed Join Container allows you to combine data from two containers, which can be from the same source datastore or different source datastores (e.g., a database table vs. a file system container).</p> <p>Note</p> <p>For more information, please refer to the Computed Join Documentation.</p>"},{"location":"container/overview/#container-attributes","title":"Container Attributes","text":""},{"location":"container/overview/#totals","title":"Totals","text":"<p>Totals are calculated from sampled data, not the full dataset. Values may differ from actual totals across all records.</p> <p>Note</p> <p>For more information, please refer to the container attributes documentation.</p>"},{"location":"container/overview/#actions-on-container","title":"Actions on Container","text":"<p>Users can perform various operations on containers to manage datasets effectively. The actions are divided into three main sections: Settings, Add, and Run. Each section contains specific options to perform different tasks.</p> <p>Note</p> <p>For more information, please refer to the actions on container documentation.</p>"},{"location":"container/overview/#field-profiles","title":"Field Profiles","text":"<p>After profiling a container, individual field profiles offer granular insights:</p> <p>Note</p> <p>For more information, please refer to the field profiles documentation.</p>"},{"location":"container/computed-fields/add-computed-fields/","title":"Add Computed Fields","text":"<p>Step 1: Log in to Your Qualytics Account, navigate to the side menu, and select the source datastore where you want to create a computed field.</p> <p></p> <p>Step 2: Select the Container within the chosen datastore where you want to create the computed field. This container holds the data to which the new computed field will be applied, enabling you to enhance your data analysis within that specific datastore.</p> <p>For demonstration purposes, we have selected the Bank Dataset-Staging source datastore and the bank_transactions_.csv container within it to create a computed field.</p> <p></p> <p>Step 3: After selecting the container, click on the Add button and select Computed Field from the dropdown menu to create a new computed field. </p> <p></p> <p>A modal window will appear, allowing you to enter the details for your computed field. </p> <p></p> <p>Step 4: Enter the Name for the computed field, select Transformation Type from the dropdown menu, and optionally add Additional Metadata.</p> REF. FIELDS ACTION 1 Field Name (Required) Add a unique name for your computed field. 2 Transformation Type (Required) The type of transformation you want to apply from the available options. 3 Additional Metadata (Optional) Enhance the computed field definition by setting custom metadata. Click the plus icon (+) to open the metadata input form and add key-value pairs. <p></p> <p>Info</p> <p>Transformations are changes made to data, like converting formats, doing calculations, or cleaning up fields. In Qualytics, you can use transformations to meet specific needs, such as cleaning entity names, converting formatted numbers, or applying custom expressions. With various transformation types available, Qualytics enables you to customize your data directly within the platform, ensuring it\u2019s accurate and ready for analysis.</p> Transformation Types Purpose Reference Cast Converts an existing field to a different data type (such as number, date, timestamp, or boolean) so it can be used correctly in checks, aggregations, and calculations. For more information, please refer to the guide cast section. Cleaned Entity Name Removes business signifiers (such as 'Inc.' or 'Corp') from an entity name. For more information, please refer to the guide cleaned entity name section. Convert Formatted Numeric Removes formatting (such as parentheses for denoting negatives or commas as delimiters) from values that represent numeric data, converting them into a numerically typed field. For more information, please refer to the guide convert formatted numeric section. Custom Expression Allows you to create a new field by applying any valid Spark SQL expression to one or more existing fields. For more information, please refer to the guide custom expression section. <p></p> <p>Step 5: After selecting the appropriate Transformation Type, click the Save button.</p> <p></p> <p>Step 6: After clicking on the Save button, your computed field is created and a success flash message will display saying The computed field has been successfully created.</p> <p></p> <p>You can find your computed field by clicking on the dropdown arrow next to the container you selected when creating the computed field.</p> <p></p>"},{"location":"container/computed-fields/computed-fields-details/","title":"Computed Fields Details","text":"<p>Computed Field Details provides a quick overview of the metrics generated from a computed field. The Totals section summarizes the results produced by this computed field and displays a report that reflects only the data output of this specific computed field.</p>"},{"location":"container/computed-fields/computed-fields-details/#totals","title":"Totals","text":"<p>1 Quality Score: This provides a comprehensive assessment of the overall health of the data, factoring in multiple checks for accuracy, consistency, and completeness. A higher score, closer to 100, indicates optimal data quality with minimal issues or errors detected. A lower score may highlight areas that require attention and improvement.</p> <p>2 Sampling: This shows the percentage of data that was evaluated during profiling. A sampling rate of 100% indicates that the entire dataset was analyzed, ensuring a complete and accurate representation of the data\u2019s quality across all records, rather than just a partial sample.  </p> <p>3 Completeness: This metric measures how fully the data is populated without missing or null values. A higher completeness percentage means that most fields contain the necessary information, while a lower percentage indicates data gaps that could negatively impact downstream processes or analysis.</p> <p>4 Active Checks: This refers to the number of ongoing quality checks being applied to the dataset. These checks monitor aspects such as format consistency, uniqueness, and logical correctness. Active checks help maintain data integrity and provide real-time alerts about potential issues that may arise.</p> <p>5 Active Anomalies: This tracks the number of anomalies or irregularities detected in the data. These could include outliers, duplicates, or inconsistencies that deviate from expected patterns. A count of zero indicates no anomalies, while a higher count suggests that further investigation is needed to resolve potential data quality issues.</p> <p></p>"},{"location":"container/computed-fields/computed-fields-details/#profile","title":"Profile","text":"<p>This provides detailed insights into the characteristics of the field, including its type, distinct values, and length. You can use this information to evaluate the data's uniqueness, length consistency, and complexity.</p> No Profile Description 1 Declared Type Indicates whether the type is declared by the source or inferred. 2 Distinct Values Count of distinct values observed in the dataset. 3 Min Length Shortest length of the observed string values or lowest value for numerics. 4 Max Length Greatest length of the observed string values or highest value for numerics. 5 Mean Mathematical average of the observed numeric values. 6 Median The median of the observed numeric values. 7 Standard Deviation Measure of the amount of variation in observed numeric values. 8 Kurtosis Measure of the \u2018tailedness\u2019 of the distribution of observed numeric values. 9 Skewness Measure of the asymmetry of the distribution of observed numeric values. 10 Q1 The first quartile; the central point between the minimum and the median. 11 Q3 The third quartile; the central point between the median and the maximum. 12 Sum Total sum of all observed numeric values. <p></p> <p>You can hover over the (i) button to view the native field properties, which provide detailed information such as the field's type (numeric), size, decimal digits, and whether it allows null values.</p> <p></p>"},{"location":"container/computed-fields/computed-fields-details/#last-profile","title":"Last Profile","text":"<p>The Last Profile timestamp helps users understand how up to date the field is. When you hover over the time indicator shown on the right side of the Last Profile label (e.g., \"8 months ago\"), a tooltip displays the complete date and time the field was last profiled.</p> <p></p> <p>This visibility ensures better context for interpreting profile metrics like mean, completeness, and anomalies.</p>"},{"location":"container/computed-fields/overview/","title":"Computed Fields","text":"<p>Computed Fields allow you to enhance data analysis by applying dynamic transformations directly to your data. These fields let you create new data points, perform calculations, and customize data views based on your specific needs, ensuring your data is both accurate and actionable.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"container/computed-fields/overview/#add-computed-fields","title":"Add Computed Fields","text":"<p>Step 1: Log in to Your Qualytics Account, navigate to the side menu, and select the source datastore where you want to add a computed field.</p> <p>Note</p> <p>For next steps please refer to the add computed field documentation.</p>"},{"location":"container/computed-fields/overview/#computed-fields-details","title":"Computed Fields Details","text":""},{"location":"container/computed-fields/overview/#totals","title":"Totals","text":"<p>Note</p> <p>For more information, please refer to the computed fields details documentation.</p>"},{"location":"container/computed-fields/overview/#types-of-transformations","title":"Types of Transformations","text":"<p>Note</p> <p>For more information, please refer to the types of transformations.</p>"},{"location":"container/computed-fields/transformation-types/","title":"Types of Transformations","text":""},{"location":"container/computed-fields/transformation-types/#cast","title":"Cast","text":"<p>The Cast transformation converts field values to a specified target data type using Spark SQL casting rules. This is commonly used when a field is ingested as a string but must behave as a numeric, date, timestamp, or boolean value for profiling, checks, or downstream computations.</p>"},{"location":"container/computed-fields/transformation-types/#when-to-use-cast","title":"When to Use Cast","text":"<p>Use Cast when:</p> <ul> <li>A numeric field is ingested as a string (for example, <code>\"1000\"</code> instead of <code>1000</code>)</li> <li>Dates or timestamps need explicit typing</li> <li>Boolean logic is required on string values</li> <li>Quality checks or aggregations require strict data types</li> </ul>"},{"location":"container/computed-fields/transformation-types/#options-for-cast","title":"Options for Cast","text":"REF. FIELD WHAT IT MEANS 1 Name A clear, descriptive name for the new computed field you are creating. 2 Transformation Type Choose Cast to convert the data type of an existing field. 3 Field The existing column whose values you want to convert into another data type. 4 Target Type The data type you want the field to become (for example: number, date, timestamp, decimal, or boolean). 5 Format Optional, Used only when converting text into dates or timestamps, so the system knows how to read the date format correctly. 6 Additional Metadata Optional information you can attach to the field to provide extra context or documentation."},{"location":"container/computed-fields/transformation-types/#target-type-examples","title":"Target Type Examples","text":"Input Value Target Type Result <code>\"1000\"</code> <code>int</code> <code>1000</code> <code>\"1234.56\"</code> <code>decimal(10,2)</code> <code>1234.56</code> <code>\"true\"</code> <code>boolean</code> <code>true</code> <code>\"2023-12-31\"</code> <code>date</code> <code>2023-12-31</code>"},{"location":"container/computed-fields/transformation-types/#format-examples-date-timestamp-casting","title":"Format Examples (Date / Timestamp Casting)","text":"<p>The Format field is only required when casting string values into <code>date</code> or <code>timestamp</code>.</p> <p>Common examples:</p> Format Pattern Example Input <code>MM/dd/yyyy</code> <code>12/31/2023</code> <code>dd/MM/yyyy</code> <code>31/12/2023</code> <code>yyyy-MM-dd HH:mm:ss</code> <code>2023-12-31 14:30:00</code>"},{"location":"container/computed-fields/transformation-types/#cleaned-entity-name","title":"Cleaned Entity Name","text":"<p>This transformation removes common business signifiers from entity names, making your data cleaner and more uniform.</p>"},{"location":"container/computed-fields/transformation-types/#options-for-cleaned-entity-name","title":"Options for Cleaned Entity Name","text":""},{"location":"container/computed-fields/transformation-types/#computed-field-configuration","title":"Computed Field Configuration","text":"No. Field Name Description 1 Name Defines a unique name for the computed field. This name is used to identify and reference the computed field. 2 Transformation Type Select Cleaned Entity Name to create a standardized and cleaned version of the selected source field value. 3 Field Specifies the source field whose values will be cleaned and transformed. 4 Term Settings Defines how specific terms are removed from the source field value. This includes:\u2022 Drop from Prefix \u2013 Removes specified terms from the beginning of the entity name.\u2022 Drop from Suffix \u2013 Removes specified terms from the end of the entity name.\u2022 Drop from Interior \u2013 Removes specified terms from anywhere within the entity name. 5 Additional Terms to Drop Allows you to define custom terms that should be removed from the field value. 6 Terms to Ignore Specifies terms that should be excluded from the cleaning process and retained in the final output. 7 Additional Metadata Enables you to add custom metadata to enhance the computed field definition, such as descriptions or key-value attributes."},{"location":"container/computed-fields/transformation-types/#example-for-cleaned-entity-name","title":"Example for Cleaned Entity Name","text":"Example Input Transformation Output 1 \"TechCorp, Inc.\" Drop from Suffix: \"Inc.\" \"TechCorp\" 2 \"Global Services Ltd.\" Drop from Prefix: \"Global\" \"Services Ltd.\" 3 \"Central LTD &amp; Finance Co.\" Drop from Interior: \"LTD\" \"Central &amp; Finance Co.\" 4 \"Eat &amp; Drink LLC\" Additional Terms to Drop: \"LLC\", \"&amp;\" \"Eat Drink\" 5 \"ProNet Solutions Ltd.\" Terms to Ignore: \"Ltd.\" \"ProNet Solutions\""},{"location":"container/computed-fields/transformation-types/#convert-formatted-numeric","title":"Convert Formatted Numeric","text":"<p>This transformation converts formatted numeric values into a plain numeric format, stripping out any characters like commas or parentheses that are not numerically significant.</p>"},{"location":"container/computed-fields/transformation-types/#options-for-converted-formatted-numeric","title":"Options for Converted Formatted Numeric","text":"No. Field Name Description 1 Name Specifies the name of the computed field. This name is used to identify the computed field. 2 Transformation Type Select Convert Formatted Numeric to convert formatted numeric values into a standard numeric format. 3 Field Selects the source field on which the transformation is applied. 4 Additional Metadata Allows adding custom metadata to enhance the computed field definition (for example, descriptions or key-value attributes)."},{"location":"container/computed-fields/transformation-types/#example-for-convert-formatted-numeric","title":"Example for Convert Formatted Numeric","text":"Example Input Transformation Output 1 \"$1,234.56\" Remove non-numeric characters: \",\", \"$\" \"1234.56\" 2 \"(2020)\" Remove non-numeric characters: \"(\", \")\" \"-2020\" 3 \"100%\" Remove non-numeric characters: \"%\" \"100\""},{"location":"container/computed-fields/transformation-types/#custom-expression","title":"Custom Expression","text":"<p>Enables the creation of a field based on a custom computation using Spark SQL. This is useful for applying complex logic or transformations that are not covered by other types.</p>"},{"location":"container/computed-fields/transformation-types/#options-for-using-custom-expression","title":"Options for Using Custom Expression","text":"No. Field Name Description 1 Name Specifies the name of the computed field. This name is used to identify the computed field. 2 Transformation Type Select Custom Expression to define a computed field using a custom SQL expression. 3 SQL Expression Enter the SQL logic used to compute the field. Use <code>Ctrl + Space</code> to view available hints and functions. 4 Additional Metadata Allows adding custom metadata to enhance the computed field definition (for example, descriptions or key-value attributes)."},{"location":"container/computed-fields/transformation-types/#using-custom-expression","title":"Using Custom Expression:","text":"<p>You can combine multiple fields, apply conditional logic, or use any valid Spark SQL functions to derive your new computed field.</p> <p>Example: To create a field that sums two existing fields, you could use the expression <code>SUM(field1, field2)</code>.</p> <p>Advanced Example: You need to ensure that a log of leases has no overlapping dates for an asset, but your data only captures a single lease's details like: </p> LeaseID AssetID Lease_Start Lease_End 1 42 1/1/2025 2/1/2026 2 43 1/1/2025 2/1/2026 3 42 1/1/2026 2/1/2026 4 43 2/2/2026 2/1/2027 <p>You can see in this example that Lease 1 has overlapping dates with Lease 3 for the same Asset. This can be difficult to detect without a full transformation of the data. However, we can accomplish our goal easily with a Computed Field. We'll simply add a Computed Field to our table named \"Next_Lease_Start\" and define it with the following custom expression so that our table will now hold the new field and render it as shown below.</p> <p><code>LEAD(Lease_Start, 1) OVER (PARTITION BY AssetID ORDER BY Lease_Start)</code> </p> LeaseID AssetID Lease_Start Lease_End Next_Lease_Start 1 42 1/1/2025 2/1/2026 1/1/2026 2 43 1/1/2025 2/1/2026 2/2/2026 3 42 1/1/2026 2/1/2026 4 43 2/2/2026 2/1/2027 <p>Now you can author a Quality Check stating that Lease_End should always be less than \"Next_Lease_Start\" to catch any errors of this type. In fact, Qualytics will automatically infer that check for you at Level 3 Inference!</p>"},{"location":"container/computed-fields/transformation-types/#more-examples-for-custom-expression","title":"More Examples for Custom Expression","text":"Example Input Fields Custom Expression Output 1 <code>field1 = 10</code>, <code>field2 = 20</code> <code>SUM(field1, field2)</code> <code>30</code> 2 <code>salary = 50000</code>, <code>bonus = 5000</code> <code>salary + bonus</code> <code>55000</code> 3 <code>hours = 8</code>, <code>rate = 15.50</code> <code>hours * rate</code> <code>124</code> 4 <code>status = 'active'</code>, <code>score = 85</code> <code>CASE WHEN status = 'active' THEN score ELSE 0 END</code> <code>85</code>"},{"location":"container/computed-tables-and-files/computed-files/","title":"Computed Files","text":"<p>Use Computed Files when you want to perform the following operations on your selected source datastore:</p> <ul> <li>Data Preparation and Transformation: Efficiently clean and restructure raw data stored in a DFS.</li> <li>Column-Level Transformations: Utilize Spark SQL functions to manipulate and clean individual columns.</li> <li>Filtering Data: Extract specific data subsets within a DFS container using Spark SQL's WHERE clause.</li> </ul> <p>Note</p> <p>Computed Files can only reference base files from the datastore. They cannot reference other Computed Files or be used as inputs to Computed Joins.</p>"},{"location":"container/computed-tables-and-files/computed-files/#add-computed-files","title":"Add Computed Files","text":"<p>Step 1: Log in to your Qualytics account and select a DFS-type source datastore from the side menu on which you would like to add a computed file.</p> <p></p> <p>Step 2: After clicking on your preferred source datastore, you will be redirected to the source datastore operations page. From this page, click on the Add button and select the Computed File option from the dropdown menu.</p> <p></p> <p>Step 3: A modal window will appear prompting you to enter a name for your computed file, select a source file pattern, choose the expression, and optionally define a filter clause and add additional metadata.</p> REF. FIELDS ACTION 1 Name (Required) Enter a name for your computed file. The name should be descriptive and meaningful to help you easily identify the file later (e.g., add a meaningful name like Customer_Order_Statistics). 2 Source File Pattern (Required) Select a source file pattern from the dropdown menu to match files that have a similar naming convention. 3 Select Expression (Required) Select the expression to define the data you want to include in the computed file. 4 Filter Clause (Optional) Add a WHERE clause to filter the data that meets certain conditions. 5 Additional Metadata (Optional) Enhance the computed file definition by setting custom metadata. Click the plus icon (+) next to this section to open the metadata input form, where you can add key-value pairs. <p></p> <p>Step 4: Click on the Validate button to quickly check your query or expression before saving.</p> <p></p> <p>Step 5: Once validation is successful, click on the Save button to add the computed file to your selected source datastore.</p> <p></p> <p>After clicking the Save button, a success notification appears on the screen showing the action was completed successfully.</p>"},{"location":"container/computed-tables-and-files/computed-files/#limitations","title":"Limitations","text":""},{"location":"container/computed-tables-and-files/computed-files/#referencing-other-computed-files","title":"Referencing Other Computed Files","text":"<p>A computed file cannot directly reference another computed file in its expression. This is similar to how computed tables work\u2014the computed file is defined within Qualytics but is not created as an actual file or dataset in your DFS datastore that can be queried by name.</p>"},{"location":"container/computed-tables-and-files/computed-files/#workarounds","title":"Workarounds","text":"<p>If you need to combine data from multiple sources:</p>"},{"location":"container/computed-tables-and-files/computed-files/#option-1-combine-logic-in-a-single-computed-file","title":"Option 1: Combine Logic in a Single Computed File","text":"<p>If your transformations can be expressed as a single query, use the Select Expression and Filter Clause to perform all necessary operations on the source file pattern directly.</p>"},{"location":"container/computed-tables-and-files/computed-files/#option-2-materialize-intermediate-results","title":"Option 2: Materialize Intermediate Results","text":"<p>If you need to chain transformations:</p> <ol> <li>Create the intermediate result as an actual file in your DFS (using your data pipeline tools).</li> <li>Catalog that file in Qualytics.</li> <li>Use the cataloged file as the source for your computed file.</li> </ol>"},{"location":"container/computed-tables-and-files/computed-files/#using-computed-files-in-computed-joins","title":"Using Computed Files in Computed Joins","text":"<p>Computed files cannot be used as inputs in a Computed Join. Computed Joins are designed to work with base containers only\u2014physical tables, views, or files that exist in your datastore catalog. For more details, see the Computed Join documentation.</p>"},{"location":"container/computed-tables-and-files/computed-table-vs-file/","title":"Computed Table vs Computed File","text":"<p>Computed Tables and Computed Files both allow you to generate transformed datasets within Qualytics, but they differ in how the output is stored, processed, and consumed. Computed Tables produce table-like results inside JDBC datastores and support SQL-based relational operations such as joins. Computed Files, on the other hand, generate file-based outputs stored in DFS datastores and support Spark SQL operations suited for large-scale file processing. This comparison helps you choose the right option for your transformation needs.</p> Feature Computed Table (JDBC) Computed File (DFS) Source Data JDBC source datastores DFS source datastores Query Language SQL (database-specific functions) Spark SQL Supported Operations Joins, where clauses, and database functions Column transforms, where clauses (no joins), Spark SQL functions <p>Note</p> <p>Computed tables and files function like regular tables. You can profile them, create checks, and detect anomalies.</p> <ul> <li>Updating a computed table\u2019s query will trigger a profiling operation.  </li> <li>Updating a computed file\u2019s select or where clause will trigger a profiling operation.  </li> <li>When you create a computed table or file, a basic profile of up to 1000 records is automatically generated.</li> </ul>"},{"location":"container/computed-tables-and-files/computed-tables/","title":"Computed Tables","text":"<p>Use Computed Tables when you want to perform the following operations on your selected source datastores:</p> <ul> <li>Data Preparation and Transformation: Clean, shape, and restructure raw data from JDBC source datastores.</li> <li>Complex Calculations and Aggregations: Perform calculations not easily supported by standard containers.</li> <li>Data Subsetting: Extract specific data subsets based on filters using SQL's WHERE clause.     </li> <li>Joining Data Across Source Datastores: Combine data from multiple JDBC source datastores using SQL joins.</li> </ul> <p>Note</p> <p>Computed Tables can only reference base containers (physical tables or views) from the datastore. They cannot reference other Computed Tables or Computed Files.</p>"},{"location":"container/computed-tables-and-files/computed-tables/#add-computed-tables","title":"Add Computed Tables","text":"<p>Step 1: Log in to your Qualytics account and select a JDBC-type source datastore from the side menu on which you would like to add a computed table.</p> <p></p> <p>Step 2: After selecting your preferred source datastore, you will be redirected to the source datastore operations page. From this page, click on the Add button and select the Computed Table option from the dropdown menu.</p> <p></p> <p>Step 3: A modal window will appear prompting you to enter a name for your computed table, a valid SQL query that supports your selected source datastore, and optionally, additional metadata.</p> REF. FIELDS ACTIONS 1 Name (Required) Enter a name for your computed table. The name should be descriptive and meaningful to help you easily identify the table later (e.g., add a meaningful name like <code>Customer_Order_Statistics</code>). 2 Query (Required) Write a valid SQL query that supports your selected source datastore. The query helps to perform joins and aggregations on your selected source datastore. 3 Additional Metadata (Optional) Add custom metadata to enhance the definition of your computed table. Click the plus icon (+) next to this section to open the metadata input form, where you can add key-value pairs. <p>Warning</p> <p>When creating a Computed Table for SQL Server, Oracle, or Redshift datastores, you must specify the table using a fully qualified name (<code>SCHEMA.TABLE</code>). The table selection field displays schema-prefixed table names to help you select the correct table.</p> <p></p> <p>Step 4: Click on the Validate button to instantly check the syntax and semantics of your SQL query. This ensures your query runs successfully and prevents errors before saving.</p> <p></p> <p>Step 5: Once validation is successful, click on the Save button to add the computed table to your selected source datastore.</p> <p></p>"},{"location":"container/computed-tables-and-files/computed-tables/#limitations","title":"Limitations","text":""},{"location":"container/computed-tables-and-files/computed-tables/#referencing-other-computed-tables","title":"Referencing Other Computed Tables","text":"<p>A computed table cannot directly reference another computed table in its SQL query. For example, attempting to write a query like this will fail:</p> <pre><code>SELECT * FROM my_computed_table JOIN another_computed_table ON ...\n</code></pre> <p>Why this happens: Computed tables are defined and stored within Qualytics, but they are not created as actual database objects (like views or tables) in your data warehouse. When your SQL query runs, the database engine cannot find <code>my_computed_table</code> because it doesn't exist as a named object in the warehouse catalog.</p>"},{"location":"container/computed-tables-and-files/computed-tables/#workarounds","title":"Workarounds","text":"<p>If you need to combine logic from multiple computed tables, you have several options:</p>"},{"location":"container/computed-tables-and-files/computed-tables/#option-1-combine-logic-using-ctes","title":"Option 1: Combine Logic Using CTEs","text":"<p>Merge the SQL logic into a single computed table using Common Table Expressions (CTEs):</p> <pre><code>WITH dataset_a AS (\n    SELECT id, name, amount\n    FROM source_table_1\n    WHERE status = 'active'\n),\ndataset_b AS (\n    SELECT id, category, region\n    FROM source_table_2\n    WHERE year = 2024\n)\nSELECT\n    a.id,\n    a.name,\n    a.amount,\n    b.category,\n    b.region\nFROM dataset_a a\nJOIN dataset_b b ON a.id = b.id\n</code></pre> <p>This approach works because the entire query is executed as a single unit by the database engine.</p>"},{"location":"container/computed-tables-and-files/computed-tables/#option-2-create-a-view-in-your-data-warehouse","title":"Option 2: Create a View in Your Data Warehouse","text":"<p>If you need to reuse intermediate results across multiple computed tables:</p> <ol> <li>Create a view or table directly in your data warehouse (e.g., using your warehouse's SQL interface or a tool like dbt).</li> <li>Catalog that view or table in Qualytics.</li> <li>Reference the cataloged object in your computed table queries.</li> </ol> <p>This works because the intermediate result becomes a real, queryable object in your warehouse.</p>"},{"location":"container/computed-tables-and-files/computed-tables/#using-computed-tables-in-computed-joins","title":"Using Computed Tables in Computed Joins","text":"<p>Computed tables cannot be used as inputs in a Computed Join. Computed Joins are designed to work with base containers only\u2014physical tables, views, or files that exist in your datastore catalog. For more details, see the Computed Join documentation.</p>"},{"location":"container/computed-tables-and-files/overview/","title":"Computed Tables &amp; Files","text":"<p>Computed Tables and Computed Files are powerful virtual tables within the Qualytics platform, each serving distinct purposes in data manipulation. Computed Tables are created using SQL queries on JDBC source datastores, enabling advanced operations like joins and where clauses. Computed Files, derived from Spark SQL transformations on DFS source datastores, allow for efficient data manipulation and transformation directly within the DFS environment.</p> <p>This guide explains how to add Computed Tables and Computed Files and discusses the differences between them.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"container/computed-tables-and-files/overview/#computed-tables","title":"Computed Tables","text":"<p>Use Computed Tables when you want to perform the following operations on your selected source datastores:</p> <p>Note</p> <p>For more information, please refer to the computed tables documentation.</p>"},{"location":"container/computed-tables-and-files/overview/#add-computed-tables","title":"Add Computed Tables","text":"<p>Step 1: Log in to your Qualytics account and select a JDBC-type source datastore from the side menu on which you would like to add a computed table.</p> <p>Note</p> <p>For more information, please refer to the add computed tables section.</p>"},{"location":"container/computed-tables-and-files/overview/#computed-files","title":"Computed Files","text":"<p>Use Computed Files when you want to perform the following operations on your selected source datastore:</p> <p>Note</p> <p>For more information, please refer to the computed files documentation.</p>"},{"location":"container/computed-tables-and-files/overview/#add-computed-files","title":"Add Computed Files","text":"<p>Step 1: Log in to your Qualytics account and select a DFS-type source datastore from the side menu on which you would like to add a computed file.</p> <p>Note</p> <p>For more information, please refer to the add computed files section.</p>"},{"location":"container/computed-tables-and-files/overview/#computed-table-vs-computed-file","title":"Computed Table Vs. Computed File","text":"<p>Note</p> <p>For more information, please refer to the computed tables vs file documentation.</p>"},{"location":"container/computed-tables-and-files/overview/#editing-from-checks","title":"Editing from Checks","text":"<p>Computed tables and fields can also be edited directly from check interfaces. This allows you to quickly update computed logic when resolving check failures.</p> <p>Note</p> <p>Learn more about editing computed assets from checks</p>"},{"location":"container/computed-tables-and-files/overview/#view-assigned-teams","title":"View Assigned Teams","text":"<p>By hovering over the information icon, users can view the assigned teams for enhanced collaboration and data transparency.</p> <p></p>"},{"location":"container/enrichment-operation/export-operation/","title":"Export Operation","text":"<p>Qualytics metadata export feature lets you capture the changing states of your data. You can export metadata for Quality Checks, Field Profiles, and Anomalies from selected profiles into an enrichment datastore so that you can perform deeper analysis, identify trends, detect issues, and make informed decisions based on your data.</p> <p>To keep things organized, the exported files use specific naming patterns:</p> <ul> <li>Anomalies: Saved as <code>_&lt;enrichment_prefix&gt;_anomalies_export</code> </li> <li>Quality Checks: Saved as <code>_&lt;enrichment_prefix&gt;_checks_export</code> </li> <li>Field Profiles: Saved as <code>_&lt;enrichment_prefix&gt;_field_profiles_export</code></li> </ul> <p>Note</p> <p>Ensure that an enrichment datastore is already set up and properly configured to accommodate the exported data. This setup is essential for exporting anomalies, quality checks, and field profiles successfully. </p> <p>Let\u2019s get started \ud83d\ude80</p> <p>Step 1: Select a source datastore from the side menu from which you would like to export the metadata.</p> <p></p> <p>For demonstration purposes, we have selected the \u201cCOVID-19 Data\u201d Snowflake source datastores.</p> <p></p> <p>Step 2: After selecting a datastore, a bottom-up menu appears on the right side of the interface. Click Enrichment Operations next to the Enrichment Datastore and select Export.</p> <p></p> <p>Step 3: After clicking Export, the Export Operation modal window appears, allowing you to extract metadata from the selected source datastore to the enrichment datastore.</p> <p></p> <p>Step 4: Select the tables you wish to export: All, Specific, or Tag and click Next to proceed.</p> <p></p> <p>Step 5: After clicking Next, select the assets you want to export to your Enrichment Datastore: Anomalies, Quality Checks, or Field Profiles, and click Export to proceed with the export process.</p> <p></p> <p>After clicking Export, a confirmation message appears stating \"Export in motion.\" In a couple of minutes, the metadata will be available in your Enrichment Datastore.</p> <p></p>"},{"location":"container/enrichment-operation/export-operation/#schedule-operation","title":"Schedule Operation","text":"<p>Step 1: Click Schedule to configure scheduling options for the Export Operation.</p> <p></p> <p>Step 2: Configure the scheduling preferences for the Export Operation.</p> <ul> <li> <p>Hourly: Runs every set number of hours at a specified minute. (e.g., Every 1 hour at 00 minutes).</p> </li> <li> <p>Daily: Runs once per day at a specific UTC time. (e.g., Every day at 00:00 UTC).</p> </li> <li> <p>Weekly: Runs on selected weekdays at a set time. (e.g., Every Sunday and Friday at 00:00 UTC).</p> </li> <li> <p>Monthly: Runs on a specific day of the month at a set time. (e.g., 1st day of every month at 00:00 UTC).</p> </li> <li> <p>Advanced: Use Cron expressions for custom scheduling. (e.g., <code>0 12 * * 1-5</code> runs at 12 PM, Monday to Friday).</p> </li> </ul> <p></p> <p>Step 3: Define the Schedule Name to identify the scheduled Export Operation when it runs.</p> <p></p> <p>Step 4: Click Schedule to finalize and schedule the Export Operation.</p> <p></p> <p>After clicking Schedule, a confirmation message appears stating \"Operation Scheduled\". Go to the Activity tab to see the progress of the export operation.</p> <p></p>"},{"location":"container/enrichment-operation/export-operation/#review-exported-data","title":"Review Exported Data","text":"<p>Step 1: Once the metadata has been exported, navigate to the \u201cEnrichment Datastores\u201d located on the left menu.</p> <p></p> <p>Step 2: In the \u201cEnrichment Datastores\u201d section, select the datastore where you exported the metadata. The exported metadata will now be visible in the selected datastore.</p> <p></p> <p>Step 3: Click on the exported files to view the metadata. For demonstration purposes, we have selected the \u201cexport_field_profiles\u201d file to review the metadata.</p> <p>The exported metadata is displayed in a table format, showing key details about the field profiles from the datastore. It typically includes columns that indicate the uniqueness of data, the completeness of the fields, and the data structure. You can use this metadata to check data quality, prepare for analysis, ensure compliance, and manage your data.</p> <p></p>"},{"location":"container/enrichment-operation/materialize-operation/","title":"Materialize Operation","text":"<p>Materialize Operation captures snapshots of selected containers from a source datastore and exports them to an enrichment datastore for seamless data loading. Users can run it instantly or schedule it at set intervals, ensuring structured data is readily available for analysis and integration.</p>"},{"location":"container/enrichment-operation/materialize-operation/#materialize-naming-conventions","title":"Materialize Naming Conventions","text":"<p>To keep materialized data organized and compatible across different enrichment datastores, the system applies specific naming conventions. These conventions ensure that the resulting container names remain valid, readable, and free of conflicts.</p>"},{"location":"container/enrichment-operation/materialize-operation/#default-naming-convention","title":"Default Naming Convention","text":"<p>Used when the container name is safe to use as-is.</p> <p><code>&lt;enrichment_prefix&gt;_mat_&lt;container_name&gt;</code></p> <p>This naming format is applied when:</p> <ul> <li>The container name length is 60 characters or less.</li> <li>The container name does not contain special characters that may cause invalid table or file names.</li> </ul> <p>Example:</p> <p>If the enrichment prefix is <code>sales</code> and the container name is <code>orders_2024</code>:</p> <p><code>sales_mat_orders_2024</code></p>"},{"location":"container/enrichment-operation/materialize-operation/#fallback-naming-convention","title":"Fallback Naming Convention","text":"<p>If the container name contains characters that may cause issues in downstream systems, the system switches to a safer naming structure by using the container ID instead.</p> <p><code>&lt;enrichment_prefix&gt;_materialize_&lt;container_id&gt;</code></p> <p>This fallback is used when:</p> <ul> <li>The container name exceeds 60 characters.</li> <li>The container name includes restricted or special characters.    (e.g., symbols, glob patterns when moving from DFS to JDBC).</li> </ul> <p>Example:</p> <p>If the enrichment prefix is <code>sales</code> and the container ID is <code>1023456</code>:</p> <p><code>sales_materialize_1023456</code></p> <p>Note</p> <p>The fallback naming ensures successful loading into the enrichment datastore by preventing invalid or non-compliant table names.</p> <p>Let\u2019s get started \ud83d\ude80</p> <p>Step 1: Select a source datastore from the side menu to capture and export containers for the Materialize Operation.</p> <p></p> <p>For demonstration purposes, we have selected the \u201cCOVID-19 Data\u201d Snowflake source datastore.</p> <p></p> <p>Step 2: After selecting a datastore, a bottom-up menu appears on the right side of the interface. Click Enrichment Operations next to the Enrichment Datastore and select Materialize.</p> <p></p> <p>Step 3: After clicking Materialize, a modal window appears, allowing you to configure the data export settings for the Materialize Operation.</p> <p></p> <p>Step 4: Select tables to materialize all tables, specific tables, or tables by tag, then click Next.</p> <p></p> <p>Step 5: Configure Record Limit: Set the maximum number of records to be materialized per table.</p> <p></p>"},{"location":"container/enrichment-operation/materialize-operation/#run-now","title":"Run Now","text":"<p>Click Run Now to instantly materialize selected containers.</p> <p></p> <p>After clicking Run Now, a confirmation message appears stating \"Operation Triggered\". Go to the Activity tab to see the progress of materialize operation.</p> <p></p>"},{"location":"container/enrichment-operation/materialize-operation/#schedule","title":"Schedule","text":"<p>Step 1: Click Schedule to configure scheduling options for the Materialize Operation.</p> <p></p> <p>Step 2: Configure the scheduling preferences for the Materialize Operation.</p> <ul> <li> <p>Hourly: Runs every set number of hours at a specified minute. (e.g., Every 1 hour at 00 minutes).</p> </li> <li> <p>Daily: Runs once per day at a specific UTC time. (e.g., Every day at 00:00 UTC).</p> </li> <li> <p>Weekly: Runs on selected weekdays at a set time. (e.g., Every Sunday and Friday at 00:00 UTC).</p> </li> <li> <p>Monthly: Runs on a specific day of the month at a set time. (e.g., 1st day of every month at 00:00 UTC).</p> </li> <li> <p>Advanced: Use Cron expressions for custom scheduling. (e.g., <code>0 12 * * 1-5</code> runs at 12 PM, Monday to Friday).</p> </li> </ul> <p></p> <p>Step 3: Define the Schedule Name to identify the scheduled Materialize Operation when it runs.</p> <p></p> <p>Step 4: Click Schedule to finalize and schedule the Materialize Operation.</p> <p></p> <p>After clicking Schedule, a confirmation message appears stating \"Operation Scheduled\". Go to the Activity tab to see the progress of materialize operation.</p> <p></p>"},{"location":"container/enrichment-operation/materialize-operation/#review-materialized-data","title":"Review Materialized Data","text":"<p>Step 1: Once the selected containers are materialized, go to Enrichment Datastores from the left menu. </p> <p></p> <p>Step 2: In the Enrichment Datastores section, select the datastore where you materialized the snapshot. The materialized containers will now be visible.</p> <p></p> <p>Step 3: Click on the materialized files to review the snapshot. For demonstration, we have selected the \"materialized_field_profiles\" file.</p> <p>The materialized data is displayed in a table format, showing key details about the selected containers. It typically includes columns indicating data structure, completeness, and uniqueness. You can use this data for analysis, validation, and integration.</p> <p></p>"},{"location":"container/manage-tables-and-files/add-checks/","title":"Add Checks","text":"<p>Add Check allows you to create rules to validate the data within a particular table. You can choose the type of rule, link it directly to the selected table, and add descriptions or tags. This ensures that the table's data remains accurate and compliant with the required standards.</p> <p>Info</p> <p>For more information about checks, please refer to the checks documentation.</p> <p>Step 1: Click on the vertical ellipse next to the table name and select Add Checks.</p> <p></p> <p>A modal window will appear to add checks against the selected table.</p> <p></p> <p>Note</p> <p>To understand how to add checks, you can follow the remaining steps from the documentation Checks Template.</p>"},{"location":"container/manage-tables-and-files/delete/","title":"Delete","text":"<p>Delete allows you to remove a table from the connected source datastore. While the table and its associated data will be deleted, it is not permanent, as the table can be recreated if you run a catalog with the \"recreate\" option. </p> <p>Note</p> <p>Deleting a table is a reversible action if a catalog with the \"recreate\" option is run later. </p> <p>Step 1: Select the tables in your connected source datastore that you would like to delete, then click on Delete.</p> <p></p> <p>Step 2: A confirmation modal window will appear; click on the Delete button to remove the table from the system.</p> <p></p> <p>Step 3: After clicking on the delete button, your table is successfully deleted and a success flash message will appear saying \"Profile has been successfully deleted\"</p> <p></p>"},{"location":"container/manage-tables-and-files/export/","title":"Export","text":"<p>Export feature lets you capture changes in your tables. You can export metadata for Quality Checks, Field Profiles, and Anomalies from selected tables to an enrichment datastore. This helps you analyze data trends, find issues, and make better decisions based on the table data.</p> <p>Step 1: Select the tables in your JDBC datastore that you would like to export, then click on Export.</p> <p></p> <p>A modal window will appear with the Export Operation setting.</p> <p></p> <p>For the next steps, detailed information on the Export Operation is available in the Export Operation section of the documentation.</p>"},{"location":"container/manage-tables-and-files/mark-tables-%26-files-as-favorite/","title":"Mark Tables &amp; Files as Favorite","text":"<p>Marking tables and files as favorite allows you to quickly access important items. This feature helps you prioritize and manage the tables and files you use frequently, making data management more efficient.</p> <p>Info</p> <p>Favoriting a table or file is user-specific. Only you can see your favorites; it does not affect other users.</p> <p>Step 1: Locate the tables and files you want to mark as favorites and click on the bookmark icon to mark the table and file as a favorite.</p> <p></p> <p>After clicking on the bookmark icon, your table or file is successfully marked as a favorite and a success flash message will appear stating \u201cThe Table has been favorited\u201d.</p> <p></p> <p>Step 2: To unmark tables and files, simply click on the bookmark icon of the marked tables and files. This will remove it from your favorites.</p> <p></p>"},{"location":"container/manage-tables-and-files/materialize/","title":"Materialize","text":"<p>Materialize Operation captures snapshots of selected containers from a source datastore and exports them to an enrichment datastore for seamless data loading. Users can run it instantly or schedule it at set intervals, ensuring structured data is readily available for analysis and integration.</p> <p>Step 1: Select the tables in your JDBC datastore that you would like to capture and export for the Materialize Operation, then click on Materialize.</p> <p></p> <p>A modal window will appear with the Materialize Operation setting.</p> <p></p> <p>For the next steps, detailed information on the Materialize Operation is available in the Materialize Operation section of the documentation.</p>"},{"location":"container/manage-tables-and-files/observability-settings/","title":"Observability Settings","text":"<p>Observability helps you track and monitor data performance in your connected source datastore\u2019s tables and files. It provides insights into data volume, detects anomalies, and ensures smooth data processing by identifying potential issues early. This makes it easier to manage and maintain data quality over time.</p> <p>Step 1: Select the table in your JDBC datastore that you would like to monitor, then click on Observability. </p> <p></p> <p>A modal window for Observability Settings will appear. Here you can view the details of the table and datastore where actions have been applied.</p> <p></p> <p>Step 2: Check the Volume Tracking to enable trend analysis and anomaly detection in data volumes over time and check the Freshness Tracking to ensure data timeliness and to identify pipeline delays.</p> <p>Volume Tracking monitors and records daily volume metrics for this data asset. This feature enables trend analysis and anomaly detection in data volumes over time. Freshness Tracking measures and records the last time data was added or updated in the data asset. This feature helps ensure data timeliness and identifies pipeline delays.</p> <p></p> <p>Step 3: Click on the Save button.</p> <p></p> <p>After clicking on the Save button, a success flash message will appear stating \"Profile has been successfully updated\".</p> <p></p>"},{"location":"container/manage-tables-and-files/run/","title":"Run","text":"<p>Execute various operations like profiling or scanning your table or file. It helps validate data quality and ensures that the table meets the defined checks and rules, providing insights into any anomalies or data issues that need attention.</p> <p>Step 1: Click on the vertical ellipse next to the table name and select Run.</p> <p></p> <p>Under Run, choose the type of operation you want to perform:</p> <ul> <li> <p>Profile: To collect metadata and profile the table's contents.</p> </li> <li> <p>Scan: To validate the data against defined rules and checks.</p> </li> </ul> <p></p> <p>To understand how a profile operation is performed, you can follow the remaining steps from the documentation Profile Operation..</p> <p>To understand how a scan operation is performed, you can follow the remaining steps from the documentation Scan Operation..</p>"},{"location":"container/manage-tables-and-files/setting-for-dfs-files-pattern/","title":"Settings For DFS Files Pattern","text":"<p>Settings allow you to edit how data is processed and analyzed for a specific file pattern in your connected source datastore. This includes selecting fields for incremental and partitioning strategies, grouping data, excluding certain fields from scans, and adjusting general behaviors.</p> <p>Step 1: Click on the vertical ellipse next to the file pattern of your choice and select Settings from the dropdown list.</p> <p></p> <p>A modal window will appear for File Pattern Settings.</p> <p></p> <p>Step 2: Modify the table setting based on:</p> <ul> <li> <p>Group Criteria</p> </li> <li> <p>Excluding</p> </li> <li> <p>General</p> </li> </ul>"},{"location":"container/manage-tables-and-files/setting-for-dfs-files-pattern/#group-criteria","title":"Group Criteria","text":"<p>Group Criteria allow you to organize data into specific groups for more precise analysis. By grouping fields, you can gain better insights and enhance the accuracy of your profiling. </p> <p></p> <p>For information about Group Criteria, you can refer to the documentation on Grouping..</p>"},{"location":"container/manage-tables-and-files/setting-for-dfs-files-pattern/#excluding","title":"Excluding","text":"<p>Excluding allows you to choose specific fields from a file pattern that you want to exclude from data checks. This helps focus on the fields that matter most for validation while ignoring others that are not relevant to the current analysis.</p> <p></p> <p>For information about Excluding, you can refer to the documentation on Excluding Settings..</p>"},{"location":"container/manage-tables-and-files/setting-for-dfs-files-pattern/#general","title":"General","text":"<p>You can control how file patterns behave by checking or unchecking options to make data processing easier and more consistent. These settings help the system automatically adjust file structures for better integration and analysis.</p> <p></p> <ul> <li>Inferring Data Types: When enabled, the system automatically determines the correct data type for each field and applies it. This keeps data consistent and reduces errors, saving you time on manual fixes.</li> </ul> <p></p> <ul> <li>First Row as Field Names: Turning this on uses the first row of a file as headers, making it simple to map and organize data in the right format.</li> </ul> <p></p> <ul> <li>Treating Empty Values as Nulls:  The \"Treat empty values as nulls\" setting controls how empty fields in files like Excel and CSV are handled. If enabled (true), empty fields are treated as NULL (missing data). If disabled (false), they are stored as empty strings (\"\"), meaning the field exists but is blank. This affects reporting, calculations, and data processing, as NULL values are ignored while empty strings may still be counted.</li> </ul> <p></p> <p>Step 3: Once you have configured the file pattern settings, click on the Save button.</p> <p></p> <p>After clicking on the Save button, your table is successfully updated and a success flash message will appear stating \"File Pattern has been successfully updated\".</p> <p></p>"},{"location":"container/manage-tables-and-files/setting-for-jdbc-table/","title":"Settings For JDBC Table","text":"<p>Settings allow you to edit how data is processed and analyzed for a specific table in your connected source datastore. This includes selecting fields for incremental and partitioning strategies, grouping data, excluding certain fields from scans, and adjusting general behaviors.</p> <p>Step 1: Click on the vertical ellipse next to the table of your choice and select Settings from the dropdown list.</p> <p></p> <p>A modal window will appear for Table Settings.</p> <p></p> <p>Step 2: Modify the table settings based on the following options:</p> <ul> <li> <p>Identifiers</p> </li> <li> <p>Group Criteria</p> </li> <li> <p>Excluding</p> </li> <li> <p>General</p> </li> </ul>"},{"location":"container/manage-tables-and-files/setting-for-jdbc-table/#identifiers","title":"Identifiers","text":"<p>An Identifier is a field that can be used to help load the desired data from a Table in support of analysis. For more details about identifiers, you can refer to the documentation on Identifiers..</p>"},{"location":"container/manage-tables-and-files/setting-for-jdbc-table/#incremental-strategy","title":"Incremental Strategy","text":"<p>This is crucial for tracking changes at the row level within tables. This approach is essential for efficient data processing, as it is specifically used to track which records have already been scanned. This allows for scan operations to focus exclusively on new records that have not been previously scanned, thereby optimizing the scanning process and ensuring that only the most recent and relevant data is analyzed.</p> <p>Note</p> <p>If you have connected a DFS datastore, no manual setup is needed for the incremental strategy, the system automatically tracks and processes the latest data changes.</p> <p></p> <p>For information about incremental strategy, you can refer to the Incremental Strategy section in the Identifiers documentation.   </p>"},{"location":"container/manage-tables-and-files/setting-for-jdbc-table/#incremental-field","title":"Incremental Field","text":"<p>Incremental Field lets you select a field that tracks changes in your data. This ensures only new or updated records are scanned, improving efficiency and reducing unnecessary processing.</p> <p></p>"},{"location":"container/manage-tables-and-files/setting-for-jdbc-table/#partition-field","title":"Partition Field","text":"<p>Partition Field is used to divide the data in a table into distinct segments, or dataframes. These partitions allow for parallel analysis, improving efficiency and performance. By splitting the data, each partition can be processed independently. This approach helps optimize large-scale data operations.</p> <p></p> <p>For information about Partition Field, you can refer to the Partition Field section in the Identifiers documentation. </p>"},{"location":"container/manage-tables-and-files/setting-for-jdbc-table/#group-criteria","title":"Group Criteria","text":"<p>Group Criteria allow you to organize data into specific groups for more precise analysis. By grouping fields, you can gain better insights and enhance the accuracy of your profiling. </p> <p></p> <p>For information about Group Criteria, you can refer to the documentation on Grouping..</p>"},{"location":"container/manage-tables-and-files/setting-for-jdbc-table/#excluding","title":"Excluding","text":"<p>Excluding allows you to choose specific fields from a table that you want to exclude from data checks. This helps focus on the fields that matter most for validation while ignoring others that are not relevant to the current analysis.</p> <p></p> <p>For information about Excluding, you can refer to the documentation on Excluding Settings..</p>"},{"location":"container/manage-tables-and-files/setting-for-jdbc-table/#general","title":"General","text":"<p>You can control the default behavior of the specific table by checking or unchecking the option to infer the data type for each field. When checked, the system will automatically determine and cast the data types as needed for accurate data processing.</p> <p></p> <p>For information about General, you can refer to the documentation on General Settings..</p> <p>Step 3: Once you have configured the table settings, click on the Save button.</p> <p></p> <p>After clicking on the Save button, your table is successfully updated and a success flash message will appear stating \"Table has been successfully updated\".</p> <p></p>"},{"location":"container/settings/general/","title":"General &amp; Excluding Overview","text":"<p>General and excluding fields in Qualytics simplify data analysis by organizing key information and removing irrelevant or sensitive data. This ensures efficient management, protects privacy, and supports customized configurations for specific needs.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"container/settings/general/#manage-general-excluding","title":"Manage General &amp; Excluding","text":"<p>Step 1: Log in to your Qualytics account and select the source datastore (JDBC or DFS) from the left menu that you want to manage.</p> <p></p> <p>Step 2: Select Tables (if JDBC datastore is connected) or File Patterns (if DFS datastore is connected) from the Navigation tab on the top.</p> <p></p> <p>Step 3: You will view the full list of tables or files belonging to the selected source datastore.</p> <p></p> <p>Step 4: Click on the vertical ellipse next to the table of your choice and select Settings from the dropdown list.</p> <p></p> <p>A modal window will appear for Table Settings, where you can manage general and excluding for the selected table.</p> <p></p>"},{"location":"container/settings/general/#excluding-fields","title":"Excluding Fields","text":"<p>This configuration allows you to selectively exclude specific fields from containers. These excluded fields will be omitted from check creation during profiling operations and will also be hidden in data previews immediately, without requiring a profile run.</p> <p>This can be helpful when dealing with sensitive data, irrelevant information, or large datasets where you want to focus on specific fields.</p> <p></p>"},{"location":"container/settings/general/#benefits-of-excluding-fields","title":"Benefits of Excluding Fields","text":""},{"location":"container/settings/general/#targeted-analysis","title":"Targeted Analysis","text":"<p>Focus your analysis on the fields that matter most by removing distractions from excluded fields.</p>"},{"location":"container/settings/general/#data-privacy","title":"Data Privacy","text":"<p>Protect sensitive information by excluding fields that contain personal data or confidential information.</p>"},{"location":"container/settings/general/#important-considerations","title":"Important Considerations","text":"<p>Excluding fields will permanently remove them from profile creation and data preview until you re-include them and re-profile the container.</p>"},{"location":"container/settings/general/#infer-data-type","title":"Infer Data Type","text":"<p>The \"infer data type\" option in containers allows the system to automatically determine the appropriate data types (e.g., fractional, integer, date) for columns within your data containers. This setting is configurable for both JDBC and DFS containers.</p> <p></p>"},{"location":"container/settings/general/#behavior-in-jdbc-datastores","title":"Behavior in JDBC Datastores","text":"<ul> <li>Default: Disabled  </li> <li>Reason: JDBC datastores provide inherent schema information from the database tables. Qualytics leverages this existing schema for accurate data typing.  </li> <li>Override: You can optionally enable this setting if encountering issues with automatic type detection from the source database.</li> </ul>"},{"location":"container/settings/general/#behavior-in-dfs-datastores","title":"Behavior in DFS Datastores","text":"<ul> <li>Default: </li> <li>Enabled for CSV files  </li> <li> <p>Disabled for other file formats (Parquet, Delta, Avro, ORC, etc.)  </p> </li> <li> <p>Reason: </p> </li> <li>CSV files lack a defined schema. Data type inference helps ensure correct data interpretation.  </li> <li> <p>File formats like Parquet, Delta, Avro, and ORC have embedded schemas, making inference unnecessary.  </p> </li> <li> <p>Override: You can adjust the default behavior based on your specific data sources and requirements.</p> </li> </ul>"},{"location":"container/settings/general/#rule-for-the-infer-data-type","title":"Rule for the \"Infer Data Type\"","text":""},{"location":"container/settings/general/#schema-based-data-sources","title":"Schema-Based Data Sources","text":"<p>If the data source has a defined schema (JDBC, Delta, Parquet, Avro, ORC), the flag is set to \"False\".</p>"},{"location":"container/settings/general/#schema-less-data-sources","title":"Schema-less Data Sources","text":"<p>If the data source lacks a defined schema (CSV), the flag is set to \"True\".</p>"},{"location":"container/settings/general/#override-file-pattern-for-dfs-datastores","title":"Override file pattern for DFS datastores","text":"<p>Override the file pattern to include files with the same schema but don't match the automatically generated pattern from the initial cataloging.</p> <p>In some cases, you may have multiple files that share the same schema but don't match the automatically generated file pattern during the initial cataloging process. To address this, Qualytics has the ability to override file patterns in the UI. This allows you to specify a custom pattern that encompasses all files with the shared schema, ensuring they are properly included in profiling and analysis.</p>"},{"location":"container/settings/general/#explore-deeper-knowledge","title":"Explore Deeper Knowledge","text":"<p>If you want to go deeper into the knowledge or if you are curious and want to learn more about DFS filename globbing, you can explore our comprehensive guide here: How DFS Filename Globbing Works.</p>"},{"location":"container/settings/general/#important-considerations_1","title":"Important Considerations","text":"<p>Subsequent catalog operations without pruning (<code>Disabled</code>) will use the new pattern.</p>"},{"location":"container/settings/grouping/","title":"Grouping Overview","text":"<p>Grouping is a fundamental aspect of data analysis, allowing users to organize data into meaningful categories for in-depth examination. With the ability to set grouping on Containers, users can define how data within a container should be grouped, facilitating more focused and efficient analysis.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"container/settings/grouping/#managing-a-grouping","title":"Managing a Grouping","text":"<p>Step 1: Log in to your Qualytics account and select the source datastore (JDBC or DFS) from the left menu that you want to manage.</p> <p></p> <p>Step 2: Select Tables (if JDBC datastore is connected) or File Patterns (if DFS datastore is connected) from the Navigation tab on the top.</p> <p></p> <p>Step 3: You will view the full list of tables or files belonging to the selected source datastore.</p> <p></p> <p>Step 4: Click on the vertical ellipse next to the table of your choice and select Settings from the dropdown list.</p> <p></p> <p>A modal window will appear for Table Settings, where you can manage grouping for the selected table. Use the Grouping section to organize fields, with a warning to avoid large row groupings to maintain performance. Add grouping logic via Group Criteria.</p> <p></p>"},{"location":"container/settings/grouping/#usage","title":"Usage","text":"<p>The <code>grouping</code> parameter accepts a list of lists of field names. Each inner list holds the field names in the order that they will be applied as grouping criteria. This flexibility allows users to customize the grouping behavior based on their specific analytical requirements.</p>"},{"location":"container/settings/grouping/#example","title":"Example","text":"<p>Consider the following examples of <code>grouping</code> configurations:</p> <p>1 <code>[\"store_id\"]</code>: Groups data within the container by the <code>store_id</code> field. 2 <code>[\"store_id\", \"month\"]</code>: Groups data first by <code>store_id</code>, then by <code>month</code>. 3 <code>[\"store_id\", \"state\"]</code>: Groups data first by <code>store_id</code>, then by <code>state</code>.</p> <p>By specifying different combinations of fields in the <code>grouping</code> parameter, users can tailor the grouping behavior to suit their analytical needs.</p>"},{"location":"container/settings/grouping/#impact-on-data-profiles","title":"Impact on Data Profiles","text":"<p>The grouping configuration has implications for various aspects or Grouping has implications for various aspects.</p>"},{"location":"container/settings/grouping/#field-profiles","title":"Field Profiles","text":"<p>Field Profiles are now produced with filters determined by the <code>grouping</code> specified on the Profile Operation. This means that the profiles generated will reflect the characteristics of data within each group defined by the grouping criteria.</p>"},{"location":"container/settings/grouping/#inferred-quality-checks","title":"Inferred Quality Checks","text":"<p>The inferred checks produced by the analytics engine will also hold the filter defined by the <code>grouping</code>. This ensures that data access controls and constraints are applied consistently across different groupings of data within the container.</p>"},{"location":"container/settings/grouping/#inferred-quality-check-filters","title":"Inferred Quality Check Filters","text":"<p>Quality Check filters, represented as Spark SQL where clauses, are set based on the <code>grouping</code> specified on the Profile Operation. This ensures that quality checks are applied appropriately to the data within each group, allowing for comprehensive data validation and quality assurance.</p>"},{"location":"container/settings/grouping/#conclusion","title":"Conclusion","text":"<p>The introduction of Grouping for Containers in Qualytics enhances data organization and analysis capabilities, allowing users to define custom grouping criteria and analyze data at a granular level. By leveraging <code>grouping</code>, users can gain deeper insights into their data and streamline the analytical process, ultimately driving more informed decision-making and improving overall data quality and reliability.</p>"},{"location":"container/settings/identifiers/","title":"Identifiers","text":"<p>An Identifier is a field that can be used to help load the desired data from a table in support of the analysis. There are two types of identifiers that can be declared for a table:</p> <ul> <li> <p>Incremental Field: Track records in the table that have already been scanned in order to support Scan operations that only analyze new (not previously scanned) data.</p> </li> <li> <p>Partition Field: Divides the data in the table into distinct dataframes that can be analyzed in parallel.</p> </li> </ul>"},{"location":"container/settings/identifiers/#managing-an-identifier","title":"Managing an Identifier","text":"<p>Step 1: Log in to your Qualytics account and select the source datastore (JDBC or DFS) from the left menu that you want to manage.</p> <p></p> <p>Step 2: Select Tables (if JDBC datastore is connected) or File Patterns (if DFS datastore is connected) from the Navigation tab on the top.</p> <p></p> <p>Step 3: You will view the full list of tables or files belonging to the selected source datastore.</p> <p></p> <p>Step 4: Click on the vertical ellipse next to the table of your choice and select Settings from the dropdown list.</p> <p></p> <p>A modal window will appear for Table Settings, where you can manage identifiers for the selected table.</p> <p></p>"},{"location":"container/settings/identifiers/#incremental-strategy","title":"Incremental Strategy","text":"<p>The Incremental Strategy configuration in Qualytics is crucial for tracking changes at the row level within tables.</p> <p>This approach is essential for efficient data processing, as it is specifically used to track which records have already been scanned.</p> <p>This allows for scan operations to focus exclusively on new records that have not been previously scanned, thereby optimizing the scanning process and ensuring that only the most recent and relevant data is analyzed.</p> <p></p> No. Strategy Option Description 1 None No incremental strategy, runs full table scans. 2 Last Modified - Available types are Date or Timestamp was last modified.- Uses a \"last modified column\" to track changes in the data set.- This column typically contains a timestamp or date value indicating when a record was last modified.- The system compares the \"last modified column\" to a previous timestamp or date, updating only the records modified since that time. 3 Batch Value - Available types are Integral or Fractional.- Uses a \"batch value column\" to track changes in the data set.- This column typically contains an incremental value that increases as new data is added.- The system compares the current \"batch value\" with the previous one, updating only records with a higher \"batch value\".- Useful when data comes from a system without a modification timestamp. 4 Postgres Commit Timestamp Tracking - Utilizes commit timestamps for change tracking. <p>Availability based on technologies: </p> Option Availability Last Modified All Batch Value All Postgres Commit Timestamp Tracking PostgreSQL <p>Info</p> <ul> <li>All options are useful for incremental strategy; it depends on the availability of the data and how it is modeled. </li> <li>The three options will allow you to track and process only the data that has changed since the last time the system was run, reducing the amount of data that needs to be read and processed, and increasing the efficiency of your system.</li> </ul>"},{"location":"container/settings/identifiers/#incremental-strategy-with-dfs-distributed-file-system","title":"Incremental Strategy with DFS (Distributed File System)","text":"<p>For DFS in Qualytics, the incremental strategy leverages the last modified timestamps from the file metadata. </p> <p>This automated process means that DFS users do not need to manually configure their incremental strategy, as the system efficiently identifies and processes the most recent changes in the data.</p>"},{"location":"container/settings/identifiers/#example","title":"Example","text":"<p>Objective: Identify and process new or modified records in the ORDERS table since the last scan using an Incremental Strategy.</p> <p>Sample Data</p> O_ORDERKEY O_PAYMENT_DETAILS LAST_MODIFIED 1 {\"date\": \"2023-09-25\", \"amount\": 250.50, \"credit_card\": \"5105105105105100\"} 2023-09-25 10:00:00 2 {\"date\": \"2023-09-25\", \"amount\": 150.75, \"credit_card\": \"4111-1111-1111-1111\"} 2023-09-25 10:30:00 3 {\"date\": \"2023-09-25\", \"amount\": 200.00, \"credit_card\": \"1234-5678-9012-3456\"} 2023-09-25 11:00:00 4 {\"date\": \"2023-09-25\", \"amount\": 175.00, \"credit_card\": \"5555-5555-5555-4444\"} 2023-09-26 09:00:00 5 {\"date\": \"2023-09-25\", \"amount\": 300.00, \"credit_card\": \"2222-2222-2222-2222\"} 2023-09-26 09:30:00"},{"location":"container/settings/identifiers/#incremental-strategy-explanation","title":"Incremental Strategy Explanation","text":"<p>In this example, an Incremental Strategy would focus on processing records that have a LAST_MODIFIED timestamp after a certain cutoff point. For instance, if the last scan was performed on 2023-09-25 at 11:00:00, then only records with O_ORDERKEY 4 and 5 would be considered for the current scan, as they have been modified after the last scan time.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Orders Since Last Scan]\nB --&gt; C{Record Modified After Last Scan?}\nC --&gt;|Yes| D[Process Record]\nC --&gt;|No| E[Skip Record]\nD --&gt; F[Move to Next Record/End]\nE --&gt; F</code></pre> <pre><code>-- An illustrative SQL query to identify and process new or modified records in the ORDERS table since the last scan.\nselect\n    o_orderkey,\n    o_payment_details,\n    last_modified\nfrom orders\nwhere\n    last_modified &gt; '2023-09-25 11:00:00'\n</code></pre>"},{"location":"container/settings/identifiers/#partition-field","title":"Partition Field","text":"<p>The Partition Field is a fundamental feature for organizing and managing large datasets. It is specifically designed to divide the data within a table into separate, distinct dataframes. </p> <p>This segmentation is a key strategy for handling and analyzing data more effectively. By creating these individual dataframes, Qualytics allows for parallel processing, which significantly accelerates the analysis.</p> <p>Each partition can be analyzed independently, enabling simultaneous examination of different segments of the dataset.</p> <p>This not only increases the efficiency of data processing but also ensures a more streamlined and scalable approach to handling large volumes of data, making it an indispensable tool in data analysis and management.</p> <p>The ideal Partition Identifier is an Incremental Identifier of type datetime such as a last-modified field, however, alternatives are automatically identified and set during a Catalog operation.</p> <p></p> <p>Info</p> <ul> <li>Partition Field Selection: When selecting a partition field for a table during catalog operation, we will attempt to select a field with no nulls where possible. </li> <li>User-Specified Partition Fields: Users are permitted to specify partition fields manually. While we ensure that the user selects a field of a supported data type, we do not currently enforce non-nullability or completeness. Care should be given to select partition fields with no or a low percentage of nulls in order to avoid unbalanced partitioning.</li> </ul> <p>Warning</p> <p>If no appropriate partition identifier can be selected, then repeatable ordering candidates (order by fields) are used for less efficient processing of containers with a very large number of rows.</p>"},{"location":"container/settings/identifiers/#example_1","title":"Example","text":"<p>Objective: Identify an efficient process and analyze the ORDERS table by partitioning the data based on the O_ORDERDATE field, allowing parallel processing of different date segments.</p> <p>Sample Data</p> O_ORDERKEY O_CUSTKEY O_ORDERSTATUS O_TOTALPRICE O_ORDERDATE 1 123 'O' 173665.47 2023-09-01 2 456 'O' 46929.18 2023-09-01 3 789 'F' 193846.25 2023-09-02 4 101 'O' 32151.78 2023-09-02 5 202 'F' 144659.20 2023-09-03"},{"location":"container/settings/identifiers/#partition-field-explanation","title":"Partition Field Explanation","text":"<p>In this example, the O_ORDERDATE field is used to partition the ORDERS table. Each partition represents a distinct date, allowing for the parallel processing of orders based on their order date. This strategy enhances the efficiency of data analysis by distributing the workload across different partitions.</p> FlowchartSQL <pre><code>graph TD\nA[Start] --&gt; B[Retrieve Orders Data]\nB --&gt; C{Partition by O_ORDERDATE}\nC --&gt; D[Distribute Partitions for Parallel Processing]\nC --&gt; E[Identify Date Segments]\nD --&gt; F[Analyze Each Partition Independently]\nE --&gt; F\nF --&gt; G[Combine Results/End]</code></pre> <pre><code>-- An illustrative SQL query to partition the ORDERS table by the O_ORDERDATE field for parallel processing.\nSELECT\n    O_ORDERKEY,\n    O_CUSTKEY,\n    O_ORDERSTATUS,\n    O_TOTALPRICE,\n    O_ORDERDATE,\n    O_ORDERPRIORITY\nFROM\n    orders\nDISTRIBUTE BY\n    O_ORDERDATE;\n</code></pre>"},{"location":"datastore-checks/activate-archived-checks/","title":"Activate Archived Checks","text":"<p>You can activate archived checks individually or in bulk.</p>"},{"location":"datastore-checks/activate-archived-checks/#method-i-activate-specific-check","title":"Method I: Activate Specific Check","text":"<p>Step 1: Navigate to the Archived checks section and click the vertical ellipsis (\u22ee) next to the archived check you want to activate, and select \"Activate\"\u202ffrom the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Metric\" check.</p> <p></p> <p>Step 2: A confirmation modal window \u201cActivate Check\u201d will appear. Click the \u201cActivate\u201d button to activate the archived check.</p> <p></p> <p>Step 3: After clicking the Activate button, your check has been successfully moved to active checks, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/activate-archived-checks/#method-ii-activate-archived-checks-in-bulk","title":"Method II: Activate Archived Checks in Bulk","text":"<p>Step 1: Hover over the archived checks and click the checkbox to select multiple checks in bulk.</p> <p></p> <p>When multiple checks are selected, an action toolbar appears, displaying the total number of checks chosen along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click the vertical ellipsis (\u22ee) and choose \"Activate\" from the dropdown menu to activate the selected checks.</p> <p></p> <p>Step 3: A confirmation modal window \u201cBulk Activate Checks\u201d will appear. Click the \u201cActivate\u201d button to activate the archived checks.</p> <p></p> <p>After clicking the Activate button, your archived checks will be activated, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/activate-draft-check/","title":"Activate Draft Checks","text":"<p>There are two ways to activate draft checks: you can activate specific checks or activate multiple checks in bulk.</p>"},{"location":"datastore-checks/activate-draft-check/#method-i-activate-specific-check","title":"Method I. Activate Specific Check","text":"<p>Step 1: Navigate to the Draft check section and click the vertical ellipsis (\u22ee) next to the draft check you want to activate, and select Edit\u202ffrom the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Time Distribution Size\" check.</p> <p></p> <p>A modal window will appear with the check details. If you want to make any changes to the check details, you can edit them.</p> <p></p> <p>Step 2: Click the down arrow icon with the Update button. A dropdown menu will appear. Click the Activate button.  </p> <p></p> <p>After clicking the Activate button, your check has been successfully moved to active checks, and a success message will appear on the screen.</p> <p>Another option is to activate the check straight from the draft list without opening it. Click the vertical ellipsis (\u22ee) next to the draft check and selecting \"Activate\" from the dropdown menu.</p> <p></p> <p>A confirmation modal window \u201cActivate Check\u201d will appear. Click the \u201cActivate\u201d button to activate the draft check.</p> <p></p> <p>You can also activate a draft check directly from its Overview page. Open the check, click the setting button in the top-right corner, and select Activate from the dropdown menu.</p> <p></p> <p>A confirmation modal will appear. Once completed, a success message will appear.</p> <p></p>"},{"location":"datastore-checks/activate-draft-check/#method-ii-activate-draft-checks-in-bulk","title":"Method II. Activate Draft Checks in Bulk","text":"<p>Step 1: Hover over the draft checks and click the checkbox to select multiple checks in bulk.</p> <p></p> <p>When multiple checks are selected, an action toolbar appears, displaying the total number of checks chosen along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click the vertical ellipsis (\u22ee) and choose \"Activate\" from the dropdown menu to activate the selected checks.</p> <p></p> <p>Step 3: A confirmation modal window \u201cBulk Activate Checks\u201d will appear. Click the \u201cActivate\u201d button to activate the draft checks.</p> <p></p> <p>After clicking the Activate button, your draft checks will be activated, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/archive-check/","title":"Archive Checks","text":"<p>There are two ways to archive checks: you can archive individual checks or archive multiple checks in bulk.</p>"},{"location":"datastore-checks/archive-check/#method-i-archive-specific-check","title":"Method I: Archive Specific Check","text":"<p>You can archive a specific check using two ways: either by directly clicking the archive button on the check or by opening the check and selecting the archive option from the action menu.</p>"},{"location":"datastore-checks/archive-check/#1-archive-directly","title":"1. Archive Directly","text":"<p>Step 1: Locate the check (whether Active or Draft) that you want to archive and click the vertical ellipsis (\u22ee) next to it and select \"Archive\" from the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Greater Than Field\" check.</p> <p></p> <p>Step 2: A modal window titled \"Archive Check\" will appear, providing you with the following archive options:</p> <ul> <li> <p>Discarded: Select this option if the check is no longer relevant or suitable for the current business rules or data requirements. This helps in archiving checks that are obsolete but still exist for historical reference.</p> </li> <li> <p>Invalid: Choose this option if the check is not valid and should be retired from future inference. This helps the system learn from invalid checks and improves its ability to infer valid checks in the future.</p> </li> </ul> <p></p> <p>Step 3: Once you've made your selection, click the Archive button to proceed.</p> <p></p> <p>After clicking the Archive button, your check is moved to the archive, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/archive-check/#2-archive-from-action-menu","title":"2. Archive from Action Menu","text":"<p>Step 1: Locate the check (whether Active or Draft) that you want to archive and click the vertical ellipsis (\u22ee) next to it and select \"Edit\" from the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Greater Than Field\" check.</p> <p></p> <p>Step 2: A modal window will appear displaying the check details. Click the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window and click \"Archive\" from the dropdown menu.</p> <p></p> <p>Step 3: A modal window titled \u201cArchive Check\u201d will appear, providing you with the following archive options:</p> <ul> <li> <p>Discarded: Select this option if the check is no longer relevant or suitable for the current business rules or data requirements. This helps in archiving checks that are obsolete but still exist for historical reference.</p> </li> <li> <p>Invalid: Choose this option if the check is not valid and should be retired from future inference. This helps the system learn from invalid checks and improves its ability to infer valid checks in the future.</p> </li> </ul> <p></p> <p>Step 4: Once you've made your selection, click the Archive button to proceed.</p> <p></p> <p>After clicking the Archive button, your check is moved to the archive, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/archive-check/#method-ii-archive-checks-in-bulk","title":"Method II: Archive Checks in Bulk","text":"<p>You can archive multiple checks in a single step, deactivating and storing them for future reference or restoration while keeping your active checks uncluttered.</p> <p>Step 1: Hover over the checks (whether Active or Draft) and click the checkbox to select multiple checks.</p> <p></p> <p>When multiple checks are selected, an action toolbar appears, displaying the total number of selected checks along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click the vertical ellipsis (\u22ee) and choose \"Archive\" from the dropdown menu to archive the selected checks.</p> <p></p> <p>A modal window will appear, providing you with the following archive options:</p> <p>1. Delete all anomalies associated with the checks: Toggle this option \"On\" if you want to delete any anomalies related to the selected checks when archiving them.</p> <p>2. Archive Options: You are presented with two options to categorize why the checks are being archived:</p> <ul> <li> <p>Discarded: Select this option if the check is no longer relevant or suitable for the current business rules or data requirements. This helps in archiving checks that are obsolete but still exist for historical reference.</p> </li> <li> <p>Invalid: Choose this option if the check is not valid and should be retired from future inference. This helps the system learn from invalid checks and improves its ability to infer valid checks in the future.</p> </li> </ul> <p></p> <p>Step 3: Once you've made your selections, click the \"Archive\" button to confirm and archive the checks.  </p> <p></p> <p>After clicking the \"Archive\" button, your selected checks (whether Active or Draft) will be successfully archived, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/check-favorite/","title":"Mark Check as Favorite","text":"<p>Locate the check you want to mark as a favorite and click the bookmark icon located on the right side of the check.</p> <p></p> <p>After clicking the bookmark icon, your check is successfully marked as a favorite, and a success message will appear on the screen.</p> <p>To unmark a check, simply click the bookmark icon of the marked check. This will remove it from your favorites.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/","title":"Checks in Datastore","text":"<p>Checks are validation rules applied to datasets to ensure data quality and integrity. They can be categorized as Active, Draft, or Archived based on their status and usage. Each check includes detailed metadata such as importance, scan history, anomalies, and assertion results. This section guides you through viewing, managing, and analyzing these checks within your datastore.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"datastore-checks/checks-datastore/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and select the datastore from the left menu on which you want to manage your checks.</p> <p></p> <p>Step 2: Click on the \"Checks\" from the Navigation Tab.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#categories-checks","title":"Categories Checks","text":"<p>You can categorize your checks based on their status, such as Active, Draft, Archived (Invalid and Discarded), or All, according to your preference. This categorization offers a clear view of the data quality validation process, helping you manage checks efficiently and maintain data integrity.</p>"},{"location":"datastore-checks/checks-datastore/#all","title":"All","text":"<p>By selecting All Checks, you can view a comprehensive list of all the checks in the datastore, including both active and draft checks, allowing you to focus on the checks that are currently being managed or are in progress. However, archived checks are not displayed in this.  </p> <p></p>"},{"location":"datastore-checks/checks-datastore/#active","title":"Active","text":"<p>By selecting Active, you can view checks that are currently applied and being enforced on the data. These operational checks are used to validate data quality in real time, allowing you to monitor all active checks and their performance.</p> <p></p> <p>You can also categorize the active checks based on their importance and favorites to streamline your data quality monitoring.</p> <p>1. Important: Shows only checks that are marked as important. These checks are prioritized based on their significance, typically assigned a weight of 7 or higher.</p> <p>Note</p> <p>Important checks are prioritized based on a weight of 7 or higher.</p> <p></p> <p>2. Favorite: Displays checks that have been marked as favorites. This allows you to quickly access checks that you use or monitor frequently.</p> <p></p> <p>3. All: Displays a comprehensive view of all active checks, including important, favorite, and any checks that do not fall under these specific categories.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#draft-checks","title":"Draft Checks","text":"<p>By selecting Draft, you can view checks that have been created but have not yet been applied to the data. These checks are in the drafting stage, allowing for adjustments and reviews before activation. Draft checks provide flexibility to experiment with different validation rules without affecting the actual data.</p> <p></p> <p>You can also categorize the draft checks based on their importance and favorites to prioritize and organize them effectively during the review and adjustment process.</p> <p>1. Important: Shows only checks that are marked as important. These checks are prioritized based on their significance, typically assigned a weight of 7 or higher. </p> <p></p> <p>2. Favorite: Displays checks that have been marked as favorites. This allows you to quickly access checks that you use or monitor frequently.  </p> <p></p> <p>3. All: Displays a comprehensive view of all draft checks, including important, favorite, and any checks that do not fall under these specific categories.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#archived-checks","title":"Archived Checks","text":"<p>By selecting Archived, you can view checks that have been marked as discarded or invalid from use but are still stored for future reference or restoration. Although these checks are no longer active, they can be restored if needed.</p> <p></p> <p>You can also categorize the archived checks based on their status as Discarded, Invalid, or view All archived checks to manage and review them effectively.</p> <p>1. Discarded: Shows checks that have been marked as no longer useful or relevant and have been discarded from use.</p> <p></p> <p>2. Invalid: Displays checks that are deemed invalid due to errors or misconfigurations, requiring review or deletion.</p> <p></p> <p>3. All: Provides a view of all archived checks within this category, including discarded and invalid checks.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#check-info","title":"Check Info","text":"<p>Check Details provides important information about each check in the system. It shows when a check was last run, how often it has been used, when it was last updated, who made changes to it, and when it was created. This section helps users understand the status and history of the check, making it easier to manage and track its use over time.</p> <p>Step 1: Locate the check you want to review, then hover over the info icon to view the Check Details.</p> <p></p> <p>A popup will appear with additional details about the check.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#last-asserted","title":"Last Asserted","text":"<p>Last Asserted At shows the most recent time the check was run, indicating when the last validation occurred. For example, the check was last asserted on Oct 17, 2023, at 2:37 AM (GMT+5:30).</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#scans","title":"Scans","text":"<p>Scans show how many times the check has been used in different operations. It helps you track how often the check has been applied. For example, the check was used in 30 operations.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#updated-at","title":"Updated At","text":"<p>Updated At shows the most recent time the check was modified or updated. It helps you see when any changes were made to the check\u2019s configuration or settings. For example, the check was last updated on Sep 9, 2024, at 3:18 PM (GMT+5:30).</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#last-editor","title":"Last Editor","text":"<p>Last Editor indicates who most recently made changes to the check. It helps track who is responsible for the latest updates or modifications. This is useful for accountability and collaboration within teams.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#created-at","title":"Created At","text":"<p>Created At shows when the check was first made. It helps you know how long the check has been in use. This is useful for tracking its history. For example, the check was created on Oct 17, 2023, at 2:19 PM (GMT+5:30).</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#check-details","title":"Check Details","text":"<p>Check Detail View displays all key information related to a specific data quality check. It shows what the check is monitoring, how it's configured, where it's applied in the dataset, and whether any issues have been found. It also includes sections for viewing the check\u2019s recent performance, related activities, and any additional metadata. This view helps users easily understand the purpose and current state of the check.</p> <p>Step 1: Click on the check that you want to see the details of.</p> <p></p> <p>You will be navigated to the detail section, where you can view the Summary, Observability, Properties, Activity, and Metadata information.</p> <p></p> <p>Info</p> <p>In addition to viewing the check details, you can also monitor and manage any anomalies associated with this check \u2014 all from the same page, without needing to navigate elsewhere.</p>"},{"location":"datastore-checks/checks-datastore/#summary-section","title":"Summary Section","text":"<p>The Summary section shows that a data quality check is applied to a field and is currently active. It indicates whether the check was created automatically by the system or manually by a user and is being applied to the entire dataset and has a defined importance level. It also shows when the check last ran and whether there are any current issues found in the data.</p> <p>1. Check &amp; Status : The type of check applied to the data. In this case, it's a Volumetric check and the check is Active, meaning it's currently being applied.</p> <p></p> <p>2. Type : This check is Authored, meaning it was manually created by the users.</p> <p></p> <p>When you hover over the time period written below the type of the check, a pop-up appears displaying the complete date and time.</p> <p></p> <p>3. Last Asserted : Shows when the check was last run \u2013 3 months ago in this case.</p> <p></p> <p>When you hover over the time the check last ran, a pop-up appears displaying the complete date and time.</p> <p></p> <p>Last Asserted Details</p> <p>Click on the info icon to view the last asserted details.</p> <p></p> <p>A popup will appear with Scans details. Scans show how many times the check has been used in different operations. It helps you track how often the check has been applied. For example, the check was used in 19 operations.</p> <p></p> <p>4. Weight : Indicates the importance or priority of this check \u2013 the weight is 13.</p> <p></p> <p>5. Coverage : How much data this check applies to \u2013 here it's 100%, meaning it applies to the whole dataset.</p> <p></p> <p>6. Active Anomalies : Number of current issues found \u2013 0 anomalies are active right now.</p> <p></p> <p>7. Description : Explains the rule or condition that the check is validating.</p> <p></p> <p>8. Tags : Displays any tags linked to the check. Users can also add new tags by clicking on the tag area.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#copy-the-check-link","title":"Copy the Check Link","text":"<p>Click on the Copy Check Link icon(represented by share icon) located at the right corner of the summary section to copy a direct link to the selected check. This link can be shared with other users for quick access to the specific check within the platform.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#favorite-the-check","title":"Favorite the check","text":"<p>Click on the bookmark icon located at the right corner of the summary section to mark the check as favorite.</p> <p></p> <p>To unmark a check, simply click on the bookmark icon of the marked check. This will remove it from your favorites.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#observability-section","title":"Observability Section","text":"<p>Observability provides a visual overview of how a check performs over time by tracking assertion results. It helps identify trends, failures, or anomalies using daily status indicators across a selected timeframe.</p> <p></p> <p>Users can hover over any date in the timeline. It provides a comprehensive view of assertion statuses, including passed, failed, and anomalous results. By hovering over a specific date, users can access detailed information such as the result status, the number of asserted records, and any anomalies identified. Highlighting all available status types ensures a clearer understanding of the data quality over time.</p> <p></p> <p>Additionally, clicking the Latest Assertion Scan button (e.g., #48151) will navigate users directly to the Scan Results page for that specific assertion.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#selecting-report-date-and-timeframe","title":"Selecting Report Date and Timeframe","text":"<p>The Observability section helps you monitor how your check assertion metrics change over time. You can customize the view by selecting a specific report date and timeframe to analyze trends over different periods.</p>"},{"location":"datastore-checks/checks-datastore/#select-the-report-date","title":"Select the Report Date","text":"<p>Step 1: Locate the Report Date field at the top-right of the Observability section.</p> <p></p> <p>Step 2: Click on the calendar icon. A date picker will appear. Select the desired report date to update the Assertion Over Time graph accordingly.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#choose-the-timeframe","title":"Choose the Timeframe","text":"<p>Step 1: Locate the Timeframe field at the top-right of the Observability section.</p> <p></p> <p>Step 2: Choose a timeframe for your assertion data view:</p> <ul> <li> <p>Week \u2013 Shows assertion metrics distributed over a 7-day period.</p> </li> <li> <p>Month \u2013 Displays daily or weekly assertions throughout the selected month.</p> </li> <li> <p>Quarter \u2013 Covers a three-month range (e.g., Q1: Jan\u2013Mar, Q2: Apr\u2013Jun), useful for quarterly reporting and insights.</p> </li> <li> <p>Year \u2013 Presents assertion data trends for an entire calendar year, allowing for broad, high-level performance monitoring.</p> </li> </ul> <p></p> <p>Once a timeframe is selected, the Assertion Over Time chart below will automatically adjust to reflect assertion activity within the chosen window.</p>"},{"location":"datastore-checks/checks-datastore/#properties-section","title":"Properties Section","text":"<p>The Properties section explains where this check is applied. In this case, the check is applied to a table called supplier, specifically to the s_comment field of type String. There is no filter added, so the check is applied to all rows in the table. This helps maintain clean and trustworthy data, especially when phone numbers must be unique per customer.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#activity-section","title":"Activity Section","text":"<p>The Activity section displays a chronological history of all actions performed on the quality check, including creation, updates, and automated adjustments. It provides visibility into how the check has evolved over time, capturing the exact configuration, properties, and tags associated with each event.</p> <p></p> <p>You can view the exact version of the check as it existed at that point in time by clicking the check icon on the right side of the activity entry.</p> <p></p> <p>A right side panel will open with the historical configuration of the check.</p> <p></p> <p>The Version At field displays the exact date and time when that version of the check was created. For example, July 8, 2025, at 5:42 AM (GMT+5:30) indicates when the configuration shown was active in the system.</p> <p></p>"},{"location":"datastore-checks/checks-datastore/#metadata-section","title":"Metadata Section","text":"<p>Currently, there is no extra metadata added to this check. Metadata can include additional notes or properties, but in this case, it's left blank.</p> <p></p>"},{"location":"datastore-checks/clone-check/","title":"Clone Check","text":"<p>Step 1: Click the vertical ellipsis (\u22ee) next to the check (whether Active or Draft) that you want to clone and select \"Edit\" from the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Between\" check.</p> <p></p> <p>Step 2: A modal window will appear, displaying the check details. Click the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window and select \"Clone\" from the dropdown menu.</p> <p></p> <p>Step 3: After clicking the Clone button, a modal window will appear. This window allows you to adjust the cloned check's details.</p> <p></p> <p>1. If you toggle on the \"Associate with a Check Template\" option, the cloned check will be linked to a specific template.</p> <p></p> <p>Choose a Template from the dropdown menu that you want to associate with the cloned check. The check will inherit properties from the selected template.</p> <ul> <li> <p>Locked: The check will automatically sync with any future updates made to the template, but you won't be able to modify the check's properties directly.</p> </li> <li> <p>Unlocked: You can modify the check, but future updates to the template will no longer affect this check.</p> </li> </ul> <p></p> <p>2. If you toggle off the \"Associate with a Check Template\" option, the cloned check will not be linked to any template, which allows you full control to modify the properties independently.</p> <p></p> <p>Select the appropriate Rule Type for the check from the dropdown menu.</p> <p></p> <p>Step 4: Once you have selected the template or rule type, fill out the remaining check details as required. </p> <p></p> <p>Step 5: After completing all the check details, click the \"Validate\" button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct. It ensures that the check will work as expected by running it against the data without committing any changes.</p> <p></p> <p>If the validation is successful, a green message saying \"Validation Successful\" will appear. </p> <p></p> <p>If the validation fails, a red message saying \"Failed Validation\" will appear. This typically occurs when the check logic or parameters do not match the data properly.</p> <p></p> <p>Step 6: Once you have a successful validation, click the \"Save\" button. The system will save any modifications you've made to the check and create a clone of that check based on your changes.  </p> <p></p> <p>After clicking the \"Save\" button, your check is successfully created, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/delete-checks/","title":"Delete Checks","text":"<p>There are two methods for deleting checks: you can either delete individual checks or delete multiple checks in bulk.</p> <p>Note</p> <p>You can only delete archived checks. If you want to delete an active or draft check, you must first move it to the archive, and then you can delete it.</p> <p>Warning</p> <p>Deleting a check is a one-time action. It cannot be restored after deletion.</p>"},{"location":"datastore-checks/delete-checks/#method-i-delete-specific-check","title":"Method I. Delete Specific Check","text":"<p>Step 1: Click Archived from the navigation bar in the Checks section to view all archived checks.</p> <p></p> <p>Step 2: Locate the check that you want to delete and click the vertical ellipsis (\u22ee) and select Delete from the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Contains Email\" check.</p> <p></p> <p>Step 3: A confirmation modal window will appear. Click the Delete button to permanently remove the check from the system.</p> <p></p> <p>After clicking the Delete button, your check is successfully deleted, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/delete-checks/#method-ii-delete-checks-in-bulk","title":"Method II. Delete Checks in Bulk","text":"<p>You can permanently delete multiple checks from the system in one action. This process is irreversible, so it should be used when you are certain that the checks are no longer needed.</p> <p>Step 1: Hover over the archived checks and click the checkbox to select checks in bulk.  </p> <p></p> <p>When multiple checks are selected, an action toolbar appears, displaying the total number of selected checks along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click the vertical ellipsis (\u22ee) and choose \"Delete\" from the dropdown menu to delete the selected checks.  </p> <p></p> <p>Step 3: A confirmation modal window will appear. Click the \"Delete\" button to permanently delete the selected checks.</p> <p></p> <p>After clicking the Delete button, your selected checks will be permanently deleted, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/draft-archived-checks/","title":"Draft Archived Checks","text":"<p>You can draft archived checks individually or in bulk.</p>"},{"location":"datastore-checks/draft-archived-checks/#method-i-draft-specific-check","title":"Method I: Draft Specific Check","text":"<p>Step 1: Navigate to the Archived checks section. Click the vertical ellipsis (\u22ee) next to the archived check you want to move to the draft state and select \"Draft\" from the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Contains Email\" check.</p> <p></p> <p>After clicking \"Draft\", the check will be successfully moved to the draft state, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/draft-archived-checks/#method-ii-draft-archived-checks-in-bulk","title":"Method II: Draft Archived Checks in Bulk","text":"<p>Step 1: Hover over the archived checks and click the checkbox to select multiple checks in bulk.</p> <p></p> <p>When multiple checks are selected, an action toolbar appears, displaying the total number of checks chosen along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click the vertical ellipsis (\u22ee) and select \"Draft\" from the dropdown menu to move archived checks to the draft state.</p> <p></p> <p>Step 3: A confirmation modal window \"Bulk Update Checks to Draft\" will appear. Click the \"Update\" button to move the selected archived checks to draft.</p> <p></p> <p>After clicking the \"Update\" button, your archived checks will be moved to draft, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/draft-checks/","title":"Draft Checks","text":"<p>There are two methods to move your active checks to draft: you can either draft specific checks or draft multiple checks in bulk.</p>"},{"location":"datastore-checks/draft-checks/#method-i-draft-specific-check","title":"Method I: Draft Specific Check","text":"<p>Step 1: Click the vertical ellipsis (\u22ee) next to the active check you want to move to the draft state and select \"Edit\" from the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Between\" check.</p> <p></p> <p>Step 2: A modal window will appear displaying the check details. Click the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window and select \"Draft\" from the dropdown menu.</p> <p></p> <p>After clicking \"Draft\", the selected item will move to the draft state, and a success message will appear on the screen.</p> <p>Alternatively, you can move an active check to the draft state by clicking the vertical ellipsis (\u22ee) next to the check and selecting \u201cDraft\u201d from the dropdown menu.</p> <p></p>"},{"location":"datastore-checks/draft-checks/#method-ii-draft-checks-in-bulk","title":"Method II. Draft Checks in Bulk","text":"<p>You can move multiple checks into the draft state in one action, allowing you to pause or make adjustments to several checks without affecting your active validation process.</p> <p>Step 1: Hover over the active checks and click the checkbox to select multiple checks.</p> <p></p> <p>Step 2: Click the vertical ellipsis (\u22ee) and select \"Draft\" from the dropdown menu to move active checks to the draft state.</p> <p></p> <p>A confirmation modal window titled \"Bulk Update Checks to Draft\" will appear, indicating the number of checks being moved to draft. </p> <p></p> <p>Step 3: Click the \"Update\" button to move the selected active checks to draft.</p> <p></p> <p>After clicking the \"Update\" button, your selected checks will be moved to draft, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/dry-run/","title":"Dry Run","text":"<p>Step 1: Click the check you want to test using the Dry Run feature.</p> <p></p> <p>Step 2: Click the Settings icon located at the top-right corner of the interface and select \u201cDry Run\u201d from the dropdown menu.</p> <p></p> <p>A modal window titled Dry Run Results will appear.</p> <p></p> <p>This window enables you to confidently evaluate and refine data quality checks before running full scans, helping maintain high-quality standards without unnecessary noise or misconfiguration.</p> <p></p> No. Field Description 1 Status Indicates whether the dry run completed successfully. 2 Timing Displays the total time taken to execute the dry run. 3 Sampling Limit Shows the number of records sampled during the dry run (default is 10K records). 4 Check ID and Name The unique identifier and name of the data quality check. This provides both a reference ID and a descriptive label indicating the rule type. 5 Description A concise explanation of the check rule being tested. For example, \u201cPS_SUPPLYCOST is greater than PS_AVAILQTY.\u201d 6 Table The name of the table on which the check is being applied. 7 Field The specific column or field within the table that the rule targets."},{"location":"datastore-checks/dry-run/#anomalies","title":"Anomalies","text":"<p>Highlights any violations detected during the dry run, such as constraint breaches or unexpected value patterns.</p> <p></p> No. Field Description 1 Violation Clearly states the reason for failure. This message helps users quickly understand what went wrong and why the data didn't pass the quality check. 2 Asserted Records Displays the total number of records evaluated in the dry run. 3 Anomalous Records Shows how many of those records violated the constraint logic. <p></p>"},{"location":"datastore-checks/dry-run/#source-records","title":"Source Records","text":"<p>The Source Records section presents a detailed, tabular view of all records that were evaluated by the selected quality check. This section is designed to help users investigate the underlying data issues that may have led to anomalies, offering clear visibility into the records that failed to meet the defined constraint.</p> <p></p>"},{"location":"datastore-checks/dry-run/#sort-options","title":"Sort Options","text":"<p>Users can sort the records based on different fields using the Sort By dropdown.</p> <p></p> No. Sort By Description 1 Name Sorts the records alphabetically based on the field name. 2 Weight Sorts records based on the weight or severity of the failure. Higher-weighted issues appear first. 3 Quality Score Sorts records by their quality score, helping you prioritize records with the lowest data quality."},{"location":"datastore-checks/dry-run/#download-source-records","title":"Download Source Records","text":"<p>The Download Source Records option allows users to export the records evaluated during the Dry Run process for further offline analysis or documentation purposes. A file containing the asserted records and their anomaly status will be downloaded in CSV format.</p> <p></p> <p>Note</p> <p>When no issues are detected, users receive a clear confirmation message indicating no anomalies were identified.</p> <p>Info</p> <p>You can perform a Dry Run on draft checks to validate the logic before they are finalized and published.</p>"},{"location":"datastore-checks/edit-checks/","title":"Edit Check","text":"<p>There are two methods for editing checks: you can either edit specific checks or edit multiple checks in bulk.</p> <p>Note</p> <p>When editing multiple checks in bulk, only the filter clause, tags, and metadata can be modified. </p>"},{"location":"datastore-checks/edit-checks/#method-i-edit-specific-check","title":"Method I. Edit Specific Check","text":"<p>Step 1: Click the vertical ellipsis (\u22ee) next to the check you want to edit whether it is an active or draft check, and select \"Edit\" from the dropdown menu.</p> <p>For demonstration purposes, we have selected the \"Greater Than Field\" check.</p> <p></p> <p>A modal window will appear with the check details. </p> <p></p> <p>Step 2: Modify the check details as needed based on your requirements.</p> <p></p> <p>Step 3: Once you have edited the check details, click the \"Validate\" button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct.</p> <p></p> <p>If the validation is successful, a green message saying \"Validation Successful\" will appear. </p> <p></p> <p>If the validation fails, a red message saying \"Failed Validation\" will appear. This typically occurs when the check logic or parameters do not match the data properly.</p> <p></p> <p>Step 4: Once you have a successful validation, click the \"Update\" button. The system will update the changes you've made to the check, including changes to the fields, filter clause, coverage, description, tags, or metadata.</p> <p></p> <p>After clicking the \"Update\" button, your check is successfully updated, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/edit-checks/#method-ii-edit-checks-in-bulk","title":"Method II. Edit Checks in Bulk","text":"<p>You can easily apply changes to multiple checks at once, saving time by editing several checks simultaneously without having to modify each one individually.</p> <p>Step 1: Hover over the checks (whether Active or Draft) and click the checkbox to select multiple checks.</p> <p></p> <p>When multiple checks are selected, an action toolbar appears, displaying the total number of selected checks along with a vertical ellipsis for additional bulk action options.</p> <p></p> <p>Step 2: Click the vertical ellipsis (\u22ee) and select \"Edit\" from the dropdown menu to make changes to the selected checks.</p> <p></p> <p>Step 3: A modal window titled \"Bulk Edit Checks\" will appear. Here you can only modify the filter clause, tags, and metadata of the selected checks.</p> <p></p> <p>Step 4: Toggle on the options (Filter Clause, Tags, or Additional Metadata) that you want to modify for the selected checks, and make the necessary changes.</p> <p>Note</p> <p>This action will overwrite the existing data for the selected checks.</p> <p></p> <p>Step 5: Once you have made the changes, click the \"Save\" button.</p> <p></p> <p>After clicking the \"Save\" button, your selected checks will be updated with the new changes, and a success message will appear on the screen.</p>"},{"location":"datastore-checks/filter-and-sort/","title":"Filter and Sort","text":"<p>You can easily organize your checks using the available sort and filter options.</p>"},{"location":"datastore-checks/filter-and-sort/#sort","title":"Sort","text":"<p>You can sort your checks by Active Anomalies, Coverage, Created Date, Last Asserted, Rules, and Weight to easily organize and prioritize them according to your needs.</p> <p></p> No Sort By Option Description 1 Active Anomalies Sort checks based on the number of active anomalies. 2 Coverage Sort checks by data coverage percentage. 3 Created Date Sort checks according to the date they were created. 4 Last Asserted Sorts by the last time the check was executed. 5 Rules Sort checks based on specific rules applied to the checks. 6 Weight Sort checks by their assigned weight or importance level. <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.</p> <p></p>"},{"location":"datastore-checks/filter-and-sort/#filter","title":"Filter","text":"<p>You can filter your checks based on values like Check Type, Asserted State, Rule, Tags, Table, Field, and Template.</p> <p>Info</p> <p>Users can search across all filter inputs using typos, partial terms, or abbreviations. The system intelligently matches relevant results, making it easier to find what they need without exact inputs.</p> <p></p> <p></p> No Filter Filter Value Description 1 Check Type All Displays all types of checks, both inferred and authored. Inferred Shows system-generated checks that automatically validate data based on detected patterns or logic. Authored Displays user-created checks, allowing the user to focus on custom validations tailored to specific requirements. 2 Asserted State All Displays all checks, regardless of their asserted status. This provides a full overview of both passed, failed, and not asserted checks. Passed Shows checks that have been asserted successfully, meaning no active anomalies were found during the validation process. Failed Displays checks that have failed assertion, indicating active anomalies or issues that need attention. Not Asserted Filters out checks that have not yet been asserted, either because they haven\u2019t been processed or validated yet. 3 Rule N/A Select this to filter the checks based on a specific rule type for data validation, such as checking non-null values, matching patterns, comparing numerical ranges, or verifying date-time constraints. By clicking on the caret down button next to the Rule field, the available rule types will be dynamically populated based on the rule types present in the results. The rules displayed are based on the current dataset and provide more granular control over filtering. Each rule type will show a counter next to it, displaying the total number of occurrences for that rule in the dataset. For example, the rule type After Date Time is displayed with a total of 2 occurrences. <p></p> No Filter Filter Value Description 4 Tag N/A Tag Filter displays only the tags associated with the currently visible items, along with their color icon, name, type, and the number of matching records. Selecting one or more tags refines the list based on your selection. If no matching items are found, a 'No options found' message is displayed. 5 Table N/A Filters checks by the table to which they are applied. 6 Field N/A Filters checks by the specific field/column name within a table. 7 Template N/A This filter allows users to view and apply predefined check templates."},{"location":"datastore-checks/overview/","title":"Overview","text":"<p>Managing your checks within a datastore is important to maintain data integrity and ensure quality. You can categorize, create, update, archive, restore, delete, and clone checks, making it easier to apply validation rules across the datastores. The system allows for checks to be set as active, draft, or archived based on their current state of use. You can also define reusable templates for quality checks to streamline the creation of multiple checks with similar criteria. With options for important and favorite, users have full flexibility to manage data quality efficiently.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"datastore-checks/overview/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and select the datastore from the left menu on which you want to manage your checks.</p> <p></p> <p>Step 2: Click \"Checks\" from the navigation tab.</p> <p></p> <p>You will be navigated to the Checks section within the selected datastore. Here, you can view checks categorized as Active, Draft, Archived (including Invalid and Discarded), or All.</p> <p></p>"},{"location":"datastore-checks/overview/#status-management-of-checks","title":"Status Management of Checks","text":""},{"location":"datastore-checks/overview/#set-check-as-draft","title":"Set Check as Draft","text":"<p>You can move an active check into a draft state, allowing you to work on the check, make adjustments, and refine the validation rules without affecting live data. This is useful when you need to temporarily deactivate a check for review and updates.</p> <p>For more information on how to set a check as draft, please refer to the Draft Checks documentation.</p>"},{"location":"datastore-checks/overview/#activate-draft-check","title":"Activate Draft Check","text":"<p>You can activate the draft checks after you have worked on the check, made adjustments, and refined the validation rules. Activating the draft check and making it live ensures that the defined criteria are enforced on the data.</p> <p>For more information on how to activate a draft check, please refer to the Activate Draft Check documentation.</p>"},{"location":"datastore-checks/overview/#set-check-as-archived","title":"Set Check as Archived","text":"<p>You can move an active or draft check into the archive when it is no longer relevant but may still be needed for historical purposes or future use. Archiving helps keep your checks organized without permanently deleting them.</p> <p>For more information on how to set a check as archived, please refer to the Archive Checks documentation.</p>"},{"location":"datastore-checks/overview/#activate-archived-checks","title":"Activate Archived Checks","text":"<p>You can activate archived checks when you need to restore previously defined validation rules. This is useful if a check was archived temporarily and is now relevant again for data quality enforcement.</p> <p>For more information on how to activate archived checks, please refer to the Activate Archived Checks documentation.</p>"},{"location":"datastore-checks/overview/#draft-archived-checks","title":"Draft Archived Checks","text":"<p>You can move archived checks to the draft state when you want to update or refine them before activation. This is useful if a check is no longer archived but needs adjustments before going live.</p> <p>For more information on how to draft archived checks, please refer to the Draft Archived Checks documentation.</p>"},{"location":"datastore-checks/overview/#restore-archived-checks","title":"Restore Archived Checks","text":"<p>If a check has been archived, then you can restore it back to an active state or in a draft state. This allows you to reuse your checks that were previously archived without having to recreate them from scratch.</p> <p>For more information on how to restore archived checks, please refer to the Restore Archived Checks documentation.</p>"},{"location":"datastore-checks/overview/#edit-check","title":"Edit Check","text":"<p>You can edit an existing check to modify its properties, such as the rule type, coverage, filter clause, or description. Updating a check ensures that it stays aligned with evolving data requirements and maintains data quality as conditions change.</p> <p>For more information on how to edit a check, please refer to the Edit Check documentation.</p>"},{"location":"datastore-checks/overview/#delete-checks","title":"Delete Checks","text":"<p>You can delete a check permanently, removing it from the system, and this is an irreversible action. Once you delete it, the check cannot be restored. By deleting the check, you ensure it will no longer appear in active or archived lists, making the system more streamlined and organized.</p> <p>For more information on how to delete checks, please refer to the Delete Checks documentation.</p>"},{"location":"datastore-checks/overview/#dry-run","title":"Dry Run","text":"<p>The Dry Run feature allows users to simulate the behavior of a Data Quality Check before enforcing it during a scan. This helps validate the check logic and preview potential anomalies without persisting the results or affecting any data.</p> <p>For more information on dry run, please refer to the Dry Run documentation.</p>"},{"location":"datastore-checks/overview/#clone-check","title":"Clone Check","text":"<p>You can clone both active and draft checks to create a duplicate copy of an existing check. This is useful when you want to create a new check based on the structure of an existing one, allowing you to make adjustments without affecting the original check.</p> <p>For more information on how to clone a check, please refer to the Clone Check documentation.</p>"},{"location":"datastore-checks/overview/#create-a-quality-check-template","title":"Create a Quality Check Template","text":"<p>You can add checks as templates, which allows you to create a reusable framework for quality checks. By using templates, you standardize the validation process, enabling the creation of multiple checks with similar rules and criteria across different datastores. This ensures consistency and efficiency in managing data quality checks.</p> <p>For more information on how to create a quality check template, please refer to the Quality Check Template documentation.</p>"},{"location":"datastore-checks/overview/#mark-check-as-favorite","title":"Mark Check as Favorite","text":"<p>Marking a check as a favorite helps you quickly access and prioritize important checks during your data validation process. Favorited checks appear in the \"Favorite\" category, making them easier to manage and monitor.</p> <p>For more information on how to mark a check as favorite, please refer to the Mark Check as Favorite documentation.</p>"},{"location":"datastore-checks/overview/#filter-and-sort","title":"Filter and Sort","text":"<p>Filter and Sort options allow you to organize your checks by various criteria, such as Weight, Active Anomalies, Coverage, Created Date, and Rules. You can also apply filters to refine your list of checks based on Check Type, Asserted State (Passed, Failed, Not Asserted), Tags, Tables, and Fields.</p> <p>For more information on how to filter and sort, please refer to the Filter and Sort documentation.</p>"},{"location":"datastore-checks/overview/#quality-check-migration","title":"Quality Check Migration","text":"<p>Quality Check Migration allows you to transfer authored quality checks from one container to another, even across different datastores. This feature helps you reuse existing quality rules without manually recreating them in the target container. </p> <p>For more information about Quality Check Migration, please refer to the Quality Check Migration documentation.</p>"},{"location":"datastore-checks/quality-check-migration/","title":"Quality Check Migration","text":"<p>Quality Check Migration allows you to transfer authored quality checks from one container to another, even across different datastores. This feature helps you reuse existing quality rules without manually recreating them in the target container. This feature is useful when you want to:</p> <ul> <li>Reuse existing authored quality checks in another container or datastore.  </li> <li>Quickly set up quality checks for similar datasets without starting from scratch.  </li> <li>Standardize quality rules across multiple data stores.</li> </ul> <p>Note</p> <p>Archived and inferred checks are excluded from migration to ensure only active, relevant, authored checks are moved. All migrated checks are set to Draft status, allowing you to review and activate them in the new container.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"datastore-checks/quality-check-migration/#how-it-works","title":"How It Works","text":"<p>The Migrate Quality Checks process consists of two main steps:</p>"},{"location":"datastore-checks/quality-check-migration/#1-select-checks","title":"1. Select Checks","text":"<p>Choose which authored quality checks to migrate:</p> REF. FIELDS DESCRIPTION 1. All Migrates all authored quality checks available in the source container, excluding archived ones. 2. Specific Lets you manually select individual authored quality checks from a list. Useful when you only need certain checks in the target container. 3. Tag Migrates all authored quality checks that match the selected tags, allowing for automated grouping. <p></p>"},{"location":"datastore-checks/quality-check-migration/#2-destination-settings","title":"2. Destination Settings","text":"<p>Define where the selected checks will be migrated:</p> REF. FIELDS DESCRIPTION 1. Source Datastore The datastore where the selected quality checks will be migrated. 2. Table The specific target container (e.g., table) within the datastore where the checks will be added. 3. Assign Additional Tags Lets you add tags to migrated checks to help with categorization and filtering in the target datastore. <p></p> <p>Note</p> <p>Migrated checks are set to Draft. Field(s) will be automatically matched by name when possible; unmatched fields remain unassigned. </p>"},{"location":"datastore-checks/quality-check-migration/#example-use-case","title":"Example Use Case","text":"<p>Scenario</p> <p>You have two tables in the COVID-19 Data datastore:</p> <ul> <li> <p>CDC_INPATIENT_BEDS_ALL \u2013 contains all hospital inpatient bed records.</p> </li> <li> <p>CDC_INPATIENT_BEDS_COVID \u2013 contains only records related to COVID-19 cases.</p> </li> </ul> <p>The first table already has 12 authored quality checks to verify important fields like hospital_id, report_date, and available_beds.</p> <p>The second table doesn\u2019t have these checks yet, but it uses the same structure and fields.</p> <p>Instead of creating all 12 checks again, you migrate them from CDC_INPATIENT_BEDS_ALL to CDC_INPATIENT_BEDS_COVID.</p> <p>This way, both tables follow the same validation rules, saving time and keeping data quality consistent.</p> <p>Before and After Migration</p> Item Before Migration After Migration Source Table CDC_INPATIENT_BEDS_ALL (authored checks already exist). CDC_INPATIENT_BEDS_ALL (unchanged). Destination Table CDC_INPATIENT_BEDS_COVID (no authored checks). CDC_INPATIENT_BEDS_COVID (authored checks in Draft status). Check Count 12 authored checks. 12 authored checks (copied). Status of Checks Active in source. Draft in destination. Benefit N/A Saves time, ensures consistency, and avoids manual recreation."},{"location":"datastore-checks/quality-check-migration/#visual-diagram","title":"Visual Diagram","text":"Flowchart <pre><code>graph TD\nA[Start] --&gt; B[CDC_INPATIENT_BEDS_ALL Source: 12 Authored Checks]\nB --&gt;|Migrate| C[CDC_INPATIENT_BEDS_COVID Destination: Draft Checks]\nC --&gt; D[End]</code></pre>"},{"location":"datastore-checks/quality-check-migration/#tips","title":"Tips","text":"<ul> <li> <p>Review before activation: Migrated checks are saved as Draft, so you can make adjustments before using them.</p> </li> <li> <p>Use tags for tracking: Assign a tag like Migrated_Aug2025 to easily find migrated checks later.</p> </li> <li> <p>Keep field names the same: The system only assigns fields when names match. If a field name differs during migration, it will not be mapped\u2014another reason checks start in Draft status and require user review.</p> </li> </ul>"},{"location":"datastore-checks/quality-check-template/","title":"Create a Quality Check template","text":"<p>Step 1: Locate the check (whether Active or Draft) that you want to convert into a template and click that check.</p> <p>For demonstration purposes, we have selected the \"Between\" check.</p> <p></p> <p>Step 2: A modal window will appear displaying the check details. Click the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window and select \"Template\" from the dropdown menu.</p> <p></p> <p>After clicking the \"Template\" button, the check will be saved and created as a template in the library, and a success message will appear on the screen. This allows you to reuse the template for future checks, streamlining the validation process.</p>"},{"location":"datastore-checks/restore-archived-checks/","title":"Restore Archived Checks","text":"<p>Step 1: Click Archived from the navigation bar in the Checks section to view all archived checks.</p> <p></p> <p>Step 2: Click the archived check that you want to restore.</p> <p></p> <p>Step 3: You will be directed to the check details page. Click the Settings icon located at the top-right corner of the interface and select \u201cEdit\u201d from the dropdown menu.</p> <p></p> <p>A modal window will appear with the check details.</p> <p></p> <p>Step 4: If you want to make any changes to the check, you can edit it. Otherwise, click the Restore button to restore it as an active check.</p> <p></p> <p>To restore the check as a draft, click the arrow icon next to the Restore button. A dropdown menu will appear\u2014select Restore as Draft from the options.</p> <p></p> <p>After clicking the Restore button, the check will be successfully restored as either an active or draft check, depending on your selection, and a success message will appear on the screen.</p>"},{"location":"deployments/databricks-deployment/","title":"Dataplane Deployment Guide for Databricks","text":"<p>This guide explains how to deploy the Qualytics Dataplane as a continuous Databricks job using a Qualytics-provided Databricks Asset Bundle.</p> <p>While the dataplane is typically deployed as a Kubernetes service using the Kubeflow Spark Operator, Databricks deployment provides:</p> <ul> <li> <p>Photon-accelerated compute</p> </li> <li> <p>Native Unity Catalog integration</p> </li> <li> <p>Higher performance compared to JDBC-based integrations</p> </li> </ul> <p>The public Helm chart for Kubernetes deployments is available at qualytics-self-hosted.</p> <p> </p> Deployment Architecture with Databricks"},{"location":"deployments/databricks-deployment/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>Access to a Databricks workspace with job creation permissions</li> <li>Databricks CLI installed</li> <li>RabbitMQ credentials provided by Qualytics</li> <li>Docker registry credentials provided by Qualytics</li> </ul>"},{"location":"deployments/databricks-deployment/#enable-rabbitmq-connectivity","title":"Enable RabbitMQ Connectivity","text":"<p>Qualytics will provide:</p> <ul> <li>RabbitMQ DNS endpoint</li> <li>RabbitMQ username</li> <li>RabbitMQ password</li> </ul> <p>These credentials are required for the dataplane to communicate with Qualytics services.</p>"},{"location":"deployments/databricks-deployment/#step-1-install-and-authenticate-databricks-cli","title":"Step 1: Install and Authenticate Databricks CLI","text":"<p>Install the Databricks CLI by following the official instructions:</p> <p>Install or update the Databricks CLI</p> <p>Authenticate using:</p> <pre><code>databricks auth\n</code></pre>"},{"location":"deployments/databricks-deployment/#step-2-create-a-secrets-scope","title":"Step 2: Create a Secrets Scope","text":"<p>Create a Databricks secrets scope to store sensitive credentials:</p> <pre><code>databricks secrets create-scope qualytics\n</code></pre>"},{"location":"deployments/databricks-deployment/#step-3-add-required-secrets","title":"Step 3: Add Required Secrets","text":"<p>Add the following secrets to the <code>qualytics</code> scope.</p>"},{"location":"deployments/databricks-deployment/#rabbitmq-password","title":"RabbitMQ Password","text":"<p><pre><code>databricks secrets put-secret qualytics rabbitmq-password\n</code></pre> When prompted, enter the RabbitMQ password: <code>[RABBIT_PASSWORD_TO_BE_PROVIDED]</code></p>"},{"location":"deployments/databricks-deployment/#docker-registry-username","title":"Docker Registry Username","text":"<pre><code>databricks secrets put-secret qualytics docker-username\n</code></pre>"},{"location":"deployments/databricks-deployment/#docker-registry-token","title":"Docker Registry Token","text":"<p><pre><code>databricks secrets put-secret qualytics docker-token\n</code></pre> When prompted, enter the Docker Hub token: <code>[DOCKER_TOKEN_TO_BE_PROVIDED]</code></p> <p>The CLI will prompt you to enter each value securely.</p>"},{"location":"deployments/databricks-deployment/#step-4-customize-the-databricks-asset-bundle","title":"Step 4: Customize the Databricks Asset Bundle","text":"<p>Create or update your Databricks Asset Bundle configuration file (for example, databricks.yml).</p> <p>Replace all values wrapped in angle brackets (&lt;&gt;) with values specific to your environment.</p> <pre><code>bundle:\n  name: qualytics-dataplane\n\ntargets:\n  prod:\n    mode: production\n    workspace:\n      host: &lt;workspace-url&gt;\n      root_path: /Users/${workspace.current_user.userName}/.bundle/${bundle.name}/${bundle.target}\n\n    variables:\n      rabbitmq_host: &lt;rabbit-mq-host&gt;\n      rabbitmq_user: user\n      secrets_scope: dev\n      max_workers: 12\n      node_type: r6gd.2xlarge\n      jname: zulu21-ca-arm64\n\nvariables:\n  # RabbitMQ Configuration\n  rabbitmq_host:\n    description: RabbitMQ host endpoint\n    default: rabbitmq.us-east-1.elb.amazonaws.com\n\n  rabbitmq_user:\n    description: RabbitMQ username\n    default: user\n\n  # Spark Configuration\n  spark_version:\n    description: Databricks Spark version\n    default: 17.3.x-scala2.13\n\n  node_type:\n    description: AWS EC2 node type for workers\n    default: r6gd.2xlarge\n\n  # Autoscaling Configuration\n  min_workers:\n    description: Minimum number of workers\n    default: 1\n\n  max_workers:\n    description: Maximum number of workers\n    default: 12\n\n  # Executor Configuration\n  num_cores_per_executor:\n    description: Number of cores per executor\n    default: 8\n\n  max_memory_per_executor:\n    description: Maximum memory per executor in MB\n    default: 50000\n\n  # Docker Configuration\n  docker_image:\n    description: Docker image for the cluster\n    default: qualyticsai/dataplane-databricks:latest\n\n  docker_username:\n    description: Docker registry username\n    default: qualyticsai\n\n  # Secrets Configuration\n  secrets_scope:\n    description: Databricks secret scope name\n    default: qualytics\n\n  # Java Configuration\n  jname:\n    description: Zulu JDK directory name (zulu21-ca-amd64 or zulu21-ca-arm64)\n    default: zulu21-ca-amd64\n\nresources:\n  jobs:\n    QualyticsDataplane:\n      name: QualyticsDataplane-${bundle.target}\n\n      continuous:\n        pause_status: PAUSED\n\n      max_concurrent_runs: 1\n\n      tasks:\n        - task_key: QualyticsDataplane\n          spark_jar_task:\n            jar_uri: \"\"\n            main_class_name: io.qualytics.dataplane.SparkMothership\n            run_as_repl: true\n\n          new_cluster:\n            spark_version: ${var.spark_version}\n\n            spark_conf:\n              spark.driver.extraJavaOptions: &gt;-\n                -Dconfig.resource=prod.conf\n                -Dlog4j2.configurationFile=file:/opt/qualytics/log4j2.properties\n                -Djava.library.path=/databricks/libs:/usr/local/lib:/opt/qualytics/libs\n                --add-opens=java.base/java.lang=ALL-UNNAMED\n                --add-opens=java.base/java.util=ALL-UNNAMED\n                --add-opens=java.base/java.lang.invoke=ALL-UNNAMED\n                --add-opens=java.base/java.nio=ALL-UNNAMED\n                --add-opens=java.base/sun.nio.ch=ALL-UNNAMED\n                --add-opens=java.management/sun.management=ALL-UNNAMED\n                --add-exports=java.management/sun.management=ALL-UNNAMED\n                -Djava.security.manager=allow\n\n              spark.executor.extraJavaOptions: &gt;-\n                -Dlog4j2.configurationFile=file:/opt/qualytics/log4j2.properties\n                -Djava.library.path=/databricks/libs:/usr/local/lib:/opt/qualytics/libs\n                --add-opens=java.base/java.lang=ALL-UNNAMED\n                --add-opens=java.base/java.util=ALL-UNNAMED\n                --add-opens=java.base/java.lang.invoke=ALL-UNNAMED\n                --add-opens=java.base/java.nio=ALL-UNNAMED\n                --add-opens=java.base/sun.nio.ch=ALL-UNNAMED\n                --add-opens=java.management/sun.management=ALL-UNNAMED\n                --add-exports=java.management/sun.management=ALL-UNNAMED\n                -Djava.security.manager=allow\n\n              # Disable Python/R to reduce attack surface\n              spark.databricks.r.command: /bin/false\n              spark.executorEnv.PYSPARK_PYTHON: /bin/false\n              spark.executorEnv.PYSPARK_DRIVER_PYTHON: /bin/false\n              spark.databricks.driverNfs.clusterWidePythonLibsEnabled: \"false\"\n              spark.databricks.driverNfs.enabled: \"false\"\n              spark.databricks.sql.externalUDF.env.enabled: \"false\"\n\n              # Product marker\n              spark.databricks.isv.product: qualytics\n              spark.log.structuredLogging.enabled: \"true\"\n\n              # Query plan debugging - increase field limit (default 25 truncates wide tables)\n              spark.sql.debug.maxToStringFields: \"1000\"\n\n            aws_attributes:\n              first_on_demand: 1\n              availability: SPOT_WITH_FALLBACK\n              zone_id: auto\n              spot_bid_price_percent: 100\n\n            node_type_id: ${var.node_type}\n\n            autoscale:\n              min_workers: ${var.min_workers}\n              max_workers: ${var.max_workers}\n\n            spark_env_vars:\n              MOTHERSHIP_NUM_CORES_PER_EXECUTOR: ${var.num_cores_per_executor}\n              MOTHERSHIP_MAX_MEMORY_PER_EXECUTOR: ${var.max_memory_per_executor}\n              MOTHERSHIP_MAX_EXECUTORS: ${var.max_workers}\n              MOTHERSHIP_RABBIT_HOST: ${var.rabbitmq_host}\n              MOTHERSHIP_LIBPOSTAL_DATA_PATH: /opt/libpostal\n              JNAME: ${var.jname}\n              MOTHERSHIP_RABBIT_USER: ${var.rabbitmq_user}\n              MOTHERSHIP_RABBIT_PASS: \"{{secrets/dev/rabbitmq-password}}\"\n              MOTHERSHIP_DEPLOYMENT_MODE: databricks\n\n            enable_elastic_disk: false\n\n            docker_image:\n              url: ${var.docker_image}\n              basic_auth:\n                username: \"{{secrets/dev/docker-username}}\"\n                password: \"{{secrets/dev/docker-token}}\"\n\n            data_security_mode: SINGLE_USER\n            runtime_engine: PHOTON\n            kind: CLASSIC_PREVIEW\n            is_single_node: false\n\n          libraries:\n            - jar: file:///opt/qualytics/qualytics-dataplane.jar\n\n      queue:\n        enabled: true\n</code></pre>"},{"location":"deployments/databricks-deployment/#step-5-deploy-the-dataplane-job","title":"Step 5: Deploy the Dataplane Job","text":"<p>Deploy the Databricks Asset Bundle to your workspace:</p> <p><pre><code>databricks bundle deploy --target prod\n</code></pre> This command creates the continuous Databricks job and associated cluster configuration.</p>"},{"location":"deployments/databricks-deployment/#step-6-start-the-dataplane-job","title":"Step 6: Start the Dataplane Job","text":"<p>Start the dataplane job using:</p> <p><pre><code>databricks bundle run --target prod QualyticsDataplane\n</code></pre> Once started, the job runs continuously and automatically reconnects on restarts.</p>"},{"location":"deployments/databricks-deployment/#configuration-notes","title":"Configuration Notes","text":"<ul> <li>RabbitMQ Connection: The dataplane connects to <code>rabbitmq.us-east-1.elb.amazonaws.com</code> with user <code>user</code></li> <li>Cluster Configuration: Uses <code>r6gd.2xlarge</code> instances with autoscaling from 1-12 workers</li> <li>Docker Image: Uses <code>qualyticsai/dataplane-databricks:latest</code> with provided authentication</li> </ul>"},{"location":"deployments/databricks-deployment/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues:</p> <ul> <li>Verify secrets are properly configured: <code>databricks secrets list-secrets qualytics</code></li> <li>Check job logs in the Databricks UI</li> <li>Ensure the JAR file is uploaded to the correct location</li> <li>Verify network connectivity to the RabbitMQ endpoint</li> </ul>"},{"location":"deployments/databricks-deployment/#support","title":"Support","text":"<p>For additional support or questions, please contact the Qualytics team.</p>"},{"location":"deployments/overview/","title":"Deployment Options","text":""},{"location":"deployments/overview/#introduction","title":"Introduction","text":"<p>This document serves as a primer for organizations looking to decide which deployment model of Qualytics is right for them. It provides an overview of the two primary deployment models and considerations when making this decision.</p>"},{"location":"deployments/overview/#overview","title":"Overview","text":"<p>The following two deployment models are supported for the Qualytics platform:</p> <ul> <li>Model 1: Platform as a Service Deployment: to a single-tenant virtual private cloud (VPC) provisioned by Qualytics on infrastructure that Qualytics manages</li> <li>Model 2: Customer-Managed Deployment: to a CNCF compliant Kubernetes control plane on Customer managed infrastructure (including on-premises options)</li> </ul>"},{"location":"deployments/overview/#across-both-models-the-following-is-true","title":"Across both models, the following is true:","text":"<ul> <li>Raw customer data is not stored at-rest but derivative data and select values may be held in the dedicated VPC</li> <li>READ access is required to connect a datastore to the Qualytics Platform</li> <li>WRITE access to Customer chosen datastore is required for enrichment data</li> </ul>"},{"location":"deployments/overview/#databricks-deployment","title":"Databricks Deployment","text":"<p>For organizations using Databricks as their data processing platform, Qualytics provides a specialized dataplane deployment option. This allows the Qualytics dataplane to run directly within your Databricks environment as a continuous job.</p> <p>For detailed instructions on setting up the Qualytics dataplane in Databricks, see our Databricks Deployment Guide.</p>"},{"location":"deployments/overview/#model-1-platform-as-a-service-paas-deployment","title":"Model 1: Platform as a Service (PaaS) Deployment","text":""},{"location":"deployments/overview/#overview_1","title":"Overview","text":"<p>In this model, the Qualytics platform is deployed to a single-tenant virtual private cloud provisioned by Qualytics and with the provider and in the region of Customer's choosing. This VPC is not shared (single-tenant) and contains a single Customer Qualytics deployment.</p> <p> </p> PaaS Deployment Architecture"},{"location":"deployments/overview/#supported-cloud-providers","title":"Supported Cloud Providers","text":"<p>Depending on Customer's cloud infrastructure, this option uses one of the following:</p> <ul> <li>EKS (Elastic Kubernetes Service)</li> <li>AKS (Azure Kubernetes Service)</li> <li>GKE (Google Kubernetes Engine)</li> <li>Oracle OKE (Oracle Container Engine for Kubernetes)</li> </ul>"},{"location":"deployments/overview/#network-requirements","title":"Network Requirements","text":"<p>This model requires that the provisioned VPC have the ability to access Customer's datastore(s). In the case of publicly routable datastores such as Snowflake or S3, no extra configuration is required. In the case of private datastore(s) with no public IP address or route, the hosted VPC will require private routing using: PrivateLink, Transit Gateway peering, point to point VPN, or similar support to enable network access to that private datastore.</p>"},{"location":"deployments/overview/#considerations","title":"Considerations","text":"<p>This is Qualytics' preferred model of deployment. In this model, Qualytics is fully responsible for the provisioning and operation of the Qualytics platform. Customer is only responsible for granting the Qualytics platform necessary access.</p>"},{"location":"deployments/overview/#model-2-customer-managed-deployment","title":"Model 2: Customer-Managed Deployment","text":""},{"location":"deployments/overview/#overview_2","title":"Overview","text":"<p>In this model, the Qualytics platform is deployed to a CNCF compliant Kubernetes control plane on Customer managed infrastructure, which can include on-premises deployments. This chart will deploy a single-tenant instance of the qualytics platform to a CNCF compliant kubernetes control plane.</p> <p> </p> Customer-Managed Deployment Architecture"},{"location":"deployments/overview/#system-requirements","title":"System Requirements","text":"<p>This option supports deployments to any Kubernetes control plane that meets the following system requirements:</p> <ul> <li>Any Kubernetes version that is officially supported for patches running any CNCF compliant control plane</li> <li>A minimum 16 cores and 80 gigabytes of memory available for workload allocation</li> <li>Assigned a Customer resolvable fully-qualified domain name for the https ingress to the Qualytics UI</li> <li>(optional) Grant Qualytics an admin-level ServiceAccount to the cluster for pushing automated upgrades</li> </ul>"},{"location":"deployments/overview/#network-requirements_1","title":"Network Requirements","text":"<p>This model requires that the Kubernetes nodes supporting Qualytics' analytics engine have the ability to access Customer's datastore(s). Because Customer hosts the Qualytics deployment, Customer is solely responsible for ensuring the necessary network configuration and support.</p>"},{"location":"deployments/overview/#considerations_1","title":"Considerations","text":"<p>This model supports organizations that due to regulatory or other restrictions cannot permit READ access to their datastore(s) from a third-party hosted product. This model requires Customer to manage and operate the appropriate infrastructure and ensure it is granted all necessary access to the targeted datastore(s).</p> <p>For deployments to supported commercial Kubernetes control planes (EKS, AKS, GKE, OKE) and at the Customer's discretion, Qualytics will provision the deployment and transfer ownership of the applicable infrastructure to the Customer. Otherwise, the Customer shall be responsible for both the provisioning of a cluster meeting the requisite system requirements and the deployment of the Qualytics platform via Qualytics provided Helm chart.</p> <p>Terraform Templates Available: To simplify infrastructure provisioning, we provide ready-to-use Terraform templates for AWS, GCP, and Azure. These templates create Kubernetes clusters with the correct node pools, labels, storage classes, and networking configuration required for Qualytics deployments.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/","title":"Filename Globbing","text":""},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#overview","title":"Overview","text":"<p>Our data quality product offers a sophisticated feature that facilitates the organization and categorization of files on a distributed filesystem. This feature, known as Multi-Token Filename Globbing, enables the system to recursively scan files and intelligently group them based on shared filename conventions. It achieves this through a combination of filename pattern analysis and globbing techniques.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#process","title":"Process","text":"<p>Delimiter Identification: The first step involves identifying a common delimiter in filenames, such as an underscore (_) or dash (-). This delimiter is used to split the filenames into tokens. Tokenization and Grouping: Once the filenames are tokenized, the system groups them based on shared tokens. This is achieved through a method called applyMultiTokenGlobbing. Glob Pattern Formation: The core of this feature lies in forming glob patterns that represent groups of files sharing a schema. These patterns are created using the tokens derived from the filenames.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#methodology","title":"Methodology","text":"<ul> <li>Initial Token Grouping: The method begins by grouping filenames based on each token. It considers the number of tokens and processes each token index separately.</li> <li> <p>Left or Right Side Grouping Decision: The system decides whether to group tokens starting from the left side or the right side of the filename, based on the distribution of tokens.</p> </li> <li> <p>Pattern Creation Logic:</p> </li> <li> <p>For filenames with a single token, the system avoids globbing and keeps the filenames as they are.</p> </li> <li>For multi-token filenames, the method constructs a container name (glob pattern) by iterating through each token.</li> <li> <p>At each token, the method decides whether to include the token as-is or replace it with a wildcard (*). This decision is based on several factors, such as:</p> <ul> <li>The uniqueness of the token in the context of other filenames.</li> <li>The nature of the token (e.g., all letters).</li> <li>The comparison of token counts in adjacent indexes.</li> </ul> </li> <li> <p>Special Cases Handling: The method includes logic to handle special cases, such as all-letter tokens, tokens at the beginning or end of a filename, and unique tokens.</p> </li> <li>Glob Pattern Optimization: Finally, the system optimizes the glob patterns, ensuring that each pattern uniquely represents a group of files with a shared schema. This is done by comparing new patterns with existing ones and updating them based on the latest file modifications.</li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#detailed-methodology-multi-token-filename-globbing","title":"Detailed Methodology: Multi-Token Filename Globbing","text":""},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#step-by-step-process","title":"Step-by-Step Process","text":""},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#delimiter-identification-and-tokenization","title":"Delimiter Identification and Tokenization","text":"<p>The system identifies a common delimiter in the filenames, typically an underscore (_) or dash (-), and splits the filenames into tokens.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#token-grouping-and-indexing","title":"Token Grouping and Indexing","text":"<ul> <li>Each token in a filename is indexed (0, 1, 2, ...).</li> <li>Filenames are grouped based on the value of tokens at each index.</li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#determining-grouping-strategy","title":"Determining Grouping Strategy","text":"<ul> <li>The system decides whether to group tokens from the left (start of filename) or right (end of filename) based on the distribution and variation of tokens at each index.</li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#pattern-creation-logic","title":"Pattern Creation Logic","text":"<ul> <li>Single-Token Filenames: No globbing is applied to filenames with only one token.</li> <li>Multi-Token Filenames: The method constructs glob patterns by analyzing each token. It considers factors like token uniqueness, commonality, and special cases like all-letter tokens.</li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#uniqueness-vs-commonality","title":"Uniqueness vs. Commonality:","text":"<ul> <li>Unique tokens (unique in their position across all filenames) are replaced with a wildcard \"*\".</li> <li>Common tokens across many files are kept as they are in the pattern.</li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#special-considerations-for-all-letter-tokens","title":"Special Considerations for All-Letter Tokens:","text":"<ul> <li>Tokens comprising entirely of letters are often grouped together, unless they are unique identifiers.</li> <li>Tokens at the start or end of a filename are treated with contextual logic, considering their potential roles (like identifiers or file types).</li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#adjacent-token-group-sizes","title":"Adjacent Token Group Sizes:","text":"<p>The method compares the group sizes of adjacent tokens to determine if a token leads to a tighter grouping, influencing whether it's kept as literal or replaced with a wildcard.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#constructing-container-names-glob-patterns","title":"Constructing Container Names (Glob Patterns)","text":"<ul> <li> <p>For each token index, the method constructs a container name, deciding whether to include the token as-is or replace it with \"*\".</p> </li> <li> <p>This decision is influenced by factors like the uniqueness of the token, the nature of the token (all letters or not), and the comparison of token counts in adjacent indexes.</p> </li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#optimization-and-finalization","title":"Optimization and Finalization","text":"<ul> <li>The system optimizes the glob patterns to ensure each pattern uniquely represents a group of files with a shared schema.</li> <li>It compares new patterns with existing ones and updates them based on the latest file modifications.</li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#example-scenarios","title":"Example Scenarios","text":"<ul> <li>Filename: <code>\"project_data_2023_v1.csv\"</code><ul> <li>Potential Pattern: <code>\"project_data_*_*.csv\"</code> (if \"2023\" and \"v1\" vary across files).</li> </ul> </li> <li>Filename: <code>\"user_123_profile_2023-06-01.json\"</code><ul> <li>Potential Pattern: <code>\"user_*_profile_*.json\"</code> (if \"123\" and dates vary, and \"user\" and \"profile\" are consistent).</li> </ul> </li> <li>Filename: <code>\"log2023-06_error.txt\"</code><ul> <li>Potential Pattern: <code>\"*_error.txt\"</code> (if dates vary but \"error\" is a constant token).</li> </ul> </li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#limitations","title":"Limitations","text":""},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#context","title":"Context","text":"<p>While the Multi-Token Filename Globbing feature is a powerful tool for organizing files in distributed filesystems, including object storage systems like AWS S3, Google Cloud Storage (GCS), and Azure Blob Storage, it's important to understand the limitations of using glob patterns with wildcards in these environments.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#wildcard-mechanics-in-directory-listings","title":"Wildcard Mechanics in Directory Listings","text":""},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#wildcard-character","title":"Wildcard Character (*):","text":"<p>In glob patterns, the asterisk (*) is used as a wildcard that matches any character, any number of times. This flexibility is powerful for grouping a wide range of file patterns but has limitations in precision.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#behavior-in-object-storage-systems","title":"Behavior in Object Storage Systems:","text":"<ul> <li>Systems like AWS S3, GCS, and Azure Blob interpret the wildcard in a glob pattern to match any sequence of characters in a filename.</li> <li>This means a pattern with a wildcard can encompass a broad range of filenames, potentially grouping files that were not intended to be grouped together.</li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#specific-limitation-example","title":"Specific Limitation Example","text":"<p>Consider the following scenario to illustrate this limitation:</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#intended-file-grouping-patterns","title":"Intended File Grouping Patterns:","text":"<ul> <li>Pattern A: <code>project_data_*.txt</code></li> <li>Pattern B: <code>project_data_*_*.txt</code></li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#example-filenames","title":"Example Filenames:","text":"<ul> <li><code>project_data_1234.txt</code></li> <li><code>project_data_1234_suffix.txt</code></li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#limitation-in-practice","title":"Limitation in Practice:","text":"<ul> <li> <p>In this case, Pattern A (<code>project_data_*.txt</code>) is intended to match files like project_data_1234.txt. However, due to the nature of the wildcard, this pattern will also inadvertently match <code>project_data_1234_suffix.txt</code>.</p> </li> <li> <p>The wildcard in Pattern A extends to any length of characters following project_data_, making it impossible to exclusively group files that strictly follow the <code>project_data_1234.txt</code> format without including those with additional suffixes like <code>project_data_1234_suffix.txt</code>.</p> </li> </ul>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#addressing-the-limitations","title":"Addressing the Limitations:","text":"<p>Understanding the inherent limitations of glob patterns, particularly when dealing with wildcards in object storage systems, is crucial for effective file management. </p> <p>When users encounter scenarios where filenames within a folder are incompatible due to these limitations, several practical options are available.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#ensure-appropriate-file-grouping","title":"Ensure appropriate file grouping:","text":""},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#separation-into-distinct-folders","title":"Separation into Distinct Folders:","text":"<p>One effective strategy is to organize files with conflicting name formats into separate folders. </p> <p>By doing so, the resultant glob patterns within each folder will be distinct and won\u2019t overlap, ensuring precise file grouping.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#leveraging-folder-globbing-feature","title":"Leveraging Folder-Globbing Feature:","text":"<p>For added flexibility, users can also utilize our folder-globbing feature. </p> <p>This feature simplifies the grouping process by aggregating all files in the same folder, regardless of their filename patterns. This approach is particularly useful in scenarios where filename-based grouping is less critical or when dealing with a wide variety of filename formats within the same directory.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#customized-filename-conventions","title":"Customized Filename Conventions:","text":"<p>Users are encouraged to adopt filename conventions that align better with the capabilities and limitations of glob patterns. By designing filenames with clear, distinct segments and predictable structures, users can more effectively leverage the globbing feature for accurate file categorization.</p>"},{"location":"dfs-globbing/how-dfs-filename-globbing-works/#conclusion","title":"Conclusion","text":"<p>The Multi-Token Filename Globbing feature stands out as a powerful and efficient tool for organizing and categorizing files within a distributed filesystem. </p> <p>By astutely analyzing filename patterns and forming optimized glob patterns, this feature significantly streamlines the process of managing files that share common schemas, thereby elevating the overall data quality and accessibility within the system.</p>"},{"location":"enrichment/api-payload-examples/","title":"API Payload Examples","text":""},{"location":"enrichment/api-payload-examples/#retrieving-enrichment-datastore-tables","title":"Retrieving Enrichment Datastore Tables","text":""},{"location":"enrichment/api-payload-examples/#endpoint-get","title":"Endpoint (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/listing</code> (get)</p> Example result response <pre><code>    [\n        {\n            \"name\":\"_datastore_prefix_scan_operations\",\n            \"label\":\"scan_operations\",\n            \"datastore\":{\n                \"id\":123,\n                \"name\":\"My Datastore\",\n                \"store_type\":\"jdbc\",\n                \"type\":\"postgresql\",\n                \"enrich_only\":false,\n                \"enrich_container_prefix\":\"_datastore_prefix\",\n                \"favorite\":false\n            }\n        },\n        {\n            \"name\":\"_datastore_prefix_source_records\",\n            \"label\":\"source_records\",\n            \"datastore\":{\n                \"id\":123,\n                \"name\":\"My Datastore\",\n                \"store_type\":\"jdbc\",\n                \"type\":\"postgresql\",\n                \"enrich_only\":false,\n                \"enrich_container_prefix\":\"_datastore_prefix\",\n                \"favorite\":false\n            }\n        },\n        {\n            \"name\":\"_datastore_prefix_failed_checks\",\n            \"label\":\"failed_checks\",\n            \"datastore\":{\n                \"id\":123,\n                \"name\":\"My Datastore\",\n                \"store_type\":\"jdbc\",\n                \"type\":\"postgresql\",\n                \"enrich_only\":false,\n                \"enrich_container_prefix\":\"_datastore_prefix\",\n                \"favorite\":false\n            }\n        },\n        {\n            \"name\": \"_datastore_prefix_remediation_container_id\",\n            \"label\": \"table_name\",\n            \"datastore\": {\n                \"id\": 123,\n                \"name\": \"My Datastore\",\n                \"store_type\": \"jdbc\",\n                \"type\": \"postgresql\",\n                \"enrich_only\": false,\n                \"enrich_container_prefix\": \"_datastore_prefix\",\n                \"favorite\": false\n            }\n        }\n    ]\n</code></pre>"},{"location":"enrichment/api-payload-examples/#retrieving-enrichment-datastore-source-records","title":"Retrieving Enrichment Datastore Source Records","text":""},{"location":"enrichment/api-payload-examples/#endpoint-get_1","title":"Endpoint (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?path={_source-record-table-prefix}</code> (get)</p>"},{"location":"enrichment/api-payload-examples/#endpoint-with-filters-get","title":"Endpoint With Filters (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?filter=anomaly_uuid='{uuid}'&amp;path={_source-record-table-prefix}</code> (get)</p> Example result response <pre><code>    {\n        \"source_records\": \"[{\\\"source_container\\\":\\\"table_name\\\",\\\"source_partition\\\":\\\"partition_name\\\",\\\"anomaly_uuid\\\":\\\"f11d4e7c-e757-4bf1-8cd6-d156d5bc4fa5\\\",\\\"context\\\":null,\\\"record\\\":\\\"{\\\\\\\"P_NAME\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"strategize intuitive systems\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"P_TYPE\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"Radiographer, therapeutic\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"P_RETAILPRICE\\\\\\\":\\\\\\\"-24.69\\\\\\\",\\\\\\\"LAST_MODIFIED_TIMESTAMP\\\\\\\":\\\\\\\"2023-09-29 11:17:19.048\\\\\\\",\\\\\\\"P_MFGR\\\\\\\":null,\\\\\\\"P_COMMENT\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"Other take so.\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"P_PARTKEY\\\\\\\":\\\\\\\"845004850\\\\\\\",\\\\\\\"P_SIZE\\\\\\\":\\\\\\\"4\\\\\\\",\\\\\\\"P_CONTAINER\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"MED BOX\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"P_BRAND\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"PLC\\\\\\\\\\\\\\\"\\\\\\\"}\\\"}]\"\n    }\n</code></pre>"},{"location":"enrichment/api-payload-examples/#retrieving-enrichment-datastore-remediation","title":"Retrieving Enrichment Datastore Remediation","text":""},{"location":"enrichment/api-payload-examples/#endpoint-get_2","title":"Endpoint (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?path={_remediation-table-prefix}</code> (get)</p>"},{"location":"enrichment/api-payload-examples/#endpoint-with-filters-get_1","title":"Endpoint With Filters (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?filter=anomaly_uuid='{uuid}'&amp;path={_remediation-table-prefix}</code> (get)</p> Example result response <pre><code>    {\n        \"source_records\": \"[{\\\"source_container\\\":\\\"table_name\\\",\\\"source_partition\\\":\\\"partition_name\\\",\\\"anomaly_uuid\\\":\\\"f11d4e7c-e757-4bf1-8cd6-d156d5bc4fa5\\\",\\\"context\\\":null,\\\"record\\\":\\\"{\\\\\\\"P_NAME\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"strategize intuitive systems\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"P_TYPE\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"Radiographer, therapeutic\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"P_RETAILPRICE\\\\\\\":\\\\\\\"-24.69\\\\\\\",\\\\\\\"LAST_MODIFIED_TIMESTAMP\\\\\\\":\\\\\\\"2023-09-29 11:17:19.048\\\\\\\",\\\\\\\"P_MFGR\\\\\\\":null,\\\\\\\"P_COMMENT\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"Other take so.\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"P_PARTKEY\\\\\\\":\\\\\\\"845004850\\\\\\\",\\\\\\\"P_SIZE\\\\\\\":\\\\\\\"4\\\\\\\",\\\\\\\"P_CONTAINER\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"MED BOX\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"P_BRAND\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"PLC\\\\\\\\\\\\\\\"\\\\\\\"}\\\"}]\"\n    }\n</code></pre>"},{"location":"enrichment/api-payload-examples/#retrieving-enrichment-datastore-failed-checks","title":"Retrieving Enrichment Datastore Failed Checks","text":""},{"location":"enrichment/api-payload-examples/#endpoint-get_3","title":"Endpoint (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?path={_failed-checks-table-prefix}</code> (get)</p>"},{"location":"enrichment/api-payload-examples/#endpoint-with-filters-get_2","title":"Endpoint With Filters (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?filter=anomaly_uuid='{uuid}'&amp;path={_failed-checks-table-prefix}</code> (get)</p> Example result response <pre><code>    {\n        \"source_records\": \"[{\\\"quality_check_id\\\":155481,\\\"anomaly_uuid\\\":\\\"1a937875-6bce-4bfe-8701-075ba66be364\\\",\\\"quality_check_message\\\":\\\"{\\\\\\\"SNPSHT_TIMESTAMP\\\\\\\":\\\\\\\"2023-09-03 10:26:15.0\\\\\\\"}\\\",\\\"suggested_remediation_field\\\":null,\\\"suggested_remediation_value\\\":null,\\\"suggested_remediation_score\\\":null,\\\"quality_check_rule_type\\\":\\\"greaterThanField\\\",\\\"quality_check_tags\\\":\\\"Time-Sensitive\\\",\\\"quality_check_parameters\\\":\\\"{\\\\\\\"field_name\\\\\\\":\\\\\\\"SNPSHT_DT\\\\\\\",\\\\\\\"inclusive\\\\\\\":false}\\\",\\\"quality_check_description\\\":\\\"Must have a value greater than the value of SNPSHT_DT\\\",\\\"operation_id\\\":28162,\\\"detected_time\\\":\\\"2024-03-29T15:08:07.585Z\\\",\\\"source_container\\\":\\\"ACTION_TEST_CLIENT_V3\\\",\\\"source_partition\\\":\\\"ACTION_TEST_CLIENT_V3\\\",\\\"source_datastore\\\":\\\"DB2 Dataset\\\"}]\"\n    }\n</code></pre>"},{"location":"enrichment/api-payload-examples/#retrieving-enrichment-datastore-scan-operations","title":"Retrieving Enrichment Datastore Scan Operations","text":""},{"location":"enrichment/api-payload-examples/#endpoint-get_4","title":"Endpoint (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?path={_scan-operations-table-prefix}</code> (get)</p>"},{"location":"enrichment/api-payload-examples/#endpoint-with-filters-get_3","title":"Endpoint With Filters (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?filter=operation_id='{operation-id}'&amp;path={_scan-operations-table-prefix}</code> (get)</p> Example result response <pre><code>    {\n        \"source_records\": \"[{\\\"operation_id\\\":22871,\\\"datastore_id\\\":850,\\\"container_id\\\":7239,\\\"partition_name\\\":\\\"ACTION_TEST_CLIENT_V3\\\",\\\"incremental\\\":true,\\\"records_processed\\\":0,\\\"enrichment_source_record_limit\\\":10,\\\"max_records_analyzed\\\":-1,\\\"anomaly_count\\\":0,\\\"start_time\\\":\\\"2023-12-04T20:35:54.194Z\\\",\\\"end_time\\\":\\\"2023-12-04T20:35:54.692Z\\\",\\\"result\\\":\\\"success\\\",\\\"message\\\":null}]\"\n    }\n</code></pre>"},{"location":"enrichment/api-payload-examples/#retrieving-enrichment-datastore-exported-metadata","title":"Retrieving Enrichment Datastore Exported Metadata","text":""},{"location":"enrichment/api-payload-examples/#endpoint-get_5","title":"Endpoint (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?path={_export-metadata-table-prefix}</code> (get)</p>"},{"location":"enrichment/api-payload-examples/#endpoint-with-filters-get_4","title":"Endpoint With Filters (Get)","text":"<p><code>/api/datastores/{enrichment-datastore-id}/source-records?filter=container_id='{container-id}'&amp;path={_export-metadata-table-prefix}</code> (get)</p> Example result of export anomalies responseExample result of export checks responseExample result of field profiles response <pre><code>    {\n        \"source_records\": \"[{\\\"container_id\\\":13511,\\\"created\\\":\\\"2024-06-10T17:07:20.751438Z\\\",\\\"datastore_id\\\":1198,\\\"generated_at\\\":\\\"2024-06-11 18:42:31+0000\\\",\\\"global_tags\\\":\\\"\\\",\\\"id\\\":224818,\\\"source_container\\\":\\\"PARTSUPP-FORMATTED.csv\\\",\\\"source_datastore\\\":\\\"TPCH GCS\\\",\\\"status\\\":\\\"Active\\\",\\\"type\\\":\\\"shape\\\",\\\"uuid\\\":\\\"f2d4fae3-982b-45a1-b289-5854b7af4b03\\\"}]\"\n    }\n</code></pre> <pre><code>    {\n        \"source_records\": \"[{\\\"additional_metadata\\\":null,\\\"container_id\\\":13515,\\\"coverage\\\":1.0,\\\"created\\\":\\\"2024-06-10T16:27:05.600041Z\\\",\\\"datastore_id\\\":1198,\\\"deleted_at\\\":null,\\\"description\\\":\\\"Must have a numeric value above &gt;= 0\\\",\\\"fields\\\":\\\"L_QUANTITY\\\",\\\"filter\\\":null,\\\"generated_at\\\":\\\"2024-06-11 18:42:38+0000\\\",\\\"global_tags\\\":\\\"\\\",\\\"has_passed\\\":false,\\\"id\\\":196810,\\\"inferred\\\":true,\\\"is_new\\\":false,\\\"is_template\\\":false,\\\"last_asserted\\\":\\\"2024-06-11T18:04:24.480899Z\\\",\\\"last_editor\\\":null,\\\"last_updated\\\":\\\"2024-06-10T17:07:43.248644Z\\\",\\\"num_container_scans\\\":4,\\\"properties\\\":null,\\\"rule_type\\\":\\\"notNegative\\\",\\\"source_container\\\":\\\"LINEITEM-FORMATTED.csv\\\",\\\"source_datastore\\\":\\\"TPCH GCS\\\",\\\"template_id\\\":null,\\\"weight\\\":7.0}]\"\n    }\n</code></pre> <pre><code>    {\n        \"source_records\": \"[{\\\"approximate_distinct_values\\\":106944.0,\\\"completeness\\\":0.7493389459,\\\"container_container_type\\\":\\\"file\\\",\\\"container_id\\\":13509,\\\"created\\\":\\\"2024-06-10T16:23:48.457907Z\\\",\\\"datastore_id\\\":1198,\\\"datastore_type\\\":\\\"gcs\\\",\\\"entropy\\\":null,\\\"field_global_tags\\\":\\\"\\\",\\\"field_id\\\":145476,\\\"field_name\\\":\\\"C_ACCTBAL\\\",\\\"field_profile_id\\\":882170,\\\"field_quality_score\\\":\\\"{\\\\\\\"total\\\\\\\": 81.70052209952111, \\\\\\\"completeness\\\\\\\": 74.93389459101233, \\\\\\\"coverage\\\\\\\": 66.66666666666666, \\\\\\\"conformity\\\\\\\": null, \\\\\\\"consistency\\\\\\\": 100.0, \\\\\\\"precision\\\\\\\": 100.0, \\\\\\\"timeliness\\\\\\\": null, \\\\\\\"volumetrics\\\\\\\": null, \\\\\\\"accuracy\\\\\\\": 100.0}\\\",\\\"field_type\\\":\\\"Fractional\\\",\\\"field_weight\\\":1,\\\"generated_at\\\":\\\"2024-06-11 18:42:32+0000\\\",\\\"histogram_buckets\\\":null,\\\"is_not_normal\\\":true,\\\"kll\\\":null,\\\"kurtosis\\\":-1.204241522,\\\"max\\\":9999.99,\\\"max_length\\\":null,\\\"mean\\\":4488.8079264033,\\\"median\\\":4468.34,\\\"min\\\":-999.99,\\\"min_length\\\":null,\\\"name\\\":\\\"C_ACCTBAL\\\",\\\"q1\\\":1738.87,\\\"q3\\\":7241.17,\\\"skewness\\\":0.0051837205,\\\"source_container\\\":\\\"CUSTOMER-FORMATTED.csv\\\",\\\"source_datastore\\\":\\\"TPCH GCS\\\",\\\"std_dev\\\":3177.3005493585,\\\"sum\\\":5.0501333575999904E8,\\\"type_declared\\\":false,\\\"unique_distinct_ratio\\\":null}]\"\n    }\n</code></pre>"},{"location":"enrichment/delete-enrichment/","title":"Delete Enrichment","text":"<p>Step 1: Click on the Delete icon.</p> <p></p> <p>A modal window Delete Enrichment Datastore will appear.</p> <p></p> <p>When deleting an enrichment datastore, the confirmation dialog displays the number of linked source datastores. </p> <p></p> <p>Step 2: Enter the name of the enrichment datastore in the given field (confirmation check) and then click on the I\u2019M SURE, DELETE THIS ENRICHMENT DATASTORE button to delete the enrichment datastore.</p> <p></p> <p>After clicking the I\u2019M SURE, DELETE THIS ENRICHMENT DATASTORE button, a success notification appears confirming the deletion.</p>"},{"location":"enrichment/edit-enrichment/","title":"Edit Enrichment","text":"<p>Step 1: Click on the Edit option.</p> <p></p> <p>Step 2: After selecting the Edit option, a modal window will appear, displaying the connection details. This window allows you to modify any specific connection details.</p> <p></p> <p>Step 3: After editing the connection details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided connection details are verified, a success message will be displayed indicating that the connection has been verified.</p> <p>Step 4: Click on the Save button.</p> <p></p> <p>After clicking the Save button, a success notification appears on the screen showing the action was completed successfully.</p>"},{"location":"enrichment/enrichment-actions/","title":"Enrichment Actions","text":"<p>Enrichment Actions in Qualytics help you manage enrichment datastores efficiently\u2014whether you're adding a new source, updating existing settings, or removing. These actions keep your enrichment workflows accurate, current, and easy to maintain.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"enrichment/enrichment-actions/#navigation","title":"Navigation","text":"<p>Log in to your Qualytics account and click the Enrichment Datastores button on the left side panel of the interface.</p> <p></p>"},{"location":"enrichment/enrichment-actions/#add-enrichment","title":"Add Enrichment","text":"<p>Use this action to create a new enrichment datastore by entering details such as name, connector type, and authentication credentials.</p> <p>For more information, refer to the Add Enrichment Documentation.</p>"},{"location":"enrichment/enrichment-actions/#edit-delete-enrichment","title":"Edit &amp; Delete Enrichment","text":"<p>Use Edit to update existing configuration details, and Delete to permanently remove a datastore and its linked components when no longer needed.</p> <p>Step 1: Select the specific enrichment datastore you want to edit or delete.</p> <p></p> <p>Step 2: Click the Settings icon located at the top right corner of the interface, then choose Edit or Delete depending on the action you want to perform.</p> <p></p>"},{"location":"enrichment/enrichment-actions/#edit","title":"Edit","text":"<p>Use this action to modify an existing enrichment datastore\u2014update its connection details or reconfigure any required fields.</p> <p>For more information, refer to the Edit Enrichment Documentation.</p>"},{"location":"enrichment/enrichment-actions/#delete-enrichment","title":"Delete Enrichment","text":"<p>Use this action to permanently delete an enrichment datastore that is no longer required.</p> <p>For more information, refer to the Delete Enrichment Documentation.</p>"},{"location":"enrichment/enrichment-datastore-creation/","title":"Add Enrichment Datastore","text":"<p>Step 1: Click on the Add Enrichment Datastore button located at the top-right corner of the interface.</p> <p></p> <p>Step 2: A modal window- Add Enrichment Datastore will appear, providing you with the options to add enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Name Specify the name of the enrichment datastore 2. Toggle Button Toggle ON to create a new enrichment datastore from scratch, or toggle OFF to reuse credentials from an existing connection 3. Connector Select connector from the dropdown list."},{"location":"enrichment/enrichment-datastore-creation/#option-i-add-enrichment-datastore-with-a-new-connection","title":"Option I: Add Enrichment Datastore with a new Connection","text":"<p>If the toggle for Add New connection is turned on, then this will prompt you to add and configure the enrichment datastore from scratch without using existing connection details.</p> <p>Step 1: Select the connector from the dropdown list and add connection details such as Secrets Management, temp dataset ID, service account key, project ID, and dataset ID.</p> <p>For demonstration purposes we have selected the Snowflake connector.</p> <p></p> <p>Secrets Management: This is an optional connection property that allows you to securely store and manage credentials by integrating with HashiCorp Vault and other secret management systems. Toggle it ON to enable Vault integration for managing secrets.</p> <p>Note</p> <p>After configuring HashiCorp Vault integration, you can use ${key} in any Connection property to reference a key from the configured Vault secret. Each time the Connection is initiated, the corresponding secret value will be retrieved dynamically.  </p> REF FIELDS ACTIONS 1. Login URL Enter the URL used to authenticate with HashiCorp Vault. 2. Credentials Payload Input a valid JSON containing credentials for Vault authentication. 3. Token JSONPath Specify the JSONPath to retrieve the client authentication token from the response (e.g., $.auth.client_token). 4. Secret URL Enter the URL where the secret is stored in Vault. 5. Token Header Name Set the header name used for the authentication token (e.g., X-Vault-Token). 6. Data JSONPath Specify the JSONPath to retrieve the secret data (e.g., $.data). <p></p> <p>Step 2: The configuration form, requesting credential details before add the enrichment datastore.</p> <p>Note</p> <p>Different connectors have different sets of fields and options appearing when selected. </p> <p></p> REF FIELDS ACTIONS 1. Account (Required) Define the account identifier to be used for accessing the Snowflake. 2. Role (Required) Specify the user role that grants appropriate access and permissions. 3. Warehouse (Required) Provide the warehouse name that will be used for computing resources. 4. Authentication (Required) You can choose between Basic authentication or Keypair authentication for validating and securing the connection to your Snowflake instance.  Basic Authentication: This method uses a username and password combination for authentication. It is a straightforward method where the user's credentials are directly used to access Snowflake. <ul><li>Type: Select the authentication type from the dropdown menu.</li><li>User: Enter the username that Qualytics will use to connect to Snowflake.</li><li>Password: Enter the password associated with the specified user account.</li></ul> Keypair Authentication: This method uses a combination of a private key and a corresponding public key for authentication. This is a more secure method compared to basic authentication, as it involves asymmetric cryptography <ul> <li>Type: Select \"Keypair\" from the dropdown menu.</li><li>User: Enter the username that Qualytics will use to connect to Snowflake.</li> <li>Private Key: Upload the private key file that will be used for authentication. This key is part of a public-private key pair used to securely authenticate the user.</li> <li>Private Key Password (Optional): Enter the password associated with the private key, if any </li> </ul> 5. Database Specify the database name to be accessed. 6. Schema Define the schema within the database that should be used. 7. Teams Select one or more teams from the dropdown to associate with this source datastore. <p>Step 3: After adding the details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p> <p>Step 4: Click on the Save button.</p> <p></p> <p>A modal window appears and shows a success message that the enrichment was updated successfully.</p> <p>Step 5: Close the success dialog. Here, you can view a list of all the enrichment datastores you have added to the system. For demonstration purposes, we have created an enrichment datastore named Snowflake_demo, which is visible in the list.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-creation/#option-ii-use-an-existing-enrichment-datastore","title":"Option II: Use an Existing Enrichment Datastore","text":"<p>If the toggle for Add New connection is turned off, then this will prompt you to add and configure the enrichment datastore using existing connection details.</p> <p>Step 1: Select a connection to reuse existing credentials.</p> <p>Note</p> <p>If you are using existing credentials, you can only edit the details such as Database, Schema, and Teams.  </p> <p></p> <p>Step 2: After adding the details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p> <p>Step 3:  Click on the Save button.</p> <p></p> <p>A modal window appears and shows a success message that the enrichment was updated successfully.</p> <p>Step 4: Close the success dialog. Here, you can view a list of all the enrichment datastores you have added to the system. For demonstration purposes, we have created an enrichment datastore named Snowflake_demo, which is visible in the list.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/","title":"Data Preview","text":"<p>Data Preview in Qualytics makes it simple to explore data tables and fields within a selected enrichment dataset. It supports filtering, field selection, and record downloads for deeper analysis, ensuring streamlined and efficient data management.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"enrichment/enrichment-datastore-preview/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the Enrichment Datastores button on the left side panel of the interface.</p> <p></p> <p>Step 2: You will see a list of available enrichment datastores. Click on the specific datastore you want to preview its details and data.</p> <p></p> <p>For Demonstration purposes, we have selected Netsuite Financials Enrich enrichment datastore.</p> <p>Step 3: After clicking on your selected enrichment datastore, you will be able to preview its enrichment, export, materialize, remediation, including all data tables, and unlinked data tables.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#data-preview-tab","title":"Data Preview Tab","text":"<p>Data Preview Tab provides a clear visualization of enriched dataset tables and fields like  <code>_FAILED_CHECKS</code>, <code>_SOURCE_RECORDS</code>, and <code>_SCAN_OPERATIONS</code>. Users can explore remediation data, export data, materialized datasets, and unlinked objects, refine data with filters, select specific fields, and download records for further analysis. This tab ensures efficient data review and management for enhanced insights.</p>"},{"location":"enrichment/enrichment-datastore-preview/#all","title":"All","text":"<p>By selecting All, users can access a comprehensive list of data tables associated with the selected enrichment datastore. This includes all relevant tables categorized under Enrichment, Remediation, Export, Materialize, and Unlinked sections, enabling users to efficiently explore and manage the data. Click on a specific table or dataset within the All section to access its detailed information.</p> <p></p> <p>After clicking on a specific table or dataset, a detailed view opens, displaying fields such as <code>_FAILED_CHECKS</code>, <code>_SOURCE_RECORDS</code>, <code>_SCAN_OPERATIONS</code>, remediation tables (e.g., <code>_ENRICHMENT_CONTAINER_PREFIX_REMEDIATION_CONTAINER_ID</code>), exported tables, materialized outputs, and unlinked objects (orphaned data) for review and action.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#enrichment","title":"Enrichment","text":"<p>By selecting Enrichment users can access a comprehensive view of the table or data associated with the selected enrichment datastore. Click on specific table or dataset within the Enrichment section to access its detailed information.</p> <p></p> <p>After clicking on a specific table or dataset, a detailed view opens, displaying fields of the selected table or dataset.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#remediation","title":"Remediation","text":"<p>By selecting Remediation users can access a comprehensive view of the table or data associated with the selected enrichment datastore. Click on specific table or dataset within the Remediation section to access its detailed information.</p> <p></p> <p>After clicking on a table or dataset, a detailed view opens, displaying all the fields and data associated with the selected table or dataset.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#export","title":"Export","text":"<p>By selecting Export, users can access a comprehensive view of the exported tables or data associated with the selected enrichment datastore. Click on a specific table or dataset within the Export section to access its detailed information.</p> <p></p> <p>After clicking on a table or dataset, a detailed view opens, displaying all the fields and data associated with the selected table or dataset.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#materialize","title":"Materialize","text":"<p>By selecting Materialize, users can access a comprehensive view of the materialized tables or data associated with the selected enrichment datastore. Click on a specific table or dataset within the Materialize section to access its detailed information.</p> <p></p> <p>After clicking on a table or dataset, a detailed view opens, displaying all the fields and data associated with the selected table or dataset.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#unlinked","title":"Unlinked","text":"<p>By selecting Unlinked users can access a comprehensive view of the table or data associated with the selected enrichment datastore. Click on specific table or dataset within the Unlinked section to access its detailed information.</p> <p></p> <p>After clicking on a table or dataset, a detailed view opens, displaying all the fields and data associated with the selected table or dataset.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#filter-clause-and-refresh","title":"Filter Clause and Refresh","text":"<p>Data Preview tab includes a filter functionality that enables users to focus on specific fields by applying filter clauses. This refines the displayed rows based on specific criteria, enhancing data analysis and providing more targeted insights and a Refresh button to update the data view with the latest data.</p>"},{"location":"enrichment/enrichment-datastore-preview/#filter-clause","title":"Filter Clause","text":"<p>Use the Filter Clause to narrow down the displayed rows by applying specific filter clauses, allowing for focused and precise data analysis.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#refresh","title":"Refresh","text":"<p>Click Refresh button to update the data view with the latest information, ensuring accuracy and relevance.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#select-specific-fields","title":"Select Specific Fields","text":"<p>Select specific fields to display, allowing you to focus on the most relevant data for analysis.To focus on relevant data for analysis, click on the Select Fields to Show dropdown. Choose specific fields you want to review by checking or unchecking options.</p> <p></p>"},{"location":"enrichment/enrichment-datastore-preview/#download-records","title":"Download Records","text":"<p>Download Records feature in Qualytics allows users to easily export all source records from the selected enrichment dataset. This functionality is essential for performing deeper analysis outside the platform or for sharing data with external tools and teams.</p> <p></p>"},{"location":"enrichment/enrichment-tables/","title":"Enrichment Tables","text":"<p>When anomalies are detected, the platform writes metadata into four primary enrichment tables:</p> <ul> <li>&lt;enrichment_prefix&gt;_check_metrics</li> <li>&lt;enrichment_prefix&gt;_failed_checks</li> <li>&lt;enrichment_prefix&gt;_source_records</li> <li>&lt;enrichment_prefix&gt;_scan_operations</li> </ul>"},{"location":"enrichment/enrichment-tables/#_check_metrics_table","title":"_CHECK_METRICS_Table","text":"<p>Captures and logs detailed metrics for every data quality check performed within the Qualytics Platform, providing insights into asserted and anomalous records across datasets.</p> <p>Columns</p> Name Data Type Description OPERATION_ID NUMBER Unique Identifier for the check metric. CONTAINER_ID NUMBER Identifier for the container associated with the check metric. SOURCE_DATASTORE STRING Datastore where the source data resides. SOURCE_CONTAINER STRING Name of the source data container. SOURCE_PARTITION STRING Partition of the source data. ASSERTION_RESULT STRING Result of the check assertion: one of <code>passed</code>, <code>failed</code>, or <code>unasserted</code>. ASSERTION_DETAILS STRING Text description explaining any warnings, errors, or notes from the check. QUALITY_CHECK_ID NUMBER Unique identifier for the quality check performed. ASSERTED_RECORDS_COUNT NUMBER Count of records expected or asserted in the source. ANOMALOUS_RECORDS_COUNT NUMBER Count of records identified as anomalous. _QUALYTICS_SOURCE_PARTITION STRING Partition information specific to Qualytics metrics."},{"location":"enrichment/enrichment-tables/#_failed_checks-table","title":"_FAILED_CHECKS Table","text":"<p>Acts as an associative entity that consolidates information on failed checks, associating anomalies with their respective quality checks.</p> <p>Columns</p> Name Data Type Description QUALITY_CHECK_ID NUMBER Unique identifier for the quality check. ANOMALY_UUID STRING UUID for the anomaly detected. QUALITY_CHECK_MESSAGE STRING Message describing the quality check outcome. SUGGESTED_REMEDIATION_FIELD STRING Field suggesting remediation. SUGGESTED_REMEDIATION_VALUE STRING Suggested value for remediation. SUGGESTED_REMEDIATION_SCORE FLOAT Score indicating confidence in remediation. QUALITY_CHECK_RULE_TYPE STRING Type of rule applied for quality check. QUALITY_CHECK_TAGS STRING Tags associated with the quality check. QUALITY_CHECK_PARAMETERS STRING Parameters used for the quality check. QUALITY_CHECK_DESCRIPTION STRING Description of the quality check. QUALITY_CHECK_FIELDS STRING Names of the fields targeted by the quality check. QUALITY_CHECK_METADATA STRING Optional JSON string containing additional check metadata. OPERATION_ID NUMBER Identifier for the operation detecting anomaly. DETECTED_TIME TIMESTAMP Timestamp when the anomaly was detected. SOURCE_CONTAINER STRING Name of the source data container. SOURCE_PARTITION STRING Partition of the source data. SOURCE_DATASTORE STRING Datastore where the source data resides. FINGERPRINT INTEGER Unique identifier created when Reactivate Recurring Anomalies is enabled. ANOMALOUS_RECORDS_COUNT NUMBER The total number of records that triggered this anomaly. For row-level checks, this is 1 per anomaly row. For shape-level checks, this is the total count of anomalous records, independent of any source record sampling limit. <p>Info</p> <p>This table is not characterized by unique <code>ANOMALY_UUID</code> or <code>QUALITY_CHECK_ID</code> values alone. Instead, the combination of <code>ANOMALY_UUID</code> and <code>QUALITY_CHECK_ID</code> serves as a composite key, uniquely identifying each record in the table.</p>"},{"location":"enrichment/enrichment-tables/#_source_records-table","title":"_SOURCE_RECORDS Table","text":"<p>Stores source records in JSON format, primarily to enable the preview source record feature in the Qualytics App.</p> <p>Columns</p> Name Data Type Description SOURCE_CONTAINER STRING Name of the source data container. SOURCE_PARTITION STRING Partition of the source data. ANOMALY_UUID STRING UUID for the associated anomaly. CONTEXT STRING Contextual information for the anomaly. RECORD STRING JSON representation of the source record."},{"location":"enrichment/enrichment-tables/#_scan_operations-table","title":"_SCAN_OPERATIONS Table","text":"<p>Captures and stores the results of every scan operation conducted on the Qualytics Platform.</p> <p>Schema Change (December 2024)</p> <p>The <code>CONTAINER_SCAN_ID</code> column was removed from this table. Enrichment tables created after this change will not contain this column. The container and datastore can be identified using the <code>CONTAINER_ID</code> and <code>DATASTORE_ID</code> columns respectively.</p> <p>Columns</p> Name Data Type Description OPERATION_ID NUMBER Unique identifier for the scan operation. DATASTORE_ID NUMBER Identifier for the source datastore associated with the operation. CONTAINER_ID NUMBER Identifier for the container associated with the operation. PARTITION_NAME STRING Name of the source partition on which the scan operation is performed. INCREMENTAL BOOLEAN Boolean flag indicating whether the scan operation is incremental. RECORDS_PROCESSED NUMBER Total number of records processed during the scan operation. ENRICHMENT_SOURCE_RECORD_LIMIT NUMBER Maximum number of records written to the enrichment for each anomaly detected. MAX_RECORDS_ANALYZED NUMBER Maximum number of records analyzed in the scan operation. ANOMALY_COUNT NUMBER Total number of anomalies identified in the scan operation. START_TIME TIMESTAMP Timestamp marking the start of the scan operation. END_TIME TIMESTAMP Timestamp marking the end of the scan operation. RESULT STRING Textual representation of the scan operation's status. MESSAGE STRING Detailed message regarding the process of the scan operation."},{"location":"enrichment/metadata-tables/","title":"Metadata Tables","text":"<p>The Qualytics platform enables users to manually export metadata into the enrichment datastore, providing a structured approach to data analysis and management. These metadata tables are structured to reflect the evolving characteristics of data entities, primarily focusing on aspects that are subject to changes.</p> <p>Currently, the following assets are available for exporting:</p> <ul> <li>_&lt;enrichment_prefix&gt;_export_anomalies</li> <li>_&lt;enrichment_prefix&gt;_export_checks</li> <li>_&lt;enrichment_prefix&gt;_export_field_profiles</li> </ul> <p>Note</p> <p>The strategy used for managing these metadata tables employs a <code>create or replace</code> approach, meaning that the export process will create a new table if one does not exist, or replace it entirely if it does. This means that any previous data will be overwritten.</p> <p>For more detailed information on exporting metadata, please refer to the export operation documentation</p>"},{"location":"enrichment/metadata-tables/#_export_anomalies-table","title":"_EXPORT_ANOMALIES Table","text":"<p>Contains metadata from anomalies in a distinct normalized format. This table is specifically designed to capture the mutable states of anomalies, emphasizing their status changes.</p> <p>Columns</p> Name Data Type Description ID NUMBER Unique identifier for the anomaly. CREATED TIMESTAMP Timestamp of anomaly creation. UUID UUID Universal Unique Identifier of the anomaly. TYPE STRING Type of the anomaly (e.g., 'shape'). STATUS STRING Current status of the anomaly (e.g., 'Active'). GLOBAL_TAGS STRING Tags associated globally with the anomaly. CONTAINER_ID NUMBER Identifier for the associated container. SOURCE_CONTAINER STRING Name of the source container. DATASTORE_ID NUMBER Identifier for the associated datastore. SOURCE_DATASTORE STRING Name of the source datastore. GENERATED_AT TIMESTAMP Timestamp when the export was generated."},{"location":"enrichment/metadata-tables/#_export_checks-table","title":"_EXPORT_CHECKS Table","text":"<p>Contains metadata from quality checks.</p> <p>Columns</p> Name Data Type Description ADDITIONAL_METADATA STRING JSON-formatted string containing additional metadata for the check. COVERAGE FLOAT Represents the expected tolerance of the rule. CREATED STRING Created timestamp of the check. DELETED_AT STRING Deleted timestamp of the check. DESCRIPTION STRING Description of the check. FIELDS STRING Fields involved in the check separated by comma. FILTER STRING Criteria used to filter data when asserting the check. GENERATED_AT STRING Indicates when the export was generated. GLOBAL_TAGS STRING Represents the global tags of the check separated by comma. HAS_PASSED BOOLEAN Boolean indicator of whether the check has passed its last assertion . ID NUMBER Unique identifier for the check. INFERRED BOOLEAN Indicates whether the check was inferred by the platform. IS_NEW BOOLEAN Flags if the check is new. LAST_ASSERTED STRING Timestamp of the last assertion performed on the check. LAST_EDITOR STRING Represents the last editor of the check. LAST_UPDATED STRING Represents the last updated timestamp of the check. NUM_CONTAINER_SCANS NUMBER Number of containers scanned. PROPERTIES STRING Specific properties for the check in a JSON format. RULE_TYPE STRING Type of rule applied in the check. WEIGHT FLOAT Represents the weight of the check. DATASTORE_ID NUMBER Identifier of the datastore used in the check. CONTAINER_ID NUMBER Identifier of the container used in the check. TEMPLATE_ID NUMBER Identifier of the template id associated to the check. IS_TEMPLATE BOOLEAN Indicates whether the check is a template or not. SOURCE_CONTAINER STRING Name of the container used in the check. SOURCE_DATASTORE STRING Name of the datastore used in the check."},{"location":"enrichment/metadata-tables/#_export_check_templates-table","title":"_EXPORT_CHECK_TEMPLATES Table","text":"<p>Contains metadata from check templates.</p> <p>Columns</p> Name Data Type Description ADDITIONAL_METADATA STRING JSON-formatted string containing additional metadata for the check. COVERAGE FLOAT Represents the expected tolerance of the rule. CREATED STRING Created timestamp of the check. DELETED_AT STRING Deleted timestamp of the check. DESCRIPTION STRING Description of the check. FIELDS STRING Fields involved in the check separated by comma. FILTER STRING Criteria used to filter data when asserting the check. GENERATED_AT STRING Indicates when the export was generated. GLOBAL_TAGS STRING Represents the global tags of the check separated by comma. ID NUMBER Unique identifier for the check. IS_NEW BOOLEAN Flags if the check is new. IS_TEMPLATE BOOLEAN Indicates whether the check is a template or not. LAST_EDITOR STRING Represents the last editor of the check. LAST_UPDATED STRING Represents the last updated timestamp of the check. PROPERTIES STRING Specific properties for the check in a JSON format. RULE_TYPE STRING Type of rule applied in the check. TEMPLATE_CHECKS_COUNT NUMBER The count of associated checks to the template. TEMPLATE_LOCKED BOOLEAN Indicates whether the check template is locked or not. WEIGHT FLOAT Represents the weight of the check."},{"location":"enrichment/metadata-tables/#_export_field_profiles-table","title":"_EXPORT_FIELD_PROFILES Table","text":"<p>Contains metadata from field profiles.</p> <p>Columns</p> Name Data Type Description APPROXIMATE_DISTINCT_VALUES FLOAT Estimated number of distinct values in the field. COMPLETENESS FLOAT Ratio of non-null entries to total entries in the field. CONTAINER_ID NUMBER Identifier for the container holding the field. SOURCE_CONTAINER STRING Name of the container holding the field. CONTAINER_STORE_TYPE STRING Storage type of the container. CREATED STRING Date when the field profile was created. DATASTORE_ID NUMBER Identifier for the datastore containing the field. SOURCE_DATASTORE STRING Name of the datastore containing the field. DATASTORE_TYPE STRING Type of datastore. ENTROPY FLOAT Measure of randomness in the information being processed. FIELD_GLOBAL_TAGS STRING Global tags associated with the field. FIELD_ID NUMBER Unique identifier for the field. FIELD_NAME STRING Name of the field being profiled. FIELD_PROFILE_ID NUMBER Identifier for the field profile record. FIELD_QUALITY_SCORE FLOAT Score representing the quality of the field. FIELD_TYPE STRING Data type of the field. FIELD_WEIGHT NUMBER Weight assigned to the field for quality scoring. GENERATED_AT STRING Date when the field profile was generated. HISTOGRAM_BUCKETS STRING Distribution of data within the field represented as buckets. IS_NOT_NORMAL BOOLEAN Indicator of whether the field data distribution is not normal. KLL STRING Sketch summary of the field data distribution. KURTOSIS FLOAT Measure of the tailedness of the probability distribution. MAX FLOAT Maximum value found in the field. MAX_LENGTH FLOAT Maximum length of string entries in the field. MEAN FLOAT Average value of the field's data. MEDIAN FLOAT Middle value in the field's data distribution. MIN FLOAT Minimum value found in the field. MIN_LENGTH FLOAT Minimum length of string entries in the field. NAME STRING Descriptive name of the field. Q1 FLOAT First quartile in the field's data distribution. Q3 FLOAT Third quartile in the field's data distribution. SKEWNESS FLOAT Measure of the asymmetry of the probability distribution. STD_DEV FLOAT Standard deviation of the field's data. SUM FLOAT Sum of all numerical values in the field. TYPE_DECLARED BOOLEAN Indicator of whether the field type is explicitly declared. UNIQUE_DISTINCT_RATIO FLOAT Ratio of unique distinct values to the total distinct values."},{"location":"enrichment/overview-of-an-enrichment-datastore/","title":"Enrichment Datastore Overview","text":"<p>An Enrichment Datastore is a user-managed storage location where the Qualytics platform records and accesses metadata through a set of system-defined tables. It is purpose-built to capture metadata generated by the platform's profiling and scanning operations.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"enrichment/overview-of-an-enrichment-datastore/#key-points","title":"Key Points","text":"<ul> <li> <p>Metadata Storage: The Enrichment Datastore acts as a dedicated mechanism for writing and retaining metadata that the platform generates. This includes information about anomalies, quality checks, field profiling, and additional details that enrich the source data.</p> </li> <li> <p>Feature Enablement: By using the Enrichment Datastore, the platform unlocks certain features such as the previewing of source records. For instance, when an anomaly is detected, the platform typically previews a limited set of affected records. For a comprehensive view and persistent access, the Enrichment Datastore captures and maintains a complete snapshot of the source records associated with the anomalies.</p> </li> <li> <p>User-Managed Location: While the Qualytics platform handles the generation and processing of metadata, the actual storage is user-managed. This means that the user maintains control over the Enrichment Datastore, deciding where and how this data is stored, adhering to their governance and compliance requirements.</p> </li> <li> <p>Insight and Reporting: Beyond storing metadata, the Enrichment Datastore allows users to derive actionable insights and develop custom reports for a variety of use cases, from compliance tracking to data quality improvement initiatives.</p> </li> </ul>"},{"location":"enrichment/overview-of-an-enrichment-datastore/#navigation","title":"Navigation","text":"<p>Log in to your Qualytics account and click the Enrichment Datastores button on the left side panel of the interface.</p> <p></p>"},{"location":"enrichment/overview-of-an-enrichment-datastore/#table-types","title":"Table Types","text":"<p>The Enrichment Datastore contains several types of tables, each serving a specific purpose in the data enrichment and remediation process. </p> <p>Note</p> <p>For more information, please refer to the table types documentation.</p>"},{"location":"enrichment/overview-of-an-enrichment-datastore/#diagram","title":"Diagram","text":"<p>The diagram below provides a visual representation of the associations between various tables in the Enrichment Datastore. It illustrates how tables can be joined to track and analyze data across different processes.</p> <p> </p>"},{"location":"enrichment/overview-of-an-enrichment-datastore/#handling-json-and-string-splitting","title":"Handling JSON and string splitting","text":"SnowflakePostgreSQLMySQL <pre><code>SELECT\n    PARSE_JSON(ADDITIONAL_METADATA):metadata_1::string AS Metadata1_Key1,\n    PARSE_JSON(ADDITIONAL_METADATA):metadata_2::string AS Metadata2_Key1,\n    PARSE_JSON(ADDITIONAL_METADATA):metadata_3::string AS Metadata3_Key1,\n    -- Add more lines as needed up to MetadataN\n    CONTAINER_ID,\n    COVERAGE,\n    CREATED,\n    DATASTORE_ID,\n    DELETED_AT,\n    DESCRIPTION,\n    SPLIT_PART(FIELDS, ',', 1) AS Field1,\n    SPLIT_PART(FIELDS, ',', 2) AS Field2,\n    -- Add more lines as needed up to FieldN\n    FILTER,\n    GENERATED_AT,\n    SPLIT_PART(GLOBAL_TAGS, ',', 1) AS Tag1,\n    SPLIT_PART(GLOBAL_TAGS, ',', 2) AS Tag2,\n    -- Add more lines as needed up to TagN\n    HAS_PASSED,\n    ID,\n    INFERRED,\n    IS_NEW,\n    IS_TEMPLATE,\n    LAST_ASSERTED,\n    LAST_EDITOR,\n    LAST_UPDATED,\n    NUM_CONTAINER_SCANS,\n    PARSE_JSON(PROPERTIES):allow_other_fields::string AS Property_AllowOtherFields,\n    PARSE_JSON(PROPERTIES):assertion::string AS Property_Assertion,\n    PARSE_JSON(PROPERTIES):comparison::string AS Property_Comparison,\n    PARSE_JSON(PROPERTIES):datetime_::string AS Property_Datetime,\n    -- Add more lines as needed up to Property\n    RULE_TYPE,\n    SOURCE_CONTAINER,\n    SOURCE_DATASTORE,\n    TEMPLATE_ID,\n    WEIGHT\nFROM \"_EXPORT_CHECKS\";\n</code></pre> <pre><code>SELECT\n    (ADDITIONAL_METADATA::json -&gt;&gt; 'metadata_1') AS Metadata1_Key1,\n    (ADDITIONAL_METADATA::json -&gt;&gt; 'metadata_2') AS Metadata2_Key1,\n    (ADDITIONAL_METADATA::json -&gt;&gt; 'metadata_3') AS Metadata3_Key1,\n    -- Add more lines as needed up to MetadataN\n    CONTAINER_ID,\n    COVERAGE,\n    CREATED,\n    DATASTORE_ID,\n    DELETED_AT,\n    DESCRIPTION,\n    (string_to_array(FIELDS, ','))[1] AS Field1,\n    (string_to_array(FIELDS, ','))[2] AS Field2,\n    -- Add more lines as needed up to FieldN\n    FILTER,\n    GENERATED_AT,\n    (string_to_array(GLOBAL_TAGS, ','))[1] AS Tag1,\n    (string_to_array(GLOBAL_TAGS, ','))[2] AS Tag2,\n    -- Add more lines as needed up to TagN\n    HAS_PASSED,\n    ID,\n    INFERRED,\n    IS_NEW,\n    IS_TEMPLATE,\n    LAST_ASSERTED,\n    LAST_EDITOR,\n    LAST_UPDATED,\n    NUM_CONTAINER_SCANS,\n    (PROPERTIES::json -&gt;&gt; 'allow_other_fields') AS Property_AllowOtherFields,\n    (PROPERTIES::json -&gt;&gt; 'assertion') AS Property_Assertion,\n    (PROPERTIES::json -&gt;&gt; 'comparison') AS Property_Comparison,\n    (PROPERTIES::json -&gt;&gt; 'datetime_') AS Property_Datetime,\n    -- Add more lines as needed up to PropertyN\n    RULE_TYPE,\n    SOURCE_CONTAINER,\n    SOURCE_DATASTORE,\n    TEMPLATE_ID,\n    WEIGHT\nFROM \"_EXPORT_CHECKS\";\n</code></pre> <pre><code>SELECT\n    (ADDITIONAL_METADATA-&gt;&gt;'$.metadata_1') AS Metadata1_Key1,\n    (ADDITIONAL_METADATA-&gt;&gt;'$.metadata_2') AS Metadata2_Key1,\n    (ADDITIONAL_METADATA-&gt;&gt;'$.metadata_3') AS Metadata3_Key1,\n    -- Add more lines as needed up to MetadataN\n    CONTAINER_ID,\n    COVERAGE,\n    CREATED,\n    DATASTORE_ID,\n    DELETED_AT,\n    DESCRIPTION,\n    SUBSTRING_INDEX(FIELDS, ',', 1) AS Field1,\n    -- Add more lines as needed up to FieldN\n    SUBSTRING_INDEX(GLOBAL_TAGS, ',', 1) AS Tag1,\n    -- Add more lines as needed up to TagN\n    HAS_PASSED,\n    ID,\n    INFERRED,\n    IS_NEW,\n    IS_TEMPLATE,\n    LAST_ASSERTED,\n    LAST_EDITOR,\n    LAST_UPDATED,\n    NUM_CONTAINER_SCANS,\n    (PROPERTIES-&gt;&gt;'$.allow_other_fields') AS Property_AllowOtherFields,\n    (PROPERTIES-&gt;&gt;'$.assertion') AS Property_Assertion,\n    (PROPERTIES-&gt;&gt;'$.comparison') AS Property_Comparison,\n    (PROPERTIES-&gt;&gt;'$.datetime_') AS Property_Datetime,\n    -- Add more lines as needed up to PropertyN\n    RULE_TYPE,\n    SOURCE_CONTAINER,\n    SOURCE_DATASTORE,\n    TEMPLATE_ID,\n    WEIGHT\nFROM \"_EXPORT_CHECKS\";\n</code></pre>"},{"location":"enrichment/overview-of-an-enrichment-datastore/#usage-notes","title":"Usage Notes","text":"<ul> <li>Both metadata tables and remediation tables, are designed to be ephemeral and thus are recommended to be used as temporary datasets. Users are advised to move this data to a more permanent dataset for long-term storage and reporting.</li> <li>The anomaly UUID in the remediation tables acts as a link to the detailed data in the _anomaly enrichment table. This connection not only shows the number of failed checks but also provides insight into each one, such as the nature of the issue, the type of rule violated, and associated check tags. Additionally, when available, suggested remediation actions, including suggested field modifications and values, are presented alongside a score indicating the suggested action's potential effectiveness. This information helps users to better understand the specifics of each anomaly related to the remediation tables.</li> <li>The Qualytics platform is configured to capture and write a maximum of 10 rows of data per anomaly by default for both the _source_records enrichment table and the remediation tables. To adjust this limit, users can utilize the <code>enrichment_source_record_limit</code> parameter within the Scan Operation settings. This parameter accepts a minimum value of 10 but allows the specification of a higher limit, up to an unrestricted number of rows per anomaly. It is important to note that if an anomaly is associated with fewer than 10 records, the platform will only write the actual number of records where the anomaly was detected.</li> </ul>"},{"location":"enrichment/overview-of-an-enrichment-datastore/#api-payload-examples","title":"API Payload Examples","text":"<p>Note</p> <p>For more information, please refer to the API Payload Example documentation.</p>"},{"location":"enrichment/remediation-tables/","title":"Remediation Tables","text":"<p>When anomalies are detected in a container, the platform has the capability to create remediation tables in the Enrichment Datastore. These tables are detailed snapshots of the affected container, capturing the state of the data at the time of anomaly detection. They also include additional columns for metadata and remediation purposes. However, the creation of these tables depends upon the chosen remediation strategy during the scan operation.</p> <p>Currently, there are three types of remediation strategies:</p> <ul> <li>None: No remediation tables will be created, regardless of anomaly detection.</li> <li>Append: Replicate source containers using an append-first strategy.</li> <li>Overwrite: Replicate source containers using an overwrite strategy.</li> </ul> <p>Note</p> <p>The naming convention for the remediation tables follows the pattern of <code>&lt;enrichment_prefix&gt;_remediation_&lt;container_id&gt;</code>, where <code>&lt;enrichment_prefix&gt;</code> is user-defined during the Enrichment Datastore configuration and <code>&lt;container_name&gt;</code> corresponds to the original source container.</p>"},{"location":"enrichment/remediation-tables/#illustrative-table","title":"Illustrative Table","text":"<p><code>_{ENRICHMENT_CONTAINER_PREFIX}_REMEDIATION_{CONTAINER_ID}</code></p> <p>This remediation table is an illustrative snapshot of the \"Orders\" container for reference purposes.</p> Name Data Type Description _QUALYTICS_SOURCE_PARTITION STRING The partition from the source data container. ANOMALY_UUID STRING Unique identifier of the anomaly. ORDERKEY NUMBER Unique identifier of the order. CUSTKEY NUMBER The customer key related to the order. ORDERSTATUS CHAR The status of the order (e.g., 'F' for 'finished'). TOTALPRICE FLOAT The total price of the order. ORDERDATE DATE The date when the order was placed. ORDERPRIORITY STRING Priority of the order (e.g., 'urgent'). CLERK STRING The clerk who took the order. SHIPPRIORITY INTEGER The priority given to the order for shipping. COMMENT STRING Comments related to the order. <p>Note</p> <p>In addition to capturing the original container fields, the platform includes two metadata columns designed to assist in the analysis and remediation process.</p> <ul> <li>_QUALYTICS_SOURCE_PARTITION</li> <li>ANOMALY_UUID</li> </ul>"},{"location":"enrichment/remediation-tables/#understanding-remediation-tables-vs-source-record-tables","title":"Understanding Remediation Tables vs. Source Record Tables","text":"<p>When managing data anomalies in containers, it's important to understand the structures of Remediation Tables and Source Record Tables in the Enrichment Datastore.</p>"},{"location":"enrichment/remediation-tables/#remediation-tables_1","title":"Remediation Tables","text":"<p>Purpose: Remediation tables are designed to capture detailed snapshots of the affected containers at the time of anomaly detection. They serve as a primary tool for remediation actions.</p> <p>Creation: These tables are generated based on the remediation strategy selected during the scan operation:</p> <ul> <li>None: No tables are created.</li> <li>Append: Tables are created with new data appended.</li> <li>Overwrite: Tables are created and existing data is overwritten.</li> </ul> <p>Structure: The structure includes all columns from the source container, along with additional columns for metadata and remediation purposes. The naming convention for these tables is <code>&lt;enrichment_prefix&gt;_remediation_&lt;container_id&gt;</code>, where <code>&lt;enrichment_prefix&gt;</code> is defined during the Enrichment Datastore configuration.</p>"},{"location":"enrichment/remediation-tables/#source-record-tables","title":"Source Record Tables","text":"<p>Purpose: The Source Record Table is mainly used within the Qualytics App to display anomalies directly to users by showing the source records.</p> <p>Structure: Unlike remediation tables, the Source Record Table stores each record in a JSON format within a single column named <code>RECORD</code>, along with other metadata columns like <code>SOURCE_CONTAINER</code>, <code>SOURCE_PARTITION</code>, <code>ANOMALY_UUID</code>, and <code>CONTEXT</code>.</p>"},{"location":"enrichment/remediation-tables/#key-differences","title":"Key Differences","text":"<ul> <li> <p>Format: Remediation tables are structured with separate columns for each data field, making them easier to use for consulting and remediation processes. </p> <p>Source Record Tables store data in a JSON format within a single column, which can be less convenient for direct data operations.</p> </li> <li> <p>Usage: Remediation tables are optimal for performing corrective actions and are designed to integrate easily with data workflows. </p> <p>Source Record Tables are best suited for reviewing specific anomalies within the Qualytics App due to their format and presentation.</p> </li> </ul>"},{"location":"enrichment/remediation-tables/#recommendation","title":"Recommendation","text":"<p>For users intending to perform querying or need detailed snapshots for audit purposes, Remediation Tables are recommended. </p> <p>For those who need to quickly review anomalies directly within the Qualytics App, Source Record Tables are more suitable due to their straightforward presentation of data.</p>"},{"location":"enrichment/schema-changelog/","title":"Enrichment Table Schema Changelog","text":"<p>This page tracks column additions, removals, and type changes to the four primary enrichment tables. If you manage downstream pipelines or queries that depend on these schemas, review this changelog when upgrading.</p> <p>Tip</p> <p>The Qualytics platform automatically migrates existing enrichment tables when schema changes are detected. New columns are added and type changes are applied during the next scan operation that writes to your enrichment datastore.</p>"},{"location":"enrichment/schema-changelog/#_scan_operations","title":"_scan_operations","text":"Date Change Column Details Nov 2023 Created \u2014 Initial table with <code>OPERATION_ID</code>, <code>CONTAINER_SCAN_ID</code>, <code>PARTITION_NAME</code>, <code>INCREMENTAL</code>, <code>RECORDS_PROCESSED</code>, <code>ENRICHMENT_SOURCE_RECORD_LIMIT</code>, <code>MAX_RECORDS_ANALYZED</code>, <code>ANOMALY_COUNT</code>, <code>START_TIME</code>, <code>END_TIME</code>, <code>RESULT</code>, <code>MESSAGE</code>. Nov 2023 Added <code>DATASTORE_ID</code>, <code>CONTAINER_ID</code> Added to enable direct identification of source datastore and container. Dec 2024 Removed <code>CONTAINER_SCAN_ID</code> Deprecated and removed. Use <code>CONTAINER_ID</code> to identify the container associated with a scan operation."},{"location":"enrichment/schema-changelog/#_failed_checks","title":"_failed_checks","text":"Date Change Column Details Jul 2022 Created \u2014 Initial table with <code>OPERATION_ID</code>, <code>ANOMALY_UUID</code>, <code>DETECTED_TIME</code>, <code>SOURCE_DATASTORE</code>, <code>SOURCE_CONTAINER</code>, <code>SOURCE_PARTITION</code>, <code>QUALITY_CHECK_ID</code>, <code>QUALITY_CHECK_RULE_TYPE</code>, <code>QUALITY_CHECK_PARAMETERS</code>, <code>QUALITY_CHECK_MESSAGE</code>, <code>QUALITY_CHECK_TAGS</code>, <code>SUGGESTED_REMEDIATION_FIELD</code>, <code>SUGGESTED_REMEDIATION_VALUE</code>, <code>SUGGESTED_REMEDIATION_SCORE</code>. May 2023 Added <code>QUALITY_CHECK_DESCRIPTION</code> Description text for the quality check. Jul 2023 Removed <code>RECORD_ID_FIELD</code>, <code>RECORD_ID_VALUE</code> These legacy fields were removed. May 2024 Added <code>ANOMALOUS_RECORDS_COUNT</code> Total anomalous record count, independent of source record sampling limits. Mar 2025 Added <code>FINGERPRINT</code> Hash fingerprint for anomaly deduplication (used with Reactivate Recurring Anomalies). Mar 2025 Type change <code>QUALITY_CHECK_ID</code> Changed from STRING to NUMBER. Existing tables are auto-migrated. Jun 2025 Added <code>QUALITY_CHECK_FIELDS</code> Names of the fields targeted by the quality check. Jan 2026 Added <code>QUALITY_CHECK_METADATA</code> Optional JSON string containing additional check metadata."},{"location":"enrichment/schema-changelog/#_check_metrics","title":"_check_metrics","text":"Date Change Column Details Sep 2024 Created \u2014 Initial table with <code>OPERATION_ID</code>, <code>QUALITY_CHECK_ID</code>, <code>ASSERTED_RECORDS_COUNT</code>, <code>ANOMALOUS_RECORDS_COUNT</code>. Sep 2024 Added <code>CONTAINER_ID</code>, <code>SOURCE_DATASTORE</code>, <code>SOURCE_CONTAINER</code>, <code>SOURCE_PARTITION</code> Added for easier joining with other tables. Mar 2025 Type change <code>OPERATION_ID</code>, <code>QUALITY_CHECK_ID</code> Changed from STRING to NUMBER. Existing tables are auto-migrated. Mar 2025 Added <code>ASSERTION_RESULT</code>, <code>ASSERTION_DETAILS</code> Assertion outcome and explanatory details for each check."},{"location":"enrichment/schema-changelog/#_source_records","title":"_source_records","text":"Date Change Column Details Jul 2022 Created \u2014 Initial table with <code>SOURCE_CONTAINER</code>, <code>SOURCE_PARTITION</code>, <code>ANOMALY_UUID</code>, <code>CONTEXT</code>, <code>RECORD</code>. No changes since creation."},{"location":"enrichment/table-types/","title":"Table Types","text":"<p>The Enrichment Datastore contains several types of tables, each serving a specific purpose in the data enrichment and remediation process. These tables are categorized into:</p> <ul> <li>Enrichment Tables</li> <li>Remediation Tables</li> <li>Metadata Tables</li> </ul>"},{"location":"enrichment/table-types/#enrichment-tables","title":"Enrichment Tables","text":"<p>When anomalies are detected, the platform writes metadata into four primary enrichment tables:</p> <p>Note</p> <p>For more information, please refer to the enrichment tables documentation.</p>"},{"location":"enrichment/table-types/#remediation-tables","title":"Remediation Tables","text":"<p>When anomalies are detected in a container, the platform has the capability to create remediation tables in the Enrichment Datastore.</p> <p>Note</p> <p>For more information, please refer to the remediation tables documentation</p>"},{"location":"enrichment/table-types/#metadata-tables","title":"Metadata Tables","text":"<p>The Qualytics platform enables users to manually export metadata into the enrichment datastore, providing a structured approach to data analysis and management.</p> <p>Note</p> <p>For more information, please refer to the metadata tables documentation.</p>"},{"location":"enrichment-support/supported-enrichment-datastores/","title":"Supported Enrichment Datastores","text":"<p>Qualytics supports enrichment datastore connectors that help enhance data discovery, profiling, and quality checks. Some connectors include enrichment capabilities, while others provide only standard connectivity.</p> <p>In this guide, we will cover:</p> <ul> <li>JDBC Connectors </li> <li>DFS Connectors </li> </ul>"},{"location":"enrichment-support/supported-enrichment-datastores/#jdbc-connectors","title":"JDBC Connectors","text":"<p>The table below shows the list of JDBC connectors and whether they support enrichment or not:</p> No. Connector Enrichment Support 01. Athena \u274c 02. Big Query \u2705 03. Databricks \u2705 04. DB2 \u2705 05. Dremio \u274c 06. Fabric Analytics \u274c 07. Hive \u274c 08. MariaDB \u2705 09. Microsoft SQL Server \u2705 10. MySQL \u2705 11. Oracle \u274c 12. PostgreSQL \u2705 13. Presto \u274c 14. Redshift \u2705 15. Snowflake \u2705 16. Synapse \u2705 17. Teradata \u274c 18. TimescaleDB \u274c 19. Trino \u2705"},{"location":"enrichment-support/supported-enrichment-datastores/#dfs-connectors","title":"DFS Connectors","text":"<p>The table below shows the list of DFS connectors and whether they support enrichment or not:</p> No. Connector Enrichment Support 01. Amazon S3 \u2705 02. Azure Datalake Storage (ABFS) \u2705 03. Google Cloud Storage (GCS) \u2705"},{"location":"explore/activity/","title":"Activity","text":"<p>Activity in Qualytics provides a comprehensive view of all operations, helping users monitor and analyze the performance and workflows across various source datastores. Activities are categorized into Runs and Schedule operations, offering distinct insights into executed and scheduled activities.</p> <p>The Rerun and Resume options depend on the type of operation. Profile and Scan support both because the system can remember where it stopped and continue from there. Catalog, Export, and Materialize only support Rerun, since the system can't pick up from where it left off and must start over. External Scan doesn't support either option, as they don't apply to it.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"explore/activity/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the Explore button in the left side panel of the interface.</p> <p></p> <p>Step 2: Click on \"Activity\" from the Navigation Tab.</p> <p></p> <p>You will be navigated to the Activity tab and here you'll see a list of operations catalog, profile, scan, and external scan across different source datastores.</p> <p></p>"},{"location":"explore/activity/#activity-categories","title":"Activity Categories","text":"<p>Activities are divided into two categories: Runs and Schedule Operations. Runs provide insights into operations that have been performed, while Schedule Operations provide insights into scheduled operations.</p> <p></p>"},{"location":"explore/activity/#runs","title":"Runs","text":"<p>Runs provide a complete record of all executed operations across various source datastores. This section enables users to monitor and review activities such as catalog, profile, scan, and external scan. Each run displays key details like the operation type, status, execution time, duration, and triggering method, offering a clear overview of system performance and data processing workflows.</p> <p></p> No. Field Description 1. Select Source Datastore Select specific source datastores to focus on their operations. 2. Search This feature helps users quickly find specific identifiers. 3. Sort By Sort By option helps users organize the list of performed operations by criteria like Duration and Created Date for quick access. 4. Filter The filter lets users easily refine the list of performed operations by choosing a specific Type Scan, Catalog, Profile, External Scan, etc. along with Status (Success, Failure, Running, and Aborted) or Has Logs to view operations that completed with logs. 5. Activity Heatmap The Activity Heatmap shows daily activity levels, with color intensity indicating operation counts. Hovering over a square reveals details for that day. 6. Operation List Shows a list of operations catalog, profile, scan, and external scan, etc performed across various source datastores."},{"location":"explore/activity/#activity-heatmap","title":"Activity Heatmap","text":"<p>The Activity Heatmap represents activity levels over a period, with each square indicating a day and the color intensity representing the number of operations or activities on that day. It is useful for tracking the number of operations performed on each day within a specific timeframe.</p> <p>Note</p> <p>You can click on any of the squares from the Activity Heatmap to filter operations.  </p> <p></p> <p>By hovering over each square, you can view additional details for that specific day, such as the exact date and the total number of operations executed.</p> <p></p>"},{"location":"explore/activity/#operation-details","title":"Operation Details","text":"<p>Step 1: Click on any successfully performed operation from the list to view its details.</p> <p>For demonstration purposes, we have selected the profile operation.</p> <p></p> <p>Step 2: After clicking, a drop-down will appear, displaying the details of the selected operation.</p> <p></p> <p>Step 3: Users can hover over abbreviated metrics to see the full value for better clarity. For demonstration purposes, we are hovering over the Records Profiled field to display the full value.</p> <p></p> <p>Step 4: Users can view the exact completion timestamp of any operation by hovering over the duration label (e.g., Took less than a minute). The tooltip displays the date and time when the operation was completed.</p> <p></p> <p>Users can also view both profiled and non-profiled File Patterns:</p> <p>Step 5: Click on the Result Button.</p> <p></p> <p>The Profile Results modal displays a list of both profiled and non-profiled containers. You can filter the view to show only non-profiled containers by toggling on the button, which will display the complete list of unprofiled containers.</p> <p></p> <p>If the operation is a Materialize or Export run, users can click the View Results button to navigate directly to the associated Enrichment Datastore.</p> <p></p> <ul> <li>When View Results is clicked on a Materialize run, the system opens the related Materialize entry so users can review the complete results.</li> </ul> <p></p> <ul> <li>When View Results is clicked on an Export run, the system opens the corresponding Export entry within the Enrichment Datastore, allowing users to view the export output.</li> </ul> <p></p>"},{"location":"explore/activity/#schedule","title":"Schedule","text":"<p>The Schedule section provides a complete record of all scheduled operations across various source datastores. This section enables users to monitor and review scheduled operations such as catalog, profile, and scan. Each scheduled operation includes key details like operation type, scheduled time, and triggering method, giving users a clear overview of system performance and data workflows.</p> <p></p> No. Field Description 1. Selected Source Datastores Select specific source datastores to focus on their operations. 2. Search This feature helps users quickly find specific identifiers. 3. Sort By Sort By option helps users organize the list of scheduled operations by criteria like Created Date and Operations for quick access. 4. Filter The filter lets users easily refine the list of scheduled operations by choosing a specific operation type: Scan, Catalog, Profile, etc. to view. 5. Operation List Shows the list of scheduled operations such as catalog, profile, scan, etc across various source datastores."},{"location":"explore/activity/#deactivate-schedule-operation","title":"Deactivate Schedule Operation","text":"<p>Users can deactivate a scheduled operation from the Activity tab. This stops the operation from running further until it is activated again.</p> <p>Step 1: Click on the Redirect (\u2197) button to open the datastore.</p> <p></p> <p>Step 2: Click on the Activity tab and select Schedule to view the scheduled operations.</p> <p></p> <p>User can hover over any operation timestamp (e.g., \"1 week ago\") to view the exact Completed at time. Clicking the Redirect link opens the operation details.</p> <p></p> <p>Step 3: Click on the vertical ellipsis (\u22ee) and select Deactivate to stop the scheduled operation.</p> <p></p>"},{"location":"explore/anomalies/","title":"Anomalies","text":"<p>Anomalies tab provides a quick overview of all detected anomalies across your source datastores. In Qualytics, An Anomaly refers to a dataset (record or column) that fails to meet specified data quality checks, indicating a deviation from expected standards or norms. These anomalies are identified when the data does not satisfy the applied validation criteria. You can filter and sort anomalies based on your preferences, making it easy to see which anomalies are active,  acknowledged, or archived. This section is designed to help you quickly identify and address any issues.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"explore/anomalies/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the Explore button on the left side panel of the interface.</p> <p></p> <p>Step 2: Click on the \"Anomalies\" from the Navigation Tab.</p> <p></p> <p>You will be navigated to the Anomalies tab, where you'll see a list of all the detected anomalies across various tables and fields from different source datastores, based on the applied data quality checks.</p> <p></p>"},{"location":"explore/anomalies/#categories-anomalies","title":"Categories Anomalies","text":"<p>Anomalies can be classified into different categories based on their status and actions taken. These categories include Open and Archived anomalies. Managing anomalies effectively helps in maintaining data integrity and ensuring quick response to issues.</p>"},{"location":"explore/anomalies/#open","title":"Open","text":"<p>By selecting Open Anomalies, you can view anomalies that have been detected but remain unacknowledged or unresolved. These anomalies require attention and may need further investigation or corrective action. </p> <p></p> <p>This option helps focus on unaddressed issues while allowing seamless navigation to All, Active, or Acknowledged anomalies as needed.</p> <p>1. Active: By selecting Active Anomalies, you can focus on anomalies that are currently unresolved or require immediate attention. These are the anomalies that are still in play and have not yet been acknowledged, archived, or resolved.</p> <p></p> <p>2. Acknowledge: By selecting Acknowledged Anomalies, you can see all anomalies that have been reviewed and marked as acknowledged. This status indicates that the anomalies have been noted, though they may still require further action.</p> <p></p> <p>3. All: By selecting All Anomalies, you can view the complete list of anomalies, regardless of their status. This option helps you get a comprehensive overview of all issues that have been detected, whether they are currently active, acknowledged, or archived.</p> <p></p>"},{"location":"explore/anomalies/#archived","title":"Archived","text":"<p>By selecting Archived Anomalies, you can view anomalies that have been resolved or moved out of active consideration. Archiving anomalies allows you to keep a record of past issues without cluttering the active list.</p> <p></p> <p>You can also categorize the archived anomalies based on their status as Resolved, Duplicate, Invalid, Discarded, and All to review them effectively.</p> <p>1. Resolved: This indicates that the anomaly was a legitimate data quality concern and has been addressed.</p> <p></p> <p>2. Duplicate: This indicates that the anomaly is a duplicate of an existing record and has already been addressed.</p> <p></p> <p>3. Invalid: This indicates that the anomaly is not a legitimate data quality concern and does not require further action.</p> <p></p> <p>4. Discarded: This indicates that the anomaly is no longer relevant or under review and has been intentionally dismissed without further action.</p> <p></p> <p>5. All: Displays all archived anomalies, including those marked as Resolved, Duplicate, Invalid, and Discarded giving a comprehensive view of all past issues.</p> <p></p>"},{"location":"explore/anomalies/#anomaly-details","title":"Anomaly Details","text":"<p>Anomaly Details window provides information about anomalies identified during scan operations. It displays details such as the anomaly ID, status, type, detection time, and where it is in the data, such as the datastore and table.  Additionally, it offers options to explore datasets, share details, and collaborate, making it easier to resolve data issues.</p> <p>Step 1: Click on the anomaly from the list of available (whether Active, Acknowledged or Archived) anomalies to view its details.</p> <p></p> <p>A modal window titled \u201cAnomaly Details\u201d will appear, displaying all the details of the selected anomaly.</p> <p></p> <p>For more details on Anomaly Details, please refer to the Anomaly Insights section in the documentation.</p>"},{"location":"explore/anomalies/#acknowledged-anomalies","title":"Acknowledged Anomalies","text":"<p>By acknowledging anomalies, you indicate that they have been reviewed or recognized. Acknowledging anomalies helps you keep track of issues that have been addressed, even if further action is still required.</p> <p>Warning</p> <p>Once an anomaly is acknowledged, it remains acknowledged and never reverts to the active state. </p> <p>Step 1: Click on the active anomaly from the list of available anomalies that you want to acknowledge.</p> <p></p> <p>Step 2: A modal window will appear displaying the anomaly details. Click on the acknowledge (\ud83d\udc41) icon located in the upper-right corner of the modal window.</p> <p></p> <p>Step 3: After clicking on the Acknowledge icon your anomaly is successfully moved to the acknowledge and a flash message will appear saying \u201cThe Anomaly has been successfully acknowledged\u201d.</p> <p></p>"},{"location":"explore/anomalies/#archive-anomalies","title":"Archive Anomalies","text":"<p>By archiving anomalies, you move them to an inactive state, while still keeping them available for future reference or analysis. Archiving helps keep your active anomaly list clean without permanently deleting the records.</p> <p>Step 1: Click on the anomaly from the list of available (whether Active or Acknowledged) anomalies that you want to archive.</p> <p></p> <p>Step 2: A modal window will appear displaying the anomaly details. Click on the archive (\ud83d\uddd1) icon located in the upper-right corner of the modal window.</p> <p></p> <p>Step 3: A modal window titled \u201cArchive Anomaly\u201d will appear, providing you with the following archive options:</p> <ul> <li> <p>Resolved: Choose this option if the anomaly was a legitimate data quality concern and has been addressed. This helps maintain a record of resolved issues while ensuring that they are no longer active.</p> </li> <li> <p>Invalid: Select this option if the anomaly is not a legitimate data quality concern and does not require further action. Archiving anomalies as invalid helps differentiate between real issues and those that can be dismissed, improving overall data quality management.  </p> </li> <li> <p>Duplicate: Choose this option if the anomaly is a duplicate of an existing record and has already been addressed. No further action is required as the issue has been previously resolved.</p> </li> <li> <p>Discarded: Select this option if the anomaly is no longer important or needs no further review. Discarded anomalies are simply ignored because they don\u2019t require any action, helping keep the anomaly list clean and easy to manage.</p> </li> </ul> <p></p> <p>Step 4: Once you've made your selection, click the Archive button to proceed.</p> <p></p> <p>Step 5: After clicking on the Archive button your anomaly is moved to the archive and a flash message will appear saying \u201cAnomaly has been successfully archived\u201d.</p> <p></p>"},{"location":"explore/anomalies/#restore-archived-anomalies","title":"Restore Archived Anomalies","text":"<p>By restoring archived anomalies, you can bring them back into the acknowledged state for further investigation or review. These anomalies will not return to the active state once they have been acknowledged.</p> <p>Step 1: Click on the anomaly that you want to restore from the list of archived anomalies.</p> <p></p> <p>Step 2: A modal window will appear displaying the anomaly details. Click on the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window, and click on \u201cRestore\u201d from the drop-down menu.</p> <p></p> <p>Step 3: After clicking on the \u201cRestore\u201d button, the selected anomaly is now restored as in acknowledged state.</p> <p></p>"},{"location":"explore/anomalies/#assign-tags","title":"Assign Tags","text":"<p>Assigning tags to an anomaly serves the purpose of labeling and grouping anomalies and driving downstream workflows.</p> <p>Step 1: Click on the Assign tags to this Anomaly or + button.</p> <p></p> <p>Step 2: A dropdown menu will appear with existing tags. Scroll through the list and click on the tag you wish to assign.</p> <p></p>"},{"location":"explore/anomalies/#delete-anomalies","title":"Delete Anomalies","text":"<p>Deleting an anomaly allows you to permanently remove a record that is no longer relevant or was logged in error. This action is done individually, ensuring that your anomaly records remain clean and up to date.</p> <p>Note</p> <p>You can only delete archived anomalies, not active or acknowledged checks. If you want to delete an active or acknowledged anomaly, you must first move it to the archive, and then you can delete it. </p> <p>Step 1: Click on Archived from the navigation bar in the Anomalies section to view all archived anomalies.</p> <p></p> <p>Step 2: Click on the archive anomaly from the list of archived anomalies that you want to delete.</p> <p></p> <p>Step 3: A modal window will appear displaying the anomaly details. Click on the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window, and click on \u201cDelete\u201d from the drop-down menu.</p> <p></p> <p>Step 4: A confirmation modal window will appear, click on the Delete button to permanently remove the anomaly from the system.</p> <p></p> <p>Step 5: After clicking on the delete button, your anomaly is successfully deleted and a success flash message will appear saying \u201cAnomaly has been successfully deleted\u201d.</p> <p></p>"},{"location":"explore/anomalies/#filter-and-sort","title":"Filter and Sort","text":"<p>Filter and Sort options allow you to organize your anomaly by various criteria, such as Weight, Anomalous Record, Created Date. You can also apply filters to refine your list of anomaly based on Selected Source Datastores, Selected Tag, Timeframe, Type and Rule .</p>"},{"location":"explore/anomalies/#sort","title":"Sort","text":"<p>You can sort your anomalies by Anomalous Record, Created Date, and Weight to easily organize and prioritize them according to your needs.</p> <p></p> No Sort By Option Description 1 Anomalous Record Sorts anomalies based on the number of anomalous records identified. 2 Created Date Sorts anomalies according to the date they were detected. 3 Weight Sort anomalies by their assigned weight or importance level. <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.</p> <p></p>"},{"location":"explore/anomalies/#filter","title":"Filter","text":"<p>You can filter your anomalies based on values like Source Datastores, Timeframe, Type, Rule, and Tags.</p> <p></p> <p>Info</p> <p>Users can search across all filter inputs using typos, partial terms, or abbreviations. The system intelligently matches relevant results, making it easier to find what they need without exact inputs.</p> <p></p> <p></p> No. Filter Description 1 Selected Source Datastore Select specific source datastores to focus on their anomalies. 2 Select Tags Filter anomalies by specific tags to categorize and prioritize issues effectively. 3 Timeframe Filtering anomalies detected within specific time ranges (e.g., anomalies detected in the last week or year). 4 Type Filter anomalies based on anomaly type (Record or Shape). 5 Rule Filter anomalies based on specific rules applied to the anomaly. By clicking on the caret down button next to the Rule field, the available rule types will be dynamically populated based on the rule types present in the results. The rules displayed are based on the current dataset and provide more granular control over filtering.   Each rule type will show a counter next to it, displaying the total number of occurrences for that rule in the dataset.  For example, the rule type After Date Time is displayed with a total of 514 occurrences."},{"location":"explore/checks/","title":"Checks","text":"<p>Checks tab provides a quick overview of the various checks applied across different tables and fields in multiple source datastores. In Qualytics, checks act as rules applied to data tables and fields to ensure accuracy and maintain data integrity. You can filter and sort the checks based on your preferences, making it easy to see which checks are active, in draft, or archived. This section is designed to simplify the review of applied checks across datasets without focusing on data quality or performance.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"explore/checks/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the Explore button on the left side panel of the interface.</p> <p></p> <p>Step 2: Click on the \"Checks\" from the Navigation Tab.</p> <p></p> <p>You will be navigated to the Checks tabs here and you'll see a list of all the checks that have been applied to various tables and fields across different source datastores.</p>"},{"location":"explore/checks/#categories-check","title":"Categories Check","text":"<p>You can categorize your checks based on their status, such as Active, Draft, Archived (Invalid and Discarded), or All, according to your preference. This categorization offers a clear view of the data quality validation process, helping you manage checks efficiently and maintain data integrity.</p>"},{"location":"explore/checks/#all","title":"All","text":"<p>By selecting All Checks, you can view a comprehensive list of all the checks in the datastores, including both active and draft checks, allowing you to focus on the checks that are currently being managed or are in progress. However, archived checks are not displayed in this.</p> <p></p>"},{"location":"explore/checks/#active","title":"Active","text":"<p>By selecting Active, you can view checks that are currently applied and being enforced on the data. These operational checks are used to validate data quality in real-time, allowing you to monitor all active checks and their performance.</p> <p></p> <p>You can also categorize the active checks based on their importance, favorites, or specific metrics to streamline your data quality monitoring.</p> <p>1. Important: Shows only checks that are marked as important. These checks are prioritized based on their significance, typically assigned a weight of 7 or higher.</p> <p>Note</p> <p>Important checks are prioritized based on a weight of 7 or higher.</p> <p></p> <p>2. Favorite: Displays checks that have been marked as favorites. This allows you to quickly access checks that you use or monitor frequently. </p> <p></p> <p>3. All: Displays a comprehensive view of all active checks, including important, favorite and any checks that do not fall under these specific categories. </p> <p></p>"},{"location":"explore/checks/#draft-checks","title":"Draft Checks","text":"<p>By selecting Draft, you can view checks that have been created but have not yet been applied to the data. These checks are in the drafting stage, allowing for adjustments and reviews before activation. Draft checks provide flexibility to experiment with different validation rules without affecting the actual data.</p> <p></p> <p>You can also categorize the draft checks based on their importance, favorites, or specific metrics to prioritize and organize them effectively during the review and adjustment process.</p> <p>1. Important: Shows only checks that are marked as important. These checks are prioritized based on their significance, typically assigned a weight of 7 or higher.</p> <p></p> <p>2 Favorite: Displays checks that have been marked as favorites. This allows you to quickly access checks that you use or monitor frequently.  </p> <p></p> <p>3. All: Displays a comprehensive view of all draft checks, including important, favorite and any checks that do not fall under these specific categories.</p> <p></p>"},{"location":"explore/checks/#archived-checks","title":"Archived Checks","text":"<p>By selecting Archived, you can view checks that have been marked as discarded or invalid from use but are still stored for future reference or restoration. Although these checks are no longer active, they can be restored if needed.</p> <p></p> <p>You can also categorize the archived checks based on their status as Discarded, Invalid, or view All archived checks to manage and review them effectively.</p> <p>1. Discarded: Shows checks that have been marked as no longer useful or relevant and have been discarded from use.</p> <p></p> <p>2. Invalid: Displays checks that are deemed invalid due to errors or misconfigurations, requiring review or deletion.</p> <p></p> <p>3. All: Provides a view of all archive checks within this category including discarded and invalid checks.</p> <p></p>"},{"location":"explore/checks/#details","title":"Details","text":"<p>Check Details provides important information about each check in the system. It shows when a check was last run, how often it has been used, when it was last updated, who made changes to it, and when it was created. This section helps users understand the status and history of the check, making it easier to manage and track its use over time. </p> <p>Step 1: Locate the check you want to review, then hover over the info icon to view the Check Details.</p> <p></p> <p>A popup will appear with additional details about the check.</p> <p></p>"},{"location":"explore/checks/#last-asserted","title":"Last Asserted","text":"<p>Last Asserted At shows the most recent time the check was run, indicating when the last validation occurred. For example, the check was last asserted on Mar 27, 2025, at 2:16 AM (GMT+5:30).</p> <p></p>"},{"location":"explore/checks/#scans","title":"Scans","text":"<p>Scans show how many times the check has been used in different operations. It helps you track how often the check has been applied. For example, the check was used in 17 operations.</p> <p></p>"},{"location":"explore/checks/#updated-at","title":"Updated At","text":"<p>Updated At shows the most recent time the check was modified or updated. It helps you see when any changes were made to the check\u2019s configuration or settings. For example, the check was last updated on Nov 8, 2024, at 6:37 PM (GMT+5:30).</p> <p></p>"},{"location":"explore/checks/#last-editor","title":"Last Editor","text":"<p>Last Editor indicates who most recently made changes to the check. It helps track who is responsible for the latest updates or modifications. This is useful for accountability and collaboration within teams.</p> <p></p>"},{"location":"explore/checks/#created-at","title":"Created At","text":"<p>Created At shows when the check was first made. It helps you know how long the check has been in use. This is useful for tracking its history. For example, the check was created on Oct 17, 2024, at 11:13 AM (GMT+5:30).</p> <p></p>"},{"location":"explore/checks/#created-by","title":"Created By","text":"<p>Created By shows the user or system that originally created the check. This helps identify ownership and understand whether the check was created manually by a user or automatically by a service. For example, the check was created by Qualytics Service.</p> <p></p>"},{"location":"explore/checks/#status-management-of-checks","title":"Status Management of Checks","text":""},{"location":"explore/checks/#set-check-as-draft","title":"Set Check as Draft","text":"<p>You can move an active check into a draft state, allowing you to work on the check, make adjustments, and refine the validation rules without affecting live data. This is useful when you need to temporarily deactivate a check for review and updates.</p> <p>Step 1: Click on the active check that you want to move to the draft state.</p> <p></p> <p>Step 2: A modal window will appear displaying the check details. Click on the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window, and select \"Draft\" from the drop-down menu.</p> <p></p> <p>Step 3: After clicking on \"Draft\", the check will be successfully moved to the draft state, and a success flash message will appear stating, \"The checks have been successfully updated.\"</p> <p></p>"},{"location":"explore/checks/#activate-draft-check","title":"Activate Draft Check","text":"<p>You can activate the draft checks after when you have worked on the check, make adjustments, and refine the validation rules. By activating the draft check and making it live, ensures that the defined criteria are enforced on the data. </p> <p>Step 1: Navigate to the Draft check section, and click on the drafted check that you want to activate, whether you have made changes or wish to activate it as is.</p> <p></p> <p>A modal window will appear with the check details. If you want to make any changes to the check details, you can edit them.</p> <p></p> <p>Step 2: Click on the down arrow icon with the Update button. A dropdown menu will appear, click on the Activate button.  </p> <p></p> <p>Step 3: After clicking on the activate button, your check is now successfully moved to the active checks and a success flash message will appear stating \"Check successfully updated\".</p> <p></p>"},{"location":"explore/checks/#set-check-as-archived","title":"Set Check as Archived","text":"<p>You can move an active or draft check into the archive when it is no longer relevant but may still be needed for historical purposes or future use. Archiving helps keep your checks organized without permanently deleting them.</p> <p>Step 1: Click on the check from the list of available (whether Active or Draft) checks that you want to archive.</p> <p></p> <p>Step 2: A modal window will appear displaying the check details. Click on the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window, and click on the \"Archive\" from the drop-down menu. </p> <p></p> <p>Step 3: A modal window titled \u201cArchive Check\u201d will appear, providing you with the following archive options:</p> <ul> <li> <p>Discarded: Select this option if the check is no longer relevant or suitable for the current business rules or data requirements. This helps in archiving checks that are obsolete but still exist for historical reference.</p> </li> <li> <p>Invalid: Choose this option if the check is not valid and should be retired from future inference. This helps the system learn from invalid checks and improves its ability to infer valid checks in the future.</p> </li> </ul> <p></p> <p>Step 4: Once you've made your selection, click the Archive button to proceed.</p> <p></p> <p>Step 5: After clicking on the Archive button your check is moved to the archive and a flash message will appear saying \"The Check has been successfully archived\".</p> <p></p>"},{"location":"explore/checks/#restore-archived-checks","title":"Restore Archived Checks","text":"<p>If a check has been archived, then you can restore it back to an active state or in a draft state. This allows you to reuse your checks that were previously archived without having to recreate them from scratch.</p> <p>Step 1: Click on Archived from the navigation bar in the Checks section to view all archived checks.</p> <p></p> <p>Step 2: Click on the archived check which you want to restore as an active or draft check.</p> <p>For Demonstration purpose, we have selected the \"Metric\" check.</p> <p></p> <p>A modal window will appear with the check details.</p> <p></p> <p>Step 3: If you want to make any changes to the check, you can edit it. Otherwise, click on the Restore button to restore it as an active check.</p> <p></p> <p>To restore the check as a draft, click on the arrow icon next to the Restore button. A dropdown menu will appear\u2014select Restore as Draft from the options.</p> <p></p> <p>After clicking the Restore button, the check will be successfully restored as either an active or draft check, depending on your selection. A success message will appear confirming, \"Check successfully updated.\"</p> <p></p>"},{"location":"explore/checks/#edit-check","title":"Edit Check","text":"<p>You can edit an existing check to modify its properties, such as the rule type, coverage, filter clause, or description. Updating a check ensures that it stays aligned with evolving data requirements and maintains data quality as conditions change.</p> <p>Step 1: Click on the check you want to edit, whether it is an active or draft check.</p> <p></p> <p>A modal window will appear with the check details. </p> <p></p> <p>Step 2: Modify the check details as needed based on your preferences.</p> <p></p> <p>Step 3: Once you have edited the check details, then click on the Validate button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct.</p> <p></p> <p>If the validation is successful, a green message saying \"Validation Successful\" will appear. </p> <p></p> <p>If the validation fails, a red message saying \"Failed Validation\" will appear. This typically occurs when the check logic or parameters do not match the data properly.</p> <p></p> <p>Step 3: Once you have a successful validation, click the \"Update\" button. The system will update the changes you've made to the check, including changes to the fields, filter clause, coverage, description, tags, or metadata.</p> <p></p> <p>After clicking on the Update button, your check is successfully updated and a success flash message will appear stating \"Check successfully updated\".</p> <p></p>"},{"location":"explore/checks/#mark-check-as-favorite","title":"Mark Check as Favorite","text":"<p>Marking a check as a favorite allows you to quickly access and prioritize the checks that are most important to your data validation process. This helps streamline workflows by keeping frequently used or critical checks easily accessible, ensuring you can monitor and manage them efficiently. By marking a check as a favorite, it will appear in the \"Favorite\" category for faster retrieval and management.</p> <p>Step 1: Locate the check which you want to mark as a favorite and click on the bookmark icon located on the right side of the check.</p> <p></p> <p>After Clicking on the bookmark icon your check is successfully marked as a favorite and a success flash message will appear stating \"Check has been favorited\".</p> <p></p> <p>To unmark a check, simply click on the bookmark icon of the marked check.</p> <p></p> <p>This will remove it from your favorites.A success flash message will appear stating \"The Check has been unfavorited\".</p> <p></p>"},{"location":"explore/checks/#clone-check","title":"Clone Check","text":"<p>You can clone both active and draft checks to create a duplicate copy of an existing check. This is useful when you want to create a new check based on the structure of an existing one, allowing you to make adjustments without affecting the original check.</p> <p>Step 1: Click on the check (whether Active or Draft) that you want to clone.</p> <p></p> <p>Step 2: A modal window will appear displaying the check details. Click on the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window, and select \"Clone\" from the drop-down menu.</p> <p></p> <p>Step 3: After clicking the Clone button, a modal window will appear. This window allows you to adjust the cloned check's details.</p> <p></p> <p>1. If you toggle on the \"Associate with a Check Template\" option, the cloned check will be linked to a specific template.</p> <p></p> <p>Choose a Template from the dropdown menu that you want to associate with the cloned check. The check will inherit properties from the selected template.</p> <ul> <li> <p>Locked: The check will automatically sync with any future updates made to the template, but you won't be able to modify the check's properties directly.</p> </li> <li> <p>Unlocked: You can modify the check, but future updates to the template will no longer affect this check.</p> </li> </ul> <p></p> <p>2. If you toggle off the \"Associate with a Check Template\" option, the cloned check will not be linked to any template, which allows you full control to modify the properties independently.</p> <p></p> <p>Select the appropriate Rule type for the check from the dropdown menu.</p> <p></p> <p>Step 4: Once you have selected the template or rule type, fill up the remaining check details as required. </p> <p></p> <p>Step 5: After completing all the check details, click on the \"Validate\" button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct. It ensures that the check will work as expected by running it against the data without committing any changes.</p> <p></p> <p>If the validation is successful, a green message saying \"Validation Successful\" will appear. </p> <p></p> <p>If the validation fails, a red message saying \"Failed Validation\" will appear. This typically occurs when the check logic or parameters do not match the data properly.</p> <p></p> <p>Step 6: Once you have a successful validation, click the \"Save\" button. The system will save any modifications you've made to the check, and create a clone of that check on basis of your changes.  </p> <p></p> <p>After clicking on the \"Save\" button your check is successfully created and a success flash message will appear stating \"Check successfully created\".</p> <p></p>"},{"location":"explore/checks/#create-a-quality-check-template","title":"Create a Quality Check template","text":"<p>You can add checks as a Template, which allows you to create a reusable framework for quality checks. By using templates, you standardize the validation process, enabling the creation of multiple checks with similar rules and criteria across different datastores. This ensures consistency and efficiency in managing data quality checks.</p> <p>Step 1: Locate the check (whether Active or Draft) you want to archive and click on that check.</p> <p></p> <p>Step 2: A modal window will appear displaying the check details. Click on the vertical ellipsis (\u22ee) located in the upper-right corner of the modal window, and select \"Template\" from the drop-down menu.</p> <p></p> <p>After clicking the \"Template\" button, the check will be saved and created as a template in the library, and a success flash message will appear stating, \"The quality check template has been successfully created.\" This allows you to reuse the template for future checks, streamlining the validation process.</p> <p></p>"},{"location":"explore/checks/#filter-and-sort","title":"Filter and Sort","text":"<p>Filter and Sort options allow you to organize your checks by various criteria, such as Weight, Anomalies, Coverage, Created Date, and Rules. You can also apply filters to refine your list of checks based on Selected Source Datastores, Check Type, Asserted State (Passed, Failed, Not Asserted), Tags, Files, and Fields.</p>"},{"location":"explore/checks/#sort","title":"Sort","text":"<p>You can sort your checks by Active Anomalies, Coverage, Created Date, Last Asserted, Rules, and Weight to easily organize and prioritize them according to your needs.</p> <p></p> No Sort By Option Description 1 Active Anomalies Sort checks based on the number of active anomalies. 2 Coverage Sort checks by data coverage percentage. 3 Created Date Sort checks according to the date they were created. 4 Last Asserted Sorts by the last time the check was executed. 5 Rules Sort checks based on specific rules applied to the checks. 6 Weight Sort checks by their assigned weight or importance level. <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.</p> <p></p>"},{"location":"explore/checks/#filter","title":"Filter","text":"<p>You can filter your checks based on values like Source Datastores Check Type, Asserted State, Rule, Tags, File, Field, and Template.</p> <p>Info</p> <p>Users can search across all filter inputs using typos, partial terms, or abbreviations. The system intelligently matches relevant results, making it easier to find what they need without exact inputs.</p> <p></p> <p></p> No Filter Filter Value Description 1 Selected Source Datastores N/A Select specific source datastores to focus on their checks. 2 Select Tags N/A Filter checks by specific tags to categorize and refine results. <p></p> No Filter Filter Value Description 3 Check Type All Displays all types of checks, both inferred and authored. Inferred Shows system-generated checks that automatically validate data based on detected patterns or logic. Authored Displays user-created checks, allowing the user to focus on custom validations tailored to specific requirements. 4 Asserted State All Displays all checks, regardless of their asserted status. This provides a full overview of both passed, failed, and not asserted checks. Passed Shows checks that have been asserted successfully, meaning no active anomalies were found during the validation process. Failed Displays checks that have failed assertion, indicating active anomalies or issues that need attention. Not Asserted Filters out checks that have not yet been asserted, either because they haven\u2019t been processed or validated yet. 5 Rule N/A Select this to filter the checks based on specific rule type for data validation, such as checking non-null values, matching patterns, comparing numerical ranges, or verifying date-time constraints. By clicking on the caret down button next to the Rule field, the available rule types will be dynamically populated based on the rule types present in the results. The rules displayed are based on the current dataset and provide more granular control over filtering. Each rule type will show a counter next to it, displaying the total number of occurrences for that rule in the dataset. For example, the rule type After Date Time is displayed with a total of 46 occurrences. 6 Template N/A This filter allows users to view and apply predefined check templates. 7 Created By Select a user Filters checks based on the user or system that created them. The list is dynamically populated based on creators present in the current results, making it easier to review ownership or focus on checks created by specific users or automated services."},{"location":"explore/insights/","title":"Insights","text":"<p>Insights in Qualytics provides a quick and clear overview of your data's health and performance. It shows key details like Quality Scores, active checks, profiles, scans, and anomalies in a simple and effective way. This makes it easy to monitor and track data quality, respond to issues, and take action quickly. Additionally, users can monitor specific source datastores and check for a particular report date and time frame.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"explore/insights/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the Explore button on the left side panel of the interface.</p> <p></p> <p>You will be navigated to the Insights tab to view a presentation of your data, pulled from the connected source datastore.</p> <p></p>"},{"location":"explore/insights/#filtering-controls","title":"Filtering Controls","text":"<p>Filtering Controls allow you to refine the data displayed on the Insights page. You can customize the data view based on Source Datastores, Tags, Report Date, and Timeframe, ensuring you focus on the specific information that matters to you.</p> <p></p> No Filter Description 1. Select Source Datastores Select specific source datastores to focus on their data. 2. Tags Filter data by specific tags to categorize and refine results. 3. Report Date Set the report date to view data from a particular day. 4. Timeframe Choose a timeframe to view data for a specific period (week, month, quarter, or year) <p>Info</p> <p>Users can search across all filter inputs using typos, partial terms, or abbreviations. The system intelligently matches relevant results, making it easier to find what they need without exact inputs.</p> <p></p>"},{"location":"explore/insights/#understanding-timeframes-and-timeslices","title":"Understanding Timeframes and Timeslices","text":"<p>When analyzing data on the Insights, two key concepts help you uncover trends: timeframes and timeslices. These work together to give you both a broad view and a detailed breakdown of your data.</p>"},{"location":"explore/insights/#timeframes","title":"Timeframes","text":"<p>Timeframe is the total range of time you select to view your data. For example, you can choose to see data:</p> <ul> <li> <p>Weekly: Summarize data for an entire week.</p> </li> <li> <p>Monthly: Group data by months.</p> </li> <li> <p>Quarterly: Cover three months at a time.</p> </li> <li> <p>Yearly: Show data for the entire year.</p> </li> </ul>"},{"location":"explore/insights/#how-metrics-behave-over-a-timeframe","title":"How Metrics Behave Over a Timeframe","text":"<ul> <li>Quality Score and other similar metrics display an average for the selected timeframe.</li> </ul> <p>Example: If you select weekly, the Quality Score shown will be the average score for the entire week.</p> <p></p> <ul> <li>Historical Graphs (like Profiles or Scans) show cumulative totals over time.</li> </ul> <p>Example:If you view a graph for a monthly timeframe, the graph shows how data grows or changes month by month.</p> <p></p>"},{"location":"explore/insights/#timeslices","title":"Timeslices","text":"<p>Timeslice breaks your selected timeframe into smaller parts. It helps you see more detailed trends within the overall timeframe.</p> <p>For example:</p> <ul> <li> <p>A weekly timeframe shows each day of the week.</p> </li> <li> <p>A monthly timeframe breaks into weekly segments.</p> </li> <li> <p>A quarterly timeframe highlights months within that quarter.</p> </li> <li> <p>A yearly timeframe divides into quarters and months.</p> </li> </ul>"},{"location":"explore/insights/#how-timeslices-work","title":"How Timeslices Work","text":"<ul> <li> <p>When you choose a timeframe, the graph automatically breaks it into timeslices.</p> </li> <li> <p>Each bar or point on the graph represents one timeslice.</p> </li> </ul> <p>Example:</p> <ul> <li>If you choose a Weekly timeframe, each bar in the graph will represent one day of the week.</li> </ul> <p></p> <ul> <li>If you choose a Monthly timeframe, each bar will represent one week in that month.</li> </ul> <p></p>"},{"location":"explore/insights/#metrics-within-a-timeslice","title":"Metrics Within a Timeslice","text":"<p>Metrics like Quality Score, Profiles, or Scans are displayed for each timeslice, allowing you to identify trends and patterns over smaller intervals.</p>"},{"location":"explore/insights/#quality-score","title":"Quality Score","text":"<p>Quality Score gives a clear view of your data's overall quality. It shows important measures like Completeness, Conformity, Consistency, Precision, Timeliness, Volumetrics, and Accuracy, each represented by a percentage. This helps you quickly understand the health of your data, making it easier to identify areas that need improvement.  </p> <p></p>"},{"location":"explore/insights/#score-summary","title":"Score Summary","text":"<p>This section shows your overall Quality Score along with a breakdown of each dimension. You\u2019ll also see how much each score has improved or decreased.</p> <p></p> <p>You can hover over any quality dimension (such as Completeness, Precision, or Timeliness) to see additional details about that score.</p>"},{"location":"explore/insights/#score-progression","title":"Score Progression","text":"<p>The new Score Progression chart helps you track how your Quality Score changes over time. Users can:</p> <ul> <li>Switch between Day, Week, or Month views.</li> <li>Click individual dimensions on the right to see the trend for that specific metric.</li> <li>Compare how each dimension is performing over time.</li> <li>This makes it easier to understand your data\u2019s health, spot improvements, and identify areas that need attention.</li> </ul> <p></p>"},{"location":"explore/insights/#data-overview","title":"Data Overview","text":"<p>The Data Overview gives you a high-level snapshot of your data for the selected period. It shows:</p> <ul> <li>Data Under Management \u2014 the total number of rows managed in Qualytics  </li> <li>Source Datastores \u2014 how many active datastores are connected  </li> <li>Containers \u2014 the number of containers within those datastores  </li> </ul> <p></p> <p>This helps you quickly understand the scale of your data and how it\u2019s growing over time.</p>"},{"location":"explore/insights/#data-volume","title":"Data Volume","text":"<p>The Data Volume chart helps you track how your stored data is increasing or decreasing over time. This makes it easier to:</p> <ul> <li>Spot unexpected growth or drops in data size  </li> <li>Monitor ingestion patterns  </li> <li>Plan storage, scaling, and resource allocation</li> <li>Detect irregularities that may affect overall system performance  </li> </ul> <p>You can change the Group By option (Day, Week, Month) to view the trend with the level of detail you need.</p> <p></p>"},{"location":"explore/insights/#checks-profiling","title":"Checks &amp; Profiling","text":"<p>The Checks &amp; Profiling section provides a consolidated view of your active checks, their status, and profiling activity.</p> <p></p> <p>1. Passing Check: Displays the real-time number of passed checks that were successfully completed during the scan or profile operation, indicating that the data met the set quality criteria.</p> <p></p> <p>2. Failing Checks: This shows the real-time number of checks that did not pass during the scan or profile operation, indicating data that did not meet the quality criteria.</p> <p></p> <p>3. Not Asserted Checks: This shows the real-time number of checks that haven't been processed or validated yet, meaning their status is still pending and they have not been confirmed as either passed or failed.</p> <p></p> <p>4. Inferred Checks: This shows the real-time number of system-generated Inferred Checks.These checks are automatically created during a Profile operation using statistical analysis and machine learning methods.</p> <p></p> <p>5. Authored Check: This shows the Authored Checks that are manually created by users within the Qualytics platform or API. These checks can range from simple templates for common validations to complex rules using Spark SQL and User-Defined Functions (UDF) in Scala. </p> <p></p> <p>The count for each category can be viewed by hovering over the relevant check, providing real-time ratios of checks. Users can also click on these checks to navigate directly to the corresponding checks\u2019 dedicated page in the Explore section. </p> <p>6. Records Profiled: This represents the total number of records that were included in the profiling process.</p> <p></p> <p>7. Fields Profiled: This shows how many field profiles were updated as a result of the profiling operation.</p> <p></p>"},{"location":"explore/insights/#anomalies-scanning","title":"Anomalies &amp; Scanning","text":"<p>The Anomalies &amp; Scanning section provides a high-level summary of all detected anomalies, along with the scanning activity that helps generate those results. This view helps you understand the number of issues identified and how they are distributed across different statuses, while also showing how much data was scanned during the selected period.</p> <p></p> <p>1. Active Anomalies: Shows the number of unresolved anomalies that require immediate attention. These anomalies are still present and have not been acknowledged, archived, or resolved in the system.</p> <p></p> <p>2. Acknowledged Anomalies:  These are anomalies that have been reviewed and recognized by users but are not yet resolved. Acknowledging anomalies helps keep track of issues that have been addressed, even if further actions are still needed. </p> <p></p> <p>3. Resolved Anomalies: Represent anomalies that were valid data quality issues and have been successfully addressed. These anomalies have been resolved, indicating the data now meets the required quality standards.</p> <p></p> <p>4. Duplicate Anomalies: These anomalies have been identified as duplicates of existing anomalies. This status helps prevent redundant issue tracking and ensures that duplicate records are consolidated into a single entry.</p> <p></p> <p>5. Invalid Anomalies: These anomalies have been reviewed and determined to be false positives or not relevant. Marking an anomaly as invalid removes it from active consideration, preventing unnecessary investigations.</p> <p></p> <p>6. Discarded Anomalies: Shows the number of anomalies that were intentionally removed from the workflow because they are not useful, not valid, or no longer need attention.</p> <p></p> <p>Info</p> <p>Users can see the checks using the redirect link (the redirect only show the current check statuses).</p> <p>The count for each category can be viewed by hovering over the relevant anomalies, providing real-time ratios of anomalies. Users can also click on these anomalies to navigate directly to the corresponding anomalies\u2019 dedicated page in the Explore section. </p> <p>7. Container Scanned: Shows the total number of containers that were scanned during the selected period.</p> <p></p> <p>8. Records Scanned: This refers to the number of records that were checked during a scan operation. The scan performs data quality checks on collections like tables, views, and files.</p> <p></p>"},{"location":"explore/insights/#check-distribution","title":"Check Distribution","text":"<p>The Check Distribution section shows a breakdown of checks by rule type, helping you quickly understand which types of rules are most commonly applied across your datastore. Each rule type is represented by a different color for easy visual comparison.</p> <p></p> <p>By clicking the caret down \ud83d\udd3d button, users can choose either the top 5 or top 10 rule types to view in the insights, based on their analysis needs.</p> <p></p>"},{"location":"explore/insights/#failed-check-distribution","title":"Failed Check Distribution","text":"<p>The Failed Check Distribution shows a breakdown of all failed checks by rule type. This helps you quickly understand which rule types are failing most often and may require attention.</p> <p></p> <p>By clicking the caret down \ud83d\udd3d button, users can choose either the top 5 or top 10 rule types to view in the insights, based on their analysis needs.</p> <p></p>"},{"location":"explore/insights/#profiles","title":"Profiles","text":"<p>Profiles section provides a clear view of data profiling activities over time, showing how often profiling is performed and the amount of data (records) analyzed.</p> <p></p> <p>Profile Runs shows how many times data profiling has been done over a certain period. Each run processes a specific source datastore or table, helping users see how often profiling happens. The graph gives a clear view of the changes in profile runs over time, making it easier to track  profiling activity.</p> <p></p> <p>Click on the caret down \ud83d\udd3d button to choose between viewing Records Profiled or Fields Profiled, depending on your preference.</p> <p></p>"},{"location":"explore/insights/#record-profile","title":"Record Profile","text":"<p>Record Profiled shows the total number of records processed during the profile runs. It provides insight into the amount of data that has been analyzed during those runs. The bars in the graph show the comparison of the number of records profiled over the selected days.</p> <p></p>"},{"location":"explore/insights/#field-profiled","title":"Field Profiled","text":"<p>Field Profiled shows the number of fields processed during the profile runs. It shows how many individual fields within datasets have been analyzed during those runs. The bars in the graph provide a comparison of the fields profiled over the selected days.</p> <p></p>"},{"location":"explore/insights/#scans","title":"Scans","text":"<p>Scans section provides a clear overview of all scanning activities within a selected period. It helps users keep track of how many scans were performed and how many anomalies were detected during those scans. This section makes it easier to understand the scanning process and manage data by offering insight into how often scans occur. </p> <p></p> <p>Scan Runs show how often data scans are performed over a certain period. These scans check the quality of data across tables, views, and files, helping users monitor their data regularly and identify any issues. The process can be customized to scan tables or limit the number of records checked, ensuring that data stays accurate and up to standard.</p> <p></p> <p>Click on the caret down \ud83d\udd3d button to choose between viewing Anomalies Identified or Records Scanned, depending on your preference.</p> <p></p>"},{"location":"explore/insights/#anomalies-identified","title":"Anomalies Identified","text":"<p>Anomalies Identified shows the total number of anomalies detected during the scan runs. The bars in the graph allow users to compare the number of anomalies found across different days, helping them spot trends or irregularities in the data.</p> <p></p>"},{"location":"explore/insights/#records-scanned","title":"Records Scanned","text":"<p>Records Scanned shows the total number of records that were scanned during the scan runs. It gives users insight into how much data has been processed and allows them to compare the scanned records over the selected period.</p> <p></p>"},{"location":"explore/insights/#export","title":"Export","text":"<p>Export button allows you to quickly download the data from the Insights page. You can export data according to the selected Source Datastores, Tags, Report Date, and Timeframe. This makes it easy to save the data for offline use or share it with others.</p> <p></p> <p>After exporting, the data appears in a structured format, making it easy to save for offline use or to share with others.</p> <p></p>"},{"location":"explore/insights/#refresh","title":"Refresh","text":"<p>Refresh button allows users to quickly update the Insights data. When clicked, it fetches the latest information, ensuring that users always have the most up-to-date insights.</p> <p></p> <p>A label indicates when the page was last refreshed, helping users track data updates. This feature ensures accuracy and keeps the insights current without requiring a full page reload.</p> <p></p>"},{"location":"explore/observability/","title":"Observability","text":"<p>Observability provides a structured way to monitor data behavior, detect anomalies, and identify trends across datastores and tables. It supports consistent tracking through Measures and Metrics, including daily data volumes, data freshness, and specific field-level values measured against predefined thresholds. Automated scans, heatmaps, and visual insights make it easier to spot issues, set thresholds, and adjust monitoring settings to maintain data integrity.</p> <p>Note</p> <p>For more information please refer to the observability overview documentation</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"explore/observability/#manage-observability","title":"Manage Observability","text":"<p>In this section, you can manage the observability settings, including editing checks, thresholds, maximum ages, and marking checks as favorites.</p> <p>Note</p> <p>For more information please refer to the manage observability overview documentation</p>"},{"location":"explore/overview-of-explore/","title":"Explore","text":"<p>Explore page in  Qualytics is where you can easily view and manage all your data. It provides easy access to important features through tabs like Insights, Activity, Profiles, Observability, Checks, and Anomalies. Each tab shows a different part of your data, such as its quality, activities, structure, checks, and issues. You can sort and filter the data by datastore and time frame, making it easier to track performance, spot problems, and take action. The Explore section helps you manage and understand your data all in one place.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"explore/overview-of-explore/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the Explore button on the left side panel of the interface.</p> <p></p> <p>Step 2: After click on the Explore button, you will see the following tabs: Insights, Activity, Profiles, Observability, Checks, and Anomalies.</p> <p></p>"},{"location":"explore/overview-of-explore/#insights","title":"Insights","text":"<p>Insights tab provides a quick and clear overview of your data's health and performance. It shows key details like Quality Scores, active checks, profiles, scans, and anomalies in a simple and effective way. This makes it easy to monitor and track data quality, respond to issues, and take action quickly. Additionally, users can monitor specific source datastores and check for a particular report date and time frame.</p> <p>For more details on Insights, please refer to the Insights documentation.</p>"},{"location":"explore/overview-of-explore/#activity","title":"Activity","text":"<p>Activity tab provides a comprehensive view of all operations, helping users monitor and analyze the performance and workflows across various source datastores. Activity are categorized into Runs and Schedule operations, offering distinct insights into executed and scheduled activities.</p> <p>For more details on Activity, please refer to the Activity documentation.</p>"},{"location":"explore/overview-of-explore/#profiles","title":"Profiles","text":"<p>Profiles tab helps you explore and manage your containers and fields. With features like filtering, sorting, tagging, and detailed profiling, it provides a clear understanding of data quality and structure. This simplifies navigation and enhances data management for quick, informed decisions.</p> <p>For more details on Profiles, please refer to the Profiles documentation.</p>"},{"location":"explore/overview-of-explore/#observability","title":"Observability","text":"<p>Observability tab gives users an easy way to track changes in data volume over time. It introduces two types of checks: Volumetric and Metric. The Volumetric check automatically monitors the number of rows in a table and flags unusual changes, while the Metric check focuses on specific fields, providing more detailed insights from scan operations. Together, these tools help users spot data anomalies quickly and keep their data accurate.</p> <p>For more details on Observability, please refer to the Observability documentation.</p>"},{"location":"explore/overview-of-explore/#checks","title":"Checks","text":"<p>Checks tab provides a quick overview of the various checks applied across different tables and fields in multiple source datastores. In Qualytics, checks act as rules applied to data tables and fields to ensure accuracy and maintain data integrity. You can filter and sort the checks based on your preferences, making it easy to see which checks are active, in draft, or archived. This section is designed to simplify the review of applied checks across datasets without focusing on data quality or performance.</p> <p>For more details on Checks, please refer to the Checks documentation.</p>"},{"location":"explore/overview-of-explore/#anomalies","title":"Anomalies","text":"<p>Anomalies tab provides a quick overview of all detected anomalies across your source datastores. In Qualytics, An Anomaly refers to a dataset (record or column) that fails to meet specified data quality checks, indicating a deviation from expected standards or norms. These anomalies are identified when the data does not satisfy the applied validation criteria. You can filter and sort anomalies based on your preferences, making it easy to see which anomalies are active, acknowledged, or archived. This section is designed to help you quickly identify and address any issues.</p> <p>For more details on Anomalies, please refer to the Anomalies documentation. </p>"},{"location":"explore/profiles/","title":"Profiles","text":"<p>Profiles in Qualytics helps you explore and manage your containers and fields. With features like filtering, sorting, tagging, and detailed profiling, it provides a clear understanding of data quality and structure. This simplifies navigation and enhances data management for quick, informed decisions.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"explore/profiles/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the Explore button on the left side panel of the interface.</p> <p></p> <p>Step 2: Click on the \"Profiles\" from the Navigation Tab.</p> <p></p> <p>You will be navigated to the Profiles section. Here, you will see the data organized into two sections: Containers and Fields, allowing you to explore and analyze the datasets efficiently.</p> <p></p>"},{"location":"explore/profiles/#containers","title":"Containers","text":"<p>By selecting the Containers section, you can explore structured datasets that are organized as either JDBC or DFS containers. JDBC containers represent tables or views within relational databases, while DFS containers include files such as CSV, JSON, or Parquet, typically stored in distributed systems like Hadoop or cloud storage.</p> <p></p>"},{"location":"explore/profiles/#container-details","title":"Container Details","text":"<p>Containers section provides key details about each container, including the last profiled and last scanned dates. Hovering over the info icon for a specific container reveals these details instantly.</p> <p>Step 1: Locate the container you want to review, then hover over the info icon to view the container Details. </p> <p></p> <p>Step 2: A pop-up will appear with additional details about the container, such as the last profiled and last scanned dates.</p> <p></p>"},{"location":"explore/profiles/#explore-tables-and-fields","title":"Explore Tables and Fields","text":"<p>By clicking on a specific container, users can view its associated fields, including detailed profiling information. Additionally, clicking the arrow icon on the right side of a specific container allows users to navigate directly to its corresponding table for a more in-depth exploration.</p>"},{"location":"explore/profiles/#explore-fields","title":"Explore Fields","text":"<p>To explore the data within a container, you can view all its fields. This allows you to gain insights into the structure and quality of the data stored in the container.</p> <p>Step 1: Click on the specific container whose fields you want to preview.</p> <p>For demonstration purposes, we have selected the Replica of Netsuite Financials container.</p> <p></p> <p>Step 2: You will be directed to the fields of the selected container, where all the fields of the container will be displayed.</p> <p></p>"},{"location":"explore/profiles/#explore-tables","title":"Explore Tables","text":"<p>To explore the data in more detail, you can view the corresponding table of a selected container. This provides a comprehensive look at the data stored within, allowing for deeper analysis and exploration.</p> <p>Step 1: Click on the arrow icon on the right side of the container you want to preview.</p> <p></p> <p>Step 2: You will be directed to the corresponding table, providing a comprehensive view of the data stored in the container.</p> <p></p>"},{"location":"explore/profiles/#filter-and-sort","title":"Filter and Sort","text":"<p>Filter and Sort options allow you to organize your containers by various criteria, such as Name, Last Profiled, Last Scanned, Quality Score, Records, and Type. You can also apply filters to refine your list of containers based on Type.</p>"},{"location":"explore/profiles/#sort","title":"Sort","text":"<p>You can sort your containers by various criteria, such as Name, Last Profiled, Last Scanned, Quality Score, Records, and Type to easily organize and prioritize them according to your needs.</p> <p></p> No Sort By Description 1. Active Anomalies Sorts containers based on the number of currently active anomalies detected. 2. Active Checks Sorts containers by the number of active validation checks applied. 3. Completeness Sorts containers based on their data completeness percentage. 4. Created Date Sorts containers by the date they were created, showing the newest or oldest fields first. 5. Fields Sorts containers by the number of fields profiled. 6. Last Profiled Sorts by the most recent profiling container. 7. Last Scanned Sorts by the most recent scanned container. 8. Name Sorts containers alphabetically by their names. 9. Quality Score Sorts containers based on their quality score, indicating the reliability of the data in the field. 10. Records Sorts containers by the number of records profiled. 11. Type Sorts containers based on their data type (e.g., string, boolean, etc.). <p></p> <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.</p> <p></p>"},{"location":"explore/profiles/#filter","title":"Filter","text":"<p>You can filter your containers based on values like Type (Table, View, File, Computed Table and Computed File) to easily organize and prioritize them according to your needs.</p> <p></p> <p>Info</p> <p>Users can search across all filter inputs using typos, partial terms, or abbreviations. The system intelligently matches relevant results, making it easier to find what they need without exact inputs.</p> <p></p>"},{"location":"explore/profiles/#mark-as-favorite","title":"Mark as Favorite","text":"<p>Marking a container as a favorite allows you to quickly access and prioritize the containers that are most important to your work, ensuring faster navigation and improved efficiency.</p> <p>Step 1: Locate the container which you want to mark as a favorite and click on the bookmark icon located on the left side of the container.</p> <p></p> <p>After clicking on the bookmark icon your container is successfully marked as a favorite and a success flash message will appear stating \"The Table has been favorited\"</p> <p></p> <p>To unmark, simply click on the bookmark icon of the marked container. This will remove it from your favorites.</p> <p></p>"},{"location":"explore/profiles/#fields","title":"Fields","text":"<p>By selecting the Fields section in the Qualytics platform, you can view all the fields across your data sources, including their quality scores, completeness, and metadata, for streamlined data management.</p>"},{"location":"explore/profiles/#field-details","title":"Field Details","text":"<p>Field Details view in the Qualytics platform provides in-depth insights into a selected field. It displays key information, including the field\u2019s declared type, number of distinct values, minimum and maximum length of observed values, entropy, and unique/distinct ratio. This detailed profiling allows you to understand the field's data structure, quality, and variability, enabling better data governance and decision-making.</p> <p>Step 1: Click on the specific field whose field details you want to preview.</p> <p></p> <p>A modal window will appear, providing detailed information about the selected field, such as its declared type, distinct values, length range, the Last Profile timestamp to indicate when the field was last profiled, and more.</p> <p></p>"},{"location":"explore/profiles/#manage-tags-in-field-details","title":"Manage Tags in Field Details","text":"<p>Tags can now be directly managed in the field profile within the Explore section. Simply access the Field Details panel to create, add, or remove tags, enabling more efficient and organized data management.</p> <p>Step 1: Click on the specific field that you want to manage tags.</p> <p></p> <p>A Field Details modal window will appear. Click on the + button to assign tags to the selected field.</p> <p></p> <p>Step 2: You can also create the new tag by clicking on the \u2795 button.</p> <p></p> <p>A modal window will appear, providing the options to create the tag. Enter the required values to get started.</p> <p></p> <p>For more information on creating tags, refer to the Add Tag section.</p>"},{"location":"explore/profiles/#filter-and-sort_1","title":"Filter and Sort","text":"<p>Filter and Sort options allow you to organize your fields by various criteria, such as Active Anomalies, Active Checks, Completeness, Created Date, Name, Quality Score, and Type. You can also apply filters to refine your list of fields based on Profile and Type. </p>"},{"location":"explore/profiles/#sort_1","title":"Sort","text":"<p>You can sort your containers by various criteria, such as Active Anomalies, Active Checks, Completeness, Created Date, Name, Quality Score, and Type to easily organize and prioritize them according to your needs.</p> <p></p> No Sort By Description 1. Active Anomalies Sorts fields based on the number of currently active anomalies detected. 2. Active Checks Sorts fields by the number of active validation checks applied. 3. Completeness Sorts fields based on their data completeness percentage. 4. Created Date Sorts fields by the date they were created, showing the newest or oldest fields first. 5. Name Sorts fields alphabetically by their names. 6. Quality Score Sorts fields based on their quality score, indicating the reliability of the data in the field. 7. Type Sorts fields based on their data type (e.g., string, boolean, etc.). <p></p> <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.</p> <p></p>"},{"location":"explore/profiles/#filter_1","title":"Filter","text":"<p>You can filter your fields based on Profiles and Type to easily organize and prioritize them according to your needs.</p> No. Filter Description 1. Profile Filters fields based on the Profiles (e.g., accounts, accounts.csv,  etc.). 2. Type Filters fields based on the data type (e.g., string, boolean, date, etc.). <p></p>"},{"location":"flows/abort/","title":"Abort Flow","text":"<p>Users can abort a flow that is currently triggered or running to stop further execution.</p>"},{"location":"flows/abort/#abort-specific-flow","title":"Abort Specific Flow","text":"<p>Aborting a specific flow allows you to immediately stop a single flow execution that is currently in progress.</p> <p>Step 1: Open the flow execution that is currently in progress and select the settings icon.</p> <p></p> <p>Step 2: Click on the abort to stop the execution.</p> <p></p> <p>A success message appears confirming the flow is aborted.</p> <p>Alternatively, you can abort the flow from the Executions tab by clicking the Abort icon.</p> <p></p>"},{"location":"flows/abort/#abort-flow-in-bulk","title":"Abort Flow in Bulk","text":"<p>Aborting flows in bulk allows you to stop multiple running flow executions at once. This is useful when several executions need to be stopped at the same time.</p> <p>Step 1: Hover over the executions list and select the checkbox next to the flow executions you want to abort.</p> <p></p> <p>When multiple executions are selected, an action toolbar appears showing the total number of selected executions along with available bulk actions.</p> <p></p> <p>Step 2: Click on the vertical ellipsis (\u22ee), and select the Abort option from the action toolbar.</p> <p></p> <p>A confirmation prompt appears. Click Abort to confirm the action.</p> <p></p> <p>After aborting, a success message appears confirming that the selected flow executions have been aborted.</p>"},{"location":"flows/activate-flow/","title":"Activate Flow","text":"<p>Users can reactivate a flow that was previously deactivated. Once reactivated, the flow\u2019s triggers become active again, allowing it to run automatically based on the defined conditions.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the flow that you want to activate, then click on Activate from the dropdown menu.</p> <p></p> <p>After clicking the Activate button, a success notification appears confirming the activation.</p>"},{"location":"flows/add-flow/","title":"Create a New Flow","text":"<p>Flows allow you to automate actions based on data operations, anomaly detection, or manual execution. When you create a new Flow, Qualytics automatically adds the base nodes you need to begin configuring the automation.</p> <p>Note</p> <p>To create a Flow, you must have Manager role. For more details, see the Team Permissions guide.</p> <p>Step 1: Click on the Add Flow button from the top right corner.</p> <p></p> <p>A modal window will appear, allowing you to create a new Flow. Every Flow begins with two default nodes:</p> <ul> <li>Flow Node \u2013 Defines the Flow\u2019s name, description, and activation state  </li> <li>Trigger Node \u2013 Defines when the Flow will start</li> </ul> <p></p>"},{"location":"flows/add-flow/#what-you-configure-next","title":"What You Configure Next","text":"<p>A Flow consists of three main node types. After creating a new Flow, you will configure each one:</p>"},{"location":"flows/add-flow/#1-flow-entry","title":"1. Flow Entry","text":"<p>Defines the Flow\u2019s general information such as its name and description.</p> <p>Info</p> <p>If you want to learn more about flow entry, check out the Flow Entry guide.</p>"},{"location":"flows/add-flow/#2-trigger-node","title":"2. Trigger Node","text":"<p>Controls when the Flow starts (operation completes, anomaly detected, manual, etc.).  </p> <p>Info</p> <p>For more detailed steps, you can review the Trigger Node Documentation.</p>"},{"location":"flows/add-flow/#3-actions-node","title":"3. Actions Node","text":"<p>Defines what happens after the Flow is triggered (operations, notifications, HTTP calls, etc.).  </p> <p>Info</p> <p>To explore this in more depth, take a look at the Actions Node Documentation.</p>"},{"location":"flows/anomaly/","title":"Anomaly","text":"<p>Flows let you automate how anomalies are cleaned up over time. Under Anomaly actions, you can choose between two cleanup options:</p> <ul> <li>Archive \u2192 Move inactive anomalies out of the active queue  </li> <li>Delete \u2192 Permanently remove inactive anomalies  </li> </ul> <p>Use these actions to keep your anomaly list lean, relevant, and manageable\u2014without manually closing out each record.</p> <p></p>"},{"location":"flows/anomaly/#archive","title":"Archive","text":"<p>Use the Archive action to automatically move anomalies into an archived state when they\u2019ve been inactive for a set amount of time. This is ideal for organizations that prefer to keep a historical record but don\u2019t need old anomalies cluttering the active view.</p> <p></p>"},{"location":"flows/anomaly/#how-it-works","title":"How It Works","text":"<p>The Archive action identifies anomalies with no updates or comments during the chosen Inactivity Period and archives them using the reason you specify.</p> <p>\ud83d\udca1 Why archive instead of delete? Archiving lets you clean up the active queue without losing historical context.</p>"},{"location":"flows/anomaly/#inactivity-period","title":"Inactivity Period","text":"<p>Choose how long an anomaly must remain untouched before being archived.</p> <p>Options include:</p> <ul> <li>Week</li> <li>Month</li> <li>Quarter</li> <li>Year</li> </ul> <p></p> <p>Note</p> <p>Anomalies are archived automatically if they have no updates or comments during the selected inactivity period.</p>"},{"location":"flows/anomaly/#archive-reason","title":"Archive Reason","text":"<p>Define why these anomalies are being archived:</p> <ul> <li>Resolved \u2014 The anomaly represents a valid data quality issue that has been fixed.  </li> <li>Invalid \u2014 The anomaly wasn\u2019t a legitimate issue.  </li> <li>Discarded \u2014 The anomaly is no longer relevant or under review. (Most common for automation.)</li> </ul> <p></p>"},{"location":"flows/anomaly/#assign-additional-tags","title":"Assign Additional Tags","text":"<p>Automatically apply tags to anomalies archived through this flow. Useful for labeling cleanup runs (e.g., <code>auto-archive-q4</code>).</p> <p></p>"},{"location":"flows/anomaly/#comment-optional","title":"Comment (Optional)","text":"<p>Provide a note that will appear on every archived anomaly, giving future reviewers context.</p> <p></p>"},{"location":"flows/anomaly/#filter-conditions","title":"Filter Conditions","text":"<p>You can narrow down which anomalies should be archived:</p> <ul> <li>Source Datastores \u2014 Archive anomalies from selected datastores only  </li> </ul> <p></p> <ul> <li>Anomaly Tags \u2014 Archive anomalies labeled with specific tags  </li> </ul> <p></p>"},{"location":"flows/anomaly/#delete","title":"Delete","text":"<p>The Delete action permanently removes anomalies after a set inactivity period. Use this when you don't need to retain historical anomaly records\u2014e.g., for ephemeral data or noise cleanup.</p> <p></p> <p>Warning</p> <p>Delete permanently removes anomalies from the system. This cannot be undone.</p>"},{"location":"flows/anomaly/#how-it-works_1","title":"How It Works","text":"<p>The Delete action looks for anomalies that have had no updates or comments during the selected Inactivity Period and permanently removes them. This helps you clear out old or unnecessary anomalies that you no longer need.</p> <p>\u26a0\ufe0f Why delete instead of archive? Delete removes the anomaly completely, which is useful when you don\u2019t need to keep any record of it.</p>"},{"location":"flows/anomaly/#inactivity-period_1","title":"Inactivity Period","text":"<p>Define how long an anomaly must remain inactive before deletion.</p> <p>Options include:</p> <ul> <li>Week</li> <li>Month</li> <li>Quarter</li> <li>Year</li> </ul> <p></p> <p>Note</p> <p>Anomalies are deleted if they have no updates or comments during the selected inactivity period.</p>"},{"location":"flows/anomaly/#source-datastores","title":"Source Datastores","text":"<p>Delete anomalies originating from specific datastores.</p> <p></p>"},{"location":"flows/anomaly/#anomaly-statuses","title":"Anomaly Statuses","text":"<p>Target anomaly states such as:</p> <ul> <li>Active</li> <li>Acknowledged</li> <li>Resolved  </li> <li>Duplicate</li> <li>Invalid  </li> <li>Discarded  </li> </ul> <p></p>"},{"location":"flows/anomaly/#anomaly-tags","title":"Anomaly Tags","text":"<p>Delete anomalies that carry specific tags (e.g., <code>test-data</code>, <code>noise</code>).</p> <p></p>"},{"location":"flows/anomaly/#when-to-use-archive-vs-delete","title":"\ud83d\udcd8 When to Use Archive vs Delete","text":"Action Best for Keeps history? Suitable for Archive Cleaning up long-ignored anomalies while retaining context \u2705 Yes Audit needs, compliance workflows, long-term reference Delete Removing noise or temporary anomalies completely \u274c No Test data cleanup, non-critical anomalies, high-volume events"},{"location":"flows/clone-a-flow/","title":"Clone a Flow","text":"<p>Users can duplicate existing flows to simplify the reuse and modification of flow configurations for similar scenarios.</p> <p>Step 1: Click on the existing flow you want to clone.</p> <p></p> <p>Step 2: A new window will open displaying the flow's detailed configuration. Click the settings icon and select Clone.</p> <p></p> <p>Step 3: After selecting the clone button, click the Publish button to publish it.</p> <p></p> <p>Important</p> <p>Users with the Manager Role can create, edit, clone, or publish Flows. For details, see the Team Permission .</p> <p>After clicking the Publish button, a success notification appears confirming that the flow has been successfully added.</p>"},{"location":"flows/clone-an-action/","title":"Clone an Action","text":"<p>Users can duplicate an existing action in just a few clicks. Cloning an action allows you to quickly replicate its configuration without manually setting it up again.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) on the action you want to clone, then select the Clone option from the menu.</p> <p>Info</p> <p>You can add a maximum of 10 Operation Actions (such as Catalog, Profile, or Scan) in a single flow.</p> <p></p> <p>Step 2: After clicking the Clone option, a cloned action will be created.</p> <p></p>"},{"location":"flows/deactivate-flow/","title":"Deactivate Flow","text":"<p>Users can deactivate a flow to pause its triggers by disabling it. This prevents the flow from being executed until it is reactivated.</p> <p>Info</p> <p>To deactivate a Flow, you must have Manager role. Only users with this role can perform the deactivation action. For more details about permissions, refer to the permissions documentation.</p>"},{"location":"flows/deactivate-flow/#deactivate-specific-flow","title":"Deactivate Specific Flow","text":"<p>Deactivating a specific flow allows you to pause its triggers without deleting it. The flow will not run again until it is reactivated.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the flow that you want to deactivate, then click on Deactivate from the dropdown menu.</p> <p></p> <p>Note</p> <p>If you accidentally deactivate a flow, you can easily reactivate it by following the steps in the Activate Flow guide.</p> <p>After clicking the Deactivate button, a success notification appears confirming the deactivation.</p> <p>Warning</p> <p>Deactivating a flow only pauses its triggers \u2014 not deletes it. You can reactivate it anytime, or see the Delete Flow guide to remove it permanently.</p>"},{"location":"flows/deactivate-flow/#deactivate-flow-in-bulk","title":"Deactivate Flow in Bulk","text":"<p>Deactivating flows in bulk allows you to pause triggers for multiple flows at once.</p> <p>Step 1: Hover over the flows list and select the checkbox next to the flows you want to deactivate.</p> <p></p> <p>When multiple flows are selected, an action toolbar appears showing the total number of selected flows along with available bulk actions.</p> <p></p> <p>Step 2: Click Deactivate from the action toolbar.</p> <p></p> <p>After clicking Deactivate, a success notification appears confirming that the selected flows have been deactivated.</p>"},{"location":"flows/deactivate-flow/#faq","title":"FAQ","text":"<p>1. What happens if I deactivate a Flow?</p> <p>Once deactivated, the Flow\u2019s triggers stop executing until you reactivate it. Existing executions will not be affected.</p>"},{"location":"flows/delete-flow/","title":"Delete Flow","text":"<p>Delete Flow feature allows you to permanently remove unwanted or outdated flows from the system. This helps in maintaining a clean and organized list of active flows.</p> <p>Info</p> <p>To delete a Flow, you must have Manager role. Only users with this role can perform the deletion action. For more details about permissions, refer to the permissions documentation.</p>"},{"location":"flows/delete-flow/#delete-specific-flow","title":"Delete Specific Flow","text":"<p>Deleting a specific flow allows you to permanently remove an individual flow that is no longer needed. This helps keep your list of flows clean and up to date.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the flow that you want to delete, then click on Delete from the dropdown menu.</p> <p></p> <p>After clicking the delete button, a confirmation modal window Delete Flow will appear.</p> <p></p> <p>Step 2: Click on the Delete button to delete the flow.</p> <p></p> <p>After clicking the Delete button, a success notification appears confirming the deletion.</p> <p>Warning</p> <p>This action cannot be undone. If you only want to pause the flow instead of deleting it, see the Deactivate Flow guide.</p>"},{"location":"flows/delete-flow/#delete-flow-in-bulk","title":"Delete Flow in Bulk","text":"<p>Deleting flows in bulk allows you to permanently remove multiple flows at once. This is useful for quickly cleaning up outdated or unused flows.</p> <p>Step 1: Hover over the flows list and select the checkbox next to the flows you want to delete.</p> <p></p> <p>When multiple flows are selected, an action toolbar appears showing the total number of selected flows along with a vertical ellipsis (\u22ee) for additional actions.</p> <p></p> <p>Step 2: Click on the vertical ellipsis (\u22ee) and select Delete from the dropdown menu.</p> <p></p> <p>Step 3: A confirmation modal window appears, click on the Delete button to delete the selected flows.</p> <p></p> <p>After clicking the Delete button, a success notification appears confirming the deletion.</p>"},{"location":"flows/edit-flow/","title":"Edit Flow","text":"<p>Edit Flow feature lets users update existing flows by modifying configurations or adding actions.</p> <p>Info</p> <p>To edit a Flow, you must have Manager role. Only users with this role can perform the edit action. For more details about permissions, refer to the permissions documentation.</p> <p>Step 1: Click the flow you want to edit.</p> <p></p> <p>Step 2: After clicking the flow, a new window will open displaying the flow's detailed configuration. Click on the boxes you want to edit.</p> <p>For demonstration purposes we have selected the Flow node.</p> <p></p> <p>Tip</p> <p>If you make changes that you don\u2019t want to keep, you can undo them by discarding the unsaved configuration or refreshing the page before publishing.</p> <p>Step 3: Click the <code>Save</code> button to apply the updates.</p> <p></p> <p>Step 4: After clicking the <code>Save</code> button, click the <code>Publish</code> button located in the top right corner to finalize and publish the changes.</p> <p>Warning</p> <p>If the flow is not published, the setup will not be saved. Always publish the flow after editing to ensure your changes are retained.</p> <p></p>"},{"location":"flows/execute-manual-flows/","title":"Execute Manual Flows","text":"<p>Manual execution gives you complete control over when a Flow should run. You can start a manual Flow directly from the Flows list or from inside the Flow editor.</p>"},{"location":"flows/execute-manual-flows/#method-1-execute-from-the-flows-list","title":"Method 1: Execute From the Flows List","text":"<p>Step 1: Locate the manual flow in your list of flows.</p> <p></p> <p>Step 2: Click the vertical ellipsis (\u22ee) next to the manual flow you wish to execute, then select \"Execute\" from the dropdown menu to trigger the flow.</p> <p></p> <p>After clicking the Execute button, a success notification appears confirming that the flow has been successfully executed.</p>"},{"location":"flows/execute-manual-flows/#method-2-execute-from-inside-the-flow-editor","title":"Method 2: Execute From Inside the Flow Editor","text":"<p>You can also run a manual Flow directly from its editor page.</p> <p>Step 1: Click on the manual flow in your list to open it.</p> <p></p> <p>Step 2: You will be navigated to the Flow editor page; click the Execute button.</p> <p></p> <p>After clicking the Execute button, a success notification appears confirming that the flow has been successfully executed.</p>"},{"location":"flows/flow-entry/","title":"Flow Entry","text":"<p>The Flow Entry is the starting point of every Flow in Qualytics. Think of it as the \u201croot\u201d of your automation \u2014 it defines the Flow\u2019s name, purpose, and whether it is active. Once this node is set, you can continue building the rest of the Flow (Triggers, Actions, etc.).</p>"},{"location":"flows/flow-entry/#trigger-types","title":"Trigger Types","text":"<p>A Flow begins when one of its Triggers activates. Here are the six available trigger types:</p> Trigger Type Description Schedule Starts the Flow automatically based on a defined schedule (hourly, daily, weekly, monthly, or custom cron). Operation Completes Starts the Flow when a catalog, scan, or profile operation finishes. Anomalous Table or File Detected Starts the Flow when anomalies are detected in a table or file. Anomaly Detected Starts the Flow when a single anomaly event occurs. Anomaly Status Changed Starts the Flow when the status of an anomaly changes to a specified state (for example, Open, Acknowledged, or Resolved). Manual Starts the Flow only when the user manually executes it."},{"location":"flows/flow-entry/#configure-the-flow-node","title":"Configure the Flow Node","text":"<p>Step 1: Click on the Flow node.  </p> <p></p> <p>A panel will appear on the right-hand side, allowing you to:</p> No. Field Name Description 1. Name Enter the name for the flow. 2. Description Provide a brief description of the flow (optional) to clarify its purpose or functionality. 3. Deactivated Check the box to deactivate the flow. If selected, the flow won't start even if the trigger conditions are met. <p></p> <p>Step 2: Once the details are filled in, click the Save button to save the flow settings.  </p> <p></p>"},{"location":"flows/http/","title":"HTTP","text":"<p>Users can connect to external apps for notifications using one of these services:</p> <ul> <li> <p>Webhook.</p> </li> <li> <p>HTTP Action.</p> </li> </ul> <p></p>"},{"location":"flows/http/#webhook","title":"Webhook","text":"<p>Qualytics allows you to connect external apps for notifications using webhooks, making it easy to stay updated in real time. When you set up a webhook, it sends an instant alert to the connected app whenever a specific event or condition occurs. This means you can quickly receive notifications about important events as they happen and respond right away. By using webhook notifications, you can keep your system running smoothly, keep everyone informed, and manage your operations more efficiently.</p> <p>Step 1: Click on Webhook.</p> <p></p> <p>A Webhook Settings panel will appear on the right-hand side, enabling users to configure and send webhook notifications.</p> <p></p> No. Field Description 1. Webhook URL Enter the desired \"Webhook URL\" of the target system where you want to receive notifications. 2. Message Text area to customize the notification message content with dynamic placeholders like <code>{{ flow_name }}</code>, <code>{{ operation_type }}</code>, and <code>{{ operation_result }}</code>. <p></p> <p>Step 2: Click on the \"Test HTTP\" button to send a test notification to the webhook URL you provided. If the webhook URL is correct, you will receive a confirmation message saying \"Notification successfully sent.\" This indicates that the webhook is functioning correctly.</p> <p></p> <p>Step 3: Once you have entered all the values, then click on the Save button.</p> <p></p>"},{"location":"flows/http/#http-action","title":"HTTP Action","text":"<p>Integrating HTTP Action notifications allows users to receive timely updates or alerts directly to a specified server endpoint. By setting up HTTP Action notifications with specific trigger conditions, you can ensure that you are instantly informed about critical events, such as operation completions or anomalies detected. This approach enables you to take immediate action when necessary, helping to address issues quickly and maintain the smooth and efficient operation of your processes.</p> <p>Step 1: Click on HTTP Action.</p> <p></p> <p>An HTTP Action Settings panel will appear on the right-hand side, enabling users to configure and send HTTP Action notifications.</p> <p></p> <p>Step 2: Enter the following detail where you want the notification to be sent.</p> <p>1. Action URL: Enter the \u201cAction URL\u201d in this field. It specifies the server endpoint for the HTTP request and defines where data will be sent or retrieved. It must be correctly formatted and accessible, including the protocol (http or https), domain, and path.</p> <p>2. HTTP Verbs: HTTP verbs specify the actions performed on server resources. Common verbs include:</p> <ul> <li>POST: Use POST to send data to the server to create something new. For example, it's used for submitting forms or uploading files. The server processes this data and creates a new resource.  </li> <li>PUT: Updates or creates a resource, replacing it entirely if it already exists. For example, updating a user\u2019s profile information or creating a new record with specific details.  </li> <li>GET: Retrieves data from the server without making any modifications. For example, requesting a webpage or fetching user details from a database.</li> </ul> <p>3. Username: Enter the username needed for authentication.</p> <p>4. Auth Type: This field specifies how to authenticate requests. Choose the method that fits your needs:</p> <ul> <li>Basic: Uses a username and password sent with each request. Example: \u201cAuthorization: Basic \u201d. </li> <li>Bearer: Uses a token included in the request header to access resources. Example: \u201cAuthorization: Bearer &lt; token &gt;\u201d. </li> <li>Digest: Provides a more secure authentication method by using a hashed combination of the username, password, and request details. Example: Authorization: Digest username=\" \", realm=\" \", nonce=\" \", uri=\" \", response=\" \".</li> </ul> <p>5. Secret: Enter the password or token used for authentication. This is paired with the Username and Auth Type to securely access the server. Keep the secret confidential to ensure security.</p> <p>6. Message: Enter your custom message using variables in the Message field, where you can specify the content of the notification that will be sent out.</p> <p></p> <p>Tip</p> <p>You can write your custom notification message by utilizing the autocomplete feature. This feature allows you to easily insert internal variables such as <code>{{ flow_name }}</code>, <code>{{ operation_type }}</code>, and <code>{{ datastore_name }}</code>. As you start typing, the autocomplete will suggest and recommend relevant variables in the dropdown.</p> <p>Step 3: Click the \"Test HTTP\" button to verify the correctness of the Action URL. If the URL is correct, a confirmation message saying \"Notification successfully sent\" will appear, confirming that the HTTP action is set up and functioning properly.</p> <p></p> <p>Step 4: Once you have entered all the values, then click on the Save button.</p> <p></p> <p>Step 5: After completing all the required details in the \"Add Flow\" section, click on the Publish button to finalize the process.</p> <p></p> <p>After clicking the Publish button, a success notification appears confirming that the flow has been successfully added.</p>"},{"location":"flows/move-an-action-node/","title":"Move an Action Node","text":"<p>Users can move an existing action node within a flow to change its position and execution order. The action node moves along with its downstream actions, helping reorganize the flow without recreating actions or breaking the flow logic.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) on the action you want to move, then select Move from the menu.</p> <p></p> <p>Step 2: Drag the action to the desired location in the flow and drop it to confirm. The action and all of its downstream actions move together. Visual indicators show valid and invalid drop locations.</p> <p>Info</p> <p>Moving an action preserves the flow hierarchy and does not affect the existing configuration.</p> <p></p>"},{"location":"flows/notification-tokens/","title":"Notification Tokens","text":"<p>Qualytics allows you to customize notification messages using dynamic variables (tokens). These tokens are automatically replaced with real values when a Flow is triggered.</p> <p>Important</p> <p>The available notification tokens depend on the Flow trigger type. Only the tokens listed for a specific trigger will work in notification messages.</p> <p>Warning</p> <p>Manual and Scheduled Flow trigger types do not support notification tokens. Notification messages for these trigger types must use static text only. Any tokens included in the message will not be populated.</p>"},{"location":"flows/notification-tokens/#anomaly-triggered-flow-tokens","title":"\ud83d\udd14 Anomaly-Triggered Flow Tokens","text":"<p>The following tokens are available when a Flow is triggered by an anomaly.</p> Token Description <code>{{ flow_name }}</code> Name of the Flow <code>{{ datastore_name }}</code> Datastore where the anomaly occurred <code>{{ datastore_link }}</code> Link to the datastore <code>{{ container_name }}</code> Container (table or file) where the anomaly was detected <code>{{ container_link }}</code> Link to the container <code>{{ anomaly_type }}</code> Type of anomaly detected <code>{{ anomaly_message }}</code> System-generated message describing the anomaly <code>{{ check_description }}</code> Description of the check that detected the anomaly <code>{{ target_link }}</code> Direct link to view the anomaly details"},{"location":"flows/notification-tokens/#partition-scan-trigger-tokens","title":"\ud83d\udd0d Partition Scan Trigger Tokens","text":"<p>The following tokens are available when a Flow is triggered by a partition scan.</p> Token Description <code>{{ flow_name }}</code> Name of the Flow <code>{{ datastore_name }}</code> Datastore where the scan was executed <code>{{ datastore_link }}</code> Link to the datastore <code>{{ container_name }}</code> Container (table or file) that was scanned <code>{{ container_link }}</code> Link to the container <code>{{ scan_target_name }}</code> Partition name that was scanned (table/view for JDBC sources, file name for DFS sources) <code>{{ anomaly_count }}</code> Number of anomalies detected during the scan <code>{{ anomaly_message }}</code> Message describing the scan results <code>{{ check_description }}</code> Description of the check <code>{{ target_link }}</code> Direct link to view the scan results"},{"location":"flows/notification-tokens/#operation-triggered-flow-tokens","title":"\u2699\ufe0f Operation-Triggered Flow Tokens","text":"<p>The following tokens are available when a Flow is triggered by an operation.</p> Token Description <code>{{ flow_name }}</code> Name of the Flow <code>{{ datastore_name }}</code> Datastore involved in the operation <code>{{ datastore_link }}</code> Link to the datastore <code>{{ operation_type }}</code> Type of operation executed (Catalog, Profile, Scan) <code>{{ operation_result }}</code> Result of the operation (Success or Failure) <code>{{ operation_message }}</code> Message describing the operation execution <code>{{ target_link }}</code> Direct link to view operation details"},{"location":"flows/notification-tokens/#anomaly-archive-trigger-tokens","title":"\ud83d\uddc4\ufe0f Anomaly Archive Trigger Tokens","text":"<p>The following tokens are available when a Flow is triggered by anomaly archiving due to inactivity.</p> Token Description <code>{{ flow_name }}</code> Name of the Flow <code>{{ datastore_name }}</code> Datastore where anomalies were archived <code>{{ datastore_link }}</code> Link to the datastore <code>{{ anomaly_count }}</code> Number of anomalies archived <code>{{ inactivity_timeframe }}</code> Number of days of inactivity before archiving <code>{{ target_link }}</code> Direct link to view archived anomalies"},{"location":"flows/notification-tokens/#anomaly-delete-trigger-tokens","title":"\ud83d\uddd1\ufe0f Anomaly Delete Trigger Tokens","text":"<p>The following tokens are available when a Flow is triggered by permanent anomaly deletion due to inactivity.</p> Token Description <code>{{ flow_name }}</code> Name of the Flow <code>{{ datastore_name }}</code> Datastore where anomalies were deleted <code>{{ datastore_link }}</code> Link to the datastore <code>{{ anomaly_count }}</code> Number of anomalies deleted <code>{{ inactivity_timeframe }}</code> Number of days of inactivity before deletion <code>{{ target_link }}</code> Direct link to view deletion details <p>Note</p> <p>Notification tokens appear in the autocomplete menu only if they are valid for the selected Flow trigger type. If a token does not appear in the list, it is not supported for that trigger.</p>"},{"location":"flows/notification/","title":"Notification","text":"<p>Users can configure the application to send notifications through various channels. The available notification options include:</p> <ul> <li> <p>In App.</p> </li> <li> <p>Email.</p> </li> <li> <p>Slack.</p> </li> <li> <p>Microsoft Teams.</p> </li> <li> <p>PagerDuty.</p> </li> </ul> <p></p>"},{"location":"flows/notification/#in-app","title":"In App","text":"<p>This will send an app notification to all users that use Qualytics. Users can set a custom message using variables and modify the standard text.</p> <p>Step 1: Click on In App.</p> <p></p> <p>A panel In App Settings will appear on the right-hand side, allowing you to configure the notification message.</p> <p></p> <p>Message: Enter your custom message using variables in the Message field, where you can specify the content of the notification that will be sent out.</p> <p></p> <p>Tip</p> <p>You can write your custom notification message by utilizing the autocomplete feature. This feature allows you to easily insert internal variables such as <code>{{ flow_name }}</code>, <code>{{ container_name }}</code>, and <code>{{ datastore_name }}</code>. As you start typing, the autocomplete will suggest and recommend relevant variables in the dropdown.</p> <p>Step 2: After configuring the message, click Save to finalize the settings.</p> <p></p>"},{"location":"flows/notification/#email","title":"Email","text":"<p>Adding email notifications allows users to receive timely updates or alerts directly in their inbox. By setting up notifications with specific triggers and channels, you can ensure that you are promptly informed about critical events, such as operation completions or detected anomalies. This proactive approach allows you to take immediate action when necessary, helping to address issues quickly and maintain the smooth and efficient operation of your processes.</p> <p>Step 1: Click on Email.</p> <p></p> <p>A panel Email Settings will appear on the right-hand side, allowing you to add email addresses, specify an email subject, and configure the notification message.</p> <p></p> No. Field Description 1. Email Address Enter the email address where the notification should be sent. 2. Email Subject Enter the subject line of the notification email to help recipients identify its purpose. 3. Message Text area to customize the notification message content with dynamic placeholders like <code>{{ flow_name }}</code>, <code>{{ operation_type }}</code>, and <code>{{ operation_result }}</code>. <p></p> <p>Step 2: Click the Test Notification button to send a test email to the provided address. If the email is successfully sent, you will receive a confirmation message indicating Notification successfully sent.</p> <p></p> <p>Step 3: Once all fields are configured, click the Save button to finalize the email notification setup.</p> <p></p>"},{"location":"flows/notification/#notification-message-variables","title":"Notification Message Variables","text":"<p>Qualytics allows you to customize notification messages using dynamic variables (tokens). These tokens are automatically replaced with real values when a Flow is triggered.</p> <p>Note</p> <p>For more detailed information, review the notification tokens documentation.</p>"},{"location":"flows/notification/#slack","title":"Slack","text":"<p>Qualytics integrates with Slack to deliver real-time notifications on scan completions, anomalies, and operational statuses, ensuring teams stay informed and can act quickly. With this integration, users receive instant alerts for system events, monitor scan results, and manage data anomalies directly within Slack. They can view notifications, acknowledge issues, and take necessary actions without switching platforms.</p> <p>Step 1: Click on Slack.</p> <p></p> <p>A Slack Settings panel appears on the right side of the screen.</p> <p></p> No. Field Description 1. Channel Choose the channel where notifications should be sent using the Channel dropdown. For demonstration purposes, the channel #demo is selected. 2. Preview Shows a preview of the Slack notification that will be sent when the flow runs. <p></p> <p>Step 2: Click the Test Notification button to send a sample notification to the selected Slack channel.</p> <p></p> <p>A prompt appears stating Notification successfully sent once the notification is successfully delivered.</p> <p></p> <p>Step 3: Once the notification is successfully sent, check your connected Slack workspace to ensure it is linked to Qualytics. You will see the test notification in the selected Slack channel.</p> <p>Note</p> <p>Each trigger generates a different type of Slack notification message. The content and format of the message vary based on the specific trigger event.</p> <p></p> <p>Step 4: After confirming that the notification was received successfully, return and click the Save button.</p> <p></p>"},{"location":"flows/notification/#examples-of-trigger-messages","title":"Examples of Trigger Messages","text":"<p>Trigger messages in Slack provide real-time notifications for various system events, ensuring timely awareness and action. Each trigger message follows a unique format and conveys different types of information based on the operation performed. Below are examples highlighting distinct scenarios:</p> <p>Scenario 1: Scan Completion Notification</p> <p>When a data cataloging or scan operation completes successfully, a notification is sent to Slack. The message includes details such as the dataset name, operation type (e.g., Catalog Operation), and the result of the operation. </p> <p></p> <p>Scenario 2: Anomalous Table or File Detected</p> <p>When a scan detects a critical data anomaly, Slack sends a detailed notification highlighting the issue. The notification includes the dataset name, flow (such as Quality Monitor), and source datastore. It also provides a summary of the anomaly, specifying the number of records that differ between datasets and the container where the discrepancy was found. Additionally, the message offers an option to view detailed results.</p> <p></p> <p>Scenario 3: Anomaly Detected</p> <p>When a scan detects record anomalies, Slack sends a notification highlighting the affected container, flow, and source datastore. It specifies the number of records that differ between datasets and provides options to view or acknowledge the anomaly.</p> <p></p>"},{"location":"flows/notification/#managing-qualytics-alerts-in-slack","title":"Managing Qualytics Alerts in Slack","text":"<p>Qualytics Slack integration enables real-time monitoring and quick action on data quality issues directly from Slack. This guide outlines the different types of alerts and the actions you can take without leaving Slack.</p> <p>When an Operation Success or failure </p> <p>Step 1: A Slack notification confirms the scan completion with a Success/failure status.</p> <p>For demonstration purposes we are using Success operation.</p> <p></p> <p>Step 2: Click View Operation to be redirected automatically to the result section in Qualytics.</p> <p></p> <p>When an Anomalous File or Table is Detected </p> <p>Step 1: A Slack alert notifies about anomalies in a dataset.</p> <p></p> <p>Step 2: Click View Results to examine the identified discrepancies directly in Qualytics.</p> <p></p> <p>When a Record Anomaly is Detected</p> <p>If a shape or record anomaly is found, you'll receive a Slack notification. You can take the following actions:</p> <p></p> <ul> <li>View Anomaly \u2013  Click on view anomaly to open the details in Qualytics to investigate further.  </li> </ul> <p></p> <ul> <li>Acknowledge \u2013 Click on Acknowledge to mark it as reviewed to avoid duplicate alerts.  </li> </ul> <p> </p> <ul> <li>Horizontal ellipsis(\u22ef)  \u2013 Click on horizontal ellipsis.</li> </ul> <p></p> <p>A dropdown will open with option comment and archive :  </p> <p></p> No. Action Description 1. Comment Add Comment to collaborate with your team. 2. Archive Archive if no further action is needed."},{"location":"flows/notification/#microsoft-teams","title":"Microsoft Teams","text":"<p>Step 1: Click on Microsoft Teams.</p> <p></p> <p>A panel Microsoft Teams Settings will appear on the right-hand side, allowing you to add a webhook url and configure the notification message.</p> <p></p> No. Field Description 1. Teams Webhook URL Enter the Teams webhook URL where the notification should be sent. 2. Message Text area to customize the notification message content with dynamic placeholders like <code>{{ flow_name }}</code>, <code>{{ operation_type }}</code>, and <code>{{ operation_result }}</code>. <p></p> <p>Step 2: Click the \"Test Notification\" button to send a test message to the provided \u201cWebhook URL\u201d. If the message is successfully sent, you will receive a confirmation notification indicating \"Notification successfully sent\".</p> <p></p> <p>Step 3: Once all fields are configured, click the Save button to finalize the Microsoft Teams notification setup.</p> <p></p>"},{"location":"flows/notification/#pagerduty","title":"PagerDuty","text":"<p>Integrating PagerDuty with Qualytics ensures that your team gets instant alerts for critical data events and system issues. With this connection, you can automatically receive real-time notifications about anomalies, operation completions and other important events directly in your PagerDuty account. By categorizing alerts based on severity, it ensures the right people are notified at the right time, speeding up decision-making and resolving incidents efficiently. This helps your team respond quickly to issues, reducing downtime and keeping data operations on track.</p> <p>Step 1: Click on PagerDuty.</p> <p></p> <p>A PagerDuty Settings panel will appear on the right-hand side, enabling users to configure and send PagerDuty notifications.</p> <p></p> <p>Integration Key: Enter the Integration Key where you want the notification to be sent.</p> <p></p> <p>Severity: Select the appropriate PagerDuty severity level to categorize incidents based on their urgency and impact. The available severity levels are:</p> <ul> <li> <p>Info: For informational messages that don't require immediate action but provide helpful context.</p> </li> <li> <p>Warning: For potential issues that may need attention but aren't immediately critical.</p> </li> <li> <p>Error: For significant problems that require prompt resolution to prevent disruption.</p> </li> <li> <p>Critical: For urgent issues that demand immediate attention due to their severe impact on system operations.</p> </li> </ul> <p></p> <p>Message: Enter your custom message using variables in the Message field, where you can specify the content of the notification that will be sent out.</p> <p></p> <p>Tip</p> <p>You can write your custom notification message by utilizing the autocomplete feature. This feature allows you to easily insert internal variables such as <code>{{ flow_name }}</code>, <code>{{ operation_type }}</code>, and <code>{{ datastore_name }}</code>. As you start typing, the autocomplete will suggest and recommend relevant variables in the dropdown.</p> <p>Step 2: Click on the Test notification button to check if the integration key is functioning correctly. Once the test notification is sent, you will see a success message, \"Notification successfully sent.\"</p> <p></p> <p>Step 3: Once you have entered all the values, then click on the Save button.</p> <p></p>"},{"location":"flows/notification/#faq","title":"FAQ","text":"<p>1. Can I test notifications before publishing?</p> <p>Yes. Each notification channel\u2014Email, Slack, Teams, PagerDuty, and HTTP\u2014includes a Test Notification button that allows you to send a sample message before publishing the flow.</p>"},{"location":"flows/operations/","title":"Operation","text":"<p>Users can execute specific operations when the trigger activates. They can choose from the following options:</p> <ul> <li> <p>Catalog.</p> </li> <li> <p>Profile.</p> </li> <li> <p>Scan.</p> </li> <li> <p>Export.</p> </li> <li> <p>Materialize.</p> </li> </ul> <p></p> <p>Warning</p> <p>Only initialized datastores appear in the Source Datastore dropdown across all operation types. A datastore becomes initialized after it has successfully completed Catalog, Profile, and Scan runs at least once within the Action Node Operation section while setting up your flow.</p> <p></p>"},{"location":"flows/operations/#catalog","title":"Catalog","text":"<p>Step 1: Click on Catalog. </p> <p></p> <p>A panel Catalog Settings will appear on the right-hand side. This window allows you to configure the catalog operation.</p> No. Field Description 1. Source Datastore Select the source datastore to catalog. 2. Prune Checkbox to enable or disable the removal of named collections (tables, views, files, etc.) that no longer exist in the datastore. 3. Recreate Checkbox to enable or disable the recreation of previously deleted named collections in Qualytics for the catalog. 4. Include Checkboxes to select Tables, Views, or both, specifying the resources to include in the catalog. <p></p> <p>Step 2: After configuring the settings, click Save to apply and proceed with the catalog operation.</p> <p></p>"},{"location":"flows/operations/#profile","title":"Profile","text":"<p>Step 1: Click on Profile. </p> <p></p> <p>A panel Profile Settings will appear on the right-hand side. This window allows you to configure the Profile operation.</p> <p></p> No. Field Description 1. Source Datastore Select the source datastore to profile. 2. Select Tables Allows users to select all tables, specific tables, or tables associated with selected tags to profile. 3. Read Settings Configure the starting point for profiling and set a maximum record limit per table for profiling. 4. Inference Settings Set the level of automated checks and decide whether inferred checks should be saved in draft mode. <p></p> <p>Step 2: Click Save to finalize the profile configuration.</p> <p></p>"},{"location":"flows/operations/#scan","title":"Scan","text":"<p>Step 1: Click on Scan.</p> <p></p> <p>A panel Scan Settings will appear on the right-hand side. This window allows you to configure the Scan operation.  </p> <p></p> <p>Source Datastore: Select the datastore to be scanned.</p> <p></p> <p>Select Tables: Choose all tables, specific tables, or tables associated with selected tags to include in the scan.</p> <p></p> <p>Select Check Categories: Select categories of checks to include, such as table properties (Metadata) or value checks (Data Integrity).</p> <p></p> <p>Read Settings: Define the scan strategy: incremental scans updated records; full scans process all records.</p> <p></p> <p>Starting Threshold: Set a starting point for scanning based on an incremental identifier.</p> <p></p> <p>Record Limit: Specify the maximum number of records to scan per table.</p> <p></p> <p>Scan Settings: Choose how to manage duplicate or recurring anomalies by archiving overlaps or reactivating previously archived anomalies with fingerprint tracking.</p> <p></p> <p>Maximum Record Anomalies per Check: Set the Rollup Threshold to limit how many anomalies are created per check. When the limit is reached, anomalies will be merged into one for easier management.</p> <p></p> <p>Maximum Source Examples per Anomaly: Sets how many source records are kept per anomaly during a scan. For example, if this is set to 10, only 10 records per anomaly will be saved or downloaded. Increase this value before running the scan to access more records.</p> <p></p> <p>Step 2: Click Save to finalize the scan configuration.</p> <p></p>"},{"location":"flows/operations/#export","title":"Export","text":"<p>Step 1: Click on Export.</p> <p></p> <p>A panel Export Settings will appear on the right-hand side. This window allows you to configure the Export settings.</p> <p></p> <p>Source Datastore: Select the datastore to export data from.</p> <p></p> <p>Select file patterns to export: All (all file patterns, including future ones), Specific (manually chosen file patterns), or Tag (file patterns based on selected tags).</p> <p></p> <p>Select Metadata: Choose metadata to export anomalies, quality checks, or field profiles. Anomalies detect data issues, quality checks validate data, and field profiles store field metadata.</p> <p></p> <p>Step 2: Click Save to finalize the export configuration.</p> <p></p> <p>Export nodes display the asset type in their titles (e.g., \u201cExport Anomalies\u201d) to help you identify the exported content easily.</p> <p></p>"},{"location":"flows/operations/#materialize","title":"Materialize","text":"<p>Step 1: Click on Materialize.</p> <p></p> <p>A panel Materialize Settings will appear on the right-hand side. This window allows you to configure the Materialize settings.</p> <p></p> <p>Source Datastore: Select the datastore to materialize data from.</p> <p></p> <p>Select Tables: Choose which tables (all, specific, or tagged) to extract from your source datastore and export to the enrichment datastore.</p> <p></p> <p>Read Settings: Select the record limit to control how much data is materialized per table.</p> <p></p> <p>Step 2: Click Save to finalize the materialize configuration.</p> <p></p>"},{"location":"flows/overview-action/","title":"Action Node","text":"<p>Actions define the specific steps the system will execute after a flow is triggered. They allow users to automate tasks, send notifications, or interact with external systems.</p> <p>Step 1: After completing the \"Trigger\" node setup, users can click on the \"Actions\" node.  </p> <p></p> <p>A panel will appear on the right-hand side displaying the list of available actions. These actions define what the system will execute after the flow is triggered. The actions are categorized into four groups:</p> <ul> <li> <p>Operations.</p> </li> <li> <p>Notifications.</p> </li> <li> <p>HTTP.</p> </li> <li> <p>Ticketing.</p> </li> </ul> <p></p> <p>Info</p> <p>Inline summaries are shown within action nodes, displaying key details based on the action type\u2014for example, datastore names for operations, Slack or Teams channels for notifications, and webhook URLs for HTTP actions. This enhancement provides quick clarity during flow configuration.</p>"},{"location":"flows/overview-action/#operations","title":"Operations","text":"<p>Note</p> <p>For more detailed information, review the operations documentation.</p>"},{"location":"flows/overview-action/#notifications","title":"Notifications","text":"<p>Note</p> <p>For more detailed information, review the notifications documentation.</p>"},{"location":"flows/overview-action/#notification-message-variables","title":"Notification Message Variables","text":"<p>Note</p> <p>For more detailed information, review the notification tokens documentation.</p>"},{"location":"flows/overview-action/#http","title":"HTTP","text":"<p>Note</p> <p>For more detailed information, review the http documentation.</p>"},{"location":"flows/overview-action/#ticketing","title":"Ticketing","text":"<p>Note</p> <p>For more detailed information, review the ticketing documentation.</p>"},{"location":"flows/overview-action/#faq","title":"FAQ","text":"<p>1. Can I have multiple actions under a single flow?</p> <p>Yes. You can chain multiple actions\u2014such as notifications, operations, or HTTP steps\u2014under a single flow to perform sequential or parallel tasks.</p>"},{"location":"flows/overview/","title":"Flows","text":"<p>Flows in Qualytics help you automate everyday data tasks so you don\u2019t have to do them manually. Each Flow is a chain of connected steps that runs automatically \u2014 it starts when something happens (like a data scan finishing) and then performs one or more actions, such as sending a notification or running another check. This makes your work faster, more consistent, and less prone to errors, keeping your data processes running smoothly even when you\u2019re not monitoring them.</p>"},{"location":"flows/overview/#why-you-need-flows","title":"Why You Need Flows","text":"<p>Modern data pipelines involve dozens of repetitive steps \u2014 from scanning data and profiling tables to alerting the right people when something breaks. Doing all this manually can be time-consuming and prone to errors.</p> <p>Flows automate this entire process by:</p> <ul> <li>Reducing manual work: Automatically run follow-up operations when specific events occur.  </li> <li>Maintaining consistency: Ensure all datastores follow the same workflow and rules.  </li> <li>Improving response time: Notify teams instantly about scan results or detected anomalies.  </li> <li>Centralizing control: Manage triggers and actions from one place instead of multiple scripts or manual tasks.</li> </ul>"},{"location":"flows/overview/#how-flows-work","title":"How Flows Work","text":"<p>Every Flow in Qualytics is made up of three key parts that work together to automate your process:</p>"},{"location":"flows/overview/#flow-node","title":"Flow Node","text":"<p>Defines the name, purpose, and activation state of your Flow. This is where you decide whether the Flow is active or paused.</p>"},{"location":"flows/overview/#trigger-node","title":"Trigger Node","text":"<p>Decides when your Flow starts. Triggers can be:</p> <ul> <li>Operation-based: Starts after a Catalog, Profile, or Scan completes.  </li> <li>Anomaly-based: Starts when Qualytics detects data issues.  </li> <li>Manual: Starts only when you trigger it yourself.</li> </ul>"},{"location":"flows/overview/#action-nodes","title":"Action Node(s)","text":"<p>Controls what happens next once the trigger fires. Actions can:</p> <ul> <li>Run operations like Scan or Export.  </li> <li>Send alerts via Email, Slack, Teams, or PagerDuty.  </li> <li>Call external systems using webhooks or API requests.</li> </ul> <p>Each Flow runs in order \u2014 from Trigger \u2192 Action(s) \u2014 and you can track its progress or results in the Execution tab.</p>"},{"location":"flows/overview/#example","title":"Example","text":"<p>Imagine you manage a retail company\u2019s product database that gets updated every night.</p> <p>You can create a Flow like this:</p> <ul> <li>Trigger: When a Catalog Operation Completes on your datastore.  </li> <li>Action 1: Automatically run a Profile to detect schema or data type changes.  </li> <li>Action 2: Run a Scan to check for anomalies in product prices or availability.  </li> <li>Action 3: Send a Slack Notification to your team if anomalies are detected.</li> </ul> <p>This setup ensures every new data load is automatically checked, issues are flagged instantly, and your team can respond right away \u2014 without any manual effort.</p>"},{"location":"flows/overview/#navigation-to-flows","title":"Navigation to Flows","text":"<p>Step 1: Log in to your Qualytics account and click on Flows on the left side panel of the interface.  </p> <p></p> <p>You will navigate to the Flows interface, where you can add and manage flows. At the top, you will see two tabs:</p> <ul> <li>Definitions: Displays a list of all flows along with details like triggers, actions, tags, and the last triggered time.</li> </ul> <p></p> <ul> <li>Executions: Provides the execution history of flows, including their status and timestamps.</li> </ul> <p></p>"},{"location":"flows/overview/#add-flow","title":"Add Flow","text":"<p>Allows users to create a new Flow, starting with default nodes to define triggers and actions.</p> <p>Step 1: Click on the Add Flow button from the top right corner.</p> <p>Note</p> <p>For more detailed information, review the add flow documentation.</p>"},{"location":"flows/overview/#flow-node_1","title":"Flow Node","text":"<p>Defines the core of a Flow, including its name, description, and activation status.</p> <p>Step 1: Click on the Flow node.  </p> <p>Note</p> <p>For more detailed information, review the flow entry documentation.</p>"},{"location":"flows/overview/#trigger-node_1","title":"Trigger Node","text":"<p>Determines when the Flow should start, based on events, conditions, or manual activation.</p> <p>Step 1: After completing the \"Flow\" node setup, users can click on the \"Trigger\" node.</p> <p>Note</p> <p>For more detailed information, review the trigger node documentation.</p>"},{"location":"flows/overview/#actions-node","title":"Actions Node","text":"<p>Actions define the specific steps the system will execute after a flow is triggered. They allow users to automate tasks, send notifications, or interact with external systems.</p> <p>Note</p> <p>For more detailed information, review the action node overview documentation.</p>"},{"location":"flows/overview/#view-and-track-flow-execution","title":"View and Track Flow Execution","text":"<p>Once a flow is added, it will be visible in the Definitions tab, where you can view all the created flows.</p> <p>Note</p> <p>For more detailed information, review the view and track flow execution documentation.</p>"},{"location":"flows/overview/#execution-tab","title":"Execution Tab","text":"<p>Execution tab allows users to view the execution history and current status of a flow. It provides detailed timestamps, status updates, and a comprehensive record of flow executions for efficient tracking and analysis.</p> <p>Note</p> <p>For more detailed information, review the execution tab documentation.</p>"},{"location":"flows/sort-flow/","title":"Sort Flows","text":"<p>Qualytics allows you to sort your flows by Created Date, Name, and Last Triggered to easily organize and prioritize them according to your needs.  </p> <p></p> <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.  </p> <p></p>"},{"location":"flows/ticketing/","title":"Ticketing","text":"<p>Ticketing actions allow you to automatically create and manage tickets when anomaly-related events occur. These actions help teams track data quality issues and ensure they are routed to the appropriate owners for investigation and resolution.</p> <p>Ticketing actions are configured as Flow actions and execute when a supported trigger condition is met.</p>"},{"location":"flows/ticketing/#available-ticketing-actions","title":"Available Ticketing Actions","text":"<p>Flows support the following ticketing actions:</p> <ul> <li>Create Ticket \u2013 Creates a new ticket when the Flow is triggered.</li> <li>Update Ticket Status \u2013 Updates the status of an existing ticket.</li> </ul> <p></p> <p>These actions are available under the Ticketing section when adding an action to a Flow.</p>"},{"location":"flows/ticketing/#create-ticket","title":"Create Ticket","text":"<p>The Create Ticket action automatically creates a new ticket when the Flow is triggered. This action is typically used to open a ticket when an anomaly is detected or when an anomaly changes to a specific status.</p> <p></p>"},{"location":"flows/ticketing/#configuration","title":"Configuration","text":"<p>When configuring the Create Ticket action, provide the following details:</p> No. Field Description 1. Short Description A concise summary that clearly identifies the issue at a glance. 2. Description A detailed explanation of the data quality issue, including relevant context or observations. 3. Status Indicates the current state of the ticket when it is created (for example, New). 4. Priority Defines the importance of the issue in relation to other reported issues. 5. Urgency Specifies how quickly the issue needs to be addressed. 6. Impact Describes the potential business or operational impact of the issue. 7. Category Represents the high-level classification of the issue (for example, Data Quality). 8. Subcategory Provides a more specific classification within the selected category (for example, Validation Error). 9. Assignment Group Identifies the team or group responsible for investigating and resolving the issue. 10. Assigned To Specifies the individual responsible for working on the ticket. <p></p> <p>After completing the configuration, click Save to add the action to the Flow.</p> <p></p>"},{"location":"flows/ticketing/#update-ticket-status","title":"Update Ticket Status","text":"<p>The Update Ticket Status action updates the status of an existing ticket when the Flow is triggered. This action is commonly used to reflect progress or resolution as anomaly conditions change.</p> <p></p>"},{"location":"flows/ticketing/#configuration_1","title":"Configuration","text":"Field Description Target Ticket Status The status to apply to the ticket (for example, In Progress, Resolved, or Closed). <p>After configuring the target status, click Save to add the action to the Flow.</p> <p></p>"},{"location":"flows/ticketing/#notifications-for-ticketing-actions","title":"Notifications for Ticketing Actions","text":"<p>Flows can send notifications when ticketing actions are executed.</p>"},{"location":"flows/ticketing/#supported-notification-channels","title":"Supported Notification Channels","text":"<ul> <li>Slack</li> <li>Microsoft Teams</li> </ul> <p>Notifications are available after a Create Ticket or Update Ticket Status action is configured and executed within a Flow.</p> <p>Info</p> <p>Trigger Compatibility Ticketing actions are validated to ensure compatibility with the following trigger types:</p> <ul> <li>Anomaly</li> <li>Anomaly Status Change</li> </ul> <p>Ticketing actions are not supported with Manual or Scheduled triggers.</p> <p>Warning</p> <p>Integration Availability If required ticketing or notification integrations (such as Slack or Microsoft Teams) are unavailable during Flow execution, ticketing actions may not complete successfully. Execution details are logged to assist with troubleshooting and error analysis.</p>"},{"location":"flows/trigger-node/","title":"Trigger","text":"<p>This is where you can configure how the Flow will be activated. It can be activated automatically by configuring the settings, or you can choose to activate it manually.</p> <p>Step 1: After completing the \"Flow\" node setup, users can click on the \"Trigger\" node.</p> <p></p> <p>A panel will appear on the right-hand side, enabling users to define when the flow should start. The panel provides six options for initiating the flow. Users can choose one of the following options:</p> <ul> <li> <p>Schedule.</p> </li> <li> <p>Operation Completes.</p> </li> <li> <p>Anomalous Table or File Detected.</p> </li> <li> <p>Anomaly Detected.</p> </li> <li> <p>Anomaly Status Changed.</p> </li> <li> <p>Manual.</p> </li> </ul> <p></p>"},{"location":"flows/trigger-node/#important-information-about-triggers","title":"Important Information About Triggers","text":"<p>Note</p> <p>Each Flow can have only one trigger. If you want to execute a different action or change the trigger, you\u2019ll need to clone the Flow and configure the new trigger.</p> <p>Click here to learn how to clone a Flow.</p>"},{"location":"flows/trigger-node/#schedule","title":"Schedule","text":"<p>The Schedule trigger runs a flow automatically at a defined time interval. Use this trigger when you want the actions in a flow to execute on a recurring schedule\u2014such as hourly, daily, weekly, or monthly\u2014without manual intervention. Scheduled flows are useful for automating routine tasks, periodic scans, exports, or notifications, and they run based on the configured schedule time (UTC).</p> <p></p> <p>Schedule Configuration</p> <p>1. Hourly: This option allows you to schedule the flow to run every hour at a specified minute. You can define the frequency in hours and the exact minute within the hour the flow should start. Example: If set to Every 1 hour(s) on minute 0, the flow will run every hour at the top of the hour (e.g., 1:00, 2:00, 3:00).</p> <p></p> <p>2. Daily: This option schedules the flow to run once every day at a specific time. You specify the number of days between runs and the exact time of day in UTC. Example: If set to Every 1 day(s) at 00:00 UTC, the flow will run every day at midnight UTC.</p> <p></p> <p>3. Weekly: This option schedules the flow to run on specific days of the week at a set time. You select the days of the week and the exact time of day in UTC. Example: If configured to run on \"Sunday\" and \"Friday\" at 00:00 UTC, the flow will execute at midnight UTC on those days.</p> <p></p> <p>4. Monthly: This option schedules the flow to run once a month on a specific day at a set time. You specify the day of the month and the time of day in UTC. If set to \"On the 1st day of every 1 month(s), at 00:00 UTC,\" the flow will run on the first day of each month at midnight UTC.</p> <p></p> <p>5. Advanced: The Advanced option allows you to configure more complex and custom schedules using Cron expressions. This is useful when you need precise control over when a flow runs.</p> <p>Cron expressions are a powerful and flexible way to schedule tasks. They use a syntax that specifies the exact timing of the task based on five fields:</p> <ul> <li>Minute (0 - 59)</li> <li>Hour (0 - 23)</li> <li>Day of the month (1 - 31)</li> <li>Month (1 - 12)  </li> <li>Day of the week (0 - 6) (Sunday to Saturday)</li> </ul> <p>Each field can be defined using specific values, ranges, or special characters to create the desired schedule.</p> <p>Example: The Cron expression <code>0 0 * * *</code> schedules the flow to run at midnight (00:00) every day. Here\u2019s a breakdown of this expression:</p> <ul> <li>0 (Minute) - The task will run at the 0th minute.</li> <li>0 (Hour) - The task will run at the 0th hour (midnight).</li> <li>*(Day of the month) - The task will run every day of the month.</li> <li>*(Month) - The task will run every month.</li> <li>*(Day of the week) - The task will run every day of the week.</li> </ul> <p>Users can define other specific schedules by adjusting the Cron expression. For example:</p> <ul> <li>0 12 * * 1-5 - Runs at 12:00 PM from Monday to Friday.</li> <li>30 14 1 * * - Runs at 2:30 PM on the first day of every month.</li> <li>0 22 * * 6 - Runs at 10:00 PM every Saturday.</li> </ul> <p>To define a custom schedule, first enter the appropriate Cron expression in the \u201cCustom Cron Schedule (UTC)\u201d field, then specify the schedule name. This provides precise control over execution timing and ensures the operation runs as intended.</p> <p></p>"},{"location":"flows/trigger-node/#operation-completes","title":"Operation Completes","text":"<p>This type of flow is triggered whenever an operation, such as a catalog, profile, or scan, is completed on a source datastore. Upon completion, teams are promptly notified through in-app messages and, if configured, via external notification channels such as email, Slack, Microsoft Teams, and others. For example, the team is notified whenever the catalog operation is completed, helping them proceed with the profile operation on the datastore.</p> <p></p> <p>Filter Conditions</p> <p>Filters can be set to narrow down which operations should trigger the flow execution:</p> <ol> <li> <p>Source Datastore Tags: The flow is triggered only for source datastores that have all the selected tags assigned.</p> </li> <li> <p>Source Datastores: The flow is triggered only for the selected source datastores.</p> </li> <li> <p>Operation Types: The flow is triggered only for operations that match one or more of the selected types.</p> </li> <li> <p>Operation Status: The flow is triggered for operations with a status of either Success or Failure.</p> </li> </ol> <p></p> <p>Once the conditions are set, click the Save button to finalize the trigger configuration.</p> <p></p>"},{"location":"flows/trigger-node/#anomalous-table-or-file-detected","title":"Anomalous Table or File Detected","text":"<p>This flow is triggered when anomalies are detected within a specific table, file, and check rule types. It includes information about the number of anomalies found and the specific scan target within the datastore. This is useful for assessing the overall health of a particular datastore.  </p> <p></p> <p>Filter Conditions</p> <p>Users can optionally set filters to specify which tables or files should trigger the flow execution.</p> <ol> <li> <p>Tables / Files Tags: Only tables or files with all the selected tags assigned will trigger the flow.</p> </li> <li> <p>Source Datastores: The flow is triggered only for the selected source datastores.</p> </li> <li> <p>Check Rule Types: Only anomalies identified by one or more of the selected check rule types will initiate the flow.</p> </li> </ol> <p></p> <p>After defining the conditions, users must click the Save button to finalize the trigger configuration.  </p> <p></p>"},{"location":"flows/trigger-node/#anomaly-detected","title":"Anomaly Detected","text":"<p>This type of flow is triggered when any single anomaly is identified in the data. The flow message typically includes the type of anomaly detected and the datastore where it was found. It provides specific information about the anomaly type, which helps quickly understand the issue's nature.</p> <p></p> <p>Filter Conditions</p> <p>Users can define specific conditions to determine when the flow should be initiated.</p> <ol> <li> <p>Anomaly\u2019s Tags: Only anomalies with all selected tags assigned will trigger the flow.</p> </li> <li> <p>Source Datastores: Only triggered when anomalies are detected in the selected datastores.</p> </li> <li> <p>Check Rule Types: Only anomalies identified by one or more of the selected check rule types will initiate the flow.</p> </li> <li> <p>Anomaly Weight (Min): Only anomalies with a weight equal to or greater than the specified value will trigger the flow.</p> </li> </ol> <p></p> <p>Once the filter conditions are set, users must click the Save button to finalize the configuration.</p> <p></p>"},{"location":"flows/trigger-node/#anomaly-status-changed","title":"Anomaly Status Changed","text":"<p>This flow is triggered when the status of an anomaly changes to a specified status. It includes information about the anomaly, its previous and new status, and relevant context such as the affected asset and datastore. This is useful for automating actions based on anomaly lifecycle events (e.g., when an anomaly is resolved, acknowledged, or active).</p> <p></p> <p>Filter Conditions</p> <p>Users can optionally set filters to specify which anomaly status changes should trigger the flow execution.</p> <ol> <li> <p>Anomaly\u2019s Tags: Only anomalies with all the selected tags assigned will trigger the flow.</p> </li> <li> <p>Source Datastores: The flow is triggered only for anomalies originating from the selected source datastores.</p> </li> <li> <p>Anomaly Statuses: The flow is triggered only when an anomaly changes to one or more of the selected statuses.</p> </li> </ol> <p></p> <p>After defining the conditions, users must click the Save button to finalize the trigger configuration.</p> <p></p>"},{"location":"flows/trigger-node/#manual","title":"Manual","text":"<p>The flow starts only when the user manually triggers it. It doesn\u2019t depend on any automatic conditions or detections, giving the user full control. </p> <p>Note</p> <p>Here you can manually activate a Flow. For a full walkthrough of manual execution, refer to Execute Manual Flows documentation.</p> <p></p> <p>Once selected, users must click the Save button to confirm the manual trigger configuration.</p> <p></p> <p>Hover over the filter tooltip in trigger nodes to view the applied conditions such as tags, datastores, and operation types. This provides quick visibility into how each trigger is configured.</p> <p></p>"},{"location":"flows/trigger-node/#faq","title":"FAQ","text":"<p>1. What\u2019s the difference between a Flow and a Trigger?</p> <p>A Flow is the overall automation pipeline, while a Trigger determines when the flow starts \u2014 such as after operation completion, anomaly detection, or manually.</p>"},{"location":"flows/view-created-flows/","title":"View and Track Flow Executions","text":"<p>The Execution tab allows users to view the execution history and current status of a flow. It provides detailed timestamps, status updates, and a comprehensive record of flow executions for efficient tracking and analysis.</p>"},{"location":"flows/view-created-flows/#accessing-the-execution-tab","title":"Accessing the Execution Tab","text":"<p>Step 1: Click on the Execution tab.</p> <p></p> <p>Once clicked, you will be navigated to the Execution tab, where you can view the complete execution history of all created flows.</p> <p></p>"},{"location":"flows/view-created-flows/#see-a-flow-execution","title":"See a Flow Execution","text":"<p>Users can view flow execution in real-time by clicking on the desired flow operation. The page will show detailed operations, but no edits can be made here.</p> <p>Step 1: Click on the flow operation you want to view.</p> <p></p> <p>After clicking, you will navigate to the selected flow operation details page. The page displays all operational details in real-time. Please note, this page is view-only, and no edits can be made.</p> <p></p>"},{"location":"flows/view-created-flows/#understanding-flow-states","title":"Understanding Flow States","text":"<p>On the bottom-right corner, there is a Legend indicating the possible states of an action, such as:</p> <ul> <li>Success (Green)</li> <li>Failure (Red)</li> <li>Aborted (Orange)</li> <li>Skipped (Yellow)</li> <li>Running (Blue with dotted lines animation)</li> <li>Pending (Gray)</li> </ul> <p></p> <p>If a step is running, you will see a dot-line animation, signaling that the step is in progress. Once the action completes, the Action box will change its color to reflect the final state.</p> <p></p>"},{"location":"flows/view-created-flows/#accessing-operation-results","title":"Accessing Operation Results","text":"<p>To view detailed results of specific operations:</p> <p>Step 1: Click the Top Right Arrow button within the action operation box.</p> <p></p> <p>Step 2: You will navigate to the Activity page, where a Result Modal will open, displaying in-depth details of the operation.</p> <p></p>"},{"location":"flows/view-created-flows/#view-historical-action-settings","title":"View Historical Action Settings","text":"<p>You can also review the exact configuration that was used for each action when the flow executed. This helps you understand how the flow behaved during that run, even if the flow has been edited afterward.</p>"},{"location":"flows/view-created-flows/#how-to-view-historical-settings","title":"How to View Historical Settings","text":"<p>Step 1: Click on any action in the execution graph to view its historical settings.</p> <p></p> <p>Step 2: A panel appears on the right side showing the historical configuration for that action.</p> <p>This panel displays the action\u2019s settings exactly as they were at the moment the flow ran, including selected inputs, parameters, and options.</p> <p></p>"},{"location":"observability/edit-check/","title":"Edit Check","text":"<p>Editing a Check enables users to modify settings such as the unit of measurement, maximum age, description, and metadata. Additionally, they can add tags to streamline organization and retrieval.</p> <p>Step 1: Click the edit icon to modify the check.</p> <p></p> <p>A modal window will appear with the check details.</p> <p></p> <p>Step 2: Modify the check details as needed based on your preferences:</p> No. Field Description 1. Unit Edit the unit of measurement for the freshness check, such as milliseconds (Millis), Minutes, Hours etc. 2. Maximum Age Edit the maximum allowed age (in the specified unit) for data to be considered fresh. 3. Description Edit the Description to better explain what the check does. 4. Tags Edit the Tags to organize and easily find the check later. 5. Additional Metadata(Optional) Edit the Additional Metadata section to add any new custom details for more context. <p></p> <p>Step 3: Once you have edited the check details, then click on the Validate button. This will perform a validation operation on the check without saving it. The validation allows you to verify that the logic and parameters defined for the check are correct.</p> <p></p> <p>If the validation is successful, a green message saying \"Validation Successful\" will appear.</p> <p></p> <p>Step 4: Once you have a successful validation, click the \"Update\" button. The system will update the changes you've made to the check, including changes to the properties, description, tags, or additional metadata.</p> <p></p> <p>After clicking on the Update button, your check is successfully updated.</p>"},{"location":"observability/edit-maximum-age/","title":"Edit Maximum Age","text":"<p>Maximum Age sets the limit for how long data can remain unchanged before it\u2019s flagged as outdated. This ensures your data stays fresh and reliable for decision-making.</p> <p>Step 1: Click the Edit Maximum Age button on the right side of the graph.</p> <p></p> <p>Step 2: After clicking Edit Maximum Age, the field becomes editable, allowing you to modify the maximum age value.</p> <p></p> <p>Step 3: Once you've updated the maximum age values, click Save to apply the changes.</p> <p></p> <p>After clicking on the Save button, a success flash message will appear.</p>"},{"location":"observability/edit-threshold/","title":"Edit Threshold","text":"<p>Edit thresholds to set specific row count limits for your data checks. By defining minimum and maximum values, you ensure alerts are triggered when data goes beyond the expected range. This helps you monitor unusual changes in data volume. It gives you better control over tracking your data's behavior.</p> <p>Note</p> <p>When editing the threshold, only the min and max values can be modified.</p> <p>Step 1: Click the Edit Thresholds button on the right side of the graph.</p> <p></p> <p>Step 2: After clicking Edit Thresholds, you enter the editing mode where the Min and Max values become editable, allowing you to input new row count limits.</p> <p></p> <p>Step 3: Once you've updated the Min and Max values, click Save to apply the changes and update the thresholds.</p> <p></p> <p>After clicking on the Save button, your threshold is successfully updated.</p>"},{"location":"observability/freshness/","title":"Freshness","text":"<p>This measures the timeliness of data by monitoring when new data was last added or updated. It helps ensure that data remains up-to-date and relevant for decision-making. Users can view timestamp values in a clear date and time format, making it easier to analyze data freshness while maintaining millisecond-level precision in the background. If data updates are delayed or missing, it may indicate pipeline failures, system lag, or unexpected data source changes. Regular freshness checks prevent outdated information from impacting analytics, reporting, or automated workflows.</p> <p></p> No. Field Description 1. Search Bar This feature helps users quickly find specific identifiers or names in the data. 2. Report Date Report Date lets users pick a specific date to view data trends for that day. 3. Timeframe The time frame option lets users choose a period (week, month, quarter, or year.) to view data trends. 4. Sort By Sort By option helps users organize data by criteria like Anomalies, Checks, Created Date, Name, or Last Scanned for quick access. 5. Filters The filter lets users easily refine results by choosing specific tags or tables to view. 6. Favorite Mark this as a favorite for quick access and easy monitoring in the future. 7. Table Displays the name of the selected table being analyzed. 8. Weight Weight shows how important a check is for finding anomalies and sending alerts. 9. Anomaly Detection Represents active anomalies detected in the data. 10. Edit Check Edit the check to modify settings, or add tags for better customization. 11. Freshness (# ID) Each freshness check is assigned a unique identifier, corresponding to the specified time period it monitors (e.g., 1 Day for the customer table). This identifier facilitates precise tracking and management within the system. 12. Group By Users can also Group By specific intervals, such as day, week to observe trends over different periods. 13. Unit The unit used to measure data freshness, shown in milliseconds. 14. Maximum Age Displays the maximum recorded age of data in milliseconds. 15. Last Asserted Shows the latest date when the data was validated or checked. 16. Edit Maximum Age Edit Maximum Age lets users set custom limits for data freshness, allowing control over when alerts are triggered based on the age of the data. 17. Graph Visualization Graph illustrates consistent data patterns over time, with sudden anomalies marked by spikes in red. It reflects changes in data freshness and highlights when the data was last updated. <p>Note</p> <p>For more information please refer to the Freshness Check documentation..</p>"},{"location":"observability/manage-observability/","title":"Overview","text":"<p>In this section, you can manage the observability settings, including editing checks, thresholds, maximum ages, and marking checks as favorites. These features help you fine-tune and optimize your monitoring setup.</p>"},{"location":"observability/manage-observability/#edit-check","title":"Edit Check","text":"<p>Editing a Check enables users to modify settings such as the unit of measurement, maximum age, description, and metadata.</p> <p>Note</p> <p>For more steps refer to the edit checks documentation</p>"},{"location":"observability/manage-observability/#edit-maximum-age","title":"Edit Maximum Age","text":"<p>Maximum Age sets the limit for how long data can remain unchanged before it\u2019s flagged as outdated.</p> <p>Note</p> <p>For more steps refer to the edit maximum age documentation</p>"},{"location":"observability/manage-observability/#edit-threshold","title":"Edit Threshold","text":"<p>Edit thresholds to set specific row count limits for your data checks.</p> <p>Note</p> <p>For more steps refer to the edit threshold documentation</p>"},{"location":"observability/manage-observability/#mark-check-as-favorite","title":"Mark Check as Favorite","text":"<p>Marking a Metric Check as a favorite allows you to easily access important checks quickly.</p> <p>Note</p> <p>For more steps refer to the mark check as favorite documentation</p>"},{"location":"observability/mark-check/","title":"Mark Check as Favorite","text":"<p>Marking a Metric Check as a favorite allows you to easily access important checks quickly. This feature helps you prioritize and manage the checks you frequently use, making data monitoring more efficient.</p> <p>Click on the bookmark icon to mark the Metric Check as a favorite.</p> <p></p> <p>After clicking on the bookmark icon your check is successfully marked as a favorite.</p> <p>To unmark a check, simply click on the bookmark icon of the marked check. This will remove it from your favorites.</p> <p></p>"},{"location":"observability/measures/","title":"Measures","text":"<p>Measures focus on monitoring overall data trends to ensure consistency and reliability. This includes Volumetric Checks, which track data volume to identify trends and detect anomalies, and Freshness Tracking, which measures the last update or addition of data to ensure timeliness. These checks help maintain data integrity by highlighting unexpected changes in volume or delays in data updates. This category includes two key checks:</p>"},{"location":"observability/measures/#volumetric","title":"Volumetric","text":"<p>Volumetric help monitor data volumes over time to keep data accurate and reliable. They automatically count rows in a table and spot any unusual changes, like problems with data loading.</p> <p>For more information please, refer to the volumetric documentation.</p>"},{"location":"observability/measures/#freshness","title":"Freshness","text":"<p>This measures the timeliness of data by monitoring when new data was last added or updated. It helps ensure that data remains up-to-date and relevant for decision-making.</p> <p>For more information please, refer to the freshness documentation.</p>"},{"location":"observability/metric-check/","title":"Metric","text":"<p>Metrics track changes in data over time to ensure accuracy and reliability. They check specific fields against set limits to identify when values, like averages, go beyond expected ranges. With scheduled scans, Metrics automatically log and analyze these data points, making it easy for users to spot any issues. This functionality enhances users' understanding of data patterns, ensuring high quality and dependability. With Metrics, managing and monitoring data becomes straightforward and efficient.</p> <p></p> No Field Description 1 Search The search bar helps users find specific metrics or data by entering an identifier or description. 2 Sort By Sort By allows users to organize data by Weight, Anomalies, or Created Date for easier analysis and prioritization. 3 Filter Filter lets users refine data by Tags or Tables. Use Apply to filter or Clear to reset. 4 Metric(ID) Represents the tracked data metric with a unique ID. 5 Description A brief label or note about the metric, in this case, it's labeled as test. 6 Weight Weight shows how important a check is for finding anomalies and sending alerts. 7 Anomalies Anomalies show unexpected changes or issues in the data that need attention. 8 Favorite Mark this as a favorite for quick access and easy monitoring in the future. 9 Edit Checks Edit the check to modify settings, or add tags for better customization and monitoring. 10 Field This refers to the specific field being measured, here the max_value, which tracks the highest value observed for the metric. 11 Min This indicates the minimum value for the metric, which is set to 1. If not defined, no lower limit is applied. 12 Max This field shows the maximum threshold for the metric, set at 8. Exceeding this may indicate an issue or anomaly. 13 Created Date This field shows when the metric was first set up, in this case, June 18, 2024. 14 Last Asserted Last Asserted field shows the last time the metric was checked, in this case July 25, 2024. 15 Edit Threshold Edit Threshold lets users set custom limits for alerts, helping them control when they\u2019re notified about changes in data. 16 Group By This option lets users group data by periods like Day, Week, or Month. In this example, it's set to Day."},{"location":"observability/metric-check/#comparisons","title":"Comparisons","text":"<p>When you add a metric check, you can choose from three comparison options:</p> <ul> <li>Absolute Change  </li> <li>Absolute Value  </li> <li>Percentage Change</li> </ul> <p>These options help define how the system will evaluate your data during scan operations on the datastore.</p> <p>Once a scan is run, the system analyzes the data based on the selected comparison type. For example, Absolute Change will look for significant differences between scans, Absolute Value checks if the data falls within a predefined range, and Percentage Change identifies shifts in data as a percentage.</p> <p>Based on the chosen comparison type, the system flags any deviations from the defined thresholds. These deviations are then visually represented on a chart, displaying how the metric has fluctuated over time between scans. If the data crosses the upper or lower limits during any scan, the system will highlight this in the chart for further analysis.</p> <p>1. Absolute Change: The Absolute Change comparison checks how much a numeric field's value has changed between scans. If the change exceeds a set limit (Min/Max), it flags this as an anomaly.</p> <p></p> <p>2. Absolute Value: The Absolute Value comparison checks whether a numeric field's value falls within a defined range (between Min and Max) during each scan. If the value goes beyond this range, it identifies it as an anomaly.</p> <p></p> <p>3. Percentage Change: The Percentage Change comparison monitors how much a numeric field's value has shifted in percentage terms. If the change surpasses the set percentage threshold between scans, it triggers an anomaly.</p> <p></p>"},{"location":"observability/metric-check/#minimum-measurements-for-chart-rendering","title":"Minimum Measurements for Chart Rendering","text":"<p>To display metric charts in the UI, a minimum number of measurements must be recorded. If the required number of measurements is not met, the chart remains empty even though some measurements exist.</p> <ul> <li> <p>Absolute Value: Requires at least 2 measurements to render.</p> </li> <li> <p>Absolute Change: Requires at least 3 measurements to render.</p> </li> <li> <p>Percentage Change: Requires at least 3 measurements to render.</p> </li> </ul> <p>These thresholds ensure meaningful visual representation by preventing incomplete or misleading chart data.</p>"},{"location":"observability/observability-filter/","title":"Observability Filters","text":"<p>Filters help narrow down the displayed observability checks, making it easier to organize and focus on relevant results.</p> <p></p> No. Filter Description 1. Tags Filters checks based on tags assigned to tables or files (for example, finance, sales, or production). 2. Rule Type Filters checks based on the rule category, such as Volumetric and Freshness. <p></p>"},{"location":"observability/observability/","title":"Observability","text":"<p>Observability helps users track changes in data volume and quality over time, ensuring data accuracy and integrity. Within the Source Datastore section, the Observability tab provides visibility into observability metrics across tables or files within a specific datastore. It introduces two main categories: </p> <ul> <li>Measures </li> <li>Metric Checks</li> </ul> <p>Measures include Volumetric Checks, which monitor fluctuations in row counts, and Freshness Tracking, which ensures data is updated on time. </p> <p>Metric Checks focus on specific fields and offer deeper insights derived from scan operations. These tools work together to help detect anomalies early and maintain the reliability of your data assets.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"observability/observability/#why-we-need-observability","title":"Why We Need Observability","text":"<p>Observability is critical in tracking and understanding data behavior, providing insights into how data is moving, evolving, and being processed. By implementing observability, you can monitor key metrics like data volume, freshness, and quality across your systems. It helps you quickly detect anomalies, spot issues early, and ensure data integrity over time.</p>"},{"location":"observability/observability/#how-it-works","title":"How It Works","text":"<p>Observability in Qualytics is designed to give you a continuous, automated view of your data health. Here\u2019s how it works:</p> <ul> <li> <p>Automated Checks: The system automatically runs once every hour at the 30-minute mark to check the container volumes in your data stores.</p> </li> <li> <p>Volume Tracking: Once volume tracking is enabled, no manual intervention is required. The system calculates data volume automatically.</p> </li> <li> <p>Measurement Frequency: The observability job runs 48 times a day to keep data up-to-date, and each measurement is time-stamped according to UTC.</p> </li> <li> <p>Efficient Monitoring: The system skips redundant checks for containers that have already been measured, thus optimizing the process.</p> </li> </ul>"},{"location":"observability/observability/#use-case-understanding-automatic-volume-tracking","title":"Use Case: Understanding Automatic Volume Tracking","text":""},{"location":"observability/observability/#scenario","title":"Scenario","text":"<p>A data operations team needs to monitor table volumes daily to detect unexpected data spikes or drops. They want to understand how Qualytics automatically tracks volume without requiring constant manual intervention.</p>"},{"location":"observability/observability/#common-questions","title":"Common Questions","text":"<p>Q: How is data volume calculated? Do I need to run profiles or scans daily? Ans: Data volume is calculated automatically by the observability job once volume tracking is enabled. No manual profiling or scanning is required.</p>"},{"location":"observability/observability/#how-it-works_1","title":"How It Works:","text":"<ul> <li>The observability job runs 24 times per day (every 30 minutes)</li> <li>On each run, it checks whether a container has already been measured for \"today\" (UTC time)</li> <li>If not measured yet, it records the container's volume</li> <li>If already measured for that day, it skips the measurement</li> <li>The first daily measurement typically occurs around 00:30 UTC (the first run after midnight UTC)</li> </ul>"},{"location":"observability/observability/#setup-requirements","title":"Setup Requirements","text":"<ul> <li>Initial Cataloging: Ensure tables are cataloged in Qualytics</li> <li>Enable Volume Tracking: Turn on volume tracking for the specific container</li> <li>Automatic Monitoring: The observability job handles all subsequent measurements</li> </ul> <p>No additional profiling or scanning operations are needed after the initial setup.</p>"},{"location":"observability/observability/#time-zone-considerations","title":"Time Zone Considerations","text":"<p>The observability system currently operates on UTC time for daily volume calculations. This means:</p> <ul> <li>Daily volume resets occur at midnight UTC</li> <li>Volume measurements begin at 00:30 UTC</li> <li>The UI displays daily totals in UTC time</li> </ul> <p>Note</p> <p>\"Local Time Display\" The UI currently displays observability data in UTC. If you notice date discrepancies (for example, a scan performed on September 17 showing observability data starting September 18), this is due to UTC time zone differences.</p>"},{"location":"observability/observability/#benefits","title":"Benefits","text":"<ul> <li>Zero Maintenance: Automatic checks every 30 minutes without manual intervention</li> <li>Consistent Monitoring: Regular measurements throughout the day ensure comprehensive coverage</li> <li>Early Detection: Quickly identify volume anomalies or data pipeline issues</li> <li>Efficient Processing: Smart skip logic prevents redundant measurements</li> </ul>"},{"location":"observability/observability/#configuration","title":"Configuration","text":"<p>To enable automatic volume tracking:</p> <ol> <li>Navigate to your datastore settings</li> <li>Select the container you want to monitor</li> <li>Enable Volume Tracking</li> <li>The Observability job runs automatically every hour at minute 30.</li> </ol>"},{"location":"observability/observability/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and select the datastore from the left menu that you want to monitor.</p> <p></p> <p>Step 2: Click on the \u201cObservability\u201d from the Navigation tab.</p> <p></p> <p>Observability metrics for tables of the selected source datastore are shown, enabling you to view their detailed insights.</p> <p></p>"},{"location":"observability/observability/#observability-categories","title":"Observability Categories","text":"<p>Observability in data checks is divided into two key categories: Measures and Metric Checks. Measures focus on overall data trends and include Volumetric Checks, which monitor data volume to identify trends and anomalies, and Freshness Tracking, which tracks when data was last added or updated to ensure timeliness. Metric Checks, on the other hand, analyze specific data attributes, providing detailed insights into data quality.</p> <p></p>"},{"location":"observability/observability/#measure","title":"Measure","text":"<p>Measures focus on monitoring overall data trends to ensure consistency and reliability.</p> <p>Note</p> <p>For more information regarding measures please refer to the measure documentation.</p>"},{"location":"observability/observability/#metric","title":"Metric","text":"<p>Metrics track changes in data over time to ensure accuracy and reliability.</p> <p>Note</p> <p>For more information regarding metric please refer to the metric documentation.</p>"},{"location":"observability/volumetric/","title":"Volumetric","text":"<p>Volumetric checks help monitor data volumes over time to keep data accurate and reliable. They automatically count rows in a table and spot any unusual changes, like problems with data loading. This makes it easier to catch issues early and keep everything running smoothly. Volumetric checks also let you track data over different time periods, like daily or weekly. The system sets limits based on past data, and if the row count goes above or below those limits, an anomaly alert is triggered.</p> <p></p> No Field Description 1 Search This feature helps users quickly find specific identifiers or names in the data. 2 Report Date Report Date lets users pick a specific date to view data trends for that day. 3 Time Frame The time frame option lets users choose a period (week, month, quarter, or year) to view data trends. 4 Sort By Sort By option helps users organize data by criteria like Volumetrics Count, Name, or Last Scanned for quick access. 5 Filter The filter lets users easily refine results by choosing specific tags or tables to view. 6 Favorite Mark this as a favorite for quick access and easy monitoring in the future. 7 Table Displays the table for which the volumetric check is being performed (e.g., customer_view, nation). Each table has its own Volumetric Check. 8 Check (# ID) Each check is assigned a unique identifier, followed by the time period it applies to (e.g., 1 Day for the customer table). This ID helps in tracking the specific check in the system. 9 Weight Weight shows how important a check is for finding anomalies and sending alerts. 10 Anomaly Detection The Volumetric Check detects anomalies when row counts exceed set min or max thresholds, triggering an alert for sudden changes. 11 Edit Checks Edit the check to modify settings, or add tags for better customization and monitoring. 12 Group By Users can also Group By specific intervals, such as day, week, or month, to observe trends over different periods. 13 Measurement Period Defines the time period over which the volumetric check is evaluated. It can be customized to 1 day, week, or other timeframes. 14 Comparison These indicate the type of comparison used, indicating the \"Absolute Value\" method. 15 Min Values These indicate the minimum thresholds for the row count of the table being checked (e.g., 150,139 Rows). 16 Max Values These indicate the maximum thresholds for the row count of the table being checked. 17 Last Asserted This shows the date the last check was asserted, which is the last time the system evaluated the Volumetric Check (e.g., Oct 02, 2024). 18 Edit Threshold Edit Threshold lets users set custom limits for alerts, helping them control when they\u2019re notified about changes in data. 19 Graph Visualization The graph provides a visual representation of the row count trends. It shows fluctuations in data volume over the selected period. This visual allows users to quickly identify any irregularities or anomalies."},{"location":"observability/volumetric/#observability-heatmap","title":"Observability Heatmap","text":"<p>The heatmap provides a visual overview of data anomalies by day, using color codes for quick understanding:</p> <p></p> <ul> <li>Blue square: Blue squares represent days with no anomalies, meaning data stayed within the expected range.</li> <li>Orange square: Orange squares indicate days where data exceeded the minimum or maximum threshold range but didn\u2019t qualify as a critical anomaly.</li> <li>Red square: Red squares highlight days with anomalies, signaling significant deviations from expected values that need further investigation.</li> </ul> <p></p> <p>By hovering over each square, you can view additional details for that specific day, including the date, anomaly count, last row count, and last modification time allowing you to easily pinpoint and analyze data issues over time.</p>"},{"location":"operation-automation/automated-setup-using-qualytics-cli/","title":"Automated Setup Using Qualytics CLI","text":"<p>Easily automate scheduled exports with the Qualytics CLI on both Linux and Windows. This setup generates the required scripts and cron/task entries for you, with simple placeholders to customize.</p>"},{"location":"operation-automation/automated-setup-using-qualytics-cli/#for-linux-and-windows-users","title":"For Linux and Windows Users","text":"<p>Use the Qualytics CLI to schedule a task automatically.</p> <pre><code>qualytics schedule export-metadata --crontab \"&lt;cronjob-expression&gt;\" --datastore &lt;datastore-id&gt; --containers &lt;container-ids&gt; --options &lt;metadata-options&gt;\n</code></pre> <p>Replace placeholders as needed.</p>"},{"location":"operation-automation/automated-setup-using-qualytics-cli/#behaviour-on-linux","title":"Behaviour on Linux:","text":"<p>It will create the files inside your <code>home/user/.qualytics</code> folder.</p> <p>The schedule operations commands are going to be located in <code>home/user/.qualytics/schedule-operation.txt</code>.</p> <p>You can see some files with the <code>option</code> you selected with the logs of the cronjob run.</p> <p>It will already create for you a cronjob expression, you can run <code>crontab -l</code> to list all cronjobs.</p>"},{"location":"operation-automation/automated-setup-using-qualytics-cli/#behaviour-on-windows","title":"Behaviour on Windows:","text":"<p>It will create the files inside your <code>home/user/.qualytics</code> folder.</p> <p>The script files will be located in <code>home/user/.qualytics</code> with a pattern <code>task_scheduler_script_&lt;option-you-selected&gt;_&lt;datastore-number&gt;.ps1</code> and it's just a matter for you to follow the step above to create the Task Scheduler.</p>"},{"location":"operation-automation/automated-setup-using-qualytics-cli/#explanation-of-placeholders","title":"Explanation of Placeholders:","text":"<ul> <li> <p><code>&lt;cronjob-expression&gt;</code>: Replace this with your desired cron expression. For example, <code>*/5 * * * *</code> means \"every 5 minutes.\" You can check <code>crontab.guru</code> for more examples.</p> </li> <li> <p><code>&lt;your-instance&gt;</code>: Replace with the actual Qualytics instance URL.</p> </li> <li> <p><code>&lt;operation&gt;</code>: Replace with the specific operation (e.g., \"anomalies\", \"checks\" or \"field-profiles\").</p> </li> <li> <p><code>&lt;datastore-id&gt;</code>: Replace with the ID of the target datastore.</p> </li> <li> <p><code>&lt;container-id-one&gt;</code> and <code>&lt;container-id-two&gt;</code>: Replace with the IDs of the containers. You can add more containers as needed.</p> </li> <li> <p><code>&lt;container-ids&gt;</code>: Comma-separated list of container IDs or array-like format. Example: \"1, 2, 3\" or \"[1,2,3]\".</p> </li> <li> <p><code>&lt;options&gt;</code>: Comma-separated list of operation to export or all for everything. Example: anomalies, checks, field-profiles or all.</p> </li> <li> <p><code>&lt;your-token&gt;</code>: Replace with the access token obtained from Qualytics (<code>Settings</code> -&gt; <code>Security</code> -&gt; <code>API Keys</code>).</p> </li> <li> <p><code>&lt;path-to-show-logs&gt;</code>: Replace with the file path where you want to store the logs.</p> </li> </ul>"},{"location":"operation-automation/linux-machine/","title":"Linux machine","text":"<p>You can automate Qualytics operations on a Linux machine by scheduling them with cron jobs. This guide walks you through setting up a scheduled curl command to trigger exports at defined intervals.</p>"},{"location":"operation-automation/linux-machine/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure that you have the following:</p> <ol> <li>Access to the terminal on your machine.</li> <li>The <code>curl</code> command-line tool installed.</li> <li>The desired Qualytics instance details, including the instance URL and authentication token.</li> </ol>"},{"location":"operation-automation/linux-machine/#steps-to-create-a-scheduled-operation","title":"Steps to Create a Scheduled Operation","text":""},{"location":"operation-automation/linux-machine/#1-open-the-crontab-editor","title":"1. Open the Crontab Editor","text":"<p>Run the following command in your terminal to open the crontab editor:</p> <pre><code>crontab -e\n</code></pre>"},{"location":"operation-automation/linux-machine/#2-add-the-cron-job-entry","title":"2. Add the Cron Job Entry","text":"<p>In the crontab editor, add the following line to execute the curl command at your specified schedule:</p> <pre><code>&lt;cronjob-expression&gt; /usr/bin/curl --request POST --url 'https://&lt;your-instance&gt;.qualytics.io/api/export/&lt;operation&gt;?datastore=&lt;datastore-id&gt;&amp;containers=&lt;container-id-one&gt;&amp;containers=&lt;container-id-two&gt;' --header 'Authorization: Bearer &lt;your-token&gt;' &gt;&gt; &lt;path-to-show-logs&gt; 2&gt;&amp;1\n</code></pre>"},{"location":"operation-automation/linux-machine/#3-example","title":"3. Example:","text":"<p>For example, to run the command every 5 minutes:</p> <pre><code>*/5 * * * * /usr/bin/curl --request POST --url 'https://your-instance.qualytics.io/api/export/anomalies?datastore=123&amp;containers=14&amp;containers=16' --header 'Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...' &gt;&gt; /path/to/show/logs.txt 2&gt;&amp;1\n</code></pre>"},{"location":"operation-automation/linux-machine/#4-verify-or-list-cron-jobs","title":"4. Verify or List Cron Jobs:","text":"<pre><code>crontab -l\n</code></pre> <p>Customize the placeholders based on your specific details and requirements. Save the crontab file to activate the scheduled operation.</p>"},{"location":"operation-automation/overview/","title":"Overview","text":"<p>Users may want to create their own scheduled operations in Qualytics to automate routine tasks such as data exports or running specific operations at defined intervals. Instead of executing these operations manually, they can be scheduled to run automatically on Linux or Windows, or through the Qualytics CLI.</p> <p>Choose the setup guide that matches your environment:</p>"},{"location":"operation-automation/overview/#linux-machine","title":"Linux Machine","text":"<p>This guide explains how to configure scheduled tasks on Linux using cron jobs with curl commands. For more steps, refer to the linux machine documentation.</p>"},{"location":"operation-automation/overview/#windows-machine","title":"Windows Machine","text":"<p>This guide explains how to configure scheduled tasks on Windows using PowerShell scripts and the Windows Task Scheduler. For more steps, refer to the windows machine documentation.</p>"},{"location":"operation-automation/overview/#installation-qualytics-cli","title":"Installation \u2013 Qualytics CLI","text":"<p>This page points you to the Qualytics CLI Overview, where you can find installation and initialization instructions. For more steps, refer to the qualytics CLI documentation.</p>"},{"location":"operation-automation/overview/#automation-setup-using-qualytics-cli","title":"Automation Setup Using Qualytics CLI","text":"<p>This guide explains how to use the Qualytics CLI\u2019s scheduling commands to automate operations, including Linux and Windows behavior. For more steps, refer to the automation setup using qualytics CLI documentation.</p>"},{"location":"operation-automation/windows-machine/","title":"Windows machine","text":"<p>Automate Qualytics operations on Windows using PowerShell and Task Scheduler. This guide shows how to set up and run scheduled export tasks.</p>"},{"location":"operation-automation/windows-machine/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure that you have the following:</p> <ol> <li>Access to PowerShell on your machine.</li> <li>The desired Qualytics instance details, including the instance URL and authentication token.</li> </ol>"},{"location":"operation-automation/windows-machine/#steps-to-create-a-scheduled-operation","title":"Steps to Create a Scheduled Operation","text":""},{"location":"operation-automation/windows-machine/#1-open-your-text-editor-of-your-preference-and-add-the-script-entry","title":"1. Open your text editor of your preference and add the script entry","text":"<p>In the text editor, add the following line to execute the <code>Invoke-RestMethod</code> command:</p> <pre><code>Invoke-RestMethod -Method 'Post' -Uri https://&lt;your-instance&gt;/api/export/anomalies?datastore=&lt;datastore-id&gt;&amp;containers=&lt;container-id-one&gt;&amp;containers=&lt;container-id-two&gt; -Headers @{'Authorization' = 'Bearer &lt;your-token&gt;'; 'Content-Type' = 'application/json'}\n</code></pre>"},{"location":"operation-automation/windows-machine/#2-example","title":"2. Example:","text":"<p>For example, to run the command every 5 minutes:</p> <pre><code>Invoke-RestMethod -Method 'Post' -Uri https://your-instance.qualytics.io/api/export/anomalies?datastore=123&amp;containers=44&amp;containers=22 -Headers @{'Authorization' = 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...'; 'Content-Type' = 'application/json'}\n</code></pre> <p>Customize the placeholders based on your specific details and requirements. Save the script with the desired name with the extension <code>.ps1</code>.</p>"},{"location":"operation-automation/windows-machine/#3-add-the-script-to-the-task-scheduler","title":"3. Add the script to the Task Scheduler:","text":"<ol> <li> <p>Open Task Scheduler:</p> <ul> <li>Press <code>Win + S</code> to open the Windows search bar.</li> <li>Type \"Task Scheduler\" and select it from the search results.</li> </ul> </li> <li> <p>Create a Basic Task:</p> <ul> <li>In the Task Scheduler window, click on <code>Create Basic Task...</code> on the right-hand side.</li> </ul> </li> <li> <p>Provide a Name and Description:</p> <ul> <li>Enter a name and description for your task. Click <code>Next</code> to proceed.</li> </ul> </li> <li> <p>Choose Trigger:</p> <ul> <li>Select when you want the task to start. Options include <code>Daily</code>, <code>Weekly</code>, or <code>At log on</code>.</li> <li>Choose the one that fits your schedule. Click <code>Next</code>.</li> </ul> </li> <li> <p>Set the Start Date and Time:</p> <ul> <li>If you selected a trigger that requires a specific start date and time, set it accordingly. Click <code>Next</code>.</li> </ul> </li> <li> <p>Choose Action:</p> <ul> <li>Select <code>Start a program</code> as the action and click <code>Next</code>.</li> </ul> </li> <li> <p>Specify the Program/Script:</p> <ul> <li>In the <code>Program/script</code> field, provide the path to PowerShell executable (<code>powershell.exe</code>), typically located at <code>C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe</code>. Alternatively, you can just type <code>powershell.exe</code>.</li> <li>In the <code>Add arguments (optional)</code> field, provide the path to your PowerShell script. For example: <code>-File \"C:\\Path\\To\\Your\\GeneratedScript.ps1\"</code>.</li> <li>Click <code>Next</code>.</li> </ul> </li> <li> <p>Review Settings:</p> <ul> <li>Review your task settings. If everything looks correct, click <code>Finish</code>.</li> </ul> </li> <li> <p>Finish:</p> <ul> <li>You should now see your task listed in the Task Scheduler Library.</li> </ul> </li> </ol>"},{"location":"quality-scores/what-are-quality-scores/","title":"Quality Scores","text":"<p>Quality Scores are quantified measures of data quality calculated at the field and container levels, recorded as time-series to enable tracking of changes over time. Scores range from 0-100 with higher values indicating superior quality for the intended purpose. These scores integrate eight distinct dimensions, providing a granular analysis of the attributes that impact the overall data quality. The overall score is a composite reflecting the relative importance and configured weights of these factors:</p> <ul> <li>Completeness: Measures the average percentage of non-null values in a field throughout the measurement period. For example, if a \"phone_number\" field has values present in 90 out of 100 records, its completeness score for the measurement would be 90%.</li> <li>Coverage: Measures the number of quality checks defined for monitoring the field's quality.</li> <li>Conformity: Measures how well the data adheres to specified formats, patterns, and business rules. For example, checking if dates follow the required format (YYYY-MM-DD) or if phone numbers match the expected pattern.See Appendix: Conformity Rule Types for the full Conformity rule type list.</li> <li>Consistency: Measures uniformity in type and scale across all data representations. Verifies that data maintains the same type and representation over time. For example, ensuring that a typed numeric column does not change over time to a string.</li> <li>Precision: Evaluates the resolution of field values against defined quality checks.See Appendix: Precision Rule Types for the full Precision rule type list.</li> <li>Timeliness: Gauges data availability according to schedule.See Appendix: Timeliness Rule Types for the full Timeliness rule type list.</li> <li>Volumetrics: Analyzes consistency in data size and shape over time.See Appendix: Volumetric Rule Types for the full Volumetrics rule type list.</li> <li>Accuracy: Determines the fidelity of field values to their real-world counterparts or expected values.</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#how-completeness-precision-and-accuracy-differ","title":"How Completeness, Precision, and Accuracy Differ","text":"Dimension Focus Example Question It Answers Completeness Are values present? What % of rows in <code>phone_number</code> are non-null? Precision Are values within the expected level of detail or granularity? Are all <code>age</code> values between 0\u2013120? Do decimals have required 2-digit precision? Accuracy Are values correct compared to real-world truth or integrity checks? Is the relationship between <code>square_footage</code> and <code>price</code> maintained? <p>Important</p> <p>A data asset's quality score is a measure of its fitness for the intended use case. It is not a simple measure of error, but instead a holistic confidence measure that considers the eight fundamental dimensions of data quality as described below.  Quality scores are dynamic and will evolve as your data and business needs change over time.</p>"},{"location":"quality-scores/what-are-quality-scores/#field-level-quality-scoring","title":"Field-Level Quality Scoring","text":"<p>Each field receives individual scores for eight quality dimensions, each evaluated on a 0-100 scale.</p>"},{"location":"quality-scores/what-are-quality-scores/#completeness-dimension","title":"Completeness Dimension","text":"<p>The Completeness score measures the average percentage of non-null values in a field over the measurement period.</p> <p>How Completeness is Calculated</p> <ul> <li>Scale: 0 to 100, representing the average completeness percentage</li> <li>Measurement period: Defined by the configured decay time (default 180 days)</li> <li>Formula: Average of <code>(non-null values / total records) \u00d7 100</code> across all measurements in the period</li> <li>Example: If a \"phone_number\" field averages 90% completeness over the measurement period, its completeness score would be 90</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#coverage-dimension","title":"Coverage Dimension","text":"<p>The Coverage score measures how many distinct quality checks have been applied to a field. It is designed to reward the first few checks heavily, then taper off as more checks are added, following a curve of diminishing returns.</p> <p>How Coverage is Calculated</p> <ul> <li>Scale: The score ranges continuously from 0 to 100</li> <li>Anchor points:<ul> <li>0 checks \u2192 score of 0</li> <li>1 check \u2192 score of approximately 60</li> </ul> </li> <li>Diminishing returns: Each additional check contributes less than the previous one. As the number of checks grows, the score approaches 100 but never exceeds it</li> </ul> <p>Mathematically, the scoring curve follows an exponential growth model: <pre><code>score(n) = 100 \u00d7 (1 - e^(-k \u00d7 n))\n</code></pre> where n is the number of checks and k is tuned so that 1 check = 60.</p> <p>Why This Model?</p> <ul> <li>Strong early reward: The first check dramatically increases confidence in field coverage</li> <li>Fair balance: More checks always improve the score, but the improvement diminishes as coverage becomes robust, preventing runaway inflation</li> </ul> <p>Field vs. Container Coverage</p> <p>At the field level, Coverage reflects the number of distinct quality checks defined for that field. At the container level, Coverage is an aggregate of field-level coverage scores, further adjusted by scan frequency (more frequent scans \u2192 greater confidence).</p>"},{"location":"quality-scores/what-are-quality-scores/#conformity-dimension","title":"Conformity Dimension","text":"<p>The Conformity score measures how well the data adheres to specified formats, patterns, and business rules.</p> <p>How Conformity is Calculated</p> <ul> <li>Scale: 0 to 100 based on the ratio of conforming values</li> <li>Formula: <code>(1 - (rows with anomalous values as specified by conformity checks / min(scanned rows, container rows))) \u00d7 100</code></li> <li>Denominator: Uses the smaller of scanned row count or container row count to prevent score inflation</li> <li>Applicable rule types: Pattern matching, length constraints, type validation, schema expectations, and format-specific validationsSee Appendix: Conformity Rule Types for the full Conformity rule type list.</li> </ul> <p>Examples</p> <ul> <li>Email field where 95% of scanned/total rows match valid email pattern \u2192 Score ~95</li> <li>Date field with consistent YYYY-MM-DD format \u2192 Score ~100</li> <li>Phone field with mixed formats and invalid entries \u2192 Score ~60</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#consistency-dimension","title":"Consistency Dimension","text":"<p>The Consistency score measures how stable a field's values remain over time compared to their expected statistical profile. This highlights fields that are \"drifting\" (changing shape, format, or density).</p> <p>How Consistency is Calculated</p> <ol> <li> <p>Check for type changes</p> <ul> <li>If a field flips between types (e.g., sometimes a number, sometimes a string), score is set to 0</li> </ul> </li> <li> <p>Collect summary statistics per field type:</p> <ul> <li>Numeric fields: median and interquartile range (IQR)</li> <li>String fields: distinct count, min/max length, Shannon entropy</li> <li>Datetime fields: earliest timestamp, distinct timestamp count</li> </ul> </li> <li> <p>Measure stability</p> <ul> <li>Track variation of each statistic across the analysis window</li> <li>Normalize changes for fair comparison across different scales</li> </ul> </li> <li> <p>Apply thresholds and weights</p> <ul> <li>Each change type has an expected tolerance (e.g., \u00b110% for numeric medians)</li> <li>Variations within tolerance incur little/no penalty</li> <li>Larger variations reduce the score proportionally</li> </ul> </li> <li> <p>Combine into final score</p> <ul> <li>100: Field stayed fully consistent</li> <li>60-90: Mild to moderate changes worth monitoring</li> <li>Below 60: Meaningful shift requiring investigation</li> <li>0: Type change detected</li> </ul> </li> </ol> <p>Consistency vs. Accuracy</p> <p>Consistency checks whether a field\u2019s statistical shape and distribution remain stable over time (e.g., numeric medians, string entropy).</p> <p>Accuracy, by contrast, evaluates whether values are correct and aligned to real-world truths or integrity rules.</p> <p>Together, they capture different aspects of trustworthiness.</p> <p>Examples</p> <ul> <li>Numeric \"Price\" field with stable median and IQR \u2192 Score ~100</li> <li>String \"Country\" field where distinct values double unexpectedly \u2192 Score ~75</li> <li>Datetime field with sudden two-year backfill \u2192 Score ~60</li> <li>ID field alternating between numeric and string types \u2192 Score = 0</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#precision-dimension","title":"Precision Dimension","text":"<p>The Precision score evaluates the resolution and granularity of field values against defined quality checks.</p> <p>How Precision is Calculated</p> <ul> <li>Scale: 0 to 100 based on the ratio of values meeting precision requirements</li> <li>Formula: <code>(1 - (rows with anomalous values as specified by precision checks / min(scanned rows, container rows))) \u00d7 100</code></li> <li>Denominator: Uses the smaller of scanned row count or container row count to prevent score inflation</li> <li>Applicable rule types: Range validations, comparisons, mathematical constraints, and temporal boundariesSee Appendix: Precision Rule Types for the full Precision rule type list.</li> </ul> <p>Examples</p> <ul> <li>Decimal field maintaining required 2-digit precision \u2192 Score ~100</li> <li>Timestamp field with appropriate granularity (no future dates) \u2192 Score ~95</li> <li>Age field with values outside valid range (0-120) \u2192 Score ~85</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#accuracy-dimension","title":"Accuracy Dimension","text":"<p>The Accuracy score determines the fidelity of field values to their real-world counterparts or expected values.</p> <p>How Accuracy is Calculated</p> <ul> <li>Scale: 0 to 100 based on the overall anomaly rate across all data integrity (excludes metadata checks like schema, volume, freshness, etc..) check types</li> <li>Formula: <code>(1 - (rows with anomalous values as specified by accuracy checks / min(scanned rows, container rows))) \u00d7 100</code></li> <li>Denominator: Uses the smaller of scanned row count or container row count to prevent score inflation</li> <li>Comprehensive: Considers anomalies from all data integrity rule types</li> <li>Represents: Overall correctness and trustworthiness of the field data</li> </ul> <p>Interpretation</p> <ul> <li>95-100: Highly accurate data suitable for critical decisions</li> <li>80-94: Generally reliable with some known issues</li> <li>60-79: Moderate accuracy requiring validation for important uses</li> <li>Below 60: Significant accuracy concerns requiring remediation</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#timeliness-volumetrics-dimensions","title":"Timeliness &amp; Volumetrics Dimensions","text":"<p>Both the Timeliness and Volumetrics dimensions are measured at the container level as described below. Field-level scores are inherited from their container-level scores.</p>"},{"location":"quality-scores/what-are-quality-scores/#container-level-quality-scoring","title":"Container-Level Quality Scoring","text":"<p>A container (table, view, file, or other structured data asset or any aggregation of data assets such as assets that share a common tag) receives an overall quality score derived from its constituent fields and additional container-specific metrics.</p>"},{"location":"quality-scores/what-are-quality-scores/#how-container-scores-are-calculated","title":"How Container Scores Are Calculated","text":"<p>Your container's total Quality Score starts at a baseline of 70. Each of the eight data quality dimensions then adjusts this baseline:</p> <ul> <li>Dimension aggregation:<ul> <li>Completeness: Weighted average of all field completeness scores</li> <li>Coverage: Weighted average of field coverage scores, adjusted for scan frequency</li> <li>Conformity: Weighted average of field conformity scores, adjusted for schema-level conformity checks</li> <li>Consistency: Weighted average of field consistency scores, adjusted for profiling frequency</li> <li>Precision: Weighted average of field precision scores</li> <li>Accuracy: Weighted average of field accuracy scores</li> <li>Timeliness: Calculated using process described below</li> <li>Volumetrics: Calculated using process described below</li> </ul> </li> <li>Proportional adjustment: Each dimension adjusts the score proportionally to its 0\u2013100 rating</li> <li>Influence capping: Every dimension has maximum positive and negative impact limits</li> <li>Weight controls: Higher weights make dimensions more influential; zero weight removes effect entirely</li> <li>Missing value handling: Documented defaults substitute for unmeasurable dimensions</li> <li>Special case: If only one dimension is weighted, the Quality Score mirrors that dimension's rating</li> <li>Final clipping: Result is always constrained between 0 and 100</li> </ul> <p>Why a 70-Point Baseline?</p> <p>The 70-point baseline represents a neutral confidence starting point.</p> <ul> <li>Dimensions then adjust the baseline downward when issues are found or upward when strong quality signals exist.</li> <li>This calibration ensures that new containers without extensive checks or history begin from a reasonable midpoint rather than 0.</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#timeliness-dimension","title":"Timeliness Dimension","text":"<p>The Timeliness score gauges whether data is available according to its expected schedule.</p> <p>How Timeliness is Calculated</p> <ul> <li>Scale: 0 to 100 based on adherence to freshness requirements</li> <li>Field level: Directly inherited from the container's timeliness score</li> <li>Anomaly counting: Counts distinct anomalies from the relevant check types within the measurement period (cutoff date)</li> <li>Formula (container): Scores start at 100 and decrease based on anomaly count<ul> <li>First anomaly causes a 40-point drop (score becomes 60)</li> <li>Each additional anomaly has diminishing impact</li> <li>Formula: <code>Score = 100 - min(100 \u00d7 (1 - e^(-k \u00d7 anomaly_count)), 100)</code></li> <li>Where k is calibrated so one anomaly = 40% score reduction</li> </ul> </li> <li>Applicable rule types: Time distribution size, freshness constraintsSee Appendix: Timeliness Rule Types for the full Timeliness rule type list.</li> </ul> <p>Score Interpretation</p> <ul> <li>100: No timeliness anomalies detected</li> <li>60: One anomaly detected (40-point penalty)</li> <li>40-60: Multiple anomalies with diminishing penalties</li> <li>0-40: Significant anomaly counts indicating serious issues</li> <li>None/Null: No checks of this type configured (unmeasured)</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#volumetrics-dimension","title":"Volumetrics Dimension","text":"<p>The Volumetrics score analyzes consistency in data size and shape over time.</p> <p>Shared Scoring Formula</p> <p>Timeliness and Volumetrics both use the same exponential penalty formula for anomaly counts. This consistency ensures comparable scoring behavior across dimensions, even though the anomalies being measured differ.</p> <p>How Volumetrics is Calculated</p> <ul> <li>Scale: 0 to 100 based on volumetric stability</li> <li>Field level: Directly inherited from the container's volumetrics score</li> <li>Anomaly counting: Counts distinct anomalies from the relevant check types within the measurement period (cutoff date)</li> <li>Formula (container): Scores start at 100 and decrease based on anomaly count<ul> <li>First anomaly causes a 40-point drop (score becomes 60)</li> <li>Each additional anomaly has diminishing impact</li> <li>Formula: <code>Score = 100 - min(100 \u00d7 (1 - e^(-k \u00d7 anomaly_count)), 100)</code></li> <li>Where k is calibrated so one anomaly = 40% score reduction</li> </ul> </li> <li>Applicable rule types: Row count size, partition size constraintsSee Appendix: Volumetric Rule Types for the full Volumetric rule type list.</li> </ul> <p>Examples</p> <ul> <li>Container with consistent record counts per partition \u2192 Score ~100</li> <li>Container showing unexpected spikes or drops in volume \u2192 Score ~75</li> <li>Container with erratic or missing time distributions \u2192 Score ~50</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#additional-container-level-factors","title":"Additional Container-Level Factors","text":"<p>Beyond the eight dimensions, containers incorporate:</p> <ul> <li>Scanning frequency: More frequent scanning improves confidence and boosts coverage scores</li> <li>Profiling frequency: Regular profiling ensures statistics remain current and boosts consistency scores</li> <li>Field tag weights: Field weights are used when calculated weighted averages for container-level dimensions</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#most-impactful-dimensions","title":"Most Impactful Dimensions","text":"<p>While specific scoring weights can be customized, dimensions that typically most influence quality scores are:</p> <ul> <li>Coverage: Asserting frequent, comprehensive quality checks is critical</li> <li>Accuracy: Large volumes of anomalies severely impact scores</li> <li>Consistency: Erratic or unstable data characteristics reduce confidence</li> </ul>"},{"location":"quality-scores/what-are-quality-scores/#how-to-interpret-and-use-quality-scores","title":"How to Interpret and Use Quality Scores","text":"<p>Quality scores are dynamic measures of confidence that reflect the intrinsic quality of your data. It's important to recognize that different types of data will have varying levels of inherent quality. To illustrate this point, let's consider a standard mailing address in the USA. A typical schema representing a mailing address includes fields such as:</p> <ul> <li>Addressee</li> <li>Street</li> <li>Street 2</li> <li>City</li> <li>State</li> <li>Postal Code</li> </ul> <p>The \"State\" field, which is naturally constrained by a limited set of known values, will inherently have a higher level of quality compared to the \"Street 2\" field. \"Street 2\" typically holds free-form text ranging from secondary unit designations to \"care of\" instructions and may often be left blank. In contrast, \"State\" is a required field for any valid mailing address.</p> <p>Consider the level of confidence you would have in making business decisions based on the values held in the \"State\" field versus the \"Street 2\" field. This thought exercise demonstrates how the Qualytics Quality Score (with default configuration) should be interpreted.</p> <p>While there are steps you can take to improve the quality score of the \"Street 2\" field, it would be unrealistic to expect it to meet the same standards as the \"State\" field. Instead, your efforts should focus on the change in measured quality score over time, with the goal of raising scores to an acceptable level of quality that meets your specific business needs.</p> <p>To further explore how to respond to Quality Scores, let's consider the business requirements for capturing \"Street 2\" and its downstream use:</p> <ul> <li> <p>If the primary use case for this address is to support credit card payment processing, where \"Street 2\" is rarely, if ever, considered, there may be no business need to focus on improving the quality of this field over time. In this case, you can reduce the impact of this field on the overall measured quality of the Address by applying a Tag with a negative weight modifier.</p> </li> <li> <p>On the other hand, if the primary use case for this address is to reliably ship a physical product to an intended recipient, ensuring a higher level of quality for the \"Street 2\" field becomes necessary. In this scenario, you may take actions such as defining additional data quality checks for the field, increasing the frequency of profiling and scanning, establishing a completeness goal, and working with upstream systems to enforce it over time.</p> </li> </ul> <p>Important</p> <p>The key to effectively adopting Qualytics's Quality Scores into your data quality management efforts is to understand that it reflects both the intrinsic quality of the data and the steps taken to improve confidence that the data is fit for your specific business needs.</p> <p>Fitness for Purpose in Practice</p> <p>Remember: Quality Scores are not absolute \u201cgrades.\u201d They reflect how well your data is suited for its intended business use, influenced by weighting, tagging, and anomaly detection. Two datasets may have different scores but still both be \"fit for purpose\" depending on use case.</p>"},{"location":"quality-scores/what-are-quality-scores/#customizing-quality-score-weights-and-decay-time","title":"Customizing Quality Score Weights and Decay Time","text":"<p>The default quality score weightings and decay time represent best practice considerations as codified by the data quality experts at Qualytics and our work with enterprises of all shapes, sizes, and sectors. We recommend that both be left in their default state for all customers and use cases.</p> <p>That said, we recognize that customers may desire to alter our default scoring algorithms for a variety of reasons, and we support that optionality by allowing administrators to tailor the impact of each quality dimension on the total score by adjusting their weights. This alters the scoring algorithm to align with customized governance priorities. Additionally, the decay period for considering past data events defaults to 180 days but can be customized to fit your operational needs, ensuring the scores reflect the most relevant data quality insights for your organization.</p> <p>Use Caution When Customizing Weights</p> <p>We strongly recommend retaining default weights unless governance priorities clearly justify changes.</p> <ul> <li>Adjusting weights can significantly alter how anomalies impact overall scores.</li> <li>Misaligned weights may cause misleading signals about data quality.</li> </ul> <p>Proceed carefully, and document any custom weighting rationale.</p>"},{"location":"quality-scores/what-are-quality-scores/#appendix-rule-types","title":"Appendix: Rule Types","text":"<p>The following lists summarize which rule types contribute to each dimension\u2019s quality score.</p>"},{"location":"quality-scores/what-are-quality-scores/#conformity-rule-types","title":"Conformity Rule Types","text":"No. Rule Type 1. Matches Pattern 2. Min Length 3. Max Length 4. Data Diff 5. Is Type 6. Entity Resolution 7. Expected Schema 8. Field Count 9. Is Credit Card 10. Is Address 11. Contains Credit Card 12. Contains URL 13. Contains Email 14. Contains Social Security Number"},{"location":"quality-scores/what-are-quality-scores/#precision-rule-types","title":"Precision Rule Types","text":"No. Rule Type 1. After Date Time 2. Before Date Time 3. Between 4. Between Times 5. Equal To 6. Equal To Field 7. Greater Than 8. Greater Than Field 9. Less Than 10. Less Than Field 11. Max Value 12. Min Value 13. Not Future 14. Not Negative 15. Positive 16. Predicted By 17. Sum"},{"location":"quality-scores/what-are-quality-scores/#volumetric-rule-types","title":"Volumetric Rule Types","text":"No. Rule Type 1. Volumetric 2. Min Partition Size 3. Max Partition Size"},{"location":"quality-scores/what-are-quality-scores/#timeliness-rule-types","title":"Timeliness Rule Types","text":"No. Rule Type 1. Freshness 2. Time Distribution Size"},{"location":"settings/connections/manage-connections/","title":"Connections","text":"<p>The Connections Management section allows you to manage global configurations for various connections to different data sources. This provides you with a centralized interface for managing all the data connections, ensuring efficient data integration and enrichment processes. You can easily navigate and manage your connections by utilizing the search, sort, edit, and delete features.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"settings/connections/manage-connections/#navigation-to-connection","title":"Navigation to Connection","text":"<p>Step 1: Log in to your Qualytics account and click the Settings button on the left sidebar of the interface. </p> <p></p> <p>Step 2: By default, you will be navigated to the Connections section.</p> <p></p>"},{"location":"settings/connections/manage-connections/#manage-connection","title":"Manage Connection","text":"<p>You can effectively manage your connections by editing, deleting, and adding datastores to maintain accuracy and efficiency.</p> <p>Warning</p> <p>Before deleting a connection, ensure that all associated datastores and enrichment datastores have been removed.</p>"},{"location":"settings/connections/manage-connections/#edit-connection","title":"Edit Connection","text":"<p>You can edit connections to update details such as name, account, role, warehouse, and authentication to improve performance. This keeps connection settings up-to-date and suited to your data needs.</p> <p>Note</p> <p>You can only edit the connection name and connection details, but you are not able to edit the connector itself.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the connection that you want to edit, then click on Edit from the dropdown menu.</p> <p></p> <p>Step 2: Edit the connection details as needed.</p> <p>Note</p> <p>Connection details vary from connection to connection, which means that each connection may have its unique configuration settings.</p> <p></p> <p>Step 3: Once you have updated the values, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>Step 4: After the connection is verified, click on the Save button to save the changes.</p> <p></p>"},{"location":"settings/connections/manage-connections/#delete-connection","title":"Delete Connection","text":"<p>This allows you to remove outdated or unnecessary connections to maintain a clean and efficient network configuration.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the connection that you want to delete, then click on Delete from the dropdown menu.</p> <p></p> <p>Step 2: A modal window Delete Connection will appear.</p> <p>Warning</p> <p>Source Datastores and Enrichment Datastores that are associated must be removed before deleting the connection.</p> <p></p> <p>Step 3: Enter the Name of the Connection in the given field (confirmation check) and then click on the I\u2019M SURE, DELETE THIS CONNECTION button to delete the connection.</p> <p></p>"},{"location":"settings/connections/manage-connections/#add-datastore","title":"Add Datastore","text":"<p>You can add new or existing datastores and enrichment datastores directly from the connection, making it easy to manage and access your data while ensuring all sources are connected and available.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the connection where you want to add a datastore, then click on Add Datastore from the dropdown menu.</p> <p></p> <p>A modal window labeled Add Datastore will appear, giving you options to connect a datastore. For more information on adding a datastore, please refer to the Configuring Source Datastores section.</p> <p>Once you have successfully added a datastore to the connection, a success message will appear.</p>"},{"location":"settings/connections/manage-connections/#view-connection","title":"View Connection","text":"<p>Once you have added a new datastore and enrichment datastore, you can view them in the connections list.</p> <p></p>"},{"location":"settings/connections/manage-connections/#sort-connection","title":"Sort Connection","text":"<p>You can sort your connections by Name and Created Date to easily find and manage them.</p> <p></p>"},{"location":"settings/connections/manage-connections/#filter-connection","title":"Filter Connection","text":"<p>You can filter connections by selecting specific data source types from the dropdown menu, making it easier to locate and manage the desired connections.</p> <p></p> <p>Info</p> <p>Users can search across all filter inputs using typos, partial terms, or abbreviations. The system intelligently matches relevant results, making it easier to find what they need without exact inputs.</p> <p></p>"},{"location":"settings/integrations/overview/","title":"Integrations","text":"<p>The Qualytics platform seamlessly connects with your enterprise technology ecosystem, transforming data quality management from a standalone process into an integral part of your data operations. Our comprehensive integration capabilities ensure that data quality insights and actions flow naturally through your existing tools and workflows.</p>"},{"location":"settings/integrations/overview/#available-integrations","title":"Available Integrations","text":""},{"location":"settings/integrations/overview/#source-datastores","title":"Source Datastores","text":"<p>Connect directly to your data wherever it lives - from traditional databases to modern cloud storage platforms. Qualytics provides unified quality management across your entire data landscape through our Datastore framework.</p>"},{"location":"settings/integrations/overview/#data-catalogs","title":"Data Catalogs","text":"<p>Surface data quality insights directly within your enterprise data catalogs, enhancing data discovery and governance with rich quality metrics and real-time anomaly detection.</p>"},{"location":"settings/integrations/overview/#compute","title":"Compute","text":"<p>Leverage flexible deployment options to optimize performance and resource utilization, whether using our managed Kubernetes infrastructure or your own external compute environment.</p>"},{"location":"settings/integrations/overview/#alerting","title":"Alerting","text":"<p>Receive instant notifications about data quality events through your enterprise messaging platforms, enabling rapid response to quality issues as they emerge.</p>"},{"location":"settings/integrations/overview/#ticketing","title":"Ticketing","text":"<p>Track and manage data quality initiatives within your existing project management tools, seamlessly incorporating quality management into your team's established workflows.</p>"},{"location":"settings/integrations/overview/#workflow","title":"Workflow","text":"<p>Embed data quality checks directly into your data pipelines and transformation processes, ensuring quality gates are enforced at every stage of your data lifecycle.</p>"},{"location":"settings/integrations/overview/#analytics","title":"Analytics","text":"<p>Visualize data quality metrics and trends through your preferred business intelligence tools, providing actionable insights to stakeholders across your organization.</p>"},{"location":"settings/integrations/overview/#single-sign-on","title":"Single Sign-On","text":"<p>Enable secure, frictionless access to Qualytics through your enterprise identity provider, maintaining consistent authentication and access control policies.</p>"},{"location":"settings/integrations/overview/#ai-agents","title":"AI &amp; Agents","text":"<p>Leverage AI-powered natural language interfaces to manage data quality conversationally. Connect AI assistants via the Model Context Protocol (MCP) for interactive use, or integrate the Agentic API into your applications and automation workflows using your own LLM API key.</p>"},{"location":"settings/integrations/ai/agentic/","title":"Agentic API","text":"<p>The Qualytics Agentic API brings the same conversational AI capabilities available through MCP directly into your applications and workflows. Using your own LLM API key, you can integrate natural language data quality management into custom tools, scripts, automation pipelines, and internal platforms.</p>"},{"location":"settings/integrations/ai/agentic/#overview","title":"Overview","text":"<p>While the MCP integration is designed for interactive AI assistants like Claude Desktop, the Agentic API enables you to:</p> <ul> <li>Build custom applications that leverage natural language for data quality tasks</li> <li>Integrate AI-powered data quality into existing automation workflows</li> <li>Create internal tools and chatbots that interact with your data infrastructure</li> <li>Use your preferred LLM provider and manage your own API costs</li> </ul> <p>The Agentic API provides the same capabilities as MCP\u2014datastore exploration, data transformations, quality check creation, and anomaly investigation\u2014accessible through standard REST endpoints.</p>"},{"location":"settings/integrations/ai/agentic/#authentication","title":"Authentication","text":"<p>All Agentic API endpoints require authentication using your Qualytics Personal API Token (PAT) or a designated service account token.</p> <p>Include the token in the <code>Authorization</code> header:</p> <pre><code>Authorization: Bearer YOUR_QUALYTICS_API_TOKEN\n</code></pre> <p>For instructions on generating a token, see Tokens.</p>"},{"location":"settings/integrations/ai/agentic/#llm-configuration","title":"LLM Configuration","text":"<p>Before using the Agentic API, you must configure your LLM provider credentials. This allows Qualytics to use your API key when making calls to the language model.</p>"},{"location":"settings/integrations/ai/agentic/#supported-providers","title":"Supported Providers","text":"<p>The Agentic API supports major LLM providers including:</p> <ul> <li>Anthropic (Claude)</li> <li>OpenAI (GPT-4, GPT-4o)</li> <li>Azure OpenAI</li> <li>Other OpenAI-compatible APIs</li> </ul>"},{"location":"settings/integrations/ai/agentic/#managing-llm-configuration","title":"Managing LLM Configuration","text":""},{"location":"settings/integrations/ai/agentic/#create-configuration","title":"Create Configuration","text":"<p>Set up your LLM provider credentials:</p> <pre><code>curl -X POST \"https://your-qualytics.qualytics.io/api/agent/llm-config\" \\\n  -H \"Authorization: Bearer YOUR_QUALYTICS_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"provider\": \"anthropic\",\n    \"api_key\": \"YOUR_LLM_API_KEY\",\n    \"model\": \"claude-sonnet-4-20250514\"\n  }'\n</code></pre>"},{"location":"settings/integrations/ai/agentic/#view-configuration","title":"View Configuration","text":"<p>Check your current LLM configuration:</p> <pre><code>curl -X GET \"https://your-qualytics.qualytics.io/api/agent/llm-config\" \\\n  -H \"Authorization: Bearer YOUR_QUALYTICS_TOKEN\"\n</code></pre>"},{"location":"settings/integrations/ai/agentic/#update-configuration","title":"Update Configuration","text":"<p>Modify your LLM settings:</p> <pre><code>curl -X PATCH \"https://your-qualytics.qualytics.io/api/agent/llm-config\" \\\n  -H \"Authorization: Bearer YOUR_QUALYTICS_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-20250514\"\n  }'\n</code></pre>"},{"location":"settings/integrations/ai/agentic/#delete-configuration","title":"Delete Configuration","text":"<p>Remove your LLM configuration:</p> <pre><code>curl -X DELETE \"https://your-qualytics.qualytics.io/api/agent/llm-config\" \\\n  -H \"Authorization: Bearer YOUR_QUALYTICS_TOKEN\"\n</code></pre>"},{"location":"settings/integrations/ai/agentic/#capabilities","title":"Capabilities","text":""},{"location":"settings/integrations/ai/agentic/#chat-with-agent","title":"Chat with Agent","text":"<p>The chat endpoint provides a conversational interface for exploring and managing your data quality infrastructure. This is the most flexible endpoint, allowing free-form natural language interactions.</p> <pre><code>curl -X POST \"https://your-qualytics.qualytics.io/api/agent/chat\" \\\n  -H \"Authorization: Bearer YOUR_QUALYTICS_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"message\": \"What tables are in our sales_db datastore and what quality checks do we have on them?\"\n  }'\n</code></pre> <p>Use Cases:</p> <ul> <li>Interactive Exploration: Build chatbots or conversational interfaces that let users explore data assets naturally</li> <li>Multi-step Workflows: Handle complex requests that require understanding context and making multiple decisions</li> <li>General Assistance: Answer questions about data quality status, anomaly patterns, or check configurations</li> </ul> <p>Example Prompts:</p> <ul> <li>\"Show me the schema for the customer_orders table in our PostgreSQL datastore\"</li> <li>\"What anomalies were detected in the last 24 hours?\"</li> <li>\"Which quality checks are failing most frequently across all our datastores?\"</li> <li>\"Help me understand why the order_total check keeps failing\"</li> </ul>"},{"location":"settings/integrations/ai/agentic/#execute-prompt","title":"Execute Prompt","text":"<p>For simpler, single-turn interactions where you need a direct response without conversational context:</p> <pre><code>curl -X POST \"https://your-qualytics.qualytics.io/api/agent/prompt\" \\\n  -H \"Authorization: Bearer YOUR_QUALYTICS_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"List all containers in the analytics_warehouse datastore\"\n  }'\n</code></pre> <p>Use Cases:</p> <ul> <li>Scripted Queries: Get specific information programmatically without maintaining conversation state</li> <li>Quick Lookups: Retrieve datastore metadata, check statuses, or anomaly counts</li> <li>Report Generation: Generate summaries or descriptions for automated reporting</li> </ul>"},{"location":"settings/integrations/ai/agentic/#transform-dataset","title":"Transform Dataset","text":"<p>Create computed assets\u2014tables, files, or cross-datastore joins\u2014through natural language descriptions:</p> <pre><code>curl -X POST \"https://your-qualytics.qualytics.io/api/agent/transform-dataset\" \\\n  -H \"Authorization: Bearer YOUR_QUALYTICS_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"description\": \"Create a computed table in sales_db that aggregates daily revenue by product category from the transactions table, including only completed orders from the last 90 days\"\n  }'\n</code></pre> <p>Use Cases:</p> <ul> <li>Automated Data Preparation: Integrate dataset creation into ETL pipelines or data workflows</li> <li>Self-Service Analytics: Let business users create derived datasets without writing SQL</li> <li>Cross-System Integration: Build unified views across databases and data lakes programmatically</li> </ul> <p>Example Descriptions:</p> <ul> <li>\"Join the customers table from our Snowflake warehouse with the support_tickets table from PostgreSQL on customer_id, filtering to only active customers\"</li> <li>\"Create a computed file from our S3 landing zone that filters out test records and standardizes the date format\"</li> <li>\"Build a daily summary table that calculates average order value and order count by region\"</li> </ul>"},{"location":"settings/integrations/ai/agentic/#generate-quality-check","title":"Generate Quality Check","text":"<p>Create data quality checks by describing the business rule or validation requirement:</p> <pre><code>curl -X POST \"https://your-qualytics.qualytics.io/api/agent/generate-quality-check\" \\\n  -H \"Authorization: Bearer YOUR_QUALYTICS_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"description\": \"Ensure the email field in the customers table of sales_db is never null and matches a valid email format\"\n  }'\n</code></pre> <p>Use Cases:</p> <ul> <li>Bulk Check Creation: Programmatically establish quality checks across multiple datasets</li> <li>Rule Migration: Translate business rules from documentation into executable quality checks</li> <li>Compliance Automation: Set up regulatory validation checks based on policy descriptions</li> <li>Data Contract Implementation: Automatically create checks from data contract specifications</li> </ul> <p>Example Descriptions:</p> <ul> <li>\"The order_total in the orders table should always be positive and less than 1,000,000\"</li> <li>\"ship_date must be after order_date for all records in the shipments table\"</li> <li>\"The status field should only contain 'pending', 'processing', 'shipped', or 'delivered'\"</li> <li>\"customer_id in transactions must exist in the customers table\"</li> </ul>"},{"location":"settings/integrations/ai/agentic/#investigate-anomaly","title":"Investigate Anomaly","text":"<p>Get detailed, contextual explanations of data quality issues:</p> <pre><code>curl -X POST \"https://your-qualytics.qualytics.io/api/agent/investigate-anomaly\" \\\n  -H \"Authorization: Bearer YOUR_QUALYTICS_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"anomaly_id\": 12345\n  }'\n</code></pre> <p>Use Cases:</p> <ul> <li>Automated Alerting: Enrich alert notifications with AI-generated context and impact analysis</li> <li>Incident Reports: Generate human-readable explanations for data quality incidents</li> <li>Triage Automation: Programmatically assess anomaly severity and suggest remediation steps</li> <li>Stakeholder Updates: Create plain-language summaries for business audiences</li> </ul> <p>What You Get:</p> <ul> <li>Clear explanation of what data quality rule was violated</li> <li>Context about the affected dataset and fields</li> <li>Count and pattern of affected records</li> <li>Potential business impact</li> <li>Suggested investigation or remediation steps</li> </ul>"},{"location":"settings/integrations/ai/agentic/#integration-patterns","title":"Integration Patterns","text":""},{"location":"settings/integrations/ai/agentic/#automated-quality-check-setup","title":"Automated Quality Check Setup","text":"<p>When onboarding a new data source, automatically generate quality checks based on schema analysis:</p> <pre><code>import requests\n\nQUALYTICS_URL = \"https://your-qualytics.qualytics.io\"\nTOKEN = \"your_token\"\n\n# Discover the schema\nresponse = requests.post(\n    f\"{QUALYTICS_URL}/api/agent/prompt\",\n    headers={\"Authorization\": f\"Bearer {TOKEN}\"},\n    json={\"prompt\": \"Analyze the customers table in sales_db and suggest appropriate quality checks based on the field types and names\"}\n)\n\nsuggestions = response.json()\n\n# Create checks based on suggestions\nfor check_description in suggestions[\"recommended_checks\"]:\n    requests.post(\n        f\"{QUALYTICS_URL}/api/agent/generate-quality-check\",\n        headers={\"Authorization\": f\"Bearer {TOKEN}\"},\n        json={\"description\": check_description}\n    )\n</code></pre>"},{"location":"settings/integrations/ai/agentic/#enriched-anomaly-alerts","title":"Enriched Anomaly Alerts","text":"<p>Enhance your alerting pipeline with AI-generated context:</p> <pre><code>def handle_anomaly_alert(anomaly_id):\n    # Get AI-generated investigation\n    response = requests.post(\n        f\"{QUALYTICS_URL}/api/agent/investigate-anomaly\",\n        headers={\"Authorization\": f\"Bearer {TOKEN}\"},\n        json={\"anomaly_id\": anomaly_id}\n    )\n\n    investigation = response.json()\n\n    # Send enriched alert to Slack/Teams/PagerDuty\n    send_alert(\n        title=f\"Data Quality Issue: {investigation['summary']}\",\n        description=investigation['explanation'],\n        impact=investigation['business_impact'],\n        suggested_actions=investigation['recommendations']\n    )\n</code></pre>"},{"location":"settings/integrations/ai/agentic/#self-service-data-preparation","title":"Self-Service Data Preparation","text":"<p>Build an internal tool that lets analysts create datasets through natural language:</p> <pre><code>@app.route(\"/create-dataset\", methods=[\"POST\"])\ndef create_dataset():\n    user_request = request.json[\"description\"]\n\n    response = requests.post(\n        f\"{QUALYTICS_URL}/api/agent/transform-dataset\",\n        headers={\"Authorization\": f\"Bearer {TOKEN}\"},\n        json={\"description\": user_request}\n    )\n\n    result = response.json()\n    return {\n        \"status\": \"created\",\n        \"dataset_name\": result[\"asset_name\"],\n        \"message\": f\"Your dataset is now available for profiling and quality scanning\"\n    }\n</code></pre>"},{"location":"settings/integrations/ai/agentic/#best-practices","title":"Best Practices","text":""},{"location":"settings/integrations/ai/agentic/#prompt-design","title":"Prompt Design","text":"<p>For best results when using the Agentic API:</p> <ul> <li>Be specific about datastores and tables: Include the datastore name and container/table name when referencing data assets</li> <li>Describe business intent: Explain what you're trying to accomplish, not just the technical operation</li> <li>Include constraints: Mention filtering criteria, date ranges, or other limitations upfront</li> </ul>"},{"location":"settings/integrations/ai/agentic/#cost-management","title":"Cost Management","text":"<p>Since you're using your own LLM API key:</p> <ul> <li>Use the <code>prompt</code> endpoint for simple, single-turn queries instead of <code>chat</code> when conversation context isn't needed</li> <li>Cache responses for frequently-requested information</li> <li>Consider rate limiting in your applications to control API costs</li> </ul>"},{"location":"settings/integrations/ai/agentic/#error-handling","title":"Error Handling","text":"<p>The Agentic API returns structured responses that include:</p> <ul> <li>Success/failure status</li> <li>Generated SQL or configuration (when applicable)</li> <li>Validation errors or issues with the request</li> <li>Suggestions for how to refine ambiguous requests</li> </ul> <p>Always check response status and handle cases where the AI may need clarification or additional context.</p>"},{"location":"settings/integrations/ai/mcp/","title":"Model Context Protocol (MCP)","text":"<p>The Qualytics platform exposes an MCP (Model Context Protocol) service that enables AI assistants and LLM-powered applications to interact directly with your data quality infrastructure. This integration allows you to manage data quality through natural language conversations\u2014exploring datastores, creating transformations, building quality checks, and investigating anomalies without writing code or navigating complex interfaces.</p>"},{"location":"settings/integrations/ai/mcp/#what-is-mcp","title":"What is MCP?","text":"<p>The Model Context Protocol is an open standard that enables AI assistants to securely connect to external data sources and tools. By exposing Qualytics functionality through MCP, you can use AI-powered tools like Claude Desktop, Cursor, or custom LLM applications to manage data quality workflows conversationally.</p>"},{"location":"settings/integrations/ai/mcp/#endpoint","title":"Endpoint","text":"<p>The MCP service is available at your Qualytics instance URL:</p> <pre><code>https://your-qualytics-instance.qualytics.io/mcp\n</code></pre>"},{"location":"settings/integrations/ai/mcp/#authentication","title":"Authentication","text":"<p>The MCP service uses the same authentication mechanism as the Qualytics API. You'll need a Personal API Token (PAT) to authenticate requests.</p> <p>To generate a token:</p> <ol> <li>Navigate to Settings &gt; Tokens in your Qualytics instance</li> <li>Click Generate Token</li> <li>Copy and securely store the generated token</li> </ol> <p>For detailed instructions, see Tokens.</p>"},{"location":"settings/integrations/ai/mcp/#capabilities","title":"Capabilities","text":"<p>This video demonstrates the power of one-shot prompting using the Qualytics MCP server</p>"},{"location":"settings/integrations/ai/mcp/#datastore-exploration","title":"Datastore Exploration","text":"<p>When you connect an AI assistant to Qualytics via MCP, it gains the ability to explore your data landscape and understand the structure of your datastores. This enables conversations like:</p> <ul> <li>\"What tables are in our sales database?\"</li> <li>\"Show me the schema for the customer_orders table\"</li> <li>\"What fields are available in the transactions container?\"</li> </ul> <p>The AI assistant retrieves schema information, field definitions, and metadata to help you understand your data without manually navigating the Qualytics UI or writing queries. This is particularly valuable when onboarding to unfamiliar datasets or when you need quick answers about data structure during analysis.</p> <p>Business Use Cases:</p> <ul> <li>Data Discovery: New team members can conversationally explore available data assets and understand what information is accessible</li> <li>Ad-hoc Analysis Preparation: Analysts can quickly understand table structures before building reports or dashboards</li> <li>Documentation Support: Generate descriptions of data assets by asking the AI to summarize what it finds in your datastores</li> </ul>"},{"location":"settings/integrations/ai/mcp/#query-validation","title":"Query Validation","text":"<p>Before running queries against production systems, you can ask the AI assistant to validate your SQL. The assistant executes queries with limits through the Qualytics service account to verify syntax, table/column existence, and permissions.</p> <ul> <li>\"Can you check if this query will work against our Snowflake warehouse?\"</li> <li>\"Validate this SQL before I use it in my ETL pipeline\"</li> <li>\"Does the service account have access to the finance schema?\"</li> </ul> <p>Business Use Cases:</p> <ul> <li>Development Workflow: Validate queries during development without risking production impact</li> <li>Permission Auditing: Verify what data the Qualytics service account can access</li> <li>SQL Debugging: Get immediate feedback on query errors with AI-assisted explanations</li> </ul>"},{"location":"settings/integrations/ai/mcp/#data-transformations","title":"Data Transformations","text":"<p>One of the most powerful capabilities is creating computed assets through conversation. Instead of manually configuring transformations in the UI, you can describe what you need in plain language.</p>"},{"location":"settings/integrations/ai/mcp/#computed-tables-jdbc-datastores","title":"Computed Tables (JDBC Datastores)","text":"<p>For databases with SQL support, you can create stored queries that execute in the source database using its native dialect:</p> <ul> <li>\"Create a computed table that aggregates daily sales by region from the transactions table\"</li> <li>\"Build me a view that joins customers with their most recent orders\"</li> <li>\"Set up a transformation that filters the events table to only include completed transactions from this quarter\"</li> </ul> <p>The AI understands your database dialect and generates appropriate SQL. The resulting computed table becomes a new container in Qualytics that you can profile and scan for quality issues.</p> <p>Business Use Cases:</p> <ul> <li>Derived Metrics: Create aggregated views for KPI tracking without modifying source systems</li> <li>Data Preparation: Build filtered or transformed datasets for downstream analysis</li> <li>Business Logic Encapsulation: Encode complex business rules into reusable computed tables</li> </ul>"},{"location":"settings/integrations/ai/mcp/#computed-files-dfs-datastores","title":"Computed Files (DFS Datastores)","text":"<p>For file-based sources like S3, ADLS, or GCS, transformations are applied using Spark SQL:</p> <ul> <li>\"Transform our raw CSV files to only include records where status is 'active'\"</li> <li>\"Create a computed file that casts the amount column to decimal and renames customer_id to cust_id\"</li> <li>\"Filter the parquet files to exclude test data based on the environment flag\"</li> </ul> <p>Business Use Cases:</p> <ul> <li>Data Lake Curation: Create clean, transformed views of raw landing zone data</li> <li>Schema Standardization: Normalize field names and types across different file sources</li> <li>Data Filtering: Remove test data, PII, or irrelevant records before profiling</li> </ul>"},{"location":"settings/integrations/ai/mcp/#cross-datastore-joins","title":"Cross-Datastore Joins","text":"<p>Perhaps the most powerful transformation capability is joining data across completely different systems:</p> <ul> <li>\"Join the customer table from our PostgreSQL database with the order events from S3\"</li> <li>\"Combine Snowflake sales data with the product catalog stored in BigQuery\"</li> <li>\"Create a unified view of transactions from our legacy Oracle system and the new cloud warehouse\"</li> </ul> <p>The AI configures the join to execute in Spark, loading data from both sources and combining them without requiring you to build ETL pipelines or manually move data.</p> <p>Business Use Cases:</p> <ul> <li>360\u00b0 Customer Views: Combine customer data scattered across CRM, data warehouse, and data lake</li> <li>Cross-System Reconciliation: Compare data between systems during migrations or audits</li> <li>Unified Analytics: Enable analysis across organizational data silos without infrastructure changes</li> </ul>"},{"location":"settings/integrations/ai/mcp/#quality-check-management","title":"Quality Check Management","text":"<p>You can create and manage data quality checks through natural conversation, without needing to understand the specifics of each rule type:</p> <ul> <li>\"Make sure the email field in the customers table is never null\"</li> <li>\"Add a check that order_total is always between 0 and 1,000,000\"</li> <li>\"Verify that ship_date is always after order_date\"</li> <li>\"Create a check that ensures the status field only contains 'pending', 'shipped', or 'delivered'\"</li> <li>\"Add validation that the phone_number field matches a valid US phone format\"</li> </ul> <p>The AI assistant translates your intent into the appropriate quality check configuration, selecting the right rule type and setting up the necessary parameters.</p> <p>Business Use Cases:</p> <ul> <li>Rapid Quality Framework Setup: Quickly establish quality checks across new datasets through conversation</li> <li>Business Rule Implementation: Translate business requirements directly into quality checks without technical translation</li> <li>Compliance Validation: Set up checks for regulatory requirements by describing them in plain language</li> <li>Data Contract Enforcement: Define expectations for data produced by upstream systems</li> </ul>"},{"location":"settings/integrations/ai/mcp/#anomaly-investigation","title":"Anomaly Investigation","text":"<p>When quality issues are detected, you can investigate them conversationally:</p> <ul> <li>\"Tell me about the anomalies found in yesterday's scan\"</li> <li>\"What's wrong with anomaly 12345?\"</li> <li>\"Explain the business impact of the data quality issues in the orders table\"</li> <li>\"Which quality checks are failing most frequently?\"</li> </ul> <p>The AI retrieves comprehensive anomaly details including the failed checks, affected records count, and contextual information. It can then help you understand the business implications and suggest remediation approaches.</p> <p>Business Use Cases:</p> <ul> <li>Incident Triage: Quickly understand the scope and impact of data quality issues</li> <li>Root Cause Analysis: Investigate patterns in anomalies to identify upstream problems</li> <li>Stakeholder Communication: Generate plain-language explanations of technical issues for business audiences</li> <li>Remediation Planning: Get AI-assisted suggestions for addressing data quality problems</li> </ul>"},{"location":"settings/integrations/ai/mcp/#client-configuration","title":"Client Configuration","text":""},{"location":"settings/integrations/ai/mcp/#chatgpt","title":"ChatGPT","text":"<p>Under Apps, select Create app and complete the form as shown below:</p> <p></p> <p>Configure the fields as follows:</p> <ul> <li> <p>MCP Server URL   Enter your Qualytics MCP endpoint:   <code>https://&lt;your-qualytics-instance&gt;.qualytics.io/mcp/</code></p> </li> <li> <p>OAuth Secret   Paste your Qualytics API token.</p> </li> </ul> <p>After creating the app, ChatGPT will prompt you to authorize the connection. When prompted, paste the same Qualytics API token again.</p> <p></p> <p>Note</p> <p>The OAuth Secret and the authorization prompt both require the same Qualytics API token.</p>"},{"location":"settings/integrations/ai/mcp/#claude-desktop","title":"Claude Desktop","text":"<p>Add the following to your Claude Desktop configuration file:</p> macOSWindows <pre><code>// ~/Library/Application Support/Claude/claude_desktop_config.json\n{\n  \"mcpServers\": {\n    \"qualytics\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-remote\",\n        \"https://your-qualytics-instance.qualytics.io/api/mcp/\",\n        \"--header\",\n        \"Authorization: Bearer YOUR_API_TOKEN\"\n      ]\n    }\n  }\n}\n</code></pre> <pre><code>// %APPDATA%\\Claude\\claude_desktop_config.json\n{\n  \"mcpServers\": {\n    \"qualytics\": {\n      \"command\": \"cmd\",\n      \"args\": [\n        \"/c\",\n        \"npx\",\n        \"mcp-remote\",\n        \"https://your-qualytics-instance.qualytics.io/api/mcp/\",\n        \"--header\",\n        \"Authorization: Bearer YOUR_API_TOKEN\"\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"settings/integrations/ai/mcp/#cursor","title":"Cursor","text":"<p>Add the following to your Cursor MCP configuration:</p> <pre><code>{\n  \"mcpServers\": {\n    \"qualytics\": {\n      \"url\": \"https://your-qualytics-instance.qualytics.io/api/mcp/\",\n      \"headers\": {\n        \"Authorization\": \"Bearer YOUR_API_TOKEN\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"settings/integrations/ai/mcp/#example-conversations","title":"Example Conversations","text":""},{"location":"settings/integrations/ai/mcp/#setting-up-quality-monitoring-for-a-new-dataset","title":"Setting Up Quality Monitoring for a New Dataset","text":"<p>You: We just connected a new PostgreSQL database called \"ecommerce_prod\". Can you help me understand what's in there and set up some basic quality checks?</p> <p>AI: I found 12 tables in the ecommerce_prod datastore. The main ones appear to be <code>customers</code>, <code>orders</code>, <code>order_items</code>, <code>products</code>, and <code>inventory</code>. Would you like me to describe the schema for any of these in detail?</p> <p>You: Yes, tell me about the orders table and suggest some quality checks.</p> <p>AI: The orders table has 15 fields including order_id, customer_id, order_date, ship_date, status, and total_amount. Based on the schema, I'd recommend these quality checks:</p> <ul> <li>order_id should be unique</li> <li>customer_id should not be null</li> <li>total_amount should be positive</li> <li>ship_date should be after order_date when both are present</li> <li>status should be one of your valid status values</li> </ul> <p>Want me to create these checks?</p>"},{"location":"settings/integrations/ai/mcp/#investigating-a-data-quality-alert","title":"Investigating a Data Quality Alert","text":"<p>You: We got an alert about anomalies in the transactions table. What's going on?</p> <p>AI: I found 3 anomalies from the last scan. The most significant is a \"Not Null\" check failure on the <code>merchant_id</code> field\u2014247 records have null values where this field is expected to be populated. This started appearing after the batch that loaded at 2:00 AM. The affected records all have source_system = 'legacy_import'. Would you like me to show you the specific check that's failing or help investigate the source of these null values?</p>"},{"location":"settings/integrations/ai/mcp/#building-cross-system-analytics","title":"Building Cross-System Analytics","text":"<p>You: I need to analyze customer lifetime value but our customer data is in Salesforce (synced to Snowflake) and our transaction data is in our PostgreSQL operational database. Can you help?</p> <p>AI: I can create a computed join that combines data from both sources. I'll join the customer table from your Snowflake datastore with the transactions table from PostgreSQL on customer_id. What fields do you need in the result? At minimum I'd suggest customer details, total transaction count, total spend, and first/last transaction dates.</p> <p>You: That sounds right. Also filter out any test customers.</p> <p>AI: I've created a computed join called \"customer_lifetime_value\" that combines the data and excludes records where the customer email contains \"test\" or the customer_type is \"internal\". The join is now available as a container in Qualytics. Would you like me to set up quality checks on this new dataset or run a profile to understand the data distribution?</p>"},{"location":"settings/integrations/alerting/msft_teams/","title":"Microsoft Teams","text":"<p>Microsoft Teams integration in Qualytics enables seamless communication by connecting your Microsoft Teams workspace with data quality updates and notifications. It involves configuring Azure resources, providing necessary credentials, and establishing a direct link to your Teams workspace for alerts and communication.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"settings/integrations/alerting/msft_teams/#microsoft-teams-setup-guide","title":"Microsoft Teams Setup Guide","text":"<p>This section provides a comprehensive walkthrough to help you configure the necessary resources and retrieve the required credentials. By following this setup process, you'll have everything you need to complete the integration form.</p> <p>Warning</p> <p>Some steps in this guide may require administrator privileges in your Microsoft Azure environment. If you don't have the necessary permissions, you might need to coordinate with your IT department or someone with administrative access to your Azure tenant.</p>"},{"location":"settings/integrations/alerting/msft_teams/#creating-a-microsoft-entra-app-registration","title":"Creating a Microsoft Entra App Registration","text":"<p>The Microsoft Entra App Registration is used by Qualytics to provision Teams bot resources in your environment.</p> <p>Step 1: Log in to to the Microsoft Entra App Registrations, and select New registration from the main menu to create a new application.</p> <p></p> <p>Step 2: You will be navigated to the App registrations dashboard. Fill in the required details for the app registration:</p> <ul> <li> <p>Name: Enter a name for your app (e.g., Qualytics Bot Manager).</p> </li> <li> <p>Supported account types: Select \"Accounts in this organizational directory only (Single tenant)\".</p> </li> <li> <p>Redirect URI: Leave this field blank, as it is not required for this integration.</p> </li> </ul> <p></p> <p>Step 3: Click Register button to complete the app registration.</p> <p></p> <p>Step 4: After the app is registered, you\u2019ll be redirected to the Overview page, where the Application (client) ID is displayed. Copy this ID since it will be needed later for the Qualytics integration.</p> <p></p>"},{"location":"settings/integrations/alerting/msft_teams/#adding-api-permissions","title":"Adding API Permissions","text":"<p>The Microsoft Entra App needs the \"Application.ReadWrite.OwnedBy\" permission to create and manage bot resources.</p> <p>Step 1: In your app registration, go to the side panel and click Manage, then select API permissions from the dropdown.</p> <p></p> <p>Step 2: Click on Add permission to begin configuring access permissions for the app.</p> <p></p> <p>Step 3: A right side panel titled Request API permissions will appear. Select Microsoft Graph from the list of options.</p> <p></p> <p>Step 4: After selecting Microsoft Graph, choose Application permissions.</p> <p></p> <p>A dropdown appears search for Application.ReadWrite.OwnedBy, check the box under Application permissions, and click Add permissions.</p> <p></p> <p>Step 5: Once the permission is added, you'll return to the API permissions page. Click Grant admin consent for [Your Organization] to approve the selected permissions.</p> <p></p>"},{"location":"settings/integrations/alerting/msft_teams/#creating-a-client-secret","title":"Creating a Client Secret","text":"<p>The Client Secret authorizes Qualytics to programmatically create bot resources.</p> <p>Step 1: In your app registration, go to the side panel and click Manage, then select Certificates &amp; secrets from the dropdown.</p> <p></p> <p>Step 2: Click on + New client secret to generate a new secret key for the application.</p> <p></p> <p>Step 3: After clicking + New client secret, a panel will appear. Enter a description (e.g., Qualytics Integration) and choose an expiration period (up to 24 months). Then click Add.</p> <p></p> <p>Step 4: Once the client secret is created, copy the Value immediately and save it securely. This will be used as the App Client Secret for the Qualytics integration.</p> <p>Warning</p> <p>The client secret value is only displayed once immediately after creation. Make sure to copy and securely store it as you won't be able to retrieve it again.</p>"},{"location":"settings/integrations/alerting/msft_teams/#retrieving-the-azure-subscription-id","title":"Retrieving the Azure Subscription ID","text":"<p>The Subscription ID is required to manage bot resources in your Azure environment.</p> <p>Step 1:  Navigate to Subscriptions in the Azure Portal and select the subscription you want to use for the Teams integration.</p> <p></p> <p>Step 2: Copy the Subscription ID from the Overview section of your selected subscription. This ID is required later to assign roles and permissions for the Microsoft Teams integration.</p> <p></p>"},{"location":"settings/integrations/alerting/msft_teams/#verifying-the-microsoft-bot-service","title":"Verifying the Microsoft Bot Service","text":"<p>You need to verify if the Microsoft Bot Service resource provider is registered in your subscription.</p> <p>Step 1: In your subscription, click Settings from the left-hand menu, then select Resource providers from the dropdown.</p> <p></p> <p>Step 2:Search for Microsoft.BotService in the provider list and check that the Status is Registered.</p> <p></p> <p>Note</p> <p>The step 3 is only required if the resource provider is not already registered. If the Microsoft.BotService provider is already marked as \"Registered\" in your subscription, you can skip this step.</p> <p>Step 3 (maybe optional): Click Register to enable the resource provider if it's not already registered.</p> <p></p>"},{"location":"settings/integrations/alerting/msft_teams/#setting-up-the-resource-group","title":"Setting Up the Resource Group","text":"<p>The Resource Group will hold and manage the bot resources created by Qualytics.</p> <p>Step 1: Navigate to Resource Groups in the Azure Portal and click Create a resource to set up a new resource group if you don\u2019t already have one.</p> <p></p> <p>Step 2: Choose your Subscription, enter a Resource group name (e.g., <code>qualytics-msft-teams-rg</code>), select a Region, and then click Review + create.</p> <p></p> <p>Step 3: After clicking Review + create, you'll see a summary of the details. Once validated, click Create.</p> <p>Note</p> <p>Once created, note the Resource Group name for the Qualytics integration.</p> <p></p>"},{"location":"settings/integrations/alerting/msft_teams/#assigning-the-azure-bot-service-contributor-role","title":"Assigning the Azure Bot Service Contributor Role","text":"<p>The Microsoft Entra App needs the \"Azure Bot Service Contributor\" role to manage bot resources.</p> <p>Step 1: Navigate to your Resource Group and select Access control (IAM) from the left menu and click on  the Add and select Add Role Assignment from the dropdown.</p> <p></p> <p>Step 2: You\u2019ll be navigated to the Add role assignment tab. In the Role section, search and select Azure Bot Service Contributor Role, then click the Next button to continue.</p> <p></p> <p>Step 3: You will be navigated to the Members tab. Under Assign access to, select User, group, or service principal, then click on Select members.</p> <p></p> <p>A Select members panel will appear. Search for the Microsoft Entra app you created earlier (e.g., Qualytics Bot Manager), select it from the list, and click on the Select button.</p> <p></p> <p>Tip</p> <p>Enterprise Applications will only appear in the search results when you start typing the exact name used in your Entra App registration. If you don't see your app immediately, try typing the full name as you entered it when creating the app.</p> <p>Step 4: Click on Review + assign from the navigation bar and confirm the role assignment then click on Review + assign button.</p> <p></p>"},{"location":"settings/integrations/alerting/msft_teams/#getting-your-microsoft-teams-link","title":"Getting Your Microsoft Teams Link","text":"<p>You need to provide the link to your Microsoft Teams workspace.</p> <p>Step 1: Log in to your Microsoft Teams desktop or web application. Navigate to the team where you want to receive Qualytics notifications, then right-click on the team name and select Get link to team.</p> <p></p> <p>Step 2: A modal window titled Get a link to the team will appear. Click the Copy button to copy the team link.</p> <p></p>"},{"location":"settings/integrations/alerting/msft_teams/#integration-summary","title":"Integration Summary","text":"<p>Now that you've gathered all the necessary information and configured the Azure resources, you're ready to integrate Microsoft Teams with Qualytics.</p> <p>In the next section, we'll walk through the steps to access the Qualytics integration interface and enter these credentials to establish the connection between Qualytics and Microsoft Teams.</p>"},{"location":"settings/integrations/alerting/msft_teams/#navigation-to-integration","title":"Navigation to Integration","text":"<p>Step 1: Log in to your Qualytics account and click the \"Settings\" button on the left side panel of the interface.</p> <p></p> <p>Step 2: By default, Connections tab will open. Click on the Integrations tab.</p> <p></p>"},{"location":"settings/integrations/alerting/msft_teams/#connect-microsoft-teams-integration","title":"Connect Microsoft Teams Integration","text":"<p>Connect Microsoft Teams by providing necessary Azure credentials, configuring bot resources, and establishing a direct link to your Teams workspace for secure communication.</p> <p>Step 1: Click on the Connect button next to Microsoft Teams to connect to the Teams Integration.</p> <p></p> <p>A modal window titled \"Add Microsoft Teams Integration\" appears. Fill in the connection properties to connect to Microsoft Teams.</p> <p></p> <p>Step 2: Fill out the required provisioning properties for the Microsoft Teams integration:</p> No. Field Name Description 1. App Client ID The Application (client) ID from the Overview page of your Entra App registration. 2. App Client Secret The secret value you copied after creating a new client secret in your Entra App. 3. Azure Subscription ID The Subscription ID you copied from the Azure Subscriptions page. 4. Azure Resource Group Name The name of the Resource Group you created or selected for bot resources. 5. Microsoft Teams Link The team link you copied from Microsoft Teams using the \"Get link to team\" option. <p></p> <p>Step 3: Click the Provision and Next button to provision the app resources and proceed with publishing the Qualytics app to the Microsoft Teams App Catalog.</p> <p>Note</p> <p>Provisioning the app resources may take around 15 seconds to complete.</p> <p></p> <p>Once the app resources have been successfully provisioned, a confirmation message will appear stating, \"The Teams app resources have been successfully provisioned.\"</p> <p></p> <p>Step 4: Click the Publish button to publish the Qualytics app to your organization's Microsoft Teams App Catalog.</p> <p></p> <p>A microsoft dialog will appear asking you to accept the requested permissions. Click Accept to proceed with the publication.</p> <p>Once the app has been successfully published, a confirmation message will appear stating, \"The Teams app has been successfully published to your organization's App Catalog.\".</p> <p>Warning</p> <p>Microsoft may take up to 24 hours to make the app available in Teams after it's published.</p>"},{"location":"settings/integrations/alerting/msft_teams/#completing-the-teams-integration","title":"Completing the Teams Integration","text":"<p>After publishing the app to your organization's Teams App Catalog, the integration will show a \"Pending\" status in Qualytics until the app is installed in a Teams channel.</p> <p></p>"},{"location":"settings/integrations/alerting/msft_teams/#installing-the-app-in-microsoft-teams","title":"Installing the App in Microsoft Teams","text":"<p>To complete the integration, you need to install the Qualytics app in Microsoft Teams:</p> <p>Step 1: Log in to your Microsoft Teams desktop or web application and click on Apps in the left sidebar.</p> <p></p> <p>Step 2: After click on apps you will navigated to app dashboard. Select Built for your org to see custom apps for your organization and select the \"Qualytics\" app.</p> <p>Note</p> <p>If you don't see the app immediately, it might still be propagating through Microsoft's systems. This can take up to 24 hours.</p> <p></p> <p>Step 3: Click Add to begin the installation process.</p> <p></p> <p>Step 4: After clicking the Add button, a window will appear prompting you to select a team and channel where you want to add the Qualytics app. Once selected, click Go to complete the installation.</p> <p></p> <p>When you add the app to a team and channel, Qualytics will automatically detect the installation. You may need to refresh your browser to see the status update from \"Pending\" to \"Connected\" in the Qualytics Integrations page.</p> <p></p>"},{"location":"settings/integrations/alerting/msft_teams/#manual-verification-optional","title":"Manual Verification (optional)","text":"<p>Important</p> <p>Manual verification serves as a fallback method if Qualytics doesn't automatically detect the app installation after adding it to a channel. If the status remains \"Pending\" after installing the app and refreshing the Qualytics page, use this manual verification process to complete the integration.</p> <p>To manually verify the integration:</p> <p>Step 1: Return to the Qualytics Integrations page and click on the Verify app installation button next to the Microsoft Teams integration.</p> <p></p> <p>When the verification is successful, the integration status will change to \"Connected\", indicating that Qualytics can now send notifications to your Microsoft Teams workspace.</p> <p></p>"},{"location":"settings/integrations/alerting/msft_teams/#manage-microsoft-teams-integration","title":"Manage Microsoft Teams Integration","text":"<p>Microsoft Teams integration enables smooth communication between your platform and Teams channels. Users can easily modify connection settings, update authorization details, or disconnect the integration based on their requirements.</p>"},{"location":"settings/integrations/alerting/msft_teams/#edit-integration","title":"Edit Integration","text":"<p>Edit Integration feature allows users to modify Microsoft Teams connection settings directly from the integration panel. By selecting the Edit option from the menu, users can update configuration details and reauthorize the connection if needed .</p> <p>Step 1: Click on the vertical ellipses(\u22ee) next to the Connected button and select the Edit option .</p> <p></p> <p>Step 2: A modal window Edit Microsoft Teams Integration will appear providing you with options to edit the connection properties.</p> <p></p> <p>Step 3: After editing the connection properties, click on the Update button to apply the changes.</p> <p></p> <p>A confirmation message will appear on the screen displaying \u201cThe Integration has been successfully updated\u201d.</p> <p></p>"},{"location":"settings/integrations/alerting/msft_teams/#disconnect-integration","title":"Disconnect Integration","text":"<p>Disconnecting the Microsoft Teams integration will remove its connection from your platform. This means any existing workflows, notifications, or actions relying on Microsoft Teams may stop working, though they won\u2019t be deleted. Make sure to review any dependent flows before proceeding.</p> <p>Step 1: Click on the vertical ellipses(\u22ee) next to the connected button and select the Disconnect option to disconnect the integration.</p> <p></p> <p>Step 2: A modal window Disconnect Integration will appear allowing you to disconnect the microsoft teams integration.</p> <p></p> <p>Step 3: Click on the Disconnect button to proceed.</p> <p></p> <p>A confirmation message will appear on the screen displaying \u201cThe Integration has been successfully disconnected\u201d.</p> <p></p>"},{"location":"settings/integrations/alerting/overview/","title":"Alerting Integrations","text":"<p>The Qualytics platform integrates with popular enterprise messaging platforms, such as Slack and Microsoft Teams to enable real-time communication about data quality events. These integrations help ensure that your teams remain informed and can respond quickly when data issues occur.</p> <ul> <li>Receive instant notifications when data quality issues are detected</li> <li>Alert relevant team members about failed quality checks in real-time</li> <li>Share operational status updates and system health notifications</li> <li>Configure custom alerts based on data quality thresholds and conditions</li> <li>Route notifications to specific channels or teams based on data context</li> </ul> <p>These integrations ensure your teams stay informed about data quality events as they happen, enabling rapid response and maintaining continuous data quality awareness across your organization.</p>"},{"location":"settings/integrations/alerting/overview/#available-integration","title":"Available Integration","text":"<p>Qualytics makes it easy to deliver alerts through the communication platforms your teams already rely on. Below are the currently supported integrations:</p>"},{"location":"settings/integrations/alerting/overview/#slack","title":"Slack","text":"<p>Integrate Qualytics with Slack to send real-time alerts directly to your Slack channels. This allows teams to stay on top of data quality events without switching tools.</p> <p>For more details you can refer to the slack integration documentation.</p> <p></p>"},{"location":"settings/integrations/alerting/overview/#microsoft-teams","title":"Microsoft Teams","text":"<p>Connect Microsoft Teams to receive automated alerts about failed checks, system health updates, and threshold-based events right within your team chats.</p> <p>For more details you can refer to the microsoft teams documentation.</p> <p></p>"},{"location":"settings/integrations/alerting/slack/","title":"Slack","text":"<p>Slack integration in Qualytics enables seamless communication by connecting your Slack workspace with data quality updates and notifications. It involves generating and applying Slack API tokens, authorizing the integration, and providing options to modify and manage the connection effortlessly.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"settings/integrations/alerting/slack/#navigation-to-integration","title":"Navigation to Integration","text":"<p>Step 1: Log in to your Qualytics account and click the \"Settings\" button on the left side panel of the interface.</p> <p></p> <p>Step 2: By default, Connections tab will open. Click on the Integrations tab .</p> <p></p>"},{"location":"settings/integrations/alerting/slack/#connect-slack-integration","title":"Connect Slack Integration","text":"<p>Connect Slack by generating tokens, configuring connection properties, and authorizing the integration using OAuth for secure communication and seamless app configuration.</p> <p>Step 1 :Click on the Connect button next to Slack to connect to the Slack Integration.</p> <p></p> <p>A modal window titled \"Add Slack Integration\" appears. Fill in the connection properties to connect to Slack.</p> <p></p> <p>Step 2:First, generate the access and refresh tokens through the Slack API by signing in.</p> <p></p> <p>Alternatively, hover over the ? icon and click on the Go to Slack Tokens.</p> <p></p> <p>You will be automatically redirected to the Slack token page, where you can copy the access token for creating and configuring apps and the refresh token for rotating the access token.</p> <p></p> <p>Step 3:Fill out the copied connection properties of slack integration :</p> <p></p> No. Field Name Description 1. Access Token Enter the generated access token. 2. Refresh Token Enter the generated refresh token. <p>Step 4: Click the Create button to apply the access and refresh tokens and proceed with authorizing the Slack integration.</p> <p></p> <p>Once the integration is successfully created, a confirmation message will appear on the screen stating, \"The Integration has been successfully created.\"</p> <p></p> <p>Step 5:Click the Authorize button to complete the Slack integration using OAuth authentication.</p> <p></p> <p>Step 6:After clicking the Authorize button, a window appears requesting permission to access the Slack workspace. Click the Allow button to grant the required permissions.</p> <p></p> <p>A message appears confirming that the integration has been successfully authorized.</p> <p></p>"},{"location":"settings/integrations/alerting/slack/#manage-slack-integration","title":"Manage Slack Integration","text":"<p>Managing Slack integration involves editing or disconnecting the integration to ensure seamless communication and synchronization between platforms. Users can easily modify integration settings, reauthorize the connection, or disconnect the integration if required.</p>"},{"location":"settings/integrations/alerting/slack/#edit-integration","title":"Edit Integration","text":"<p>Editing Slack integration allows modifications to the existing configuration to ensure that the integration functions according to updated requirements. Users can update Slack details, reauthorize the connection, and apply necessary changes seamlessly</p> <p>Step 1:Click on the vertical ellipses(\u22ee) next to the Connected button and select the Edit option.</p> <p></p> <p>Step 2:A modal window Edit Slack Integration will appear providing you with options to edit the connection properties.</p> <p></p> <p>Step 3 :After editing the connection properties of the slack integration, click on the Update button to apply the changes.</p> <p></p> <p>A confirmation message will appear on the screen displaying \u201cThe Integration has been successfully updated\u201d.</p> <p></p> <p>Step 4:Click on the Authorize button to update the authorization details.</p> <p></p> <p>A confirmation message will appear on the screen displaying \u201cThe Integration has been successfully authorized\u201d.</p> <p></p>"},{"location":"settings/integrations/alerting/slack/#disconnect-integration","title":"Disconnect Integration","text":"<p>Disconnecting the Slack integration removes all associated synced assets and disables further data exchange between the platforms. To ensure a smooth disconnection process, follow the steps below to terminate the integration safely and confirm the action.</p> <p>Step 1 : Click on the vertical ellipses(\u22ee) next to the connected button and select the Disconnect option to disconnect the integration.</p> <p></p> <p>Step 2:A modal window Disconnect Integration will appear allowing you to disconnect the slack integration.</p> <p></p> <p>Step 3:Click on the Disconnect button to proceed.</p> <p></p> <p>Note</p> <p>This action will delete all synced assets by this integration. </p> <p>A confirmation message will appear on the screen displaying \u201cThe Integration has been successfully disconnected\u201d.</p> <p></p>"},{"location":"settings/integrations/analytics/analytics/","title":"Analytics Integrations","text":"<p>The Qualytics platform integrates with enterprise analytics and visualization tools, enabling organizations to:</p> <p> </p> <ul> <li>Create custom dashboards showcasing data quality metrics and trends</li> <li>Visualize data quality scores across your entire data ecosystem</li> </ul> <p> </p> <ul> <li>Monitor anomaly patterns and profiling characteristics in real-time</li> <li>Generate detailed reports on data quality at field, schema, and enterprise levels</li> <li>Share data quality insights through familiar business intelligence interfaces</li> <li>Track data quality improvement initiatives with executive-level visibility</li> </ul> <p> </p> <p>These integrations transform Qualytics' rich metadata and quality metrics into actionable insights, helping organizations understand, communicate, and improve their data quality through their preferred analytics platforms.</p>"},{"location":"settings/integrations/catalog/alation/","title":"Alation","text":"<p>Integrating Alation with Qualytics, allows you to pull metadata from Alation to Qualytics and push Qualytics metadata to Alation. Once integrated, Qualytics can stay updated with key changes in Alation, like metadata updates and anomaly alerts which helps to ensure data quality and consistency. Qualytics updates only active checks, and metadata updates in Qualytics occur if the Event-Driven option is enabled or can be triggered manually using the \"Sync\" button. During sync, Qualytics can replace existing tags in Alation or skip duplicate tags to avoid conflicts. The setup is simple\u2014just provide a refresh token for communication between the systems.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"settings/integrations/catalog/alation/#alation-setup","title":"Alation Setup","text":""},{"location":"settings/integrations/catalog/alation/#create-refresh-token","title":"Create Refresh Token","text":"<p>Before setting up Alation Integration in Qualytics, you have to generate a Refresh token. This allows Qualytics to access Alation's API and keep data in sync between the two platforms.</p> <p>Step 1: Navigate to the \"Profile Settings\".</p> <p></p> <p>Step 2: Select the \"Authentication\" tab.</p> <p></p> <p>Step 3: Click on the \"Create Refresh Token\" button.</p> <p></p> <p>Step 4: Enter a name for the token.</p> <p></p> <p>Step 5: After entering the name for the token, click on \"Create Refresh Token\".</p> <p></p> <p>Step 6: Your \"refresh\" token has been generated successfully. Please Copy and save it securely.</p> <p></p> <p>Step 7: Here you can view the token that is successfully added to the access tokens list.</p> <p></p>"},{"location":"settings/integrations/catalog/alation/#add-alation-integration","title":"Add Alation Integration","text":"<p>Step 1:  Log in to your Qualytics account and click the \"Settings\" button on the left side panel of the interface.</p> <p></p> <p>Step 2: You will be directed to the Settings page, then click on the \"Integration\" tab.</p> <p></p> <p>Step 3: Click on the \"Add Integration\" button.</p> <p></p> <p>Step 4: Complete the configuration form by choosing the Alation integration type.</p> <p></p> REF. FIELDS ACTIONS 1. Name (Required) Provide a name for the integration. 2. Type (Required) Choose the type of integration from the dropdown menu. Currently, 'Atlan' is selected 3. URL (Required) Enter the full address of the Alation instance, for example,  https://instance.alationcloud.com. 4. Refresh Token (Required) Enter the refresh token required to access the Alation API. 5. User ID (Required) Provide the user ID associated with the generated token. 6. Domains Select specific domains to filter assets for synchronization. - Acts as a filtering mechanism to sync specific assets - Uses domain information from the data catalog (e.g. Sales ). Only assets under the selected domains will synchronize. 7. Event Driven If enabled, operations, archiving anomalies, and checks will activate the integration sync. 8. Overwrite Tags If enabled, Alation tags will override Qualytics tags in cases of conflicts (when tags with the same name exist on both platforms). <p>Step 5: Click on the Save button to integrate Alation with Qualytics. </p> <p></p> <p>Step 6: Here you can view the new integration appearing in Qualytics.</p> <p></p>"},{"location":"settings/integrations/catalog/alation/#synchronization","title":"Synchronization","text":"<p>The Alation synchronization supports both push and pull operations. This includes pulling metadata from Alation to Qualytics and pushing Qualytics metadata to Alation. During the syncing process, the integration pulls tags assigned to data assets in Alation and assigns them to Qualytics assets as an external tag.</p> <p>Note</p> <p>Tag synchronization requires manual triggering.</p> <p>Step 1: To sync tags, simply click the \"Sync\" button next to the relevant integration card.</p> <p></p> <p>Step 2: After clicking the Sync button, you will have the following options:</p> <ul> <li>Pull Alation Metadata </li> <li>Push Qualytics Metadata</li> </ul> <p>Specify whether the synchronization will pull metadata, push metadata, or do both.</p> <p></p> <p>Step 3: After selecting the desired options, click on the \"Start\" button.</p> <p></p> <p>Step 4: After clicking the Start button, the synchronization process between Qualytics and Alation begins. This process pulls metadata from Alation and pushes Qualytics metadata, including tags, quality scores, anomaly counts, asset links, and many more.</p> <p></p> <p>Step 5: Once synchronization is complete, the mapped assets from Alation will display an external tag.</p> <p></p>"},{"location":"settings/integrations/catalog/alation/#alerts","title":"Alerts","text":"<p>When Qualytics detects anomalies, alerts are sent to the assets in Alation, showing the number of active anomalies and providing a link to view them.</p> <p></p>"},{"location":"settings/integrations/catalog/alation/#metadata","title":"Metadata","text":"<p>The Quality Score Total, the \"Qualytics 8\" metrics (completeness, coverage, conformity, consistency, precision, timeliness, volume, and accuracy), and counts of checks and anomalies per asset identified by Qualytics are pushed to Alation. This enables users to analyze assets based on data profiling and scanning metrics. A link to the asset in Qualytics is also provided.</p> <p></p>"},{"location":"settings/integrations/catalog/alation/#data-health","title":"Data Health","text":"<p>On the Alation tables page, there's a tab called \u201cData Health\u201d where Qualytics displays insights from data quality checks in a table format, showing the current status based on the number of anomalies per check.</p> <p></p> Column Description Rule The type of data quality check rule Object Name The Table Name Status The check status can be either \"Alert\" if there are active anomalies or \"No Issues\" if no active anomalies exist for the check. Value The current amount of active anomalies Description The data quality check description Last Updated The last synced timestamp"},{"location":"settings/integrations/catalog/atlan/","title":"Atlan","text":"<p>Integrating Atlan with Qualytics allows for easy push and pull of metadata between the two platforms. Specifically, Qualytics \"pushes\" its metadata to the data catalog and \"pulls\" metadata from the data catalog. Once connected, Qualytics automatically updates when key events happen in Atlan, such as metadata changes, anomaly updates, or archiving checks. This helps maintain data quality and consistency. During the sync process, Qualytics can either replace existing tags in Atlan or skip assets that have duplicate tags to avoid conflicts. Setting it up is simple\u2014you just need to provide an API token to allow smooth communication between the systems.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"settings/integrations/catalog/atlan/#atlan-setup","title":"Atlan Setup","text":""},{"location":"settings/integrations/catalog/atlan/#create-an-atlan-persona-and-policy","title":"Create an Atlan persona and policy","text":"<p>Before starting the integration process, it is recommended that you set up an Atlan persona. It allows access to the necessary data and metadata. While you can create this persona simultaneously as your API token, it's easier if you create it first. That way, you can link the persona directly to the token later.</p> <p>Before using Atlan with your data source, authorize the API token with access to the needed data and metadata. You do this by setting up policies within the persona for the Atlan connection that matches your Qualytics data source. Remember, you will need to do this for each data source you want to integrate.</p> <p>Step 1. Navigate to Governance, then select \u201cPersonas\u201d.</p> <p></p> <p>Step 2: Click on \u201c+ New Persona Button\u201d.</p> <p></p> <p>Step 3: Enter a Name and Description for a new persona, then click the \u201cCreate\u201d button.</p> <p></p> <p>Step 4: Here your new Atlan persona has been created.</p> <p></p> <p>Step 5: After creating a new Atlan persona you have to create policies to authorize the personal access token. Click on \"Add Policies\" to create a new policy or to add one if there isn't any available.</p> <p></p> <p>Step 6: Click on \"New Policy\" and select \"Metadata Policy\" from the dropdown menu.</p> <p></p> <p>Step 7: Enter a \"name\", and choose the \"connection\".  </p> <p></p> <p>Step 8: Customize the permissions and assets that Qualytics will access.</p> <p></p> <p>Step 9: Once the policy is created, you\u2019ll see it listed in the Policies section.</p> <p></p>"},{"location":"settings/integrations/catalog/atlan/#create-atlan-personal-access-token","title":"Create Atlan Personal Access Token","text":"<p>After you\u2019ve created the persona, the next step is to create a personal access token.</p> <p>Step 1: Navigate to the API Tokens section in the Admin Center.</p> <p></p> <p>Step 2: Click on \"Generate API Token\" button.</p> <p></p> <p>Step 3: Enter a name and description, and select the persona you created earlier.</p> <p></p> <p>Step 4: Click the \"Save\" button and make sure to store the token in a secure location.</p> <p></p>"},{"location":"settings/integrations/catalog/atlan/#add-atlan-integration","title":"Add Atlan Integration","text":"<p>Integrating Atlan with Qualytics enhances your data management capabilities, allowing seamless synchronization between the two platforms. This guide will walk you through the steps to add the Atlan integration efficiently. By following these steps, you can configure essential settings, provide necessary credentials, and customize synchronization options to meet your organization\u2019s needs.</p> <p>Step 1: Log in to your Qualytics account and click the \"Settings\" button on the left side panel of the interface.  </p> <p></p> <p>Step 2: You will be directed to the Settings page, then click on the \"Integration\" tab.</p> <p></p> <p>Step 3: Click on the Connect button next to Atlan to connect to the Atlan Integration.</p> <p></p> <p>A modal window titled Add Atlan Integration appears.</p> <p></p> <p>Fill in the connection properties to connect to Atlan.</p> REF. FIELDS ACTIONS 1. URL (Required) The complete address for the Atlan instance, for example: https://your-company.atlan.com. 2. Token (Required) Provide the authentication token needed to connect to Atlan. 3. Enable Announcements If enabled, announcements will be automatically posted to Atlan assets whenever anomalies are detected. 4. Domains Select specific domains to filter assets for synchronization. - Acts as a filtering mechanism to sync specific assets - Uses domain information from the data catalog (e.g. Sales ). Only assets under the selected domains will synchronize. 5. Event Driven If enabled, the integration sync will be activated by operations, archiving anomalies, and checks. 6. Overwrite Tags If enabled, Atlan tags will have precedence over Qualytics tags in cases of conflicts (when tags with the same name exist on both platforms). <p></p> <p>Step 4:  Click on the Create button to set up the Atlan integration. </p> <p></p> <p>Step 5: Once the Atlan integration is set up with Qualytics, it will appear in Qualytics as a new integration. </p> <p></p>"},{"location":"settings/integrations/catalog/atlan/#synchronization","title":"Synchronization","text":"<p>The Atlan synchronization supports both push and pull operations. This includes pulling metadata from Atlan to Qualytics and pushing Qualytics metadata to Atlan. During the syncing process, the integration pulls tags assigned to data assets in Atlan and assigns them to Qualytics assets as an external tag.</p> <p>Note</p> <p>Tag synchronization requires manual triggering.</p> <p>Step 1: To sync tags, click the vertical ellipsis next to Atlan and select\u202fSync from the dropdown.</p> <p></p> <p>Step 2: After clicking the \"Sync\" button, you will have the following options:</p> <ul> <li>Pull Atlan Metadata </li> <li>Push Qualytics Metadata</li> </ul> <p>Specify whether the synchronization will pull metadata, push metadata, or do both.</p> <p></p> <p>Step 3: After selecting the desired options, click on the \"Start\" button.</p> <p></p> <p>Step 4: After clicking the Start button, the synchronization process between Qualytics and Atlan begins. This process pulls metadata from Atlan and pushes Qualytics metadata, including tags, quality scores, anomaly counts, asset links, and many more.</p> <p></p> <p>Step 5: Review the logs to verify which assets were successfully mapped from Atlan to Qualytics.</p> <p></p> <p>Step 6: Once synchronization is complete, the mapped assets from \"Atlan\" will display an external tag.   </p> <p></p> <p>Step 7: When Qualytics detects anomalies, alerts are sent to the assets in Atlan, displaying the number of active anomalies and including a link to view the corresponding details</p> <p></p>"},{"location":"settings/integrations/catalog/atlan/#metadata","title":"Metadata","text":"<p>The Quality Score Total, along with the Qualytics 8 metrics (completeness, coverage, conformity, consistency, precision, timeliness, volume, and accuracy), and the count of checks and anomalies per asset identified by Qualytics, are pushed.</p> <p></p>"},{"location":"settings/integrations/catalog/collibra/","title":"Collibra","text":"<p>The Collibra integration allows Qualytics to sync data quality metadata with Collibra, making quality context available directly within governance workflows. Data quality checks, anomalies, and related metadata are aligned with governed assets to support informed decision-making. Synchronization is handled at the metadata level and does not require a specific target configuration.</p> <p></p>"},{"location":"settings/integrations/catalog/datahub/","title":"DataHub","text":"<p>The DataHub integration enables Qualytics to sync data quality metadata into DataHub, providing visibility into checks, anomalies, and quality context alongside cataloged assets. This helps teams discover and assess data with quality signals included. Synchronization operates at the metadata level and does not require a specific target configuration.</p> <p></p>"},{"location":"settings/integrations/catalog/external-tag-propagation/","title":"External Tag Propagation","text":"<p>External tags propagation in Qualytics serve as metadata labels that are automatically synchronized from an integrated data catalog, such as Atlan or Alation. This process helps maintain consistent data tagging across various platforms by using pre-existing tags from the data catalog.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"settings/integrations/catalog/external-tag-propagation/#navigation","title":"Navigation","text":"<p>Step 1: Log in to your Qualytics account and click the Settings button on the left side panel of the interface.</p> <p></p> <p>Step 2: You will be directed to the Settings page, then click on the Integration tab.</p> <p></p> <p>Step 3: Click on the Add Integration button.</p> <p></p> <p>A modal window Add Integration will appear, providing you with the options to add integration.</p> <p></p> REF. FIELDS ACTIONS 1. Name Provide a detailed description of the integration. 2. Type Choose the type of integration from the dropdown menu. Currently, 'Atlan' is selected 3. URL The complete address for the Atlan instance, for example: https://your-company.atlan.com. 4. Token Provide the authentication token needed to connect to Atlan. 5. Event Driven If enabled, the integration sync will be activated by operations, archiving anomalies, and checks. 6. Overwrite Tags If enabled, Atlan tags will have precedence over Qualytics tags in cases of conflicts (when tags with the same name exist on both platforms). <p></p> <p>For demonstration purposes we have selected Atlan integration type.</p> <p>Step 5: Click on the Save button to set up the Atlan integration.</p> <p></p> <p>Step 6: Once the Atlan integration is set up with Qualytics, it will appear in Qualytics as a new integration.</p> <p></p>"},{"location":"settings/integrations/catalog/external-tag-propagation/#synchronization","title":"Synchronization","text":"<p>Synchronization supports both push and pull operations. This includes pulling metadata from one platform to Qualytics and pushing Qualytics metadata to the other platform. During the syncing process, the integration pulls tags assigned to data assets in the source platform and assigns them to Qualytics assets as an external tag.</p> <p>For demonstration purposes we have selected Atlan synchronization.</p> <p>Note</p> <p>Tag synchronization requires manual triggering. </p> <p>Step 1: To sync tags, simply click on the Sync button next to the relevant integration card.</p> <p></p> <p>Step 2: After clicking the Sync button, you will have the following options:</p> <ul> <li>Pull Atlan Metadata </li> <li>Push Qualytics Metadata</li> </ul> <p>Specify whether the synchronization will pull metadata, push metadata, or do both.</p> <p></p> <p>Step 3: After selecting the desired options, click on the Start button.</p> <p></p> <p>Step 4: After clicking the Start button, the synchronization process between Qualytics and Atlan begins. This process pulls metadata from Atlan and pushes Qualytics metadata, including tags, quality scores, anomaly counts, asset links, and many more.</p> <p></p> <p>Step 5: Review the logs to verify which assets were successfully mapped from Atlan to Qualytics.</p> <p></p> <p>Step 6: Once synchronization is complete, the mapped assets from Atlan will display an external tag.</p> <p></p>"},{"location":"settings/integrations/catalog/overview/","title":"Data Catalog Integrations","text":"<p>The Qualytics platform seamlessly integrates with enterprise data catalogs, enabling organizations to:</p> <ul> <li>Surface data quality insights directly within existing catalog tools</li> <li>Automatically sync metadata between platforms in real-time</li> <li>Leverage data catalog tags for quality classification</li> <li>Push quality alerts and anomaly notifications to catalog users</li> <li>Maintain consistent metadata across platforms</li> <li>Track data quality metrics within your data governance framework</li> </ul> <p>These catalog integrations ensure that data quality insights are readily available to users within their preferred data discovery and governance platforms.</p>"},{"location":"settings/integrations/catalog/overview/#setting-up-catalog-integration","title":"Setting Up Catalog Integration","text":"<p>Navigate to Settings &gt; Integration to configure your data catalog connection:</p> <p></p> <p></p> <p></p>"},{"location":"settings/integrations/catalog/overview/#supported-data-catalogs","title":"Supported Data Catalogs","text":"<p>Currently, Qualytics supports integration with the following data catalog platforms:</p>"},{"location":"settings/integrations/catalog/overview/#atlan","title":"Atlan","text":"<p>The Atlan integration enables bidirectional metadata synchronization, providing:</p> <ul> <li>Automated metadata push from Qualytics to Atlan</li> <li>Real-time metadata pull from Atlan to Qualytics</li> <li>Automatic updates based on key events</li> <li>Flexible tag management options</li> <li>Simple API-based authentication</li> </ul> <p>For detailed configuration steps, see the Atlan documentation.</p>"},{"location":"settings/integrations/catalog/overview/#alation","title":"Alation","text":"<p>The Alation integration supports comprehensive metadata exchange:</p> <ul> <li>Bidirectional metadata synchronization</li> <li>Real-time quality metric updates</li> <li>Selective synchronization of active checks</li> <li>Configurable tag conflict resolution</li> <li>Token-based secure authentication</li> </ul> <p>For detailed configuration steps, see the Alation documentation.</p>"},{"location":"settings/integrations/catalog/overview/#microsoft-purview","title":"Microsoft Purview","text":"<p>The Microsoft Purview integration connects Qualytics with Azure\u2019s native data governance platform.</p> <p>Key capabilities include:</p> <ul> <li>Synchronization of data quality metadata with Purview</li> <li>Visibility of quality context alongside cataloged assets</li> <li>Automated updates based on Qualytics operations</li> <li>This integration appears as Connected once successfully configured in Qualytics.</li> </ul> <p>For detailed configuration steps, see the Microsoft Purview documentation.</p>"},{"location":"settings/integrations/catalog/overview/#collibra","title":"Collibra","text":"<p>The Collibra integration enables metadata synchronization between Qualytics and Collibra, allowing data quality insights to be surfaced directly within Collibra\u2019s governance workflows.</p> <p>Key capabilities include:</p> <ul> <li>Synchronization of data quality metadata from Qualytics to Collibra.</li> <li>Visibility of data quality context alongside governed assets.</li> <li>Alignment between data quality checks and governance policies.</li> </ul> <p>For detailed configuration steps, see the Collibra documentation.</p>"},{"location":"settings/integrations/catalog/overview/#datahub","title":"DataHub","text":"<p>The DataHub integration connects Qualytics with DataHub to make data quality insights available within DataHub\u2019s data discovery experience.</p> <p>Key capabilities include:</p> <ul> <li>Syncing data quality metadata into DataHub</li> <li>Enabling quality-aware data discovery for analytics and engineering teams</li> <li>Maintaining consistency between Qualytics checks and cataloged assets</li> </ul> <p>For detailed configuration steps, see the DataHub documentation.</p>"},{"location":"settings/integrations/catalog/overview/#synchronization-options","title":"Synchronization Options","text":"<p>Qualytics provides flexible synchronization methods to match your workflow:</p>"},{"location":"settings/integrations/catalog/overview/#manual-sync","title":"Manual Sync","text":"<p>Trigger complete metadata synchronization on-demand:</p> <p></p> <p>For detailed steps, see the Synchronization section.</p>"},{"location":"settings/integrations/catalog/overview/#event-driven","title":"Event Driven","text":"<p>Enable automatic synchronization based on platform events:</p> <p></p> Event Description Run an Operation (Profile Or Scan) Sync all target containers for the operation. Archive an Anomaly (including bulk) Sync the container in which the anomaly was identified. Archive a Check ( including bulk) Sync the container to which the check belongs."},{"location":"settings/integrations/catalog/purview/","title":"Microsoft Purview Integration","text":"<p>Integrating Microsoft Purview with Qualytics enables seamless metadata synchronization between the two platforms. Once connected, Qualytics can push quality scores, anomaly data, and metadata to your Purview catalog, providing a unified view of data quality across your organization.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"settings/integrations/catalog/purview/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that:</p> <ul> <li> <p>You already have a Microsoft Purview account</p> </li> <li> <p>You have permission to:</p> <ul> <li>Create App Registrations in Microsoft Entra ID</li> <li>Assign roles in the Purview governance portal (Collections)</li> </ul> </li> </ul>"},{"location":"settings/integrations/catalog/purview/#azure-setup","title":"Azure Setup","text":"<p>Before connecting Purview to Qualytics, you need to create a service principal in Azure and configure the necessary permissions.</p>"},{"location":"settings/integrations/catalog/purview/#create-a-service-principal","title":"Create a Service Principal","text":"<p>Qualytics uses a service principal to securely authenticate with Microsoft Purview APIs.</p> <p>Step 1: Sign in to the Azure portal.</p> <p></p> <p>Step 2: Navigate to Microsoft Entra ID and select App registrations from the left sidebar.</p> <p></p> <p>Step 3: Click New registration.</p> <p></p> <p>Step 4: Fill in the registration form with the following details:</p> Field Value Name <code>Qualytics-Purview-Integration</code> (or any descriptive name) Supported account types Accounts in this organizational directory only (Single tenant) <p></p> <p>Step 5: Click Register. After registration, save the Application (client) ID \u2014 you will need this later.</p> <p></p> <p></p>"},{"location":"settings/integrations/catalog/purview/#generate-a-client-secret","title":"Generate a Client Secret","text":"<p>Step 1: In the app registration, go to Certificates &amp; secrets in the left sidebar.</p> <p></p> <p>Step 2: Click New client secret, provide a description, select an expiration period, and click Add.</p> <p></p> <p></p> <p>Step 3: Copy the Value immediately and store it securely.</p> <p></p> <p>Important</p> <p>The client secret value is shown only once. Store it securely. This value is your <code>client_secret</code>.</p>"},{"location":"settings/integrations/catalog/purview/#configure-api-permissions","title":"Configure API Permissions","text":"<p>Step 1: In the app registration, open API permissions from the left sidebar and click Add a permission.</p> <p></p> <p></p> <p>Step 2: Search for and select Microsoft Purview, then add the following permissions:</p> Permission Type Permission Name Delegated permissions <code>Purview.DelegatedAccess</code> Application permissions <code>Purview.ApplicationAccess</code> <p></p> <p></p> <p>Step 3: Click Add permissions, then click Grant admin consent and confirm the action.</p> <p></p> <p></p> <p></p>"},{"location":"settings/integrations/catalog/purview/#purview-setup","title":"Purview Setup","text":"<p>After creating the service principal, you need to assign the appropriate roles in the Purview governance portal.</p>"},{"location":"settings/integrations/catalog/purview/#assign-collection-roles","title":"Assign Collection Roles","text":"<p>Step 1: Open the Microsoft Purview governance portal.</p> <p></p> <p>Step 2: Select Data Map from the navigation, then click Collections.</p> <p></p> <p>Step 3: Select the root collection (the top-level collection with the same name as your Purview account).</p> <p></p> <p>Step 4: Open the Role assignments tab and assign the following roles to the service principal:</p> Role Purpose Data Curator Access Catalog data plane Data Source Administrator Access Scanning data plane Collection Admin Access Account &amp; Metadata policy data planes Policy Author Access DevOps policies API <p></p> <p></p> <p>Note</p> <p>You may assign roles to a sub-collection instead of the root collection, but API access will be limited to that scope.</p>"},{"location":"settings/integrations/catalog/purview/#gather-connection-details","title":"Gather Connection Details","text":"<p>Step 1: In the Azure portal, open your Purview account and go to Settings and properties.</p> <p></p> <p>Step 2: Copy the Atlas endpoint and remove <code>/catalog</code> from the end of the URL.</p> <p>Example:</p> <pre><code>https://&lt;purview-tenant-id&gt;-api.purview-service.microsoft.com\n</code></pre> <p>Step 3: Copy the Managed Identity tenant ID from the same page.</p>"},{"location":"settings/integrations/catalog/purview/#add-purview-integration","title":"Add Purview Integration","text":"<p>Now you can configure the integration in Qualytics using the credentials you gathered.</p> <p>Step 1: Log in to your Qualytics account and click Settings on the left side panel.</p> <p>Step 2: Click on the Integration tab.</p> <p>Step 3: Click the Connect button next to Microsoft Purview.</p> <p>A modal window titled Add Purview Integration appears. Fill in the connection properties:</p> REF. Field Description 1. Purview Account URL The Atlas endpoint without <code>/catalog</code> (e.g., <code>https://your-account.purview.azure.com</code>) 2. Tenant ID The Managed Identity tenant ID from your Purview account 3. Client ID The Application (client) ID from your app registration 4. Client Secret The client secret value you generated <p></p> <p>Step 4: Click Save to create the integration.</p> <p></p> <p>Step 5: Once the integration is set up, it will appear in your integrations list.</p> <p>Integration Complete</p> <p>Your Microsoft Purview account is now successfully connected to Qualytics and ready for catalog, scanning, and governance operations.</p>"},{"location":"settings/integrations/catalog/purview/#synchronization","title":"Synchronization","text":"<p>After the integration is configured, you can synchronize metadata between Qualytics and Purview.</p> <p>Step 1: Click the vertical ellipsis next to the Purview integration and select Sync from the dropdown.</p> <p></p> <p>Step 2: Select the synchronization options and click Start.</p> <p></p>"},{"location":"settings/integrations/catalog/purview/#troubleshooting","title":"Troubleshooting","text":"Issue Solution Authentication failed Verify that the Client ID and Client Secret are correct and the secret has not expired Insufficient permissions Ensure all required roles are assigned to the service principal in Purview Invalid endpoint Confirm the Purview Account URL does not include <code>/catalog</code> at the end Tenant mismatch Verify the Tenant ID matches your Azure Active Directory tenant <p>Tip</p> <p>If you encounter persistent issues, check the Azure portal for any error messages in the app registration's sign-in logs.</p>"},{"location":"settings/integrations/compute/compute/","title":"Compute Integrations","text":"<p>The Qualytics platform offers flexible compute deployment options to optimize performance and resource utilization:</p> <ul> <li>Leverage existing Kubernetes infrastructure for seamless deployment</li> <li>Scale compute resources dynamically based on workload demands</li> <li>Deploy the data plane to external Spark environments</li> <li>Maintain data sovereignty and security compliance</li> <li>Take advantage of cloud-native performance optimizations</li> </ul> <p>These compute integration options ensure that Qualytics can adapt to your infrastructure requirements while maximizing performance and cost efficiency.</p>"},{"location":"settings/integrations/compute/compute/#running-qualytics-data-plane-on-kubernetes","title":"Running Qualytics Data Plane on kubernetes","text":"<p>The Qualytics platform's default deployment unifies control and data planes within a single Kubernetes cluster,  simplifying infrastructure management through a declarative approach.  This architecture enables dynamic scaling with cost-optimized spot instances while maintaining seamless  coordination between platform components.</p> <p>Alternatively, we support deploying the Qualytics data plane to any external Spark cluster   (external to the kubernetes cluster running the Qualytics control plane). </p>"},{"location":"settings/integrations/compute/compute/#running-qualytics-data-plane-on-databricks","title":"Running Qualytics Data Plane on Databricks","text":"<p>Deploying the Qualytics data plane within your Databricks account will allow our analytics engine to leverage  Photon acceleration while ensuring that all data transfer &amp; compute occurs inside your Databricks deployment.</p> <p></p>"},{"location":"settings/integrations/compute/compute/#running-qualytics-data-plane-on-gcp-dataproc","title":"Running Qualytics Data Plane on GCP Dataproc","text":"<p>Similarly, the Qualytics data plane can be deployed to Google Cloud's Dataproc to leverage Dataproc serverless and other  Dataproc optimizations in support of the Qualytics analytics engine.   </p>"},{"location":"settings/integrations/sso/sso/","title":"SSO (Single Sign-On) Integrations","text":"<p>The Qualytics platform provides enterprise-grade authentication integration capabilities, enabling organizations to:</p> <ul> <li>Implement secure, frictionless access across all platform components</li> <li>Leverage existing identity providers and authentication workflows</li> <li>Support both cloud-based and on-premise deployment scenarios</li> <li>Maintain compliance with corporate security policies</li> <li>Enable seamless mobile and web-based access</li> <li>Automate user provisioning and access management</li> </ul> <p>These authentication capabilities ensure that Qualytics seamlessly integrates with your organization's identity and access management infrastructure while maintaining the highest security standards.</p>"},{"location":"settings/integrations/sso/sso/#sso-for-paas-deployments","title":"SSO for PaaS Deployments","text":"<p>Qualytics platform harnesses the power of Auth0's Single Sign-On (SSO) technology to create a frictionless authentication journey for our PaaS users. Once users have successfully logged in to Qualytics, they can conveniently access all linked external applications and services without the need for additional sign-ins. Depending on the application and its compatibility with federated SSO protocols such as SAML, OIDC, or any proprietary authentication methods, Qualytics, with the help of Auth0, establishes a secure connection for user authentication. In essence, SSO allows one central domain to authenticate and then share the session across various other domains. The method of sharing may vary between SSO protocols, but the principle remains constant.</p> <p>Through Auth0's Integration Network (OIN), Qualytics extends SSO access to an extensive range of supported cloud-based applications. These integrations can utilize OpenID Connect (OIDC), SAML, SWA, or proprietary APIs for SSO. Maintenance of SSO protocols and provisioning APIs is reliably managed by Auth0.</p> <p>In addition to this, Qualytics also leverages Auth0's capabilities to provide SSO integrations for on-premises web-based applications. You have the option to integrate these applications via SWA or SAML toolkits. In addition, Auth0 supports user provisioning and deprovisioning with applications that publicly offer their provisioning APIs.</p> <p>Further enhancing our SSO integrations, Qualytics provides seamless access to mobile applications. Whether they are web applications optimized for mobile devices, native iOS apps, or Android apps, users can access web app integrations in the OIN using SSO from any mobile device. These mobile web apps can employ industry-standard OIDC, SAML, or Auth0 SWA technologies. To illustrate, Qualytics, in conjunction with Auth0, can integrate with native applications such as Box Mobile using SAML for registration and OAuth for continuous use.</p> <p>Auth0 supports the following enterprise providers out of the box: -  OAuth2 -   Active Directory/LDAP</p> <ul> <li> <p>ADFS</p> </li> <li> <p>Azure Active Directory Native</p> </li> <li> <p>Google Workspace</p> </li> <li> <p>OpenID Connect</p> </li> <li> <p>Okta</p> </li> <li> <p>PingFederate</p> </li> <li> <p>SAML</p> </li> <li> <p>Azure Active Directory</p> </li> </ul>"},{"location":"settings/integrations/sso/sso/#sso-for-on-premise-deployments","title":"SSO for On-Premise Deployments","text":"<p>In addition to the option of leveraging our robust Auth0 support for federated authentication, customer-managed deployments can choose to directly integrated with their IdP (Identity Provider such as Active Directory, ForgeRock, etc) using OpenID Connect (OIDC). Once configured for direct federated authentication using OIDC, the customer's own user login requirements fully govern the authentication process in support of a fully air-gapped deployment of Qualytics with no egress required for operations.</p>"},{"location":"settings/integrations/ticketing/jira/","title":"Jira","text":"<p>The Jira integration enables data quality teams to create and link Jira issues directly from data anomalies detected in Qualytics. This streamlines the issue resolution workflow by eliminating the need to manually create tickets in Jira and then reference them back in Qualytics.</p> <p></p>"},{"location":"settings/integrations/ticketing/jira/#key-benefits","title":"Key Benefits","text":"<ul> <li>Create Issues Directly: Create Jira issues directly from Qualytics anomalies without leaving the platform</li> <li>Automatic Linking: Automatic linking between anomalies and tickets for complete traceability</li> <li>Bidirectional Navigation: Hyperlink from Jira issue back to the source anomaly in Qualytics</li> <li>Status Synchronization: Anomaly status changes (acknowledge, archive) sync to Jira</li> <li>Comment Syncing: Comments added in Qualytics are pushed to Jira issue comments</li> </ul> <p>Integration Type</p> <p>The current release supports one-way synchronization (Qualytics \u2192 Jira). Changes made directly in Jira (e.g., issue status changes, comments) are not reflected back in Qualytics at this time.</p>"},{"location":"settings/integrations/ticketing/jira/#prerequisites","title":"Prerequisites","text":"<p>Before configuring the Jira integration, ensure the following requirements are met.</p>"},{"location":"settings/integrations/ticketing/jira/#jira-requirements","title":"Jira Requirements","text":"Requirement Details Jira Instance Active Jira Cloud or Jira Data Center instance Instance URL Format <code>https://your-domain.atlassian.net</code> (Cloud) or your Jira Data Center URL API Access REST API must be enabled for the instance Project Access User must have access to the target project(s)"},{"location":"settings/integrations/ticketing/jira/#account-requirements","title":"Account Requirements","text":"<p>A Jira account with appropriate permissions is required for the integration. This account will be used by Qualytics to authenticate and create/update issues.</p> Setting Recommendation Email The email address associated with your Jira account API Token A personal API token generated from Atlassian account settings Project Access Permission to create and edit issues in the target project(s)"},{"location":"settings/integrations/ticketing/jira/#required-jira-permissions","title":"Required Jira Permissions","text":"<p>Ensure your account has the following permissions in the target project(s):</p> Permission Purpose Browse Projects View project and its issues Create Issues Create new issues in the project Edit Issues Update issue fields and add comments Add Comments Add comments to issues"},{"location":"settings/integrations/ticketing/jira/#jira-configuration","title":"Jira Configuration","text":"<p>Complete the following steps in your Jira account before configuring the integration in Qualytics.</p>"},{"location":"settings/integrations/ticketing/jira/#step-1-access-account-settings","title":"Step 1: Access Account Settings","text":"<ol> <li>Log in to your Jira account</li> <li>Click on your profile icon in the top-right corner</li> <li>Select Account settings (or Manage account)</li> </ol>"},{"location":"settings/integrations/ticketing/jira/#step-2-navigate-to-security-settings","title":"Step 2: Navigate to Security Settings","text":"<ol> <li>In your account settings, click on the Security tab</li> </ol>"},{"location":"settings/integrations/ticketing/jira/#step-3-create-an-api-token","title":"Step 3: Create an API Token","text":"<ol> <li>Scroll down to find API token section</li> <li>Click on Create and manage API tokens</li> </ol> <ol> <li>Click on Create API token</li> </ol> <ol> <li>Enter a label for your token (e.g., <code>Qualytics Integration</code>)</li> <li>Click Create</li> </ol> <ol> <li>Important: Copy the generated API token immediately and store it securely. You won't be able to see it again.</li> </ol>"},{"location":"settings/integrations/ticketing/jira/#step-4-gather-configuration-values","title":"Step 4: Gather Configuration Values","text":"<p>Collect the following values needed for Qualytics configuration:</p> Value How to Find Example Host URL Your Jira instance URL <code>https://your-domain.atlassian.net</code> Credentials Email and API token combined with a colon <code>user@company.com:your_api_token</code>"},{"location":"settings/integrations/ticketing/jira/#qualytics-configuration","title":"Qualytics Configuration","text":""},{"location":"settings/integrations/ticketing/jira/#navigation-to-integration","title":"Navigation to Integration","text":"<p>Step 1: Log in to your Qualytics account and click the \"Settings\" button on the left side panel of the interface.</p> <p>Step 2: Click on the Integrations tab.</p>"},{"location":"settings/integrations/ticketing/jira/#connect-jira-integration","title":"Connect Jira Integration","text":"<p>Step 1: Click on the Connect button next to Jira to connect to the Jira Integration.</p> <p>A modal window titled \"Add Jira Integration\" appears. Fill in the connection properties.</p> <p></p> <p>Step 2: Enter the following connection properties:</p> REF. Field Description 1 Host (Required) Enter the complete Jira instance URL (e.g., <code>https://your-domain.atlassian.net</code>). Do not include a trailing slash. 2 Credentials (Required) Enter the credentials in the format <code>email:api_token</code> (email and API token separated by a colon). For example: <code>user@company.com:your_api_token_here</code> <p>Credentials Format</p> <p>The credentials must be entered as <code>email:api_token</code> with a colon separator. Use your Jira account email and the API token you generated.</p> <p>Step 3: Click the Create button to validate and store the credentials.</p> <p>Once the integration is successfully created, a confirmation message will appear stating \"The Integration has been successfully created.\"</p> <p></p>"},{"location":"settings/integrations/ticketing/jira/#test-the-connection","title":"Test the Connection","text":"<p>After saving, the connection status should show as Connected. If you encounter errors, verify:</p> <ul> <li>Host URL is correct and includes <code>https://</code> (no trailing slash)</li> <li>Credentials are in the correct format (<code>email:api_token</code>)</li> <li>API token is valid and not expired</li> <li>Account has required permissions in Jira</li> </ul>"},{"location":"settings/integrations/ticketing/jira/#integration-behavior-data-flow","title":"Integration Behavior &amp; Data Flow","text":""},{"location":"settings/integrations/ticketing/jira/#data-flow-direction","title":"Data Flow Direction","text":"<p>The current release supports one-way synchronization from Qualytics to Jira:</p> Action in Qualytics Result in Jira Create ticket from anomaly New Issue created Acknowledge anomaly Comment added to issue with status change Archive anomaly (resolve) Comment added to issue with resolution status Add comment to anomaly Comment pushed to Jira issue Link existing ticket Association created (anomaly details added as comment)"},{"location":"settings/integrations/ticketing/jira/#what-gets-synced","title":"What Gets Synced","text":"Direction What How Status Qualytics \u2192 Jira New ticket Creates issue with anomaly details  Active Qualytics \u2192 Jira Anomaly status change Adds comment (doesn't change issue status)  Active Qualytics \u2192 Jira Anomaly comment Adds comment  Active Jira \u2192 Qualytics Issue status Not synced  Not supported Jira \u2192 Qualytics Issue comments Not synced  Not supported Jira \u2192 Qualytics Issue closure Not synced  Not supported"},{"location":"settings/integrations/ticketing/jira/#current-limitations","title":"Current Limitations","text":"<p>The following actions in Jira are not reflected in Qualytics:</p> Action in Jira Effect in Qualytics Change issue status (e.g., In Progress, Done) Not reflected - anomaly status unchanged Add comments Not reflected - comments do not appear in Qualytics Reassign issue Not reflected Close or resolve issue Not reflected - anomaly remains in current state"},{"location":"settings/integrations/ticketing/jira/#field-mapping-reference","title":"Field Mapping Reference","text":""},{"location":"settings/integrations/ticketing/jira/#issue-fields-supported","title":"Issue Fields Supported","text":"<p>When creating a ticket from Qualytics, the following Jira issue fields can be populated:</p> Field Name Jira Field Required Notes Summary <code>summary</code> Yes Brief summary of the issue (ticket title) Description <code>description</code> No Detailed description; auto-populated with anomaly details Priority <code>priority</code> No Issue priority level"},{"location":"settings/integrations/ticketing/jira/#auto-populated-fields","title":"Auto-Populated Fields","text":"<p>The following information is automatically included when creating a ticket:</p> Field Auto-Populated Value Description Anomaly ID, type, status at detection, field/check message, failed checks, hyperlink back to Qualytics anomaly Comments (on updates) Status changes (acknowledge, archive) and comments from Qualytics with timestamps"},{"location":"settings/integrations/ticketing/jira/#ticket-creation-workflow","title":"Ticket Creation Workflow","text":""},{"location":"settings/integrations/ticketing/jira/#creating-a-new-ticket-from-an-anomaly","title":"Creating a New Ticket from an Anomaly","text":"<p>Step 1: Navigate to a datastore in Qualytics</p> <p>Step 2: Locate an anomaly you want to create a ticket for</p> <p>Step 3: Click on the anomaly to open details</p> <p>Step 4: Find the \"Linked Tickets\" section and click \"Add\" to link or create tickets</p> <p></p> <p>Step 5: A modal will appear with options to create a new ticket or link an existing one. Select \"New Ticket\"</p> <p></p> <p>Step 6: Fill in the required and optional fields:</p> <p></p> Field Description Summary Brief summary of the data quality issue (required) Description Detailed description (auto-populated with anomaly details) Priority Select the appropriate priority level <p></p> <p>Step 7: Click Create to submit the ticket</p> <p>When the ticket is created, a link is established between the anomaly and the Jira issue. The issue description will contain the anomaly details, including:</p> <ul> <li>Anomaly ID and status</li> <li>Anomaly type and detection timestamp</li> <li>Failed checks information</li> <li>Direct link back to the anomaly in Qualytics</li> </ul> <p></p> <p>The linked ticket will appear in the anomaly's Linked Tickets section:</p> <p></p>"},{"location":"settings/integrations/ticketing/jira/#linking-an-existing-ticket","title":"Linking an Existing Ticket","text":"<p>If a ticket already exists in Jira, you can link it to an anomaly:</p> <p>Step 1: Open the anomaly in Qualytics</p> <p>Step 2: Click \"Add\" in the Linked Tickets section</p> <p>Step 3: Select \"Link Existing\"</p> <p>Step 4: Search by issue key or summary</p> <p></p> <p>Step 5: Select the matching issue to link</p> <p>When linking an existing ticket:</p> <ul> <li>Anomaly details are added as a comment to the existing issue</li> <li>A link is established between the anomaly and the issue</li> </ul> <p>Tip</p> <p>Multiple tickets can be linked to a single anomaly if needed.</p>"},{"location":"settings/integrations/ticketing/jira/#viewing-linked-tickets","title":"Viewing Linked Tickets","text":"<p>Once linked, tickets appear in the \"Linked Tickets\" section of the anomaly:</p> <p></p> <ul> <li>Click \"View\" to open the ticket directly in Jira</li> <li>The issue key, status, and last sync timestamp are displayed</li> </ul>"},{"location":"settings/integrations/ticketing/jira/#key-integration-operations","title":"Key Integration Operations","text":""},{"location":"settings/integrations/ticketing/jira/#create-ticket","title":"Create Ticket","text":"<p>Creates a Jira issue from a Qualytics anomaly with:</p> <ul> <li>Required fields: <code>summary</code></li> <li>Optional fields: <code>description</code>, <code>priority</code></li> <li>Auto-included: Anomaly ID, status, type, failed checks, and link to Qualytics</li> </ul>"},{"location":"settings/integrations/ticketing/jira/#link-existing-ticket","title":"Link Existing Ticket","text":"<p>Links an existing Jira issue to an anomaly:</p> <ul> <li>Adds anomaly details as a comment to the existing issue</li> <li>Creates a link between the anomaly and the issue</li> </ul>"},{"location":"settings/integrations/ticketing/jira/#sync-anomaly-status","title":"Sync Anomaly Status","text":"<p>When anomaly status changes in Qualytics:</p> <ul> <li>Adds a timestamped comment to the linked Jira issue</li> <li>Example: <code>[2024-01-15 10:30:00 UTC] Qualytics Anomaly Status: Acknowledged</code></li> </ul> <p>Example: Acknowledging an anomaly in Qualytics:</p> <p></p> <p>The status change is reflected in Jira as a comment:</p> <p></p> <p>Note</p> <p>Status changes add comments but do not change the issue's status. This preserves your Jira workflow control.</p>"},{"location":"settings/integrations/ticketing/jira/#sync-comments","title":"Sync Comments","text":"<p>When comments are added to anomalies in Qualytics:</p> <ul> <li>The comment text is synced to linked tickets as Jira comments</li> <li>Example: <code>Comment from Qualytics: &lt;user's comment&gt;</code></li> </ul>"},{"location":"settings/integrations/ticketing/jira/#api-reference","title":"API Reference","text":"<p>This section provides technical details for administrators and developers who need to understand the underlying API operations.</p>"},{"location":"settings/integrations/ticketing/jira/#qualytics-api-endpoints","title":"Qualytics API Endpoints","text":"<p>For programmatic integration management, the following Qualytics API endpoints are available:</p>"},{"location":"settings/integrations/ticketing/jira/#ticketing-integration-endpoints","title":"Ticketing Integration Endpoints","text":"Operation Method Endpoint Description Get Ticketing Form Specification GET <code>/api/integrations/ticketing/form-specification</code> Get the form specification for creating tickets Search Tickets GET <code>/api/integrations/ticketing/tickets/search</code> Search for tickets in Jira Get Ticket By Id GET <code>/api/integrations/ticketing/tickets/{ticket_id}</code> Get a specific ticket by its ID Create Ticket For Anomaly POST <code>/api/integrations/ticketing/anomalies/{anomaly_id}/tickets</code> Create a new ticket linked to an anomaly"},{"location":"settings/integrations/ticketing/jira/#anomaly-ticket-link-endpoints","title":"Anomaly Ticket Link Endpoints","text":"Operation Method Endpoint Description Get Anomaly Ticket Links GET <code>/api/anomalies/{anomaly_id}/ticket-links</code> Retrieve all ticket links for an anomaly Create Anomaly Ticket Link POST <code>/api/anomalies/{anomaly_id}/ticket-links</code> Create a new ticket link for an anomaly Delete Anomaly Ticket Link DELETE <code>/api/anomalies/{anomaly_id}/ticket-links/{link_id}</code> Delete a specific ticket link"},{"location":"settings/integrations/ticketing/jira/#troubleshooting","title":"Troubleshooting","text":""},{"location":"settings/integrations/ticketing/jira/#common-issues","title":"Common Issues","text":"Issue Resolution Connection failed Verify host URL format (<code>https://your-domain.atlassian.net</code>), check credentials format (<code>email:api_token</code>), confirm account is active 401 Unauthorized Incorrect credentials; verify the <code>email:api_token</code> format is correct and that API token is not expired 403 Forbidden Account lacks required permissions; ensure user has access to the target project Project not found Verify the project key is correct and the user has access to the project Issue type not found Verify the issue type exists in the selected project Network timeout Firewall may be blocking; ensure Qualytics IPs are allowlisted"},{"location":"settings/integrations/ticketing/jira/#getting-support","title":"Getting Support","text":"<p>If you encounter issues not covered above, contact Qualytics support:</p> <ul> <li>Email: support@qualytics.ai</li> <li>Slack: Your dedicated customer Slack channel</li> </ul> <p>Please include: Host URL (masked if needed), error messages, and steps to reproduce.</p>"},{"location":"settings/integrations/ticketing/jira/#appendix-sample-payloads","title":"Appendix: Sample Payloads","text":""},{"location":"settings/integrations/ticketing/jira/#create-ticket-request","title":"Create Ticket Request","text":"<p>When creating a ticket from Qualytics, the following request is sent:</p> <pre><code>POST /api/integrations/ticketing/anomalies/{anomaly_id}/tickets\n\n{\n  \"title\": \"summary of ticket\",\n  \"description\": \"description of the ticket\",\n  \"priority\": \"Medium\"\n}\n</code></pre>"},{"location":"settings/integrations/ticketing/jira/#create-ticket-response","title":"Create Ticket Response","text":"<pre><code>{\n  \"ticket\": {\n    \"ticket_id\": \"10428\",\n    \"ticket_number\": \"SCRUM-40\",\n    \"title\": \"summary of ticket\",\n    \"description\": \"description of the ticket\\n ---\\n Qualytics Anomaly Linked [2026-01-23 20:22:04 UTC] Anomaly ID: 1 Status: Acknowledged Type: shape Detected: 2025-12-21 01:48:42.988233+00:00\\n Failed Checks: - 8 values in CUSTOMER_NAME were resolved to 7 distinct entities using similarity matching. 1 of those entities is assigned more than one value of CUSTOMER_ID\\n View in Qualytics: https://your-qualytics-instance.qualytics.io/datastores/1/anomalies?id=1\\n\",\n    \"status\": \"Idea\",\n    \"priority\": \"Medium\",\n    \"urgency\": null,\n    \"impact\": null,\n    \"category\": \"Task\",\n    \"subcategory\": null,\n    \"created_at\": \"2026-01-23T15:22:05\",\n    \"updated_at\": \"2026-01-23T15:22:05\",\n    \"resolved_at\": null,\n    \"closed_at\": null,\n    \"assigned_to\": null,\n    \"assigned_group\": null,\n    \"requester\": \"User Name\",\n    \"url\": \"https://your-domain.atlassian.net/browse/SCRUM-40\",\n    \"comments_count\": null,\n    \"metadata\": {\n      \"project_key\": \"SCRUM\",\n      \"issue_type\": \"Task\"\n    }\n  },\n  \"anomaly_id\": 1,\n  \"integration_id\": 2,\n  \"message\": \"Ticket SCRUM-40 created and linked to anomaly 1\"\n}\n</code></pre>"},{"location":"settings/integrations/ticketing/jira/#search-tickets-response","title":"Search Tickets Response","text":"<pre><code>GET /api/integrations/ticketing/tickets/search?query=task\n\n{\n  \"items\": [\n    {\n      \"ticket_id\": \"10002\",\n      \"ticket_number\": \"SCRUM-3\",\n      \"title\": \"Task 3\",\n      \"description\": null,\n      \"status\": \"To Do\",\n      \"priority\": null,\n      \"created_at\": \"2025-12-02T12:34:20\",\n      \"updated_at\": \"2025-12-02T12:34:21\",\n      \"assigned_to\": null,\n      \"url\": \"https://your-domain.atlassian.net/browse/SCRUM-3\"\n    }\n  ],\n  \"total\": 1,\n  \"page\": 1,\n  \"size\": 50,\n  \"pages\": 1\n}\n</code></pre>"},{"location":"settings/integrations/ticketing/servicenow/","title":"ServiceNow","text":"<p>The ServiceNow integration enables data quality teams to create and link ServiceNow Incident tickets directly from data anomalies detected in Qualytics. This streamlines the issue resolution workflow by eliminating the need to manually create tickets in ServiceNow and then reference them back in Qualytics.</p> <p></p>"},{"location":"settings/integrations/ticketing/servicenow/#key-benefits","title":"Key Benefits","text":"<ul> <li>Create Incidents Directly: Create ServiceNow Incidents directly from Qualytics anomalies without leaving the platform</li> <li>Automatic Linking: Automatic linking between anomalies and tickets for complete traceability</li> <li>Bidirectional Navigation: Hyperlink from ServiceNow ticket back to the source anomaly in Qualytics</li> <li>Status Synchronization: Anomaly status changes (acknowledge, archive) sync to ServiceNow</li> <li>Comment Syncing: Comments added in Qualytics are pushed to ServiceNow incident work notes</li> </ul> <p>Integration Type</p> <p>The current release supports one-way synchronization (Qualytics \u2192 ServiceNow). Changes made directly in ServiceNow (e.g., ticket state changes, comments) are not reflected back in Qualytics at this time.</p>"},{"location":"settings/integrations/ticketing/servicenow/#prerequisites","title":"Prerequisites","text":"<p>Before configuring the ServiceNow integration, ensure the following requirements are met.</p>"},{"location":"settings/integrations/ticketing/servicenow/#servicenow-requirements","title":"ServiceNow Requirements","text":"Requirement Details ServiceNow Instance Active ServiceNow instance accessible via HTTPS Instance URL Format <code>https://[instance-name].service-now.com</code> ITSM Module Incident Management module must be active API Access REST API must be enabled for the instance Network Access Qualytics cloud must be able to reach your ServiceNow instance (firewall/allowlist may be required)"},{"location":"settings/integrations/ticketing/servicenow/#service-account-requirements","title":"Service Account Requirements","text":"<p>A dedicated service account is required for the integration. This account will be used by Qualytics to authenticate and create/update incidents.</p> Setting Recommendation Username <code>qualytics_integration</code> or similar identifiable name Password Strong password, stored securely Web Service Access Enabled (check \"Web service access only\" if no UI login needed) Active Status Active (not locked or disabled)"},{"location":"settings/integrations/ticketing/servicenow/#required-servicenow-roles","title":"Required ServiceNow Roles","text":"<p>Assign the following roles to the service account:</p> Role Purpose <code>itil</code> Allows creation and modification of Incident records <code>rest_api_explorer</code> (optional) Useful for testing/troubleshooting API calls <p>Note</p> <p>Additional roles may be required if your organization has custom access controls on the Incident table.</p>"},{"location":"settings/integrations/ticketing/servicenow/#servicenow-configuration","title":"ServiceNow Configuration","text":"<p>Complete the following steps in your ServiceNow instance before configuring the integration in Qualytics.</p>"},{"location":"settings/integrations/ticketing/servicenow/#step-1-create-the-service-account","title":"Step 1: Create the Service Account","text":"<ol> <li>Navigate to User Administration \u2192 Users</li> <li>Click New to create a new user</li> </ol> <ol> <li>Enter the User ID (e.g., <code>qualytics_integration</code>)</li> <li>Set a secure password</li> <li>Check \"Web service access only\" if the account should not have UI access</li> <li>Save the user record</li> </ol> <ol> <li>Navigate to the Roles tab and assign the <code>itil</code> role</li> </ol>"},{"location":"settings/integrations/ticketing/servicenow/#step-2-verify-api-access","title":"Step 2: Verify API Access","text":"<p>Confirm that the REST API is enabled for your instance:</p> <ol> <li>Navigate to System Web Services \u2192 REST API Explorer</li> </ol> <p></p> <ol> <li>Verify you can access the Incident table API</li> <li>Test a simple GET request to confirm connectivity</li> </ol> <p></p> <ol> <li>Verify the response returns successfully</li> </ol> <p></p>"},{"location":"settings/integrations/ticketing/servicenow/#step-3-networkfirewall-configuration","title":"Step 3: Network/Firewall Configuration","text":"<p>If your ServiceNow instance has IP restrictions, you may need to allowlist Qualytics IP addresses. Contact Qualytics support for the current list of egress IPs used by the Qualytics cloud platform.</p>"},{"location":"settings/integrations/ticketing/servicenow/#step-4-gather-configuration-values","title":"Step 4: Gather Configuration Values","text":"<p>Collect the following values needed for Qualytics configuration:</p> Value How to Find Example Instance URL Your ServiceNow URL <code>https://your-instance.service-now.com</code> Credentials Service account username and password combined with a colon <code>qualytics_integration:YourPassword</code> Category Names Incident \u2192 Right-click Category \u2192 Show choice list Software, Network Subcategory Names Same process, filter by Category Email, Database Assignment Group SysID User Administration \u2192 Groups \u2192 Copy sys_id <code>a1b2c3d4e5f6...</code>"},{"location":"settings/integrations/ticketing/servicenow/#qualytics-configuration","title":"Qualytics Configuration","text":""},{"location":"settings/integrations/ticketing/servicenow/#navigation-to-integration","title":"Navigation to Integration","text":"<p>Step 1: Log in to your Qualytics account and click the \"Settings\" button on the left side panel of the interface.</p> <p>Step 2: Click on the Integrations tab.</p> <p></p>"},{"location":"settings/integrations/ticketing/servicenow/#connect-servicenow-integration","title":"Connect ServiceNow Integration","text":"<p>Step 1: Click on the Connect button next to ServiceNow to connect to the ServiceNow Integration.</p> <p>A modal window titled \"Add ServiceNow Integration\" appears. Fill in the connection properties.</p> <p></p> <p>Step 2: Enter the following connection properties:</p> REF. Field Description 1 Instance URL (Required) Enter the complete ServiceNow instance URL (e.g., <code>https://your-instance.service-now.com</code>). Do not include a trailing slash. 2 Credentials (Required) Enter the service account credentials in the format <code>username:password</code> (username and password separated by a colon). For example: <code>qualytics_integration:YourSecurePassword123</code> <p>Credentials Format</p> <p>The credentials must be entered as <code>username:password</code> with a colon separator. This format is used for Basic Authentication. OAuth 2.0 and HashiCorp Vault credential storage are planned for future releases.</p> <p>Step 3: Click the Create button to validate and store the credentials.</p> <p>Once the integration is successfully created, a confirmation message will appear stating \"The Integration has been successfully created.\"</p> <p></p>"},{"location":"settings/integrations/ticketing/servicenow/#test-the-connection","title":"Test the Connection","text":"<p>After saving, the connection status should show as Connected. If you encounter errors, verify:</p> <ul> <li>Instance URL is correct and includes <code>https://</code> (no trailing slash)</li> <li>Credentials are in the correct format (<code>username:password</code>)</li> <li>Service account has required roles</li> <li>Network connectivity allows Qualytics to reach ServiceNow</li> </ul> <p>When the integration is created, Qualytics validates the connection by making a test request to the ServiceNow API (<code>GET /api/now/table/incident?sysparm_limit=1</code>).</p>"},{"location":"settings/integrations/ticketing/servicenow/#integration-behavior-data-flow","title":"Integration Behavior &amp; Data Flow","text":""},{"location":"settings/integrations/ticketing/servicenow/#data-flow-direction","title":"Data Flow Direction","text":"<p>The current release supports one-way synchronization from Qualytics to ServiceNow:</p> Action in Qualytics Result in ServiceNow Create ticket from anomaly New Incident record created Acknowledge anomaly Incident work notes updated with status change Archive anomaly (resolve) Incident marked as Resolved Add comment to anomaly Comment pushed to Incident work notes Link existing ticket Association created (anomaly details appended to description)"},{"location":"settings/integrations/ticketing/servicenow/#integration-flow-diagram","title":"Integration Flow Diagram","text":"<p>The following diagram illustrates how data flows from Qualytics to ServiceNow:</p> <pre><code>flowchart TB\n    subgraph Qualytics[\"Qualytics Platform\"]\n        A[Anomaly Detected] --&gt; B{User Action}\n        B --&gt;|Create Ticket| C[Create Ticket Request]\n        B --&gt;|Acknowledge| D[Status Change Event]\n        B --&gt;|Add Comment| E[Comment Event]\n        B --&gt;|Archive/Resolve| F[Resolution Event]\n    end\n\n    subgraph Background[\"Background Processing\"]\n        C --&gt; G[Qualytics API]\n        D --&gt; H[Sync Worker]\n        E --&gt; I[Sync Worker]\n        F --&gt; J[Sync Worker]\n    end\n\n    subgraph ServiceNow[\"ServiceNow Instance\"]\n        G --&gt;|POST /incident| K[New Incident Created]\n        H --&gt;|PATCH /incident| L[Work Note Added]\n        I --&gt;|PATCH /incident| L\n        J --&gt;|PATCH /incident| M[Incident Resolved]\n    end\n\n    K --&gt; N[Ticket Link Stored]\n    N --&gt; O[Anomaly &amp; Ticket Linked]</code></pre>"},{"location":"settings/integrations/ticketing/servicenow/#what-gets-synced","title":"What Gets Synced","text":"Direction What How Status Qualytics \u2192 ServiceNow New ticket Creates incident with anomaly details  Active Qualytics \u2192 ServiceNow Anomaly status change Adds work note (doesn't change incident state)  Active Qualytics \u2192 ServiceNow Anomaly comment Adds work note  Active ServiceNow \u2192 Qualytics Ticket status Manual sync only (updates link status)  Limited ServiceNow \u2192 Qualytics Ticket comments Not synced  Not supported ServiceNow \u2192 Qualytics Ticket closure Not synced  Not supported"},{"location":"settings/integrations/ticketing/servicenow/#current-limitations","title":"Current Limitations","text":"<p>The following actions in ServiceNow are not reflected in Qualytics:</p> Action in ServiceNow Effect in Qualytics Change Incident state (e.g., In Progress, Resolved) Not reflected - anomaly status unchanged Add comments or work notes Not reflected - comments do not appear in Qualytics Reassign Incident Not reflected Close or cancel Incident Not reflected - anomaly remains in current state"},{"location":"settings/integrations/ticketing/servicenow/#servicenow-state-mappings","title":"ServiceNow State Mappings","text":"<p>ServiceNow uses numeric state values for incidents. Understanding these states helps when working with the integration:</p> State Code State Name Description 1 New Incident has been created but not yet worked on 2 In Progress Work is actively being done on the incident 3 On Hold Incident is waiting for external input or resources 6 Resolved The issue has been fixed/addressed 7 Closed Incident is complete and verified 8 Canceled Incident was canceled without resolution <p>One-Way State Updates</p> <p>Qualytics does not automatically update anomaly status based on ServiceNow state changes. This design prevents synchronization loops and maintains clear ownership: ServiceNow is the \"ticket of record\" while Qualytics is the \"anomaly of record.\"</p>"},{"location":"settings/integrations/ticketing/servicenow/#field-mapping-reference","title":"Field Mapping Reference","text":""},{"location":"settings/integrations/ticketing/servicenow/#incident-fields-supported","title":"Incident Fields Supported","text":"<p>When creating a ticket from Qualytics, the following ServiceNow Incident fields can be populated:</p> Field Name ServiceNow Field Required Notes Short Description <code>short_description</code> Yes Brief summary of the issue Description <code>description</code> No Detailed description; auto-populated with anomaly details Priority <code>priority</code> No Values: Critical, High, Moderate, Low, Planning Urgency <code>urgency</code> No Standard ServiceNow urgency values Impact <code>impact</code> No Standard ServiceNow impact values Category <code>category</code> No Must match exact category name in ServiceNow Subcategory <code>subcategory</code> No Must match exact subcategory name Assignment Group <code>assignment_group</code> No Requires <code>sys_id</code> of the group (not name) Assigned To <code>assigned_to</code> No Username of the assignee"},{"location":"settings/integrations/ticketing/servicenow/#auto-populated-fields","title":"Auto-Populated Fields","text":"<p>The following fields are automatically populated by Qualytics when creating a ticket:</p> Field Auto-Populated Value Description (body) Anomaly ID, type, status at detection, field/check message, failed checks, hyperlink back to Qualytics anomaly Caller Currently uses service account (user-based caller not yet supported) Work Notes (on updates) Status changes (acknowledge, archive) and comments from Qualytics with timestamps"},{"location":"settings/integrations/ticketing/servicenow/#how-to-find-category-subcategory-and-assignment-group-values","title":"How to Find Category, Subcategory, and Assignment Group Values","text":""},{"location":"settings/integrations/ticketing/servicenow/#category-subcategory","title":"Category / Subcategory","text":"<ol> <li>Open any existing Incident in ServiceNow</li> <li>Right-click on the Category field</li> <li>Select \"Show choice list\"</li> </ol> <ol> <li>Copy the exact Name value (case-sensitive)</li> </ol>"},{"location":"settings/integrations/ticketing/servicenow/#assignment-group-sys_id","title":"Assignment Group sys_id","text":"<ol> <li>Navigate to User Administration \u2192 Groups</li> </ol> <ol> <li>Locate the desired group</li> <li>Right-click on the group row</li> <li>Select \"Copy sys_id\" and paste into Qualytics</li> </ol> <p>Important</p> <p>Category and Subcategory names are case-sensitive and must match exactly as they appear in ServiceNow. Assignment Group requires the <code>sys_id</code>, not the display name.</p>"},{"location":"settings/integrations/ticketing/servicenow/#ticket-creation-workflow","title":"Ticket Creation Workflow","text":""},{"location":"settings/integrations/ticketing/servicenow/#creating-a-new-ticket-from-an-anomaly","title":"Creating a New Ticket from an Anomaly","text":"<p>Step 1: Navigate to a datastore in Qualytics</p> <p>Step 2: Locate an anomaly you want to create a ticket for</p> <p>Step 3: Click on the anomaly to open details</p> <p>Step 4: Find the \"Linked Tickets\" section and click \"Add\" to link or create tickets</p> <p></p> <p>Step 5: A modal will appear with options to create a new ticket or link an existing one. Select \"New Ticket\"</p> <p></p> <p>Step 6: Fill in the required and optional fields:</p> <p></p> Field Description Short Description Brief summary of the data quality issue Description Detailed description (auto-populated with anomaly details) Priority Select the appropriate priority level Urgency Select the urgency level Impact Select the impact level Category Select or enter the category (must match ServiceNow exactly) Subcategory Select or enter the subcategory Assignment Group Enter the <code>sys_id</code> of the assignment group <p></p> <p>Step 7: Click Create to submit the ticket</p> <p>When the ticket is created, a link is established between the anomaly and the ServiceNow incident. The incident in ServiceNow will contain the anomaly details in the description, including:</p> <ul> <li>Anomaly ID and status</li> <li>Anomaly type and detection timestamp</li> <li>Failed checks information</li> <li>Direct link back to the anomaly in Qualytics</li> </ul> <p></p>"},{"location":"settings/integrations/ticketing/servicenow/#linking-an-existing-ticket","title":"Linking an Existing Ticket","text":"<p>If a ticket already exists in ServiceNow, you can link it to an anomaly:</p> <p>Step 1: Open the anomaly in Qualytics</p> <p>Step 2: Click \"Add\" in the Linked Tickets section</p> <p>Step 3: Select \"Link Existing\"</p> <p>Step 4: Search by Incident number or short description</p> <p></p> <p>Step 5: Select the matching Incident to link</p> <p>When linking an existing ticket:</p> <ul> <li>Anomaly details are appended to the existing incident description</li> <li>A work note is added documenting the linkage</li> </ul> <p>Tip</p> <p>Multiple tickets can be linked to a single anomaly if needed.</p>"},{"location":"settings/integrations/ticketing/servicenow/#viewing-linked-tickets","title":"Viewing Linked Tickets","text":"<p>Once linked, tickets appear in the \"Linked Tickets\" section of the anomaly:</p> <p></p> <ul> <li>Click \"View\" to open the ticket directly in ServiceNow</li> <li>The Incident number, status, and last sync timestamp are displayed</li> </ul>"},{"location":"settings/integrations/ticketing/servicenow/#key-integration-operations","title":"Key Integration Operations","text":""},{"location":"settings/integrations/ticketing/servicenow/#create-ticket","title":"Create Ticket","text":"<p>Creates a ServiceNow incident from a Qualytics anomaly with:</p> <ul> <li>Required fields: <code>short_description</code>, <code>description</code>, <code>priority</code>, <code>urgency</code>, <code>impact</code></li> <li>Optional fields: <code>category</code>, <code>subcategory</code>, <code>assigned_to</code>, <code>assignment_group</code></li> <li>Auto-included: Anomaly ID, status, type, failed checks, and link to Qualytics</li> </ul>"},{"location":"settings/integrations/ticketing/servicenow/#link-existing-ticket","title":"Link Existing Ticket","text":"<p>Links an existing ServiceNow incident to an anomaly:</p> <ul> <li>Appends anomaly details to the existing description</li> <li>Adds a work note about the linkage</li> </ul>"},{"location":"settings/integrations/ticketing/servicenow/#sync-anomaly-status","title":"Sync Anomaly Status","text":"<p>When anomaly status changes in Qualytics:</p> <ul> <li>Adds a timestamped work note to the linked ServiceNow incident</li> <li>Example: <code>[2024-01-15 10:30:00 UTC] Qualytics Anomaly Status: Acknowledged</code></li> </ul> <p>Example: Acknowledging an anomaly in Qualytics:</p> <p></p> <p>The status change is reflected in ServiceNow as a work note:</p> <p></p> <p>Note</p> <p>Status changes add work notes but do not change the incident's state. This preserves ITSM workflow control.</p>"},{"location":"settings/integrations/ticketing/servicenow/#sync-comments","title":"Sync Comments","text":"<p>When comments are added to anomalies in Qualytics:</p> <ul> <li>The comment text is synced to linked tickets as work notes</li> <li>Example: <code>Comment from Qualytics: &lt;user's comment&gt;</code></li> </ul>"},{"location":"settings/integrations/ticketing/servicenow/#supported-features-capabilities","title":"Supported Features &amp; Capabilities","text":""},{"location":"settings/integrations/ticketing/servicenow/#current-release-features","title":"Current Release Features","text":"Feature Status Create Incident from anomaly  Supported Link existing Incident to anomaly  Supported Link multiple tickets to one anomaly  Supported Sync anomaly acknowledge to ServiceNow  Supported Sync anomaly archive (resolve) to ServiceNow  Supported Push comments to ServiceNow work notes  Supported Hyperlink from ServiceNow back to Qualytics anomaly  Supported Basic Authentication (username/password)  Supported Search existing ServiceNow tickets  Supported"},{"location":"settings/integrations/ticketing/servicenow/#known-limitations","title":"Known Limitations","text":"Feature Status Two-way sync (ServiceNow \u2192 Qualytics)  Not yet supported See ServiceNow ticket state changes in Qualytics  Not yet supported Drop-down fields populated from ServiceNow  Manual entry required Multiple ServiceNow instances  Single instance only Ticket types other than Incident (Defect, Enhancement, etc.)  Incident only Auto-populate caller from logged-in user  Uses service account OAuth 2.0 authentication  Basic auth only HashiCorp Vault credential storage  Credentials stored in Qualytics Automatic ticket creation from Flows  Not yet available"},{"location":"settings/integrations/ticketing/servicenow/#api-reference","title":"API Reference","text":"<p>This section provides technical details for administrators and developers who need to understand the underlying API operations.</p>"},{"location":"settings/integrations/ticketing/servicenow/#qualytics-api-endpoints","title":"Qualytics API Endpoints","text":"<p>For programmatic integration management, the following Qualytics API endpoints are available:</p>"},{"location":"settings/integrations/ticketing/servicenow/#ticketing-integration-endpoints","title":"Ticketing Integration Endpoints","text":"Operation Method Endpoint Description Get Ticketing Form Specification GET <code>/api/integrations/ticketing/form-specification</code> Get the form specification for creating tickets Search Tickets GET <code>/api/integrations/ticketing/tickets/search</code> Search for tickets in ServiceNow Get Ticket By Id GET <code>/api/integrations/ticketing/tickets/{ticket_id}</code> Get a specific ticket by its ID Create Ticket For Anomaly POST <code>/api/integrations/ticketing/anomalies/{anomaly_id}/tickets</code> Create a new ticket linked to an anomaly"},{"location":"settings/integrations/ticketing/servicenow/#anomaly-ticket-link-endpoints","title":"Anomaly Ticket Link Endpoints","text":"Operation Method Endpoint Description Get Anomaly Ticket Links GET <code>/api/anomalies/{anomaly_id}/ticket-links</code> Retrieve all ticket links for an anomaly Create Anomaly Ticket Link POST <code>/api/anomalies/{anomaly_id}/ticket-links</code> Create a new ticket link for an anomaly Delete Anomaly Ticket Link DELETE <code>/api/anomalies/{anomaly_id}/ticket-links/{link_id}</code> Delete a specific ticket link"},{"location":"settings/integrations/ticketing/servicenow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"settings/integrations/ticketing/servicenow/#common-issues","title":"Common Issues","text":"Issue Resolution Connection failed Verify instance URL format (<code>https://your-instance.service-now.com</code>), check credentials format (<code>username:password</code>), confirm service account is active 401 Unauthorized Incorrect credentials; verify the <code>username:password</code> format is correct and that account is not locked 403 Forbidden Service account lacks required roles; add <code>itil</code> role Ticket created but missing fields Category/Subcategory names must match exactly (case-sensitive); verify names in ServiceNow choice list Assignment Group not set Must use <code>sys_id</code>, not group name; copy <code>sys_id</code> from User Administration \u2192 Groups Network timeout Firewall may be blocking; ensure Qualytics IPs are allowlisted Work notes not appearing Verify the service account has permission to add work notes to incidents"},{"location":"settings/integrations/ticketing/servicenow/#getting-support","title":"Getting Support","text":"<p>If you encounter issues not covered above, contact Qualytics support:</p> <ul> <li>Email: support@qualytics.ai</li> <li>Slack: Your dedicated customer Slack channel</li> </ul> <p>Please include: Instance URL (masked if needed), error messages, and steps to reproduce.</p>"},{"location":"settings/integrations/ticketing/servicenow/#appendix-sample-payloads","title":"Appendix: Sample Payloads","text":""},{"location":"settings/integrations/ticketing/servicenow/#create-ticket-request","title":"Create Ticket Request","text":"<p>When creating a ticket from Qualytics, the following request is sent:</p> <pre><code>POST /api/integrations/ticketing/anomalies/{anomaly_id}/tickets\n\n{\n  \"title\": \"My ticket description\",\n  \"priority\": \"3\",\n  \"urgency\": \"3\",\n  \"impact\": \"3\"\n}\n</code></pre>"},{"location":"settings/integrations/ticketing/servicenow/#create-ticket-response","title":"Create Ticket Response","text":"<pre><code>{\n  \"ticket\": {\n    \"ticket_id\": \"7a62213283aef210f9e89780deaad366\",\n    \"ticket_number\": \"INC0010006\",\n    \"title\": \"My ticket description\",\n    \"description\": \"## Qualytics Anomaly Linked [2026-01-23 19:51:23 UTC]\\n\\n**Anomaly ID:** 824\\n**Status:** Acknowledged\\n**Type:** record\\n**Detected:** 2026-01-22 13:22:40.780346+00:00\\n\\n**Failed Checks:**\\n- The field 'max_value' has a value that falls outside the declared limits\\n\\n**View in Qualytics:** https://your-qualytics-instance.qualytics.io/datastores/23/anomalies?id=824\\n\",\n    \"status\": \"New\",\n    \"priority\": \"5\",\n    \"urgency\": \"3\",\n    \"impact\": \"3\",\n    \"category\": \"inquiry\",\n    \"subcategory\": \"\",\n    \"created_at\": \"2026-01-23T19:51:24\",\n    \"updated_at\": \"2026-01-23T19:51:24\",\n    \"resolved_at\": null,\n    \"closed_at\": null,\n    \"assigned_to\": null,\n    \"assigned_group\": null,\n    \"requester\": null,\n    \"url\": \"https://your-instance.service-now.com/nav_to.do?uri=incident.do?sys_id=7a62213283aef210f9e89780deaad366\",\n    \"comments_count\": null,\n    \"metadata\": {\n      \"work_notes\": \"\",\n      \"close_notes\": \"\"\n    }\n  },\n  \"anomaly_id\": 824,\n  \"integration_id\": 1,\n  \"message\": \"Ticket INC0010006 created and linked to anomaly 824\"\n}\n</code></pre>"},{"location":"settings/integrations/ticketing/servicenow/#search-tickets-response","title":"Search Tickets Response","text":"<pre><code>GET /api/integrations/ticketing/tickets/search?query=shared+folder\n\n{\n  \"items\": [\n    {\n      \"ticket_id\": \"57af7aec73d423002728660c4cf6a71c\",\n      \"ticket_number\": \"INC0009009\",\n      \"title\": \"Unable to access the shared folder.\",\n      \"description\": \"Unable to access the shared folder. Please provide access.\",\n      \"status\": \"Unknown (New)\",\n      \"priority\": \"4 - Low\",\n      \"created_at\": \"2018-08-30T01:06:52\",\n      \"updated_at\": \"2018-12-12T23:30:24\",\n      \"assigned_to\": null,\n      \"url\": \"https://your-instance.service-now.com/nav_to.do?uri=incident.do?sys_id=57af7aec73d423002728660c4cf6a71c\"\n    }\n  ],\n  \"total\": 1,\n  \"page\": 1,\n  \"size\": 50,\n  \"pages\": 1\n}\n</code></pre>"},{"location":"settings/integrations/ticketing/ticketing/","title":"Ticketing Integrations","text":"<p>The Qualytics platform integrates with enterprise ticketing systems to help teams track and resolve data quality issues using their existing issue and incident management workflows.</p> <p>These integrations enable teams to:</p> <ul> <li>Track and manage data quality issues within familiar ticketing tools  </li> <li>Create tickets from data quality failures for investigation and resolution  </li> <li>Assign ownership and monitor progress using standard workflows  </li> <li>Align data quality remediation with existing operational processes  </li> <li>Maintain visibility into data quality issues alongside other work items  </li> </ul> <p>This approach helps make data quality management a natural extension of your team\u2019s established workflows rather than a separate process to manage.</p>"},{"location":"settings/integrations/ticketing/ticketing/#available-integrations","title":"Available Integrations","text":"<p>Qualytics currently supports the following ticketing integrations:</p>"},{"location":"settings/integrations/ticketing/ticketing/#jira","title":"Jira","text":"<p>Integrate Qualytics with Jira to create and manage issues related to data quality failures. This allows teams to triage, assign, and track resolution progress using their existing Jira workflows.</p> <p>For more details, refer to the Jira integration documentation.</p> <p></p>"},{"location":"settings/integrations/ticketing/ticketing/#servicenow","title":"ServiceNow","text":"<p>Connect Qualytics with ServiceNow to route data quality incidents into your IT service management processes. This helps organizations handle data quality issues using established ServiceNow incident and change management workflows.</p> <p>For more details, refer to the ServiceNow integration documentation.</p> <p></p>"},{"location":"settings/integrations/workflow/workflow/","title":"Workflow Integrations","text":"<p>The Qualytics platform integrates with modern data workflow and orchestration tools, empowering teams to:</p> <p> </p> <ul> <li>Embed data quality checks directly within data pipelines</li> <li>Automate quality verification steps in ETL and transformation processes</li> <li>Trigger remediation workflows when quality issues are detected</li> <li>Schedule and coordinate data quality operations with other pipeline activities</li> <li>Ensure data quality gates are enforced before critical data movements</li> </ul> <p> </p> <p>These integrations enable organizations to make data quality an integral part of their automated data workflows, ensuring quality checks and remediation steps are seamlessly woven into their data engineering processes.</p>"},{"location":"settings/security/directory-sync/","title":"Directory Sync","text":"<p>Directory Sync, also known as User and Group Provisioning, automates the synchronization of users and groups between your identity provider (IDP) and the Qualytics platform. This ensures that your user data is consistent across all systems, improving security and reducing the need for manual updates.</p>"},{"location":"settings/security/directory-sync/#directory-sync-overview","title":"Directory Sync Overview","text":"<p>Directory Sync automates the management of users and groups by synchronizing information between an identity provider (IDP) and your application. This ensures that access permissions, user attributes, and group memberships are consistently managed across platforms, eliminating the need for manual updates.</p>"},{"location":"settings/security/directory-sync/#how-directory-sync-works-with-scim","title":"How Directory Sync Works with SCIM","text":"<p>SCIM is an open standard protocol designed to simplify the exchange of user identity information. When integrated with Directory Sync, SCIM automates the creation, updating, and de-provisioning of users and groups. SCIM communicates securely between the IDP and your platform\u2019s API using OAuth tokens to ensure only authorized actions are performed.</p>"},{"location":"settings/security/directory-sync/#general-setup-requirements","title":"General Setup Requirements","text":"<p>To set up Directory Sync, the following are required:</p> <ul> <li>Administrative access to both the identity provider and Qualytics platform  </li> <li>A SCIM-enabled identity provider or custom integration  </li> <li>The OAuth client set up in your IDP  </li> <li>SCIM URL and OAuth Bearer Token generated from the Qualytics platform</li> </ul>"},{"location":"settings/security/directory-sync/#getting-started","title":"Getting Started","text":""},{"location":"settings/security/directory-sync/#prerequisites-for-setting-up-directory-sync","title":"Prerequisites for Setting Up Directory Sync","text":"<p>Before setting up Directory Sync, ensure you have the following:</p> <ul> <li>A SCIM-supported identity provider  </li> <li>Administrative privileges for both your IDP and Qualytics  </li> <li>A SCIM URL and OAuth Bearer Token, which will be generated from your Qualytics instance</li> </ul>"},{"location":"settings/security/directory-sync/#quick-start-guide","title":"Quick Start Guide","text":"<ol> <li>Set up an OAuth client in your IDP.  </li> <li>Configure the SCIM endpoints with the SCIM URL and OAuth Bearer Token.  </li> <li>Assign users and groups to provision in the IDP.  </li> <li>Monitor the synchronization to ensure proper operation.</li> </ol>"},{"location":"settings/security/directory-sync/#what-is-scim","title":"What is SCIM?","text":"<p>SCIM is a standardized protocol used to automate the exchange of user identity information between IDPs and service providers. Its goal is to simplify the process of user provisioning and management.</p> <p>SCIM improves efficiency by automating user lifecycle management (creation, updating, and de-provisioning) and ensures that data remains consistent across platforms. It also enhances security by minimizing manual errors and ensuring proper access control.</p> <p>SCIM includes endpoints that are configured within your IDP and your platform. It uses OAuth tokens for secure communication between the IDP and the Qualytics API, ensuring that only authorized users can manage identity data.</p>"},{"location":"settings/security/directory-sync/#benefits-of-using-scim-for-user-and-group-provisioning","title":"Benefits of Using SCIM for User and Group Provisioning","text":"<p>By leveraging SCIM (System for Cross-domain Identity Management), Directory Sync simplifies user management with:</p> <ul> <li>Automated user provisioning and de-provisioning  </li> <li>Reduced manual intervention, improving efficiency and security  </li> <li>Real-time updates of user data, ensuring accuracy and compliance</li> <li>Support for scaling user management across organizations of any size</li> </ul> <p>Supported Providers</p> <p>Our API supports SCIM 2.0 (System for Cross-domain Identity Management) as defined in RFC 7643 and RFC 7644. It is designed to ensure seamless integration with any SCIM-compliant identity management system, supporting standardized user provisioning, de-provisioning, and lifecycle management. Additionally, we have verified support with the following providers:</p> <ul> <li>Microsoft Entra (Azure Active Directory) </li> <li>Okta </li> <li>OneLogin </li> <li>JumpCloud</li> </ul> <p>Unsupported Providers</p> <p>We do not support Google Workspace, as it does not offer SCIM support. Organizations using Google Workspace must use alternate methods for user provisioning.</p>"},{"location":"settings/security/directory-sync/#providers","title":"Providers","text":""},{"location":"settings/security/directory-sync/#1-microsoft-entra","title":"1. Microsoft Entra","text":""},{"location":"settings/security/directory-sync/#creating-an-app-registration","title":"Creating an App Registration","text":"<p>Step 1: Log in to the Microsoft Azure Portal, and select \u201cMicrosoft Entra ID\u201d from the main menu.</p> <p></p> <p>Step 2: Click on \u201cEnterprise Applications\u201d from the left navigation menu.</p> <p></p> <p>Step 3: If your application is already created, choose it from the list and move to the section Configuring SCIM Endpoints. If you haven't created your application yet, click on the New Application button.</p> <p></p> <p>Step 4: Click on the \u201cCreate your own application\u201d button to create your application.</p> <p></p> <p>Step 5: Give your application a name (e.g., \"Qualytics OAuth Client\" or \"Qualytics SCIM Client\").</p> <p></p> <p>Step 6: After entering the name for your application, click the Create button to finalize the creation of your app.</p> <p></p>"},{"location":"settings/security/directory-sync/#configuring-scim-endpoints","title":"Configuring SCIM Endpoints","text":"<p>Step 1: Click on Provisioning from the left-hand menu.</p> <p></p> <p>Step 2: A new window will appear, click on the Get Started button.</p> <p></p> <p>Step 3: In the Provisioning Mode dropdown, select \u201cAutomatic\u201d and enter the following details in the Admin Credentials section:</p> <ol> <li> <p>Provisioning Mode: Select Automatic.</p> </li> <li> <p>Tenant URL: <code>https://your-domain.qualytics.io/api/scim/v2</code> </p> </li> <li> <p>Secret Token: Generate this token from the Qualytics UI when logged in as an admin user. For more information on how to generate tokens in Qualytics, refer to the documentation on Tokens.</p> </li> </ol> <p></p> <p>Step 4: Click on the Test Connection button to test the connection to see if the credentials are correct.</p> <p></p> <p>Step 5: Expand the Mappings section and enable your app to enable group and user attribute mappings. The default mapping should work.</p> <p></p> <p>Step 6: Expand the Settings section and make the following changes:</p> <ol> <li>Select Sync only assigned users and groups from the Scope dropdown.  </li> <li>Confirm the Provisioning Status is set to On.</li> </ol> <p></p> <p>Step 7: Click on the Save to save the credentials. Now you've successfully configured the Microsoft Entra ID SCIM API integration.</p> <p></p>"},{"location":"settings/security/directory-sync/#assigning-users-and-groups-for-provisioning","title":"Assigning Users and Groups for Provisioning","text":"<p>Step 1: Click on the Users and groups from the left navigation menu and then click Add user/group.</p> <p></p> <p>Step 2: Click on the None Selected under the Users and Groups.</p> <p></p> <p>Step 3: From the right side of the screen, select the users and groups you want to assign to the app.</p> <p></p> <p>Step 4: Once you selected the group and users for your app, click the \u201cSelect\u201d button.</p> <p></p> <p>Step 5: Click on the Assign button to assign the users and groups to the application.</p> <p>Warning</p> <p>When you assign a group to an application, only users directly in the group will have access. The assignment does not cascade to nested groups.</p> <p></p>"},{"location":"settings/security/directory-sync/#2-okta","title":"2. Okta","text":""},{"location":"settings/security/directory-sync/#setting-up-the-oauth-client-in-okta","title":"Setting up the OAuth Client in Okta","text":"<p>Step 1: Log in to your Okta account using your administrator credentials. From the left-hand navigation menu, click Applications, then select Browse App Catalog.</p> <p></p> <p>Step 2: In the search bar, type SCIM 2.0 Test App (OAuth Bearer Token), and select the app called SCIM 2.0 Test App (OAuth Bearer Token) from the search results.</p> <p></p> <p>Step 3: On the app\u2019s details page, click Add Integration.</p> <p></p> <p>Step 4: Enter a name for your application (e.g., \"Qualytics SCIM Client\").</p> <p></p> <p>Step 5: Click on the Next button.</p> <p></p>"},{"location":"settings/security/directory-sync/#configuring-scim-endpoints_1","title":"Configuring SCIM Endpoints","text":"<p>Step 1: In the newly created app, go to the Provisioning tab and click Configure API Integration.</p> <p></p> <p>Step 2: Check the box labeled Enable API Integration, and enter the following details:</p> <ul> <li> <p>SCIM 2.0 Base URL: <code>https://your-domain.qualytics.io/api/scim/v2</code> </p> </li> <li> <p>OAuth Bearer Token: Generate this token from the Qualytics UI when logged in as an admin user. For more information on how to generate tokens in Qualytics, refer to the documentation on Tokens.</p> </li> </ul> <p></p> <p>Step 3: Click Test API Credentials to verify the connection. Once the credentials are validated, click Save.</p> <p></p> <p>Step 4: A new settings page will appear. Under the To App section, enable the following settings:</p> <ul> <li>Create Users </li> <li>Update User Attributes </li> <li>Deactivate Users</li> </ul> <p>After enabling these settings, your Okta SCIM API integration is successfully configured.</p> <p></p>"},{"location":"settings/security/directory-sync/#assigning-users-for-provisioning","title":"Assigning users for provisioning","text":"<p>Step 1: Click the Assignments tab and select Assign to People from the dropdown Assign.</p> <p></p> <p>Step 2: Select the users you want to assign to the app and click the Assign button.</p> <p></p> <p>Step 3: After you click the Assign button, you'll see a new popup window with various fields. Confirm the field values and click the Save and Go Back buttons.</p> <p></p>"},{"location":"settings/security/directory-sync/#assigning-groups-for-provisioning","title":"Assigning groups for provisioning","text":"<p>Step 1: Navigate to the tab Push Groups and select Find group by name from the dropdown Push Groups.</p> <p></p> <p>Step 2: Search for the group you want to assign to the app.</p> <p></p> <p>Step 3: After assigning the group name, then click on the Save button.</p> <p></p>"},{"location":"settings/security/directory-sync/#3-onelogin","title":"3. OneLogin","text":""},{"location":"settings/security/directory-sync/#setting-up-the-oauth-client-in-onelogin","title":"Setting up the OAuth Client in OneLogin","text":"<p>Step 1: Log in to your OneLogin account using your administrator credentials. From the top navigation menu, click Applications, then select Add App.</p> <p></p> <p>Step 2: In the search bar, type SCIM and select the app called SCIM Provisioner with SAML (SCIM V2 Enterprise) from the list of apps.</p> <p></p> <p>Step 3: Enter a name for your app, then click Save. You have successfully created the SCIM app in OneLogin.</p> <p></p>"},{"location":"settings/security/directory-sync/#configuring-scim-endpoints_2","title":"Configuring SCIM Endpoints","text":"<p>Step 1: In your created application, navigate to the Configuration tab on the left and enter the following information:</p> <ul> <li> <p>API Status: Enable the API status for the integration to work properly. </p> </li> <li> <p>SCIM Base URL: <code>https://your-domain.qualytics.io/api/scim/v2</code> </p> </li> <li> <p>SCIM Bearer Token: Generate this token from the Qualytics UI when logged in as an admin user. For more information on how to generate tokens in Qualytics, refer to the documentation on Tokens.</p> </li> </ul> <p></p> <p>Step 2: Click on the Save button to store the credentials.</p> <p></p> <p>Step 3: Navigate to the Provisioning tab, and check the box labeled Enable Provisioning.</p> <p></p> <p>Step 4: Click on Save to apply the changes.</p> <p></p> <p>Step 5: Navigate to the Parameters tab and select the row for Groups.</p> <p></p> <p>Step 5: A popup window will appear, check the box Include in User Provisioning, then click the Save button.</p> <p></p>"},{"location":"settings/security/directory-sync/#assigning-users-for-provisioning_1","title":"Assigning Users for Provisioning","text":"<p>Step 1: To assign users to your app, go to Users from the top navigation menu, and select the user you want to assign to the app.</p> <p>From the User page, click the Applications tab on the left, and click the + (plus) sign.</p> <p></p> <p>Step 3: A popup window will show a list of apps. Select the app you created earlier and click Continue.</p> <p></p> <p>Step 4: A new modal window will appear, click on the Save to confirm the assignment.</p> <p></p> <p>Step 5: If you see the status Pending in the table, click that text. A modal window will appear, where you can click Approve to confirm the assignment.</p> <p></p>"},{"location":"settings/security/directory-sync/#assigning-groups-for-provisioning_1","title":"Assigning Groups for Provisioning","text":"<p>Step 1: To push groups to your app, go to the top navigation menu, click Users, select Roles from the dropdown, and click New Role to create the role.</p> <p></p> <p>Step 2: Enter a name for the role, select the app you created earlier</p> <p></p> <p>Step 3: Click on the \u201cSave\u201d button.</p> <p></p> <p>Step 4: Click the Users tab for the role and search for the user you want to assign to the role.</p> <p></p> <p>Step 5: Click the Add To Role button to assign the user, then click Save to confirm the assignment.</p> <p></p> <p>Step 6: A modal window will appear, click on the \u201cSave\u201d button to confirm the assignment.</p> <p></p> <p>Step 7: Go back to your app and click the Rule tab on the left and click the Add Rule button.</p> <p>Give the rule a name. Under the Actions, select the Set Groups in your-app-name from the dropdown, then select each role with values that match your-app-name.</p> <p></p> <p>Step 8: Click on the Save button.</p> <p></p> <p>Step 9: Click on the Users tab on the left, you may see Pending under the provisions state. Click on it to approve the assignment.</p> <p></p> <p>Step 10: A modal window will appear, click on the Approve to finalize the assignment.</p> <p></p>"},{"location":"settings/security/directory-sync/#4-jumpcloud","title":"4. JumpCloud","text":""},{"location":"settings/security/directory-sync/#configuring-scim-endpoints_3","title":"Configuring SCIM Endpoints","text":"<p>JumpCloud supports SCIM provisioning within an existing SAML application. Follow these steps to configure SCIM provisioning:</p> <p>Step 1: Log in to JumpCloud and either choose an existing SAML application or create a new one. From the left navigation menu, click SSO and select your Custom SAML App.</p> <p></p> <p>Step 2: Click on the tab Identity Management within your SAML application.</p> <p>Under the SCIM Version, choose SCIM 2.0 and enter the following information:</p> <ol> <li> <p>Base URL: <code>https://your-domain.qualytics.io/api/scim/v2</code></p> </li> <li> <p>Token Key: Generate this token from the Qualytics UI when logged in as an admin user. For more information on how to generate tokens in Qualytics, refer to the documentation on Tokens.  </p> </li> <li> <p>Test User Email</p> </li> </ol> <p></p> <p>Step 4: Click Test Connection to ensure the credentials are correct, then click Activate to enable SCIM provisioning.</p> <p></p> <p>Step 5: Click Save to store your settings. Once saved, SCIM provisioning is successfully configured for your JumpCloud SAML application.</p> <p></p>"},{"location":"settings/security/directory-sync/#assigning-users-for-provisioning_2","title":"Assigning Users for Provisioning","text":"<p>Step 1: Click the tab User Groups within your SAML application. You can see all the available groups, select the groups you want to sync, and click Save.</p> <p></p> <p>If no existing groups are available, click User Groups from the left navigation menu and click on the plus (+) icon to create a new group.</p> <p></p> <p>Step 2: Select the Users tab and choose the users you want to assign to the group.</p> <p></p> <p>Step 3: Select the Applications tab and choose the app you want to assign the group to.</p> <p></p>"},{"location":"settings/security/overview/","title":"Security","text":"<p>You can easily manage user and team access by assigning roles and permissions within the system. This includes setting up specific access levels and roles for different users and teams. By doing so, you ensure that data and resources are accessed securely and appropriately, with only authorized individuals and groups having the necessary permissions to view or modify them. This helps maintain the integrity and security of your system.</p> <p>Note</p> <p>Only users with the Admin role have the authority to manage global platform settings, such as user permissions and team access controls.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"settings/security/overview/#navigation-to-security","title":"Navigation to Security","text":"<p>Step 1: Log in to your Qualytics account and click the Settings button on the left side panel of the interface. </p> <p></p> <p>Step 2: By default, you will be navigated to the Tags section. Click on the Security tab.</p> <p></p>"},{"location":"settings/security/overview/#user-roles","title":"User Roles","text":"<p>In Qualytics, every user is assigned a role: <code>Admin</code>,<code>Manager</code> or <code>Member</code>. Admins have the ability to edit any User selected from the user listing and change that User's role.  </p> <ul> <li>Admin: Admin users have full access to the system and can manage datastores, teams, and users. This means they can access everything in the application, as well as manage user accounts and team permissions.</li> </ul> Category Functionality Description Source Datastore Delete Permanently remove a source datastore from the system. Enrichment Datastore Delete Permanently remove an enrichment datastore from the system. Global Settings Settings Manage global system configurations and preferences. Security Manage user access and team permissions within the system. User (Manage) Add, modify, or delete user accounts, assign roles, and control access levels. Team (Manage) Manage teams by adding or removing members, and setting team-specific permissions. Health Monitor the system\u2019s health status and performance metrics. Restart Analytical Engine Restart the analytics engine to refresh data processing or resolve issues. <ul> <li>Manager: Manager role has limited administrative access over global assets but remains subject to team permissions when interacting with datastores. Managers cannot manage user roles or teams. They can list all datastores (but cannot view their content without explicit team permission) and create datastores for teams where they have Editor permission. Additionally, Managers on a team with Editor permission can manage datastore teams. They can manage global assets such as Tags, Templates, and Notifications but do not have the ability to manage user accounts or team permissions like Admins.</li> </ul> Category Functionality Description Source Datastore Create Managers can create new source datastores for data integration. List Managers can view all source datastores that are listed in the system. Add Enrichment Add enrichment processes to source datastores to enhance data quality. Teams Manage Managers on a team with Editor permission can manage datastore teams. Enrichment Datastore Create Managers can create enrichment datastores and assign them to teams with an \"Editor\" role. List Managers can view all enrichment datastores available in the system. Global Settings DataStore (Source &amp;Enrichment) Create Managers can create new source and enrichment datastores and assign them to teams with an \"Editor\" role. List Managers can view all datastores (source and enrichment) listed in the system. Library View View the checks, or assets available in the library. Manage Manage library content, such as adding, modifying, or removing checks. Tags View View tags assigned to datastores, records, or other elements in the system. Manage Manage the tags themselves \u2014 create, update (name, color, description), or delete tags. Assigning tags to assets such as datastores, fields, checks, and anomalies depends on the user\u2019s role and team permissions.Note: For Flows, only Admin and Manager roles can assign tags. Notifications Rules View View existing notification rules and actions configured for alerts. Manage Configure and manage notification rules for different actions or triggers. Settings Connections Create Create new connections for integrating external systems or databases. Update Update existing connections to modify their settings or credentials. Delete Remove existing connections that are no longer needed. Security View Users View the list of users in the system and their access details. View Teams View the teams and their roles/permissions within the system. Integration Add Add new integrations to the system for external systems or data sources. Sync Sync external data with the system to ensure the most up-to-date information. Health View View the health status of the system to monitor performance and stability. API only (ATM) Transaction History View the history of transactions made via the API for auditing and tracking. <ul> <li>Member: Members are normal users with access explicitly granted to them, usually inherited from the teams they are assigned to.</li> </ul> Category Functionality Description Library View Access and browse available checks. Tags View View tags associated with datastores, records, or other system elements. Actions View View existing notification rules and action configurations. Settings Connection Read Access connection details without modification permissions. Tokens Generate Token Create new tokens for secure access or integrations. Revoke Disable existing tokens to restrict access. Restore Reactivate previously revoked tokens. Delete Permanently remove tokens. View Access and review all token details."},{"location":"settings/security/overview/#manage-users","title":"Manage Users","text":"<p>You can easily manage users by assigning roles, teams, and deactivating users who are not active. This ensures that access control is streamlined, security is maintained, and only active users have access to resources.</p> <p>The Security section, visible only to Admins, allows for granting and revoking permissions for Member users.</p> <p>Access controls in Qualytics are assigned at the datastore level. A non-administrator user (Member) can have one of three levels of access to any datastore connected to Qualytics:</p> <ul> <li> <p>Editor: Editor role has the most advanced permissions, enabling users to manage datastore functions comprehensively. Editors can control enrichment, scoring, computed fields, operations, and more. However, they cannot add teams outside their access; only administrators can perform this task.</p> </li> <li> <p>Author: Author role focuses on managing checks and their associated metadata. This role is essential for tasks like activating, validating, and editing checks but has limited access to datastore functionalities.</p> </li> <li> <p>Drafter: Drafter role is designed for users who need to create and prepare checks without performing or finalizing them. This role focuses on adding and organizing content for future use.</p> </li> <li> <p>Viewer: Viewer role provides read-only access to anomalies and allows users to add comments or create notes. This role is ideal for those who need to monitor activities without making changes.</p> </li> <li> <p>Reporter: Reporter role has extensive access to all app report information, including dashboards, overviews, and anomalies. Reporters can view various data contexts and generate analytical insights.</p> </li> </ul> <p>Note</p> <p>Permissions are assigned to Teams rather than directly to users. Users inherit the permissions of the teams to which they are assigned.</p> <p>All users are part of the default Public team, which provides access to all Public Datastores. Admins can create and manage additional teams, assigning both users and datastores to them. When a datastore is assigned to a team, the team is granted either Read or Write access, and all team members inherit this permission.</p>"},{"location":"settings/security/overview/#view-users","title":"View Users","text":"<p>Whenever new users are added to the system, they will appear in the Users list. Click the Users tab to view the list of users.</p> <p></p>"},{"location":"settings/security/overview/#edit-users","title":"Edit Users","text":"<p>You can edit user details to update their role, and team assignments, ensuring their access and team information are current and accurate.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the user name that you want to edit, then click on Edit from the dropdown menu.</p> <p></p> <p>Step 2: Edit the user details as needed, including:</p> <ol> <li>Updating their role</li> <li>Assigning them additional teams</li> </ol> <p>Note</p> <p>All users are inside the Public team by default and that can't be changed. If users have no default access to any datastore, then no datastores should be assigned to the Public team.</p> <p></p> <p>Step 3: Once you have made the necessary changes, then click on the Save button.</p> <p></p> <p>After clicking the Save button, your changes will be updated, and a success message will appear.</p>"},{"location":"settings/security/overview/#deactivate-users","title":"Deactivate Users","text":"<p>You can deactivate users to revoke their access to the system while retaining their account information for future reactivation if needed.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the user name that you want to deactivate, then click on Deactivate from the dropdown menu.</p> <p></p> <p>Step 2: A modal window Deactivate User will appear.</p> <p></p> <p>Step 3: Enter deactivate in the given field (confirmation check) and then click on the I\u2019M SURE, DEACTIVATE THIS USER button to deactivate the user.</p> <p></p>"},{"location":"settings/security/overview/#sort-users","title":"Sort Users","text":"<p>You can sort users by various criteria, such as Created date, Name, Role, and Teams, to easily manage and organize user information.</p> <p></p>"},{"location":"settings/security/overview/#filter-users","title":"Filter Users","text":"<p>You can filter the users by their roles, deactivated and team, to quickly find and manage particular groups of users.</p> <p></p> <p>Info</p> <p>Users can search across all filter inputs using typos, partial terms, or abbreviations. The system intelligently matches relevant results, making it easier to find what they need without exact inputs.</p> <p></p>"},{"location":"settings/security/overview/#manage-teams","title":"Manage Teams","text":"<p>You can manage teams by editing their permissions, adding or removing users, and adjusting access to source and enrichment datastores. If a team is no longer needed, you can delete it from the system. This ensures that team configurations are always up-to-date and relevant, enhancing overall data management and security.</p>"},{"location":"settings/security/overview/#view-team","title":"View Team","text":"<p>Whenever new teams are added to the system, they will appear in the Teams list. Click the Teams tab to view the list of teams.</p> <p></p>"},{"location":"settings/security/overview/#edit-team","title":"Edit Team","text":"<p>You can edit a team to update its permissions, name, manage users within the team, and adjust access to source and enrichment datastores, ensuring the team's configuration is current and effective.</p> <p>Note</p> <p>The name and users of a public team cannot be edited.</p> <p>Step 1:  Click on the vertical ellipsis (\u22ee) next to the team name that you want to edit, then click on Edit from the dropdown menu.</p> <p></p> <p>Step 2:  Edit the team details as needed, including updating their permissions, users, source, and enrichment datastores.</p> <p></p> <p>Step 3: Once you have made the necessary changes, then click on the Save button.</p> <p></p> <p>After clicking on the Save button, your team is successfully updated, and a success message will appear.</p>"},{"location":"settings/security/overview/#delete-team","title":"Delete Team","text":"<p>You can delete a team from the system when it is no longer needed, removing its access and permissions to streamline management and maintain security.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the team name that you want to delete, then click on Edit from the dropdown menu.</p> <p></p> <p>A modal window Delete Team will appear.</p> <p></p> <p>Step 2: Click on the Delete button to delete the team from the system.</p> <p></p>"},{"location":"settings/security/overview/#sort-team","title":"Sort Team","text":"<p>You can sort teams by various criteria, such as name or creation date, to easily organize and manage team information.</p> <p></p> <ul> <li>Team Permissions</li> <li>Directory Sync</li> </ul>"},{"location":"settings/security/team-permissions/","title":"Team Permissions","text":"<p>Admins are not subject to Team permissions and can therefore access all data assets. By contrast, users assigned the Member and Manager roles are subject to Team permissions which control the data assets they can interact with.</p> <p>Team permissions are granted at the Datastore level and extend to all data assets under that Datastore (Tables/View/Files, Fields, Quality Checks, Anomalies, etc...)</p>"},{"location":"settings/security/team-permissions/#permission-matrix","title":"Permission Matrix","text":"<p>Legend:</p> <ul> <li><code>\u2705</code> The given Team permission grants the ability to perform the Action on associated Datastores</li> <li><code>\u274c</code> The given Team permission does not grant the ability to perform the Action on associated Datastores</li> </ul> Action Reporter Viewer Drafter Author Editor Delete Source Datastore \u274c \u274c \u274c \u274c \u274c View Source Datastore \u2705 \u2705 \u2705 \u2705 \u2705 Edit Datastore Settings \u274c \u274c \u274c \u274c \u2705 Preview Source Datastore \u274c \u2705 \u2705 \u2705 \u2705 Create/Delete Computed Asset \u274c \u274c \u274c \u274c \u2705 View Activity \u2705 \u2705 \u2705 \u2705 \u2705 Run &amp; Manage Operations \u274c \u274c \u274c \u274c \u2705 Schedule Operations \u274c \u274c \u274c \u274c \u2705 View Profiles \u2705 \u2705 \u2705 \u2705 \u2705 Delete Profiles \u274c \u274c \u274c \u274c \u2705 View Checks \u2705 \u2705 \u2705 \u2705 \u2705 Create Checks \u274c \u274c \u2705 \u2705 \u2705 Save Check to draft \u274c \u274c \u2705 \u2705 \u2705 Restore Check to draft \u274c \u274c \u2705 \u2705 \u2705 Activate / Validate Check \u274c \u274c \u274c \u2705 \u2705 Edit Check Metadata \u274c \u274c \u274c \u2705 \u2705 View Anomalies \u2705 \u2705 \u2705 \u2705 \u2705 View Anomaly Source Records \u274c \u2705 \u2705 \u2705 \u2705 Change Anomaly Status \u274c \u274c \u274c \u2705 \u2705 Add Comment to Anomaly \u274c \u2705 \u2705 \u2705 \u2705 Delete Enrichment Datastore \u274c \u274c \u274c \u274c \u274c View Enrichment Datastore \u274c \u2705 \u2705 \u2705 \u2705 Preview Enrichment Datastore \u274c \u2705 \u2705 \u2705 \u2705 View Tags \u2705 \u2705 \u2705 \u2705 \u2705 Manage Tags (create / update / delete) \u274c \u274c \u2705 \u2705 \u2705 Assign Tags to Assets (Datastore, Container, Field, Check, Anomaly) \u274c \u2705 \u2705 \u2705 \u2705 Assign Tags in Flows \u274c \u274c \u2705 \u2705 \u2705"},{"location":"settings/security/team-permissions/#add-team","title":"Add Team","text":"<p>You can create a new team for efficient and secure data management. Teams make it easier to control who has access to what, help people work together better, keep things secure with consistent rules, and simplify managing and expanding user groups. You can assign permissions to the team, such as Editor, Author, Drafter, Viewer and Reporter access, by selecting the datastore and enrichment datastore to which you want them to have access. This makes data management easier.</p> <p>Step 1: Click on the Add Team button located in the top right corner.</p> <p></p> <p>Step 2: A modal window will appear, providing the options for creating the team. Enter the required values to get started.</p> REF. FIELD ACTION EXAMPLE 1. Name Enter the name of the team Data Insights Team 2. Description Provide a brief description of the team. Analyzes data to provide actionable insights, supporting data-driven decisions <p></p>"},{"location":"settings/security/team-permissions/#permissions","title":"Permissions","text":"<p>Permissions decide what users can see, create, or manage based on their role. Each role is designed for specific tasks, giving users access to the tools and information they need without going beyond their limits. From Editors who manage advanced settings to Viewers with read-only access, these roles make it easy to use the system while keeping everything secure.</p> <p></p>"},{"location":"settings/security/team-permissions/#editor","title":"Editor","text":"<p>Editor role allows users to manage datastore functions comprehensively. They can handle tasks such as controlling enrichment, scoring, computed fields, and operations.</p> <p></p> Feature Operation Can View/Can Run Can Manage Datastores Add Datastore \u274c \u2705 Edit Settings \u274c \u2705 Enrichment Add Enrichment \u274c \u2705 Edit Enrichment \u274c \u2705 Scoring Edit Scoring \u274c \u2705 Computed Field Add Computed \u274c \u2705 Operation Run Operation \u2705 \u2705 Manage Operation \u274c \u2705 Manage Scheduled Operation \u274c \u2705 Profiles Add Computed \u274c \u2705 Delete Computed \u274c \u2705 Field Context Edit Field Context \u274c \u2705 Delete Field Context \u274c \u2705"},{"location":"settings/security/team-permissions/#author","title":"Author","text":"<p>Author role focuses on managing checks within the system. Users can activate, validate, change the status of checks, and edit their metadata. It is specifically designed for handling these functions efficiently.</p> <p></p> Feature Functionality Can View/ Can Run Can Edit Source Datastore Checks \u274c \u2705 Activate Checks \u274c \u2705 Validate Checks \u274c \u2705 Change Status of Checks \u274c \u2705 Edit Metadata \u274c \u2705 Anomalies \u274c \u2705 Anomalies Change Status of Anomalies \u274c \u2705"},{"location":"settings/security/team-permissions/#drafter","title":"Drafter","text":"<p>Drafter role is designed specifically for adding and saving data within the system. Users can create new, make edits to existing ones, and save their progress as drafts. It is focused on these basic functions without access to advanced features or management tasks.</p> <p></p> Feature Functionality Can View Can Edit Source Datastore Open Datastore \u2705 \u274c Add Checks \u274c \u2705 Profiles Add Check \u274c \u2705 Checks Create as Draft \u274c \u2705 Field Context Add Check \u274c \u2705"},{"location":"settings/security/team-permissions/#viewer","title":"Viewer","text":"<p>Viewer role is focused on viewing anomalies within the system and creating notes as needed. It offers read-only access while allowing users to add comments to document their observations.</p> <p></p> Features Functionality Can View Can Edit Source Datastore Anomalies \u2705 \u274c Add Comment \u2705 \u274c Preview (Container) \u2705 \u274c Enrichment Datastore View \u2705 \u274c Preview \u2705 \u274c Explore Anomalies \u2705 \u274c Source Records \u2705 \u274c"},{"location":"settings/security/team-permissions/#reporter","title":"Reporter","text":"<p>Reporter role provides access to all report-related information, including dashboards, overviews, checks, anomalies, fields, containers, and datastores. It is intended for users who need to view and analyze data to generate reports.</p> <p></p> Feature Operation Can View Can Edit Source Datastore List \u2705 \u274c View \u2705 \u274c Overview \u2705 \u274c Activity \u2705 \u274c Profiles \u2705 \u274c Observability \u2705 \u274c Checks \u2705 \u274c Anomalies \u2705 \u274c Fields (Containers) \u2705 \u274c Enrichment Datastores List \u2705 \u274c Explore Insights \u2705 \u274c Activity \u2705 \u274c Profiles \u2705 \u274c Observability \u2705 \u274c Checks \u2705 \u274c Anomalies \u2705 \u274c <p></p> REF. FIELD ACTION EXAMPLE 4. Users Add users to the team John, Michael 5. Source Datastores Grant access to specific source datastores (single or multiple) for the team Athena 6. Enrichment Datastores Add and grant access to additional enrichment datastores (single or multiple) for the team Bank Enrichment <p>Step 3: Click on the Save button to save your team.</p> <p></p> <p>After clicking on the Save button, your team is created, and a success message will appear saying.</p>"},{"location":"settings/service-account/create-service-account/","title":"Create a Service Account","text":"<p>Service Accounts allow automated or shared systems to securely interact with Qualytics without depending on individual user credentials. They must be created by Administrators and can be managed through both the Qualytics UI and API.</p> <p>Note</p> <p>Only users with the Administrator role can create and manage Service Accounts. For detailed information about permissions, refer to the Teams Permissions documentation.</p> <p>Let\u2019s get started \ud83d\ude80</p> <p>Step 1: Log in to your Qualytics account and click the Settings button on the left side panel of the interface.</p> <p></p> <p>Step 2: By default, you will be navigated to the Connections section. Click on the Security tab.</p> <p></p> <p>Step 3: Click on the Add button located in the top right corner and select the Service User button.</p> <p></p> <p>A modal window will appear with a two-step wizard to create the service account and generate its token.</p> <p></p> <p>Step 4: Enter the following values in the service user form:</p> No. Field Description Required 1 Name Provide a descriptive name for the service account (e.g., \"Pipeline Automation\", \"Alation Sync\", \"API Access\") \u2013 this will be used as the service account ID. Yes 2 Role Choose the role for the service account (Admin, Manager, Editor, or Member) based on the permissions needed. Start with the least privileged role. Yes 3 Teams Select the teams the service account should belong to. The Public team is automatically included. Optional <p></p> <p>Step 5: After entering the service user information, click on the Next button.</p> <p></p> <p>Step 6: In the second step, complete the token generation form:</p> No. Field Description Required 1 Name Descriptive name for the token (e.g., \"pipeline-production\", \"api-access-prod\"). Token names must be unique per service account. Yes 2 Expiration Select the expiration period from dropdown: 30 days, 60 days, 90 days, 1 year, or Never. For production tokens, 1 year with a rotation plan is recommended. Yes 3 Type Automatically set to \"Service\" (pre-filled and non-editable). N/A 4 Service User Automatically populated with the service user created in Step 4 (pre-filled and non-editable). N/A 5 SCIM Administration Token Check this box if the token should be restricted to SCIM endpoints only. Optional <p></p> <p>Step 7: After entering the token details, click on the Finish button.</p> <p></p> <p>Your service account and token have been successfully created.</p> <p></p> <p>Warning</p> <p>Make sure to copy or download the token right away. Once you navigate away from this page, you won\u2019t be able to see it again.</p> <p>You will see the newly created service account in the users list with a \"Service\" badge.</p> <p></p>"},{"location":"settings/service-account/create-service-account/#api-reference","title":"API Reference","text":"<p>For administrators who prefer API-based management or need to automate service account creation, Qualytics provides comprehensive API endpoints.</p>"},{"location":"settings/service-account/create-service-account/#creating-a-service-account-via-api","title":"Creating a Service Account via API","text":"<p>Endpoint:</p> <pre><code>POST /users\nAuthorization: Bearer {admin_token}\nContent-Type: application/json\n</code></pre> <p>Request Body:</p> <pre><code>{\n  \"name\": \"API Access\",\n  \"role\": \"Editor\",\n  \"teams\": [\"Data Engineering\", \"Data Quality\"]\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"id\": 123,\n  \"user_id\": \"api_access@service\",\n  \"user_name\": \"api_access@service\",\n  \"email\": \"service@example.com\",\n  \"name\": \"API Access\",\n  \"user_type\": \"Service\",\n  \"role\": \"Editor\",\n  \"teams\": [\"Public\", \"Data Engineering\", \"Data Quality\"]\n}\n</code></pre>"},{"location":"settings/service-account/service-account/","title":"Service Accounts","text":"<p>Service Accounts provide a secure and centralized way to authenticate automated systems and integrations with the Qualytics API. Unlike Personal Access Tokens (PATs) that are tied to individual users, Service Accounts are synthetic users designed specifically for data pipeline automation, Qualytics API/CLI access, data catalog integrations, and shared automation workflows. They eliminate the security risk of sharing personal tokens, remain active independent of individual user lifecycles, role changes, or access status, and provide clear audit trails for automated system activities.</p>"},{"location":"settings/service-account/service-account/#what-are-service-accounts","title":"What Are Service Accounts?","text":"<p>Service accounts are synthetic user accounts created specifically for automation and integrations. They have the following characteristics:</p>"},{"location":"settings/service-account/service-account/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>No interactive login: Service accounts cannot log into the Qualytics web interface  </li> <li>Administrator-managed: Only administrators can create and manage service accounts  </li> <li>Independent lifecycle: Service accounts are not tied to individual users  </li> <li>Role-based permissions: Can be assigned Admin, Manager, Editor, or Member roles  </li> <li>Team membership: Can be assigned to specific teams for scoped access  </li> </ul> <p>Info</p> <p>Every automated or system-driven integration should use a Service Account instead of a Personal Access Token.</p>"},{"location":"settings/service-account/service-account/#typical-use-cases","title":"Typical Use Cases","text":"<ul> <li>Automated ETL or pipeline execution  </li> <li>Metadata sync (Alation, Atlan, etc.)  </li> <li>Scheduled or recurring quality scanning  </li> <li>API-based workflows  </li> <li>Multi-user team automation  </li> </ul>"},{"location":"settings/service-account/service-account/#service-accounts-vs-personal-tokens","title":"Service Accounts vs Personal Tokens","text":"<p>Understanding the difference between Service Accounts and Personal Access Tokens is essential for secure and reliable automation.</p>"},{"location":"settings/service-account/service-account/#comparison-table","title":"Comparison Table","text":"Feature Service Account Personal Access Token (PAT) Created by Administrators only Individual users (self-service) Tied to Synthetic service user Human user account Best for Production integrations, data pipelines, and shared automation Personal development, testing, and ad-hoc API exploration Lifecycle Independent of individual user lifecycles Dependent on the user\u2019s access status Management Centrally managed by administrators Managed by the individual user Audit Trail Provides clear, purpose-based identification Linked to a specific user\u2019s activity <p>Warning</p> <p>Never use PATs in production pipelines \u2014 they will break when the user is offboarded, disabled, or role-changed.</p>"},{"location":"settings/service-account/service-account/#when-to-use-service-accounts","title":"When to Use Service Accounts","text":"<p>Service Accounts are intended for secure, automated, and shared access to Qualytics. They provide consistent authentication for system-level workflows, ensuring stability and centralized management without dependency on individual user credentials.</p>"},{"location":"settings/service-account/service-account/#use-service-accounts-for","title":"\u2705 Use Service Accounts For","text":"<ul> <li> <p>Data Pipeline Automation: Use Service Accounts to integrate Qualytics operations   within your automated data workflows, such as Airflow, dbt, Azure Data Factory, or other   ETL pipelines.</p> </li> <li> <p>Qualytics API Access: For programmatic interactions with Qualytics, such as managing   resources or retrieving data through API calls, Service Accounts offer reliable, long-term   access.</p> </li> <li> <p>Qualytics CLI Operations: Automate data quality workflows and recurring tasks using   the Qualytics CLI without relying on personal user tokens.</p> </li> <li> <p>Data Catalog Integrations: Enable metadata synchronization and data lineage tracking   with catalog platforms like Alation, Atlan, or other similar tools.</p> </li> <li> <p>Shared Automation: Use Service Accounts for any automation or integration shared   across multiple team members or systems to maintain consistency, security, and proper   audit control.</p> </li> </ul>"},{"location":"settings/service-account/service-account/#avoid-using-personal-access-tokens-pats-for","title":"\u274c Avoid Using Personal Access Tokens (PATs) For","text":"<p>Personal Access Tokens (PATs) are meant for individual, short-term, or development use only. Avoid using them in the following scenarios:</p> <ul> <li> <p>Shared Data Pipelines: Tokens should not be shared among users due to security risks. </p> </li> <li> <p>Production Integrations: Tokens may become invalid if the associated user is   deactivated or their role changes.  </p> </li> <li> <p>Team-wide API or CLI Automations: Difficult to manage and audit across multiple   users.  </p> </li> <li> <p>Long-running Integrations: PATs are tied to individual user lifecycles and are not   suitable for continuous system-level access.</p> </li> </ul> <p>Tip</p> <p>If an automation is shared or runs in production, always use a Service Account instead of a Personal Access Token (PAT) for improved stability, security, and governance.</p>"},{"location":"settings/service-account/service-account/#key-takeaways","title":"Key Takeaways","text":""},{"location":"settings/service-account/service-account/#use-service-accounts-for-production-integrations","title":"Use Service Accounts for Production Integrations","text":"<ul> <li>Operate independently of individual users, their access changes, or lifecycle events</li> <li>Centrally managed by administrators for consistency and control</li> <li>Provide clear, purpose-based audit trails</li> <li>Strengthen overall security posture</li> </ul>"},{"location":"settings/service-account/service-account/#administrator-responsibilities","title":"Administrator Responsibilities","text":"<ul> <li>Create, manage, and maintain service accounts</li> <li>Perform regular audits of service tokens</li> <li>Enforce token rotation and expiration policies</li> <li>Maintain documentation for all integrations and use cases</li> </ul>"},{"location":"settings/service-account/service-account/#security-first","title":"Security First","text":"<ul> <li>Store tokens securely using secrets managers</li> <li>Assign appropriate expiration periods based on the environment</li> <li>Monitor activity through Last Used timestamps</li> <li>Revoke immediately if a token is lost or compromised</li> </ul>"},{"location":"settings/service-account/service-account/#least-privilege-principle","title":"Least Privilege Principle","text":"<ul> <li>Grant only the minimum permissions required</li> <li>Restrict team membership to just the needed scopes</li> <li>Conduct recurring access reviews</li> <li>Document justification for any elevated roles</li> </ul>"},{"location":"settings/service-account/service-account/#quick-reference","title":"Quick Reference","text":"Task UI Location API Endpoint Required Role Create a service account Settings \u2192 Users POST /users Admin Create service token Users \u2192 Generate Token POST /user-tokens Admin List service tokens Settings \u2192 Tokens (Service filter) GET /user-tokens/service Admin Revoke token Token Menu \u2192 Revoke PUT /user-tokens/{id} Admin Delete token Token Menu \u2192 Delete DELETE /user-tokens/{id} Admin"},{"location":"settings/service-account/service-account/#create-a-service-account","title":"Create a Service Account","text":"<p>Follow the step-by-step UI workflow or use the API to create Service Accounts for automation workflows.</p> <p>Info</p> <p>If you need a deeper walkthrough, you can refer to the Create a Service Account documentation.</p> <p>Info</p> <p>For API-based setup, see the Creating a Service Account via API guide for step-by-step instructions.</p>"},{"location":"settings/service-account/service-account/#naming-conventions","title":"Naming Conventions","text":"<p>Learn recommended naming patterns that keep your Service Accounts clean, descriptive, and audit-friendly.</p> <p>Info</p> <p>For a deeper explanation, refer to the Naming Convention guide.</p>"},{"location":"settings/service-account/service-account/#roles-access","title":"Roles &amp; Access","text":"<p>Understand how roles and team membership control what a Service Account can access across the Qualytics platform. </p> <p>Info</p> <p>To learn more about how roles work, refer to the Role Assignment section.</p>"},{"location":"settings/service-account/service-account/#security-guidelines","title":"Security Guidelines","text":"<p>Review best practices for token storage, expiration, monitoring, and incident response for secure automation.</p> <p>Info</p> <p>You can find more details in the Security Guidelines documentation.</p>"},{"location":"settings/service-token/best-practices/","title":"Best Practices","text":""},{"location":"settings/service-token/best-practices/#service-account-naming-conventions","title":"Service Account Naming Conventions","text":"<p>Consistent naming conventions are essential for clear audit logs and smooth integrations. Follow these guidelines to keep service account names organized and manageable. Each token name must be unique within its service account, but it does not need to be globally unique.</p>"},{"location":"settings/service-token/best-practices/#auto-generated-service-account-id-format","title":"Auto-Generated Service Account ID Format","text":"<p>When you create a service account, the system automatically generates a service account ID based on the name you provide.</p> <p>Example:</p> <ul> <li>Original Name: \"Pipeline Automation\"</li> <li>Auto-Generated ID: <code>pipeline_automation@service</code></li> </ul> <p>The system applies the following sanitization rules to generate the ID:</p> <ul> <li>Lowercase letters are used.</li> <li>Spaces are converted to underscores.</li> <li>Special characters are removed.</li> </ul>"},{"location":"settings/service-token/best-practices/#naming-rules","title":"Naming Rules","text":"<p>Follow these rules to ensure consistency and clarity:</p> <ul> <li>Use lowercase letters.</li> <li>Replace spaces with underscores.</li> <li>Remove any special characters.</li> </ul>"},{"location":"settings/service-token/best-practices/#best-practices-for-naming","title":"Best Practices for Naming","text":"<p>Recommended Naming Examples:</p> <ul> <li><code>pipeline_automation@service</code> \u2014 Clear and specific purpose.</li> <li><code>alation_sync@service</code> \u2014 Reflects the integration name.</li> <li><code>finance_metrics@service</code> \u2014 Indicates the purpose of the service account.</li> </ul> <p>Names to Avoid:</p> <ul> <li><code>service1@service</code> \u2014 Not descriptive enough.</li> <li><code>temp@service</code> \u2014 Unclear and vague.</li> <li><code>test123@service</code> \u2014 Too generic and unhelpful.</li> </ul> <p>Note</p> <p>Service account names appear in audit logs, so it\u2019s essential to choose meaningful names that clearly indicate the purpose of the service account.</p>"},{"location":"settings/service-token/best-practices/#token-rotation","title":"Token Rotation","text":"<p>For security best practices, tokens should be rotated regularly, especially before they expire.</p>"},{"location":"settings/service-token/best-practices/#token-rotation-process","title":"Token Rotation Process","text":"<ol> <li> <p>Create a New Token (at least 30 days before expiration)    Generate a new token for the same service account and give it a clear name (e.g.,<code>production-integration-2025</code>).</p> </li> <li> <p>Share the New Token    Provide the newly generated token to your integration or engineering team.</p> </li> <li> <p>Update Secrets    Update your environment variables or secrets manager with the new token.</p> </li> <li> <p>Verify Integration    Confirm that your integration or automation works correctly using the new token.</p> </li> <li> <p>Monitor Token Usage    Check the Last Used timestamp to ensure the new token is active and the old token is    no longer in use.</p> </li> <li> <p>Revoke the Old Token    Once you've confirmed the new token works, revoke the old token to prevent any further    access.</p> </li> <li> <p>Observe a Grace Period    Monitor for at least 7 days to ensure no unexpected usage occurs on the old token.</p> </li> <li> <p>Delete the Old Token    After the grace period, permanently delete the revoked token.</p> </li> </ol>"},{"location":"settings/service-token/best-practices/#role-assignment","title":"Role Assignment","text":"<p>Assigning the correct role to a service account is essential for enforcing the principle of least privilege and ensuring secure integration access.</p>"},{"location":"settings/service-token/best-practices/#role-guidelines","title":"Role Guidelines","text":"Use Case Recommended Role Rationale Data catalog sync (Alation, Atlan) Manager Requires access to read datastores, containers, and quality checks Data pipeline automation Editor Needs permission to trigger operations and create containers Qualytics CLI automation Editor Must be able to run profiling and scanning operations Read-only API access Member Only needs to view metrics, anomalies, and quality results BI tool integration Member Only requires read access to data quality outputs Full platform automation Admin Requires full control over users, teams, integrations, and platform resources"},{"location":"settings/service-token/best-practices/#role-permissions","title":"Role Permissions","text":"Permission Member Editor Manager Admin Read data quality results \u2705 \u2705 \u2705 \u2705 View datastores and containers \u2705 \u2705 \u2705 \u2705 Create and modify resources \u274c \u2705 \u2705 \u2705 View all users and teams \u274c \u274c \u2705 \u2705 Manage users and service accounts \u274c \u274c \u274c \u2705 Configure integrations \u274c \u274c \u274c \u2705 <p>Recommendation: Start with the Member role whenever possible, and escalate only if the integration requires additional capabilities.</p>"},{"location":"settings/service-token/best-practices/#security-guidelines","title":"Security Guidelines","text":"<p>Proper token management is essential to maintaining a secure environment. The guidelines below outline how to safely store rotate, monitor, and review service tokens across all environments.</p>"},{"location":"settings/service-token/best-practices/#token-storage","title":"Token Storage","text":"<p>\u2705 Do:</p> <ul> <li>Use secure secrets managers such as HashiCorp Vault, AWS Secrets Manager, or Azure Key Vault</li> <li>Load tokens through environment variables in deployment configurations</li> <li>Encrypt tokens at rest</li> <li>Rotate tokens on a defined schedule</li> </ul> <p>\u274c Don\u2019t:</p> <ul> <li>Commit tokens to Git or any version control system</li> <li>Store tokens in plain or unencrypted text files</li> <li>Share tokens through email, chat, or other unsecure channels</li> <li>Reuse the same token across multiple environments</li> </ul>"},{"location":"settings/service-token/best-practices/#token-expiration-strategy","title":"Token Expiration Strategy","text":"<p>Set token expiration periods based on environment type:</p> Environment Recommended Expiration Notes Production 365 days Begin rotation at least 30 days before expiration Staging 180 days Shorter duration due to non-production usage Development 90 days Frequent rotation aligns with active development cycles Testing 7\u201330 days Ideal for temporary testing or short-lived workflows"},{"location":"settings/service-token/best-practices/#monitoring-and-auditing","title":"Monitoring and Auditing","text":"<p>Regularly monitor token activity to maintain operational security.</p> <ol> <li>Monthly Reviews</li> <li>Check \u201cLast Used\u201d timestamps for all tokens  </li> <li>Revoke tokens unused for 90+ days  </li> <li> <p>Review each service account\u2019s role to ensure it remains appropriate  </p> </li> <li> <p>Audit Trail</p> </li> <li>All API requests log the authenticated service account  </li> <li><code>created_by_id</code> identifies the admin who created the account  </li> <li> <p>Token usage is captured through the <code>last_used</code> timestamp  </p> </li> <li> <p>Alert on Anomalies</p> </li> <li>Sudden or unexpected token usage  </li> <li>Requests from unfamiliar or suspicious IPs  </li> <li>Repeated failed authentication attempts  </li> </ol>"},{"location":"settings/service-token/best-practices/#access-reviews","title":"Access Reviews","text":"<p>Conduct quarterly reviews of all service accounts and tokens.</p> <p>Review Checklist:</p> <ul> <li>Each service account has a clearly documented purpose  </li> <li>Token names accurately reflect their usage  </li> <li>No tokens are close to expiration without a rotation plan  </li> <li><code>last_used</code> timestamps show expected activity  </li> <li>Roles follow the principle of least privilege  </li> <li>Team memberships match required access  </li> <li>No orphaned tokens exist (tokens tied to deleted service accounts)  </li> </ul>"},{"location":"settings/service-token/best-practices/#security-considerations","title":"Security Considerations","text":"<p>Keeping service accounts and tokens secure is critical to protecting your data, infrastructure, and API access. The following guidelines outline recommended practices for handling tokens, responding to incidents, and maintaining compliance.</p>"},{"location":"settings/service-token/best-practices/#token-security","title":"Token Security","text":"<p>Bearer Tokens Are Sensitive</p> <ul> <li>Treat every token as securely as a password  </li> <li>Anyone with the token can fully authenticate as that service account  </li> <li>Tokens are hashed using HMAC-SHA256 before being stored  </li> <li>Lost tokens cannot be retrieved\u2014new tokens must be created  </li> </ul>"},{"location":"settings/service-token/best-practices/#immediate-revocation","title":"Immediate Revocation","text":"<ul> <li>Revoke tokens immediately if you suspect compromise  </li> <li>Token deletion is a two-step process (revoke \u2192 delete)  </li> <li>Revoked tokens stop working instantly\u2014no grace period  </li> </ul>"},{"location":"settings/service-token/best-practices/#incident-response","title":"Incident Response","text":""},{"location":"settings/service-token/best-practices/#if-a-token-is-compromised","title":"If a Token Is Compromised","text":"<ol> <li>Immediate Actions</li> <li>Revoke the affected token  </li> <li>Notify your security team  </li> <li> <p>Review audit logs  </p> </li> <li> <p>Investigation</p> </li> <li>Analyze all API requests made using the token  </li> <li>Determine scope of unauthorized access  </li> <li> <p>Check if data was altered or exfiltrated  </p> </li> <li> <p>Remediation</p> </li> <li>Generate a new token  </li> <li>Update integrations to use the new token  </li> <li>Delete the compromised token  </li> <li> <p>Document the incident  </p> </li> <li> <p>Prevention</p> </li> <li>Strengthen storage and handling practices  </li> <li>Improve monitoring and alerts  </li> <li>Provide security training if needed  </li> </ol>"},{"location":"settings/service-token/best-practices/#if-a-service-account-is-compromised","title":"If a Service Account Is Compromised","text":"<ol> <li>Immediately revoke all tokens  </li> <li>Create a new service account  </li> <li>Audit all actions performed by the compromised account  </li> <li>Rotate any downstream credentials, keys, or integrations  </li> </ol>"},{"location":"settings/service-token/best-practices/#compliance-considerations","title":"Compliance Considerations","text":""},{"location":"settings/service-token/best-practices/#separation-of-duties","title":"Separation of Duties","text":"<ul> <li>Use dedicated service accounts for production integrations  </li> <li>Personal Access Tokens should only be used in development  </li> <li>Require admin approval before creating service accounts  </li> <li>Document ownership and purpose  </li> </ul>"},{"location":"settings/service-token/best-practices/#least-privilege","title":"Least Privilege","text":"<ul> <li>Start with Member role  </li> <li>Grant only needed team access  </li> <li>Conduct regular permission reviews  </li> <li>Document justification for elevated access  </li> </ul>"},{"location":"settings/service-token/delete-service-token/","title":"Delete Service Token","text":"<p>You can delete a service token to permanently remove its access, ensuring it cannot be used again. This is important for maintaining security when a token is no longer needed, has been compromised, or to clean up unused tokens in your system.</p> <p>Note</p> <p>You can only delete revoked tokens, not active tokens. If you want to delete an active token, you must first revoke it before you can delete it.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the revoked tokens, that you want to delete, then click on the Delete button from the dropdown menu.</p> <p></p> <p>After clicking the delete button, a confirmation modal window Delete Token will appear.</p> <p></p> <p>Step 2: Click on the Delete button to delete the token.</p> <p></p> <p>After clicking on the Delete button, your token will be deleted and a confirmation message will display saying User token successfully deleted.</p> <p></p>"},{"location":"settings/service-token/delete-service-token/#deleting-a-revoked-token-via-api","title":"Deleting a Revoked Token via API","text":"<p>Endpoint:</p> <pre><code>DELETE /user-tokens/{token_id}\nAuthorization: Bearer {admin_token}\n</code></pre> <p>Note</p> <p>Can only delete tokens that are already revoked.</p>"},{"location":"settings/service-token/generate-service-token/","title":"Generate Service Token","text":"<p>Service Tokens are generated when you create a Service Account or manually through the Tokens management page. These tokens are essential for automating API interactions and securely authenticating requests made by your automated systems.</p> <p>Let\u2019s get started \ud83d\ude80</p> <p>Step 1: Log in to your Qualytics account and click the Settings button on the left side panel of the interface.</p> <p></p> <p>Step 2: By default, you will be navigated to the Connections section. Click on the Tokens tab.</p> <p></p> <p>Step 3: Click on the Generate Token button located in the top right corner.</p> <p></p> <p>A modal window will appear, providing the options for generating the token.</p> <p></p> <p>Step 4: Enter the following values:</p> No. Field Description Required 1 Name Enter a descriptive name for the token (e.g., \"svc-token-prod\", \"api-access-prod\"). Yes 2 Type Select Service for a service account token. Yes 3 Service User Select the service user for which you want to create the token. Yes 4 Expiration Select the expiration period (e.g., 30 Days, 1 Year, Never). Yes 5 SCIM Administration Token Check this box if the token should be restricted to SCIM endpoints only. Optional <p></p> <p>Step 5: Once you have entered the values, click on the Generate button.</p> <p></p> <p>Step 6: After clicking on the Generate button, your token is successfully generated.</p> <p></p> <p>Warning</p> <p>Make sure to download or copy this token. You won't be able to see it again. Keep your token confidential and avoid sharing it with anyone. Use a password manager or an encrypted vault to store your tokens.</p> <p>Step 7: After copying the token, you can close the modal. The token will now appear in the service tokens list.</p> <p></p>"},{"location":"settings/service-token/generate-service-token/#usage-of-service-tokens","title":"Usage of Service Tokens","text":"<p>Once generated, include the service token in the Authorization header of all API requests:</p> <pre><code>curl -H \"Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\" \\\n  https://acme.qualytics.io/api/datastores\n</code></pre> <p>Example Python Usage:</p> <pre><code>import requests\n\nQUALYTICS_TOKEN = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\"\nQUALYTICS_API = \"https://acme.qualytics.io/api\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {QUALYTICS_TOKEN}\",\n    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.get(f\"{QUALYTICS_API}/datastores\", headers=headers)\nprint(response.json())\n</code></pre>"},{"location":"settings/service-token/generate-service-token/#creating-a-service-token-via-api","title":"Creating a Service Token via API","text":"<p>Endpoint:</p> <pre><code>POST /user-tokens\nAuthorization: Bearer {admin_token}\nContent-Type: application/json\n</code></pre> <p>Request Body:</p> <pre><code>{\n  \"name\": \"api-production\",\n  \"user_id\": 123,\n  \"expires_in_days\": 365\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"id\": 456,\n  \"name\": \"api-production\",\n  \"active\": true,\n  \"expiration\": \"2026-11-05T12:34:56Z\",\n  \"bearer_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\",\n  \"user\": {\n    \"id\": 123,\n    \"user_id\": \"api_access@service\",\n    \"name\": \"API Access\",\n    \"user_type\": \"Service\"\n  }\n}\n</code></pre> <p>Danger</p> <p>Save the <code>bearer_token</code> immediately! It cannot be retrieved later.</p>"},{"location":"settings/service-token/overview/","title":"Service Tokens","text":"<p>Service Tokens authenticate automated systems that use Service Accounts to access the Qualytics API. They provide secure, long-lived credentials for API operations, ensuring that automation can continue running without human intervention.</p>"},{"location":"settings/service-token/overview/#what-are-service-tokens","title":"What Are Service Tokens?","text":"<p>Service Tokens are secure credentials used by Service Accounts to authenticate API requests. They support expiration settings and can be revoked or restored as necessary.  </p> <p>Note</p> <p>For more details, check out the Service Tokens documentation.</p>"},{"location":"settings/service-token/overview/#generate-service-tokens","title":"Generate Service Tokens","text":"<p>Learn how to generate tokens for Service Accounts. This process enables automated systems to securely authenticate with the Qualytics API.  </p> <p>Note</p> <p>If you want to learn more, check out the Generate Service Token documentation.</p>"},{"location":"settings/service-token/restore-service-token/","title":"Restore Service Token","text":"<p>You can restore a token to reactivate its access, allowing authorized use again. This is useful if the token was mistakenly revoked or if access needs to be temporarily re-enabled without generating a new token.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the revoked tokens, that you want to restore, then click on the Restore button from the dropdown menu.</p> <p></p> <p>Step 2: After clicking on the Restore button, your secret token will be restored and a confirmation message will display saying \"The token has been successfully restored\".</p> <p></p>"},{"location":"settings/service-token/revoking-service-token/","title":"Revoking a Service Token","text":"<p>Revoking a service token immediately disables it without permanently deleting it. This is useful if you suspect the token has been compromised or need to temporarily stop its access.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the service token that you want to revoke, then click on Revoke from the dropdown menu.</p> <p></p> <p>Step 2: After clicking the Revoke button, your service token will be successfully revoked.</p> <p></p> <p>Warning</p> <p>Revoked tokens cannot be used for API authentication. Any systems using this token will immediately lose access.</p>"},{"location":"settings/service-token/revoking-service-token/#revoking-a-token-via-api","title":"Revoking a Token via API","text":"<p>Endpoint:</p> <pre><code>PUT /user-tokens/{token_id}\nAuthorization: Bearer {admin_token}\nContent-Type: application/json\n</code></pre> <p>Request Body:</p> <pre><code>{\n  \"revoke\": true\n}\n</code></pre>"},{"location":"settings/service-token/service-token/","title":"What Are Service Tokens","text":"<p>Service Tokens are secure credentials used by Service Accounts to authenticate automated workflows within the Qualytics API. They act as long-lived access keys, enabling seamless interaction between automation systems and the API without requiring user intervention.</p>"},{"location":"settings/service-token/service-token/#key-features-of-service-tokens","title":"Key Features of Service Tokens","text":"<ul> <li> <p>Secure Authentication: Service Tokens authenticate API requests made by automated systems, ensuring that only authorized applications can access resources.</p> </li> <li> <p>Tied to Service Accounts: Unlike Personal Access Tokens (PATs), Service Tokens are linked to Service Accounts, which are synthetic users designed for automation.</p> </li> <li> <p>Expiration: Service Tokens support expiration settings, limiting the lifespan of tokens for added security.</p> </li> <li> <p>Revocation &amp; Restoration: Tokens can be revoked when no longer needed or if compromised, and can be restored if needed.</p> </li> <li> <p>Multiple Tokens: Each Service Account can have multiple tokens for different environments or purposes, providing flexibility for automation workflows.</p> </li> </ul>"},{"location":"settings/service-token/service-token/#benefits-of-service-tokens","title":"Benefits of Service Tokens","text":""},{"location":"settings/service-token/service-token/#secure-long-term-authentication","title":"Secure Long-Term Authentication","text":"<p>Service Tokens eliminate the need for manual user-based authentication. Once a token is generated, it can be used for long-term, automated access to the API, without requiring human involvement for renewal.</p>"},{"location":"settings/service-token/service-token/#role-based-access-control","title":"Role-Based Access Control","text":"<p>Service Tokens are tied to Service Accounts, which can be assigned specific roles and permissions. This ensures that automated systems only have access to the necessary resources based on the defined permissions.</p>"},{"location":"settings/service-token/service-token/#scalability","title":"Scalability","text":"<p>As your automation grows, you can create additional tokens for different purposes or environments, allowing your automated workflows to scale securely.</p>"},{"location":"settings/service-token/service-token/#how-service-tokens-work","title":"How Service Tokens Work","text":"<ol> <li> <p>Generate a Service Token: When a Service Account is created, an administrator generates a Service Token linked to that account. The token can be set to expire after a certain period, such as 30 days, 1 year, etc.</p> </li> <li> <p>Include Token in API Requests: The generated token is included in the Authorization header when making API requests    to authenticate the Service Account.</p> </li> <li> <p>Token Expiry &amp; Rotation: Tokens will expire based on the settings, and periodic rotation should be practiced to maintain security.</p> </li> <li> <p>Revoking &amp; Restoring Tokens: Tokens that are no longer needed or are suspected to be compromised can be revoked, ensuring that access is stopped immediately. Tokens can be restored if needed.</p> </li> </ol> <p>Note</p> <p>Service Tokens are essential for secure and scalable automation workflows. Ensure you follow best practices for token generation, management, and rotation to minimize security risks.</p>"},{"location":"settings/service-token/team-membership-strategy/","title":"Team Membership Strategy","text":"<p>Service accounts can be added to teams to control and limit their access to specific datastores and containers across the platform.</p>"},{"location":"settings/service-token/team-membership-strategy/#how-team-membership-works","title":"How Team Membership Works","text":"<ul> <li>Public team: Every service account is automatically part of the Public team.</li> <li>Additional teams: You can assign a service account to one or more teams based on the   access it needs.</li> <li>Access control: A service account can access only the datastores and containers that   belong to the teams it is a member of.</li> </ul>"},{"location":"settings/service-token/team-membership-strategy/#team-assignment-examples","title":"Team Assignment Examples","text":"<p>Example 1: Data Catalog Integration</p> <pre><code>{\n  \"name\": \"Alation Data Catalog Sync\",\n  \"role\": \"Manager\",\n  \"teams\": [\n    \"Data Engineering\",\n    \"Analytics\",\n    \"Finance Data\"\n  ]\n}\n</code></pre> <p>This service account will have access to:</p> <ul> <li>Public team resources (automatic)</li> <li>Data Engineering team's datastores</li> <li>Analytics team's datastores</li> <li>Finance Data team's datastores</li> </ul> <p>Example 2: Data Pipeline Automation</p> <pre><code>{\n  \"name\": \"Pipeline Automation\",\n  \"role\": \"Editor\",\n  \"teams\": [\n    \"Data Engineering\"\n  ]\n}\n</code></pre> <p>This service account can:</p> <ul> <li>Trigger profiling and scanning operations on Data Engineering team's datastores</li> <li>Create and modify containers in Data Engineering team's scope</li> </ul>"},{"location":"settings/service-token/token-usage-status/","title":"Token Usage Status","text":"<p>Each service token includes a usage status to help you understand how and when it has been used for API authentication.</p>"},{"location":"settings/service-token/token-usage-status/#last-used","title":"Last Used","text":"<p>Shows the timestamp when the token was last used for API authentication.</p> <p></p>"},{"location":"settings/service-token/token-usage-status/#not-used","title":"Not Used","text":"<p>The token has been generated but has not been used for any API requests since creation.</p> <p></p>"},{"location":"settings/status/overview-of-status/","title":"Status","text":"<p>System status provides a real-time overview of your system's resources, essential for monitoring performance and diagnosing potential issues. It provides key indicators and status updates to help you maintain system health and quickly address potential issues.</p>"},{"location":"settings/status/overview-of-status/#navigation-to-status","title":"Navigation to Status","text":"<p>Step 1: Log in to your Qualytics account and click the Settings button on the left side panel of the interface. </p> <p></p> <p>Step 2: You will be directed to the Settings page; then click on the Status section.</p> <p></p>"},{"location":"settings/status/overview-of-status/#platform-status-section","title":"Platform Status Section","text":"<p>The Platform Status Section displays the current platform version, along with the database status and RabbitMQ state.</p> REF. FIELD ACTION EXAMPLE 1 Version Shows the current version of your platform's core software. 20260231-196460 2 Cloud Platform Indicates which cloud provider the platform is hosted on. Amazon Web Services 3 Deployment Size Indicates the size of the deployment that the client has contracted. Medium 4 License Expiration Date Displays the current license expiration date and status. May 31 2026, 9:00 PM (BRT) 5 Database Verifies your database connection. An \"OK\" status means it\u2019s connected. Status:OK 6 RabbitMQ Confirms RabbitMQ (a message broker software) is running correctly with an \"OK\" state. State:OK <p></p>"},{"location":"settings/status/overview-of-status/#status-indicator","title":"Status Indicator","text":"<p>The status indicator reflects the overall system resources health. For example, in the image below, a green checkmark indicates that our system resources are healthy.</p> <p>Note</p> <p>Status indicators are simple: a green checkmark indicates \"Healthy,\" and a red exclamation mark means \"Critical.\"</p> <p></p>"},{"location":"settings/status/overview-of-status/#dataplane","title":"Dataplane","text":"<p>The Dataplane section provides advanced information about the analytics engine's configuration and current state for technical users and developers.</p> REF FIELD ACTION EXAMPLE 1 Build Date This shows the date and time when the Analytics Engine was built. Feb 13 2026, 7:20 AM (IST) 2 Engine Shows the platform used to run and manage workloads. Kubernetes 3 Max Executors Maximum number of executors allocated for processing tasks. 12 4 Max Memory Per Executor This shows the maximum amount of memory allocated to each executor. 55000 MB 5 Implementation Version The version of the analytics engine implementation being used. 2.109.3-60de4a5 6 Spark Version The version of Apache Spark that the Analytics Engine uses for processing. 4.0.2 7 Core Per Executor This shows the number of CPU cores assigned to each executor. 7 8 Max Dataframe Size The maximum size of dataframes that can be processed. 396000 MB 9 Thread Pool State Indicates the current state of the thread pool used for executing tasks. 0 running operations [] with 0 queued requests and 10 provisioned channels <p></p>"},{"location":"settings/status/overview-of-status/#private-routes","title":"Private Routes","text":"<p>Users can now utilize private routes to view their IP addresses along with relevant system messages in the Analytics Engine, ensuring greater transparency and visibility into network activity.</p> <p></p>"},{"location":"settings/status/overview-of-status/#license-management","title":"License Management","text":"<p>The License section allows administrators to monitor, renew, and update platform licensing directly from the Status page.</p> <p>Warning</p> <p>The License section is available only for non-SaaS (self-hosted) customers. SaaS environments will not display a License.</p>"},{"location":"settings/status/overview-of-status/#viewing-license-status","title":"Viewing License Status","text":"<p>The License panel displays:</p> <ul> <li>Expiration date  </li> </ul> <p></p> <ul> <li>Remaining days  </li> </ul> <p></p> <ul> <li>Restriction alerts if expired  </li> </ul> <p></p>"},{"location":"settings/status/overview-of-status/#generate-license-request","title":"Generate License Request","text":"<p>If your license is nearing expiration or has already expired, you can request a renewal by clicking the Generate License Request button. This will generate a license request code that you can share with your Qualytics account representative to receive a renewed license key.</p> <p></p>"},{"location":"settings/status/overview-of-status/#apply-a-new-license","title":"Apply a New License","text":"<p>Once you receive a valid license key from your Qualytics representative, follow these steps:</p> <p>Step 1: Click on the Update License button to apply a new or renewed license to your system.</p> <p></p> <p>A modal window will appear. Paste the provided license key into the input field and click Submit to apply the new license.</p> <p></p>"},{"location":"settings/status/overview-of-status/#manage-status-summary","title":"Manage Status Summary","text":"<p>You can perform essential tasks such as copying the status summary, refreshing it, and restarting the analytics engine. These functionalities help maintain an up-to-date overview of system performance and ensure accurate analytics.</p>"},{"location":"settings/status/overview-of-status/#copy-status-summary","title":"Copy Status Summary","text":"<p>The Copy Status Summary feature lets you duplicate all data from the Health Section for easy sharing or saving.</p> <p>Step 1: Click the vertical ellipsis from the right side of the summary section and choose Copy Status Summary from the drop-down menu.</p> <p></p> <p>Step 2: After clicking on Copy Status Summary, a success message appears saying Copied.</p> <p></p>"},{"location":"settings/status/overview-of-status/#refresh-status-summary","title":"Refresh Status Summary","text":"<p>The Refresh Status Summary option updates the Health Section with the latest data. This ensures that you see the most current performance metrics and system status.</p> <p>Step 1: Click the vertical ellipsis from the right side of the summary section and choose Refresh Status Summary to update the latest data.  </p> <p></p>"},{"location":"settings/status/overview-of-status/#restart-analytics-engine","title":"Restart Analytics Engine","text":"<p>The Restart Analytics Engine option restarts the analytics processing system. This helps resolve issues and ensures that analytics data is accurately processed.</p> <p>Step 1: Click the vertical ellipsis from the right side of the summary section and choose Restart Analytics Engine from the drop-down menu. </p> <p></p> <p>Step 2: A modal window will pop up. Click the Restart button in this window to restart the analytics engine. Restarting the engine helps resolve any issues and ensures that your analytics data is up-to-date and accurately processed.</p> <p></p> <p>Step 3: After clicking on the Restart button, a success message appears saying Successfully triggered Analytics Engine restart.</p> <p></p>"},{"location":"settings/tokens/overview-of-tokens/","title":"Tokens","text":"<p>A token is a secure way to access the Qualytics API instead of using a password. Qualytics provides two types of tokens to meet different authentication needs:</p>"},{"location":"settings/tokens/overview-of-tokens/#token-types","title":"Token Types","text":""},{"location":"settings/tokens/overview-of-tokens/#personal-access-tokens-pats","title":"Personal Access Tokens (PATs)","text":"<p>Personal Access Tokens are designed for individual users to authenticate and interact with the Qualytics API. These tokens are:</p> <ul> <li>User-specific: Each user creates and manages their own tokens</li> <li>Self-service: Can be generated through the Qualytics UI</li> <li>Ideal for: Personal development, testing, and ad-hoc API exploration</li> </ul> <p>Note</p> <p>Tokens are created only once, so you need to copy and store them safely because they cannot be retrieved again.</p>"},{"location":"settings/tokens/overview-of-tokens/#service-tokens","title":"Service Tokens","text":"<p>Service Tokens are designed for automated systems and integrations. These tokens are:</p> <ul> <li>Administrator-managed: Only administrators can create and manage service tokens</li> <li>Organization-level: Created for service accounts rather than individual users</li> <li>Ideal for: Data pipeline automation, Qualytics API/CLI access, data catalog integrations, and shared automation</li> </ul> <p>Tip</p> <p>For detailed information about creating and managing Service Accounts and Service Tokens, see the Service Accounts documentation.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"settings/tokens/overview-of-tokens/#navigation-to-tokens","title":"Navigation to Tokens","text":"<p>Step 1: Log in to your Qualytics account and click the Settings button on the left side panel of the interface. </p> <p></p> <p>Step 2: By default, you will be navigated to the Tags section. Click on the Tokens tab.</p> <p></p>"},{"location":"settings/tokens/overview-of-tokens/#generate-personal-access-tokens-pats","title":"Generate Personal Access Tokens (PATs)","text":"<p>Generating a token provides a secure method for authenticating and interacting with your platform, ensuring that only authorized users and applications can access your resources. Personal Access Tokens (PATs) are particularly useful for automated tools and scripts, allowing them to perform tasks without needing manual intervention. By using PATs, you can leverage our Qualytics CLI to streamline data management and operations, making your workflows more efficient and secure.</p> <p>Step 1: Click on the Generate Token button located in the top right corner.</p> <p></p> <p>A modal window will appear providing the options for generating the token.</p> <p></p> <p>Step 2: Enter the following values:</p> <ol> <li>Name: Enter the name for the Token (e.g., DataAccessToken). </li> <li>Type: Select \"Personal\" from the dropdown.</li> <li>Expiration: Set the expiration period for the token (e.g., 30 days).</li> <li>SCIM Administration Token: Enable this option if the token should only be used for SCIM-related operations, such as provisioning or managing user identities.</li> </ol> <p></p> <p>Step 3: Once you have entered the values, then click on the Generate button.</p> <p></p> <p>Step 4: After clicking on the Generate button, your token is successfully generated.</p> <p>Warning</p> <p>Make sure to download or copy this token. You won't be able to see it again. Keep your token confidential and avoid sharing them with anyone. Use a password manager or an encrypted vault to store your tokens.</p> <p></p>"},{"location":"settings/tokens/overview-of-tokens/#token-expiration","title":"Token Expiration","text":"<p>Tokens include an expiration period that defines how long they remain valid. When a token reaches its expiration date, it automatically stops working and must be regenerated.</p> <p>Available expiration options:</p> <ul> <li>30 Days</li> <li>60 Days</li> <li>90 Days</li> <li>1 Year</li> <li>Never</li> </ul> <p></p> <p>Choosing a time-bound expiration helps maintain security and ensures unused tokens do not stay active indefinitely.</p> <p>Warning</p> <p>Avoid using the Never expiration option, as tokens that never expire can create security risks.</p>"},{"location":"settings/tokens/overview-of-tokens/#token-usage-status","title":"Token Usage Status","text":"<p>Each personal API token displays a usage status to indicate whether it has been used for interaction with the Qualytics API:</p> <p>Last Used: This shows the token has been successfully used recently and is actively in use.</p> <p></p> <p>Not Used: The token has been generated but has not been used for any API requests since creation.</p> <p></p>"},{"location":"settings/tokens/overview-of-tokens/#revoke-token","title":"Revoke Token","text":"<p>You can revoke your token to prevent unauthorized access or actions, especially if the token has been compromised, is no longer needed, or to enhance security by limiting the duration of access.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the user token, that you want to revoke, then click on Revoke from the dropdown menu.</p> <p></p> <p>Step 2: After clicking the Revoke button, your user token will be successfully revoked. A success message will display saying The token has been successfully revoked. Following revocation, the token's status color will change from green to orange.</p> <p></p>"},{"location":"settings/tokens/overview-of-tokens/#restore-token","title":"Restore Token","text":"<p>You can restore a token to reactivate its access, allowing authorized use again. This is useful if the token was mistakenly revoked or if access needs to be temporarily re-enabled without generating a new token.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the revoked tokens, that you want to restore, then click on the Restore button from the dropdown menu.</p> <p></p> <p>Step 2: After clicking on the Restore button, your secret token will be restored and a confirmation message will display saying \"The token has been successfully restored\".</p> <p></p>"},{"location":"settings/tokens/overview-of-tokens/#delete-token","title":"Delete Token","text":"<p>You can delete a token to permanently remove its access, ensuring it cannot be used again. This is important for maintaining security when a token is no longer needed, has been compromised, or to clean up unused tokens in your system.</p> <p>Note</p> <p>You can only delete revoked tokens, not active tokens. If you want to delete an active token, you must first revoke it before you can delete it.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the revoked tokens, that you want to delete, then click on the Delete button from the dropdown menu.</p> <p></p> <p>After clicking the delete button, a confirmation modal window Delete Token will appear.</p> <p></p> <p>Step 2: Click on the Delete button to delete the token.</p> <p></p> <p>After clicking on the Delete button, your token will be deleted and a confirmation message will display saying User token successfully deleted.</p> <p></p>"},{"location":"source-datastore/assign-tags/","title":"Assign Tag","text":"<p>Assigning tags to your Datastore helps you to identify and categorize your datastore easily. Tags serve as labels that categorize and identify various data sets, enhancing efficiency and organization. By highlighting checks and anomalies, tags make it easier to monitor data quality. They also allow you to list file patterns and assign quality scores, enabling quick identification and resolution of issues.</p> <p>In this documentation, we will explore the steps to assign a tag to the datastore.</p> <p>Step 1: Login in to your Qualytics account and select the datastore from the left menu on which you want to assign a tag.</p> <p></p> <p>Step 2: Click on Assign Tag to this Datastore located at the bottom-left corner of the interface.</p> <p></p> <p>Step 3: A drop-up menu will appear, providing you with a list of tags. Assign an appropriate tag to your datastore to simplify sorting, accessing, and managing data.</p> <p></p> <p>You can also create a new tag by clicking on the call to action (\u2795) button.</p> <p></p> <p>A modal window will appear, providing the options to create the tag. Enter the required values to get started.</p> <p></p> <p>For more information on creating tags, refer to the Add Tag section.</p> <p>Step 4: Once you have assigned a tag, the tag will be instantly labeled on your source Datastore, and all related records will be updated.</p> <p>For demonstration, we have assigned the High tag for the Snowflake source datastore Covid-19 Data, so it will automatically be applied to all related tables and checks within the datastore.</p> <p></p>"},{"location":"source-datastore/catalog/","title":"Catalog Operation","text":"<p>A Catalog Operation imports named data collections like tables, views, and files into a Source Datastore. It identifies incremental fields for incremental scans, and offers options to recreate or delete these containers, streamlining data management and enhancing data discovery.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"source-datastore/catalog/#key-components","title":"Key Components","text":""},{"location":"source-datastore/catalog/#incremental-identifier","title":"Incremental Identifier","text":"<p>An incremental identifier is essential for supporting incremental scan operations, as it allows the system to detect changes since the last operation.</p>"},{"location":"source-datastore/catalog/#partition-identifier","title":"Partition Identifier","text":"<p>For large data containers or partitions, a partition identifier is necessary to process data efficiently. In DFS datastores, the default fields for both incremental and partition identifiers are set to the last-modified timestamp. If a partition identifier is missing, the system uses repeatable ordering candidates (order-by fields) to process containers, although this method is less efficient for handling large datasets with many rows.</p> <p>Info</p> <p>Attribute Overrides: After the profile operation, the qualytics engine might automatically update the containers to have partition fields and incremental fields. Those \"attributes\" can be manually overridden.</p> <p>Note</p> <p>Advanced users should be able to override these auto-detected selections and overridden options should persist through subsequent Catalog Operations.</p>"},{"location":"source-datastore/catalog/#initialization-operation-options","title":"Initialization &amp; Operation Options","text":""},{"location":"source-datastore/catalog/#automatic-catalog-operation","title":"Automatic Catalog Operation","text":"<p>While adding the datastore, tick the Initiate Cataloging checkbox to automatically perform a catalog operation on the configured source datastore.</p> <p></p> <p>With the automatic cataloging option turned on, you will be redirected to the datastore details page once the datastore (whether JDBC or DFS) is successfully added. You will observe the cataloging operation running automatically with the following default options:</p> <ul> <li> <p>Prune: Disabled \u274c  </p> </li> <li> <p>Recreate: Disabled \u274c  </p> </li> <li> <p>Include: Tables and views \u2714\ufe0f</p> </li> </ul> <p></p>"},{"location":"source-datastore/catalog/#manual-catalog-operation","title":"Manual Catalog Operation","text":"<p>If automatic cataloging is disabled while adding the datastore, users can initiate the catalog operation manually by selecting preferred options. Manual catalog operation offers users the flexibility to set up custom catalog configurations like syncing only tables or views.</p> <p>Step 1: Select a source datastore from the side menu on which you would like to perform the catalog operation.</p> <p></p> <p>Step 2: Clicking on your preferred datastore will navigate you to the datastore details page. Within the overview tab (default view), click on the Run button under Catalog to initiate the catalog operation.</p> <p></p> <p>A modal window will display Operation Triggered and you will be notified once the catalog operation is completed.  </p> <p>Note</p> <p>You will receive a notification when the catalog operation is completed.</p> <p></p> <p>Step 3: Close the Success modal window and you will observe in the UI that the Catalog operation has been completed and it has gathered the data structures, file patterns, and corresponding metadata from your configured datastore.  </p> <p></p> <p>Users might encounter a error if the schema of the datastore is empty or if the specified user for logging does not have the necessary permissions to read the objects. This ensures that proper access controls are in place and that the data structure is correctly defined.</p> <p></p>"},{"location":"source-datastore/catalog/#custom-catalog-configuration","title":"Custom Catalog Configuration","text":"<p>The catalog operation can be custom-configured with the following options:  </p> <ul> <li> <p>Prune: Remove any existing named collections that no longer appear in the datastore</p> </li> <li> <p>Recreate: Restore any previously removed named collection that does currently appear in the database</p> </li> <li> <p>Include: Include Tables and Views</p> </li> </ul> <p>Step 1: Click on the Run button from the datastore details page (top-right corner) and select Catalog from the dropdown list.</p> <p></p> <p>Step 2: When configuring the catalog operation settings, you have two options to tune:</p> <ul> <li> <p>Prune: This option allows the removal of any named collections (tables, views, files, etc.) that no longer exist in the datastore. This ensures that outdated or obsolete collections are not included in future operations, keeping the datastore clean and relevant.</p> </li> <li> <p>Recreate: This option enables the recreation of any named collections that have been previously deleted in Qualytics. It is useful for restoring collections that may have been removed accidentally or need to be brought back for analysis.</p> </li> </ul> <p></p> <p>Step 3: The user can choose whether to include only tables, only views, or both in the catalog operation. This flexibility allows for more targeted metadata analysis based on the specific needs of the data management task.</p> <p></p>"},{"location":"source-datastore/catalog/#run-instantly","title":"Run Instantly","text":"<p>Click on the \u201cRun Now\u201d button to perform the catalog operation immediately.</p> <p></p> <p>After clicking Run Now, a confirmation message appears stating \"Operation Triggered\".</p> <p></p>"},{"location":"source-datastore/catalog/#schedule","title":"Schedule","text":"<p>Step 1: Click on the \u201cSchedule\u201d button to configure the available schedule options in the catalog operation.</p> <p></p> <p>Step 2: Set the scheduling preferences for the catalog operation.</p> <p>1. Hourly: This option allows you to schedule the catalog operation to run every hour at a specified minute. You can define the frequency in hours and the exact minute within the hour the cataloging should start. Example: If set to \"Every 1 hour(s) on minute 0,\" the catalog operation will run every hour at the top of the hour (e.g., 1:00, 2:00, 3:00).</p> <p></p> <p>2. Daily: This option schedules the catalog operation to run once every day at a specific time. You specify the number of days between scans and the exact time of day in UTC. Example: If set to \"Every 1 day(s) at 00:00 UTC,\" the scan will run every day at midnight UTC.</p> <p></p> <p>3. Weekly: This option schedules the catalog operation to run on specific days of the week at a set time. You select the days of the week and the exact time of day in UTC for the catalog operation to run. Example: If configured to run on \"Sunday\" and \"Friday\" at 00:00 UTC, the scan will execute at midnight UTC on these days.</p> <p></p> <p>4. Monthly: This option schedules the catalog operation to run once a month on a specific day at a set time. You specify the day of the month and the time of day in UTC. If set to \"On the 1st day of every 1 month(s), at 00:00 UTC,\" the catalog operation will run on the first day of each month at midnight UTC.</p> <p></p> <p>5. Advanced: The advanced section for scheduling operations allows users to set up more complex and custom scheduling using Cron expressions. This option is particularly useful for defining specific times and intervals for catalog operations with precision.</p> <p>Cron expressions are a powerful and flexible way to schedule tasks. They use a syntax that specifies the exact timing of the task based on five fields:</p> <ul> <li>Minute (0 - 59)  </li> <li>Hour (0 - 23)  </li> <li>Day of the month (1 - 31)  </li> <li>Month (1 - 12)  </li> <li>Day of the week (0 - 6) (Sunday to Saturday)</li> </ul> <p>Each field can be defined using specific values, ranges, or special characters to create the desired schedule.</p> <p>Example: For instance, the Cron expression <code>0 0 * * *</code> schedules the catalog operation to run at midnight (00:00) every day. Here\u2019s a breakdown of this expression:</p> <ul> <li>0 (Minute) - The task will run at the 0th minute.  </li> <li>0 (Hour) - The task will run at the 0th hour (midnight).  </li> <li>*(Day of the month) - The task will run every day of the month.  </li> <li>*(Month) - The task will run every month.  </li> <li>*(Day of the week) - The task will run every day of the week.</li> </ul> <p>Users can define other specific schedules by adjusting the Cron expression. For example:</p> <ul> <li>0 12 * * 1-5 - Runs at 12:00 PM from Monday to Friday.  </li> <li>30 14 1 * * - Runs at 2:30 PM on the first day of every month.  </li> <li>0 22 * * 6 - Runs at 10:00 PM every Saturday.</li> </ul> <p>To define a custom schedule, enter the appropriate Cron expression in the \"Custom Cron Schedule (UTC)\" field before specifying the schedule name. This will allow for precise control over the timing of the catalog operation, ensuring it runs exactly when needed according to your specific requirements.</p> <p></p> <p>Step 3: Define the \u201cSchedule Name\u201d to identify the scheduled operation at run time.</p> <p></p> <p>Step 4: Click on the \u201cSchedule\u201d button to activate your catalog operation schedule.</p> <p></p> <p>After clicking Schedule, a confirmation message appears stating \"Operation Scheduled\".</p> <p></p> <p>Once the catalog operation is triggered, your view will be automatically switched to the Activity tab, allowing you to explore post-operation details on your ongoing/completed catalog operation.</p> <p></p>"},{"location":"source-datastore/catalog/#operations-insights","title":"Operations Insights","text":"<p>When the catalog operation is completed, you will receive a notification and can navigate to the Activity tab for the datastore on which you triggered the Catalog Operation and learn about the operation results.</p>"},{"location":"source-datastore/catalog/#top-panel","title":"Top Panel","text":"<p>1. Runs (Default View): Provides insights into the operations that have been performed.</p> <p>2. Search: Search any operation (including catalog) by entering the operation ID</p> <p>3. Sort by: Organize the list of operations based on the Created Date or the Duration.</p> <p>4. Filter: Narrow down the list of operations based on:</p> <ul> <li> <p>Operation Type</p> </li> <li> <p>Operation Status</p> </li> <li> <p>Table</p> </li> </ul> <p></p>"},{"location":"source-datastore/catalog/#activity-heatmap","title":"Activity Heatmap","text":"<p>The activity heatmap shown in the snippet below represents activity levels over a period, with each square indicating a day and the color intensity representing the number of operations or activities on that day. It is useful for tracking the number of operations performed on each day within a specific timeframe.  </p> <p>Tip</p> <p>You can click on any of the squares from the Activity Heatmap to filter operations</p> <p></p>"},{"location":"source-datastore/catalog/#operation-detail","title":"Operation Detail","text":""},{"location":"source-datastore/catalog/#running","title":"Running","text":"<p>This status indicates that the catalog operation is still running at the moment and is yet to be completed. A catalog operation having a running status reflects the following details and actions:</p> Parameter Interpretation Operation ID Unique identifier Operation Type Type of operation performed (catalog, profile, or scan) Timestamp Timestamp when the operation was started Progress Bar The progress of the operation Triggered By The author who triggered the operation Schedule Whether the operation was scheduled or not Prune Indicates whether Prune was enabled or disabled in the operation Recreate Indicates whether Recreate was enabled or disabled in the operation Table Indicates whether the Table was included in the operation or not Views Indicates whether the Views was included in the operation or not Abort Click on the Abort button to stop the catalog operation <p></p>"},{"location":"source-datastore/catalog/#aborted","title":"Aborted","text":"<p>This status indicates that the catalog operation was manually stopped before it could be completed. A catalog operation having an aborted status reflects the following details and actions:</p> Parameter Interpretation Operation ID Unique identifier Operation Type Type of operation performed (catalog, profile, or scan) Timestamp Timestamp when the operation was started Progress Bar The progress of the  operation Triggered By The author who triggered the operation Schedule Whether the operation was scheduled or not Prune Indicates whether Prune was enabled or disabled in the operation Recreate Indicates whether Recreate was enabled or disabled in the operation Table Indicates whether the Table was included in the operation or not Views Indicates whether the Views was included in the operation or not Resume Click on the Resume button to continue a previously aborted catalog operation from where it left off Rerun Click on the Rerun button to initiate the catalog operation from the beginning, ignoring any previous attempts Delete Click on the Delete button to remove the record of the catalog operation from the list <p></p>"},{"location":"source-datastore/catalog/#warning","title":"Warning","text":"<p>This status signals that the catalog operation encountered some issues and displays the logs that facilitate improved tracking of blockers and issue resolution. A catalog operation having a warning status reflects the following details and actions:</p> Parameter Interpretation Operation ID Unique identifier Operation Type Type of operation performed (catalog, profile, or scan) Timestamp Timestamp when the operation was started Progress Bar The progress of the operation Triggered By The author who triggered the operation Schedule Whether the operation was scheduled or not Prune Indicates whether Prune was enabled or disabled in the operation Recreate Indicates whether Recreate was enabled or disabled in the operation Table Indicates whether the Table was included in the operation or not Views Indicates whether the Views was included in the operation or not Rerun Click on the Rerun button to initiate the catalog operation from the beginning, ignoring any previous attempts Delete Click on the Delete button to remove the record of the catalog operation from the list Logs Logs include error messages, warnings, and other pertinent information generated during the execution of the Catalog Operation <p></p>"},{"location":"source-datastore/catalog/#success","title":"Success","text":"<p>This status confirms that the catalog operation was completed successfully without any issues. A catalog operation having a success status reflects the following details and actions:</p> Parameter Interpretation Operation ID Unique identifier Operation Type Type of operation performed (catalog, profile, or scan) Timestamp Timestamp when the operation was started Progress Bar The progress of the operation Triggered By The author who triggered the operation Schedule Whether the operation was scheduled or not Prune Indicates whether Prune was enabled or disabled in the operation Recreate Indicates whether Recreate was enabled or disabled in the operation Table Indicates whether the Table was included in the operation or not Views Indicates whether the Views was included in the operation or not Rerun Click on the Rerun button to initiate the catalog operation from the beginning, ignoring any previous attempts Delete Click on the Delete button to remove the record of the catalog operation from the list <p></p>"},{"location":"source-datastore/catalog/#post-operation-details","title":"Post-Operation Details","text":""},{"location":"source-datastore/catalog/#for-jdbc-source-datastores","title":"For JDBC Source Datastores","text":"<p>After the catalog operation is completed on a JDBC source datastore, users can view the following information:</p> <p>Container Names: These are the names of the data collections (e.g., tables, views) identified during the catalog operation.</p> <p></p> <p>Fields for Each Container: Each container will display its fields or columns, which were detected during the catalog operation.</p> <p></p> <p>Incremental Identifiers and Partition Fields: These settings are automatically configured based on the catalog operation. Incremental identifiers help in recognizing changes since the last scan, and partition fields aid in efficient data processing.</p> <p>Tree view &gt; Container node &gt; Gear icon &gt; Settings option</p> <p></p>"},{"location":"source-datastore/catalog/#for-dfs-source-datastores","title":"For DFS Source Datastores","text":"<p>After the catalog operation is completed on a DFS source datastore, users can view the following information:</p> <ul> <li> <p>Container Names: Similar to JDBC, these are the data collections identified during the catalog operation.</p> </li> <li> <p>Fields for Each Container: Each container will list its fields or metadata detected during the catalog operation.</p> </li> <li> <p>Directory Tree Traversal: The catalog operation traverses the directory tree, treating each file with a supported extension as a single-partition container. It reveals metadata such as the relative path, filename, and extension.</p> </li> <li> <p>Incremental Identifier and Partition Field: By default, both the incremental identifier and partition field are set to the last-modified timestamp. This ensures efficient incremental scans and data partitioning.</p> </li> <li> <p>\"Globbed\" Containers: Files in the same folder with the same extensions and similar naming formats are grouped into a single container, where each file is treated as a partition. This helps in managing and querying large datasets effectively.</p> </li> </ul>"},{"location":"source-datastore/catalog/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides API payload examples for initiating and checking the running status of a catalog operation. Replace the placeholder values with data specific to your setup.</p>"},{"location":"source-datastore/catalog/#running-a-catalog-operation","title":"Running a Catalog operation","text":"<p>To run a catalog operation, use the API payload example below and replace the placeholder values with your specific values:</p> <p>Endpoint (Post):  <code>/api/operations/run (post)</code></p> <pre><code>{\n  \"type\": \"catalog\",\n  \"datastore_id\": \"datastore-id\",\n  \"prune\": false,\n  \"recreate\": false,\n  \"include\": [\n    \"table\",\n    \"view\"\n  ]\n}\n</code></pre>"},{"location":"source-datastore/catalog/#retrieving-catalog-operation-status","title":"Retrieving Catalog Operation Status","text":"<p>To retrieve the catalog operation status, use the API payload example below and replace the placeholder values with your specific values:</p> <p>Endpoint (Get):  <code>/api/operations/{id} (get)</code></p> <pre><code>{\n  \"items\": [\n    {\n      \"id\": 12345,\n      \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n      \"type\": \"catalog\",\n      \"start_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n      \"end_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n      \"result\": \"success\",\n      \"message\": null,\n      \"triggered_by\": \"user@example.com\",\n      \"datastore\": {\n        \"id\": 54321,\n        \"name\": \"Datastore-Sample\",\n        \"store_type\": \"jdbc\",\n        \"type\": \"db_type\",\n        \"enrich_only\": false,\n        \"enrich_container_prefix\": \"_data_prefix\",\n        \"favorite\": false\n      },\n      \"schedule\": null,\n      \"include\": [\n        \"table\",\n        \"view\"\n      ],\n      \"prune\": false,\n      \"recreate\": false\n    }\n  ],\n  \"total\": 1,\n  \"page\": 1,\n  \"size\": 50,\n  \"pages\": 1\n}\n</code></pre>"},{"location":"source-datastore/delete-datastore/","title":"Delete Datastore","text":"<p>Step 1: Click on the Delete option in the settings icon.</p> <p></p> <p>Step 2: A modal window titled Delete Datastore will appear.</p> <p></p> <p>Step 3: Enter the name of the datastore in the given field (confirmation check) and then click on the I\u2019M SURE, DELETE THIS DATASTORE button to delete the datastore.</p> <p></p> <p>After clicking the I\u2019M SURE, DELETE THIS DATASTORE button, a success notification appears confirming the deletion.</p>"},{"location":"source-datastore/edit-datastore/","title":"Edit Datastore","text":"<p>Step 1: Click on the Edit option.</p> <p></p> <p>Step 2: After selecting the Edit option, a modal window will appear, displaying the connection details. This window allows you to modify any specific connection details.</p> <p></p> <p>Step 3: After editing the connection details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided connection details are verified, a success message will be displayed indicating that the connection has been verified.</p> <p>Step 4: Click on the Save button.</p> <p></p> <p>After clicking on the Save button, a success notification appears on the screen showing the action was completed successfully.</p>"},{"location":"source-datastore/external-scan/","title":"External Scan Operation","text":"<p>An external scan is ideal for ad hoc scenarios, where you may receive a file intended to be replicated to a source datastore. Before loading, you can perform an external scan to ensure the file aligns with existing data standards. The schema of the file must match the target table or file pattern that has already been profiled within Qualytics, allowing you to reuse the quality checks to identify any issues before data integration.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"source-datastore/external-scan/#navigation-to-external-scan-operation","title":"Navigation to External Scan Operation","text":"<p>Step 1: Select a source datastore from the side menu to perform the external scan operation.</p> <p></p> <p>Step 2: After selecting your preferred source datastore, you will be taken to the details page. From there, click on \"Tables\" and select the table you want to perform the external scan operation on.</p> <p>Note</p> <p>This example is based on a JDBC table, but the same steps apply to a DFS as well. For DFS source datastores, you will need to click on \"File Patterns\" and select a File Pattern to run the external scan.</p> <p></p> <p>For demonstration purposes, we have selected the \u201cCUSTOMER\u201d table.</p> <p></p>"},{"location":"source-datastore/external-scan/#external-scan-configuration","title":"External Scan Configuration","text":"<p>Step 1: Click on the \u201cRun\u201d button and select the \u201cExternal Scan\u201d option. </p> <p></p> <p>Step 2: After selecting the \"External Scan\" option, a modal window will appear with an input for uploading your external file. After uploading the file, click the \u201cRun\u201d button to start the operation.</p> <p></p> <p>Note</p> <p>An External Scan operation supports the following file formats: CSV, XLSX, and XLS.</p> <p>Step 3: After clicking the \"Run\" button, the external scan operation will begin, and you will receive a confirmation message if the operation is successfully triggered.</p> <p></p>"},{"location":"source-datastore/external-scan/#supported-file-formats","title":"Supported File Formats","text":"<p>External scan operation accepts CSV, XLSX, and XLS files. CSV is a simple text format, while XLSX and XLS are Excel formats that support more complex data structures. This versatility enables seamless integration of data from various sources.</p> <p>An External Scan Operation can be configured with the following file formats:</p> File Extension .csv .xls .xlsx File Format Comma-separated values Microsoft Excel 97-2003 Workbook Microsoft Excel 2007+ Workbook Header Row Required for optimal reading. It should contain column names. Recommended, but not strictly required. Recommended, but not strictly required. Empty Cells Represented as empty strings. Allowed. Allowed. Data Types Typically inferred by Spark. May require explicit specification for complex types. May require explicit specification for complex types. Nested Data Not directly supported. Consider flattening or using alternative file formats. Not directly supported. Consider flattening or using alternative file formats. Not directly supported. Consider flattening or using alternative file formats. Additional Considerations - Ensure consistent delimiter usage (usually commas). - Avoid special characters or line breaks within fields. - Enclose text fields containing commas or delimiters in double quotes. - Use a plain XLS format without macros or formatting. - Consider converting to CSV for simpler handling. - Use a plain XLSX format without macros or formatting. - Consider converting to CSV for simpler handling."},{"location":"source-datastore/external-scan/#scenario","title":"Scenario","text":"<p>A company maintains a large sales database containing information about various transactions, customers, and products. They have received a new sales data file that will be integrated into the existing database. Before loading the data, the organization wants to ensure there are no issues with the file.</p> <p>An External Scan is initiated to perform checks on the incoming file, validating that it aligns with the quality standards of the sales table.</p>"},{"location":"source-datastore/external-scan/#specific-checks","title":"Specific Checks:","text":"Check Description <code>Expected Schema</code> Verify that all columns have the same data type as the selected profile structure. <code>Exists in</code> Verify that all transactions have valid customer and product references. <code>Between Times</code> Ensure that transaction dates fall within an expected range. <code>Satisfies Expression</code> Validate that the calculated revenue aligns with the unit price and quantity sold. The formula is: <code>R = Quantity \u00d7 Unit Price</code>"},{"location":"source-datastore/external-scan/#potential-anomalies","title":"Potential Anomalies:","text":"<p>This overview highlights common issues such as data type mismatches, missing references, out-of-range dates, and inconsistent revenue calculations. Each anomaly affects data integrity and requires corrective action.</p> Anomaly Description Data type issue The external resource does not follow the data type schema. Missing References Transactions without valid customer or product references. Out-of-Range Dates Transactions with dates outside the expected range. Inconsistent Revenue Mismatch between calculated revenue and unit price times quantity."},{"location":"source-datastore/external-scan/#benefits-of-external-scan","title":"Benefits of External Scan:","text":"Benefit Description Quality Assurance Identify and rectify data inconsistencies before downstream processes. Data Integrity Ensure that all records adhere to defined schema and constraints. Anomaly Detection Uncover potential issues that might impact business analytics and reporting."},{"location":"source-datastore/external-scan/#csv-table-sales-data","title":"CSV Table (Sales Data):","text":"<p>This dataset includes transaction records with details such as Transaction_ID, Customer_ID, Product_ID, Transaction_Date, Quantity, and Unit_Price. It provides essential information for tracking and analyzing sales activities.</p> Transaction_ID Customer_ID Product_ID Transaction_Date Quantity Unit_Price 1 101 201 2023-01-15 5 20.00 2 102 202 2023-02-20 3 15.50 3 103 201 2023-03-10 2 25.00 4 104 203 2023-04-05 1 30.00 ... ... ... ... ... ... Flowchart <pre><code>graph TB\nsubgraph Init\n  A[Start] --&gt; B[Load Sales Data]\nend\n\nsubgraph Checks\n  B --&gt; C1[Expected schema]\n  B --&gt; C2[Exists in]\n  B --&gt; C3[Between times]\n  B --&gt; C4[Satisfies expression]\n\n  C1 --&gt;|Invalid| E1[Expected schema anomaly]\n  C2 --&gt;|Invalid| E2[Exists in anomaly]\n  C3 --&gt;|Invalid| E3[Between times anomaly]\n  C4 --&gt;|Invalid| E4[Satisfies expression anomaly]\nend\n\nsubgraph End\n\n  E1 --&gt; J[Finish]\n  E2 --&gt; J[Finish]\n  E3 --&gt; J[Finish]\n  E4 --&gt; J[Finish]\nend\n</code></pre>"},{"location":"source-datastore/external-scan/#api-payload-examples","title":"API Payload Examples","text":""},{"location":"source-datastore/external-scan/#running-an-external-scan-operation","title":"Running an External Scan operation","text":"<p>This section provides a sample payload for running an external scan operation. Replace the placeholder values with actual data relevant to your setup.</p>"},{"location":"source-datastore/external-scan/#endpoint-post","title":"Endpoint (Post)","text":"<p><code>/api/containers/{container-id}/scan</code> (post)</p> Running an external scan operation of a datastore <pre><code>{\n    \"name\":\"file_name.csv\",\n    \"records\": [{\\\"COLUMN_1\\\":\\\"VALUE 1\\\",\\\"COLUMN_2\\\":\\\"VALUE 1\\\"},{\\\"COLUMN_1\\\":\\\"VALUE_2\\\",\\\"COLUMN_2\\\":\\\"VALUE 2\\\"}]\n}\n</code></pre>"},{"location":"source-datastore/external-scan/#retrieving-an-external-scan-operation-status","title":"Retrieving an External Scan Operation Status","text":""},{"location":"source-datastore/external-scan/#endpoint-get","title":"Endpoint (Get)","text":"<p><code>/api/operations/{id}</code> (get)</p> Example result response <pre><code>{\n    \"items\": [\n        {\n            \"id\": 12345,\n            \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n            \"type\": \"external_scan\",\n            \"start_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n            \"end_time\": null,\n            \"result\": \"running\",\n            \"message\": null,\n            \"triggered_by\": \"user@example.com\",\n            \"datastore\": {\n                \"id\": 101,\n                \"name\": \"Datastore-Sample\",\n                \"store_type\": \"jdbc\",\n                \"type\": \"db_type\",\n                \"enrich_only\": false,\n                \"enrich_container_prefix\": \"data_prefix\",\n                \"favorite\": false\n            },\n            \"schedule\": null,\n            \"incremental\": false,\n            \"remediation\": \"none\",\n            \"max_records_analyzed_per_partition\": -1,\n            \"greater_than_time\": null,\n            \"greater_than_batch\": null,\n            \"high_count_rollup_threshold\": 10,\n            \"enrichment_source_record_limit\": 10,\n            \"status\": {\n                \"total_containers\": 1,\n                \"containers_analyzed\": 0,\n                \"partitions_scanned\": 0,\n                \"records_processed\": 0,\n                \"anomalies_identified\": 0\n            },\n            \"containers\": [\n                    {\n                    \"id\": 234,\n                    \"name\": \"Container1\",\n                    \"container_type\": \"table\",\n                    \"table_type\": \"table\"\n                    }\n                ],\n                \"container_scans\": [\n                    {\n                    \"id\": 456,\n                    \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                    \"container\": {\n                        \"id\": 234,\n                        \"name\": \"Container1\",\n                        \"container_type\": \"table\",\n                        \"table_type\": \"table\"\n                    },\n                    \"start_time\": null,\n                    \"end_time\": null,\n                    \"records_processed\": 0,\n                    \"anomaly_count\": 0,\n                    \"result\": \"running\",\n                    \"message\": null\n                    }\n                ],\n                \"tags\": []\n        }\n    ],\n    \"total\": 1,\n    \"page\": 1,\n    \"size\": 50,\n    \"pages\": 1\n}\n</code></pre>"},{"location":"source-datastore/link-enrichment/","title":"Link Enrichment","text":"<p>Step 1: Click on the Enrichment from the dropdown list.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear, providing you with two options to link an enrichment datastore.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Caret Down Button Click the caret down to select either Use Enrichment Datastore or Add Enrichment Datastore. 3. Enrichment Datastore Select an enrichment datastore from the dropdown list. 4. Maximum Record Anomaly Per Check Sets the maximum number of anomalies per check before they are merged into one anomaly. Value must be between 1 and 1,000. 5. Maximum Source Examples Per Anomaly This setting lets you choose how many source records to keep for each anomaly. When a scan finds an anomaly, only these records are saved and can be downloaded later. For example, if you set the value to 10, you\u2019ll be able to view or download only 10 records per anomaly. If you need more records, increase this value before running the scan. 6. Remediation Strategy The Remediation Strategy defines how anomalous source tables are replicated in the enrichment datastore. You can choose None (no replication), Append (append new data), or Overwrite (replace existing data)."},{"location":"source-datastore/link-enrichment/#option-i-link-new-enrichment","title":"Option I: Link New Enrichment","text":"<p>If the toggle for Add new connection is turned on, then this will prompt you to link a new enrichment datastore from scratch without using existing connection details.</p> <p>Step 1: Click on the caret button and select Add Enrichment Datastore.</p> <p></p> <p>A modal window Link Enrichment Datastore will appear. Enter the following details to create an enrichment datastore with a new connection.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Name Give a name for the enrichment datastore. 3. Toggle Button for adding new connection Toggle ON to create a new enrichment from scratch or toggle OFF to reuse credentials from an existing connection. 4. Connector Select a datastore connector from the dropdown list. 5. Maximum Record Anomaly Per Check Sets the maximum number of anomalies per check before they are merged into one anomaly. Value must be between 1 and 1,000. 6. Maximum Source Examples Per Anomaly Choose how many source records are saved for each anomaly. Only the saved records can be downloaded later. For example, if this is set to 10, only 10 source records will be available. 7. Remediation Strategy The Remediation Strategy defines how anomalous source tables are replicated in the enrichment datastore. You can choose None (no replication), Append (append new data), or Overwrite (replace existing data). <p>Step 2: Add connection details for your selected enrichment datastore connector.</p> <p>Note</p> <p>Connection details can vary from datastore to datastore. For illustration, we have demonstrated linking BigQuery as a new enrichment datastore.</p> <p></p> <p>Step 3: After adding the source datastore details, click on the Test Connection button to check and verify its connection.</p> <p></p> <p>If the credentials and provided details are verified, a success message will be displayed indicating that the connection has been verified.</p> <p>Step 4: Click on the Save button.</p> <p></p> <p>After clicking on the Save button, a success notification appears on the screen showing the action was completed successfully.</p>"},{"location":"source-datastore/link-enrichment/#option-ii-link-existing-connection","title":"Option II: Link Existing Connection","text":"<p>If the Use an existing enrichment datastore option is selected from the dropdown menu, you will be prompted to link the enrichment datastore using existing connection details.</p> <p>Step 1: Click on the caret button and select Use Enrichment Datastore.</p> <p></p> <p>Step 2: A modal window Link Enrichment Datastore will appear. Add a prefix name and select an existing enrichment datastore from the dropdown list.</p> <p></p> REF. FIELDS ACTIONS 1. Prefix Add a prefix name to uniquely identify tables/files when Qualytics writes metadata from the source datastore to your enrichment datastore. 2. Enrichment Datastore Select an enrichment datastore from the dropdown list. 3. Maximum Record Anomalies per Check Sets the maximum number of anomalies per check before they are merged into one anomaly. Value must be between 1 and 1,000. 4. Maximum Source Examples per Anomaly Sets how many source records are kept per anomaly during a scan. For example, if this is set to 10, only 10 records per anomaly will be saved or downloaded. Increase this value before running the scan to access more records. 5. Remediation Strategy The Remediation Strategy defines how anomalous source tables are replicated in the enrichment datastore. You can choose None (no replication), Append (append new data), or Overwrite (replace existing data). <p>Step 3: View and check the connection details of the enrichment datastore and click on the Save button.</p> <p></p> <p>After clicking on the Save button, a success notification appears on the screen showing the action was completed successfully.</p>"},{"location":"source-datastore/link-enrichment/#endpoint-patch","title":"Endpoint (Patch)","text":"<p><code>/api/datastores/{datastore-id}/enrichment/{enrichment-id}</code> (patch)</p>"},{"location":"source-datastore/profile/","title":"Profile Operation","text":"<p>The Profile Operation is a comprehensive analysis conducted on every record within all available containers in a datastore. This process is aimed at understanding and improving data quality by generating metadata for each field within the collections of data (like tables or files).</p> <p>By gathering detailed statistical data and interacting with the Qualytics Inference Engine, the operation not only identifies and evaluates data quality but also suggests and refines checks to ensure ongoing data integrity. Executing profile operations periodically helps maintain up-to-date and accurate data quality checks based on the latest data.</p> <p>This guide explains how to configure the profile operation with available functionalities such as tables, tags, and schedule options.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"source-datastore/profile/#how-profiling-works","title":"How Profiling Works","text":""},{"location":"source-datastore/profile/#fields-identification","title":"Fields Identification","text":"<p>The initial step involves recognizing and identifying all the fields within each data container. This step is crucial as it lays the foundation for subsequent analysis and profiling.</p>"},{"location":"source-datastore/profile/#statistical-data-gathering","title":"Statistical Data Gathering","text":"<p>After identifying the fields, the Profile Operation collects statistical data for each field based on its declared or inferred data type. This data includes essential metrics such as minimum and maximum values, mean, standard deviation, and other relevant statistics. These metrics provide valuable insights into the characteristics and distribution of the data, helping to understand its quality and consistency.</p>"},{"location":"source-datastore/profile/#metadata-generation","title":"Metadata Generation","text":"<p>The gathered statistical data is then submitted to the Qualytics Inference Engine. The engine utilizes this data to generate metadata that forms the basis for creating appropriate data quality checks. This metadata is essential for setting up robust quality control mechanisms within the data management system.</p>"},{"location":"source-datastore/profile/#data-quality-checks","title":"Data Quality Checks","text":"<p>The inferred data quality checks are rigorously tested against the actual source data. This testing phase is critical to fine-tuning the checks to the desired sensitivity levels, ensuring they are neither too strict (causing false positives) nor too lenient (missing errors). By calibrating these checks accurately, the system can maintain high data integrity and reliability.</p>"},{"location":"source-datastore/profile/#navigation-to-profile-operation","title":"Navigation to Profile Operation","text":"<p>Step 1: Select a source datastore from the side menu on which you would like to perform the profile operation.</p> <p></p> <p>Step 2: Clicking on your preferred datastore will navigate you to the datastore details page. Within the overview tab (default view), click on the Run button under Profile to initiate the profile operation.</p> <p></p>"},{"location":"source-datastore/profile/#configuration","title":"Configuration","text":"<p>Step 1: Click on the Run button to initiate the profile operation.</p> <p></p> <p>Note</p> <p>You can run Profile Operation anytime to update the inferred data quality checks, automatically based on new data in the datastore. It is recommended to schedule the profile operations periodically to update inferred rules. More details are discussed in the Schedule section below.</p> <p>Step 2: Select tables (in your JDBC datastore) or file patterns (in your DFS datastore) and tags you would like to be profiled.</p> <p>1. All Tables/File Patterns</p> <p>This option includes all tables or files currently available in the datastore for profiling. Selecting this will profile every table within the source datastore without the need for further selection.</p> <p></p> <p>2. Specific</p> <p>This option allows users to manually select individual tables or files for profiling. It provides the flexibility to focus on particular tables of interest, which can be useful if the user is only interested in a subset of the available data.</p> <p></p> <p>3. Tag</p> <p>This option automatically profiles tables associated with selected tags. Tags are used to categorize tables, and by selecting a specific tag, all tables associated with that tag will be profiled. This option helps in managing and profiling grouped data efficiently.</p> <p></p> <p>Step 3: After making the relevant selections, click on the Next button to configure the Operation Settings.</p> <p></p> <p>Step 4: Configure the following two Read Settings:</p> <ul> <li>Starting Threshold</li> <li>Record Limit</li> </ul>"},{"location":"source-datastore/profile/#starting-threshold","title":"Starting Threshold","text":"<p>This setting allows users to specify a minimum incremental identifier value to set a starting point for the profile operation. It helps in filtering data from a specific point in time or a particular batch value.  </p> <ul> <li> <p>Greater Than Time: Users can select a timestamp in UTC to start profiling data from a specific time onwards. This is useful for focusing on recent data or data changes since a particular time.  </p> </li> <li> <p>Greater Than Batch: Users can enter a batch value to start profiling from a specific batch. This option is helpful for scenarios where data is processed in batches, allowing the user to profile data from a specific batch number onwards.</p> </li> </ul> <p>Note</p> <p>The starting threshold i.e. Greater Than Time and Greater Than Batch are applicable only to the tables or files with an incremental timestamp strategy.</p> <p></p>"},{"location":"source-datastore/profile/#record-limit","title":"Record Limit","text":"<p>Define the number of records to be profiled per table: This feature allows users to manually enter a custom record limit value using a text field in the profile operation. This setting helps in controlling the scope of the profiling operation, particularly for large datasets, by capping the number of records to analyze.</p> <p></p> <p>You can also use a drop-down menu to quickly select from commonly used limits such as 1M, 10M, 100M, and All.</p> <p></p> <p>Note</p> <p>The number of records must be between 1 and 1,000,000,000.</p> <p>Step 5: After making the relevant selections, click on the Next button to configure the Inference Settings. </p> <p></p> <p>Step 6: Configure the following two Inference Settings: </p> <ul> <li>Inference Threshold</li> <li>Inference State</li> </ul>"},{"location":"source-datastore/profile/#inference-threshold","title":"Inference Threshold","text":"<p>The Inference Threshold allows you to customize the data quality checks that are automatically created and updated when your data is analyzed. This means you can adjust the data quality checks based on how complex the data rules are, giving you more control over how your data is checked and monitored.</p> <p>Default Configuration</p> <p>By default, the Inference Threshold is set to 2, which provides a comprehensive range of checks designed to ensure data integrity across different scenarios. Users have the flexibility to adjust this threshold based on their specific needs, allowing for either basic or advanced checks as required.</p> <p></p>"},{"location":"source-datastore/profile/#levels-of-check-inference","title":"Levels of Check Inference","text":"<p>The Inference Threshold ranges from 0 to 5, with each level including progressively more complex and comprehensive checks. Below is an explanation of each level:</p> <p>Note</p> <p>Each level includes all the checks from the previous levels and adds new checks specific to that level. For example, at Level 1, there are five basic checks. At Level 2, you get those five checks plus additional ones for Level 2. By the time you reach Level 5, it covers all the checks from Levels 1 to 4 and adds its own new checks for complete review. </p>"},{"location":"source-datastore/profile/#level-0-no-inference","title":"Level 0: No Inference","text":"<p>At this level, no checks are automatically inferred. This is suitable when users want complete control over which checks are applied, or if no checks are needed. Ideal for scenarios where profiling should not infer any constraints, and all checks will be manually defined.</p> <p></p>"},{"location":"source-datastore/profile/#level-1-basic-data-integrity-and-simple-value-threshold-checks","title":"Level 1: Basic Data Integrity and Simple Value Threshold Checks","text":"<p>This level includes fundamental rules for basic data integrity and simple validations. It ensures that basic constraints like completeness, non-negative numbers, and valid date ranges are applied. Included Checks:</p> <ul> <li> <p>Completeness Checks: Ensure data fields are complete if previously complete.</p> </li> <li> <p>Categorical Range Checks: Validate if values fall within a predefined set of categories. </p> </li> <li> <p>Non-Negative Numbers: Ensure numeric values are non-negative.  </p> </li> <li> <p>Non-Future Date/Time: Ensure datetime values are not set in the future.</p> </li> </ul> <p>Use Case: Suitable for datasets where basic integrity checks are sufficient.</p> <p>The following table shows the inferred checks that the Analytics Engine can generate based on the user's data. At Level 1, five checks are created.</p> Inferred Checks Reference Not Null (record) See more. Any Not Null (record) See more. Expected Values (record) See more. Not Negative See more. Not Future See more. <p></p>"},{"location":"source-datastore/profile/#level-2-value-range-and-pattern-checks","title":"Level 2: Value Range and Pattern Checks","text":"<p>Builds upon Level 1 by adding more specific checks related to value ranges and patterns. This level is more detailed and begins to enforce rules related to the nature of the data itself. Included Checks:</p> <ul> <li> <p>Date Range Checks: Ensure dates fall within a specified range. </p> </li> <li> <p>Numeric Range Checks: Validate that numeric values are within acceptable ranges.  </p> </li> <li> <p>String Pattern Checks: Ensure strings match specific patterns (e.g., email formats).  </p> </li> <li> <p>Approximate Uniqueness: Validate uniqueness of values if they are approximately unique.</p> </li> </ul> <p>Use Case: Ideal for datasets where patterns and ranges are important for ensuring data quality.</p> <p>The following table shows the inferred checks that the Analytics Engine can generate based on the user's data. At Level 2, four checks are created.</p> Checks Reference Between Times See more. Between See more. Matches Pattern See more. Unique See more. <p></p>"},{"location":"source-datastore/profile/#level-3-time-series-and-comparative-relationship-checks","title":"Level 3: Time Series and Comparative Relationship Checks","text":"<p>This level includes all checks from Level 2 and adds sophisticated checks for time series and comparative relationships between datasets. Included Checks:</p> <ul> <li> <p>Date Granularity Checks: Ensure the granularity of date values is consistent (e.g., day, month, year).</p> </li> <li> <p>Consistent Relationships: Validate that relationships between overlapping datasets are consistent.</p> </li> </ul> <p>Use Case: Suitable for scenarios where data quality depends on time-series data or when comparing data across different datasets.</p> <p>The following table shows the inferred checks that the Analytics Engine can generate based on the user's data. At Level 3, eight checks are created.</p> Inferred checks Reference Time Distribution Size See more. After Date Time See more. Before Date Time See more. Greater Than See more. Greater Than Field See more. Less Than See more. Less Than Field See more. Equal To Field See more. <p></p>"},{"location":"source-datastore/profile/#level-4-linear-regression-and-cross-datastore-relationship-checks","title":"Level 4: Linear Regression and Cross-Datastore Relationship Checks","text":"<p>This level includes all checks from Level 3 and adds even more advanced checks, including linear regression analysis and validation of relationships across different data stores. Included Checks:</p> <ul> <li> <p>Linear Regression Checks: Validate data using regression models to identify trends and outliers.</p> </li> <li> <p>Cross-Datastore Relationships: Ensure that data relationships are maintained across different data sources.</p> </li> </ul> <p>Use Case: Best for complex datasets where advanced analytical checks are necessary.</p> <p>The following table shows the inferred checks that the Analytics Engine can generate based on the user's data. At Level 4, four checks are created.</p> Inferred Checks Reference Data Diff See more. Exists In See more. Not Exists In See more. Predicted By See more. Is Replica Of (is sunsetting) See more. <p></p>"},{"location":"source-datastore/profile/#level-5-shape-checks","title":"Level 5: Shape Checks","text":"<p>The most comprehensive level includes all previous checks plus checks that validate the shape of certain distribution patterns that can be identified in your data. Included Checks:</p> <ul> <li>Shape Checks: Checks that define an expectation for some percentage of your data less than 100%.  The property \u201ccoverage\u201d holds the percentage of your data for which the expressed check should be true.</li> </ul> <p>Use Case: Ideal for scenarios where each incremental set of scanned data should exhibit the same distributions of values as the training set.  For example, a transactions table is configured for a weekly incremental scan after each week\u2019s data is loaded. A shape check could define that 80% of all transactions are expected to be performed using \u201ccash\u201d or \u201ccredit\u201d.</p> <p>This table shows the inferred checks that the Analytics Engine can generate based on the user's data. At Level 5, three checks are created.</p> Inferred Checks Reference Expected Values (Shape) See more. Matches Pattern (Shape) See more. Not Null (Shape) See more. <p></p> <p>Warning</p> <p>If the checks inferred during a profile operation do not detect any anomalies, and the check inference level decreases in the next profile operation, the checks that did not generate anomalies will be archived or discarded. However, if the checks detect any anomalies, they will be retained to continue monitoring the data and addressing potential issues.</p>"},{"location":"source-datastore/profile/#inference-state","title":"Inference State","text":"<p>Check the box labeled \"Infer As Draft\" to ensure that all inferred checks will be generated in a draft state. This allows for greater flexibility as you can review and refine these checks before they are finalized.</p> <p></p>"},{"location":"source-datastore/profile/#run-instantly","title":"Run Instantly","text":"<p>Click on the Run Now button, and perform the profile operation immediately.</p> <p></p>"},{"location":"source-datastore/profile/#schedule","title":"Schedule","text":"<p>Step 1: Click on the Schedule button to configure the available schedule options in the profile operation.</p> <p></p> <p>Step 2: Set the scheduling preferences for the profile operation.</p> <p>1. Hourly: This option allows you to schedule the profile operation to run every hour at a specified minute. You can define the frequency in hours and the exact minute within the hour the profiling should start. Example: If set to \"Every 1 hour(s) on minute 0,\" the profile operation will run every hour at the top of the hour (e.g., 1:00, 2:00, 3:00).</p> <p></p> <p>2. Daily: This option schedules the profile operation to run once every day at a specific time. You specify the number of days between scans and the exact time of day in UTC. Example: If set to \"Every 1 day(s) at 00:00 UTC,\" the scan will run every day at midnight UTC.</p> <p></p> <p>3. Weekly: This option schedules the profile operation to run on specific days of the week at a set time. You select the days of the week and the exact time of day in UTC for the profile operation to run. Example: If configured to run on \"Sunday\" and \"Friday\" at 00:00 UTC, the scan will execute at midnight UTC on these days.</p> <p></p> <p>4. Monthly: This option schedules the profile operation to run once a month on a specific day at a set time. You specify the day of the month and the time of day in UTC. If set to \"On the 1st day of every 1 month(s), at 00:00 UTC,\" the profile operation will run on the first day of each month at midnight UTC.</p> <p></p> <p>5. Advanced: The advanced section for scheduling operations allows users to set up more complex and custom scheduling using Cron expressions. This option is particularly useful for defining specific times and intervals for profile operations with precision.</p> <p>Cron expressions are a powerful and flexible way to schedule tasks. They use a syntax that specifies the exact timing of the task based on five fields:</p> <ul> <li>Minute (0 - 59)</li> <li>Hour (0 - 23)</li> <li>Day of the month (1 - 31)</li> <li>Month (1 - 12)</li> <li>Day of the week (0 - 6) (Sunday to Saturday)</li> </ul> <p>Each field can be defined using specific values, ranges, or special characters to create the desired schedule.</p> <p>Example: For instance, the Cron expression <code>0 0 * * *</code> schedules the profile operation to run at midnight (00:00) every day. Here\u2019s a breakdown of this expression:</p> <ul> <li>0 (Minute) - The task will run at the 0th minute.</li> <li>0 (Hour) - The task will run at the 0th hour (midnight).</li> <li>*(Day of the month) - The task will run every day of the month.</li> <li>*(Month) - The task will run every month.</li> <li>*(Day of the week) - The task will run every day of the week.  </li> </ul> <p>Users can define other specific schedules by adjusting the Cron expression. For example:</p> <ul> <li>0 12 * * 1-5 - Runs at 12:00 PM from Monday to Friday.</li> <li>30 14 1 * * - Runs at 2:30 PM on the first day of every month.</li> <li>0 22 * * 6 - Runs at 10:00 PM every Saturday.</li> </ul> <p>To define a custom schedule, enter the appropriate Cron expression in the Custom Cron Schedule (UTC) field before specifying the schedule name. This will allow for precise control over the timing of the profile operation, ensuring it runs exactly when needed according to your specific requirements.</p> <p></p> <p>Step 3: Define the Schedule Name to identify the scheduled operation at the running time.</p> <p></p> <p>Step 4: Click on the Schedule button to activate your profile operation schedule.</p> <p></p> <p>Note</p> <p>You will receive a notification when the profile operation is completed.</p>"},{"location":"source-datastore/profile/#operation-insights","title":"Operation Insights","text":"<p>When the profile operation is completed, you will receive the notification and can navigate to the Activity tab for the datastore on which you triggered the Profile Operation and learn about the operation results.</p>"},{"location":"source-datastore/profile/#top-panel","title":"Top Panel","text":"<ol> <li> <p>Runs (Default View): Provides insights into the operations that have been performed</p> </li> <li> <p>Schedule: Provides insights into the scheduled operations.</p> </li> <li> <p>Search: Search any operation (including profile) by entering the operation ID</p> </li> <li> <p>Sort by: Organize the list of operations based on the Created Date or the Duration.</p> </li> <li> <p>Filter: Narrow down the list of operations based on:</p> </li> <li> <p>Operation Type</p> </li> <li>Operation Status</li> <li>Table</li> </ol> <p></p>"},{"location":"source-datastore/profile/#activity-heatmap","title":"Activity Heatmap","text":"<p>The activity heatmap shown in the snippet below represents activity levels over a period, with each square indicating a day and the color intensity representing the number of operations or activities on that day. It is useful in tracking the number of operations performed on each day within a specific timeframe.</p> <p>Tip</p> <p>You can click on any of the squares from the Activity Heatmap to filter operations</p> <p></p>"},{"location":"source-datastore/profile/#operation-detail","title":"Operation Detail","text":""},{"location":"source-datastore/profile/#running","title":"Running","text":"<p>This status indicates that the profile operation is still running at the moment and is yet to be completed. A profile operation having a running status reflects the following details and actions:</p> No. Parameter Interpretation 1. Operation ID &amp; Operation Type Unique identifier and type of operation performed (catalog, profile, or scan). 2. Timestamp Timestamp when the operation was started. 3. Progress Bar The progress of the operation. 4. Triggered By The author who triggered the operation. 5. Schedule Whether the operation was scheduled or not. 6. Inference Threshold Indicates how much control you have over automatic data quality checks, adjustable based on complexity. 7. Checks Synchronized Indicates the count of Checks Synchronized in the operation. 8. Infer as Draft Indicates whether Infer as Draft was enabled or disabled in the operation. 9. Read Record Limit Defines the maximum number of records to be scanned per table after initial filtering. 10. Results Provides immediate insights into the profile operation conducted. 11. Abort The \"Abort\" button enables you to stop the ongoing profile operation. 12. Summary The \"Summary\" section provides a real-time overview of the profile operation's progress. It includes key metrics such as: <ul><li> Tables Requested: The total number of tables that were requested for profiling. Click on the adjacent magnifying glass icon to view the tables requested.</li><li> Tables Profiled: The number of tables that have been profiled so far. Click on the adjacent magnifying glass icon to view the tables profiled. </li><li> Records Profiled:  This represents the total number of records that were included in the profiling process. </li><li> Field Profiles Updates:  This number shows how many field profiles were updated as a result of the profiling operation. </li><li> Inferred Checks Synchronized:  This indicates the number of inferred checks that were synchronized based on the profile operation. </li> <li> Added: Shows the count of newly added inferred checks. </li><li> Updated: Indicates the count of checks that were updated in the operation.</li> <p></p>"},{"location":"source-datastore/profile/#aborted","title":"Aborted","text":"<p>This status indicates that the profile operation was manually stopped before it could be completed. A profile operation having an aborted status reflects the following details and actions:</p> No. Parameter Interpretation 1. Operation ID &amp; Operation Type Unique identifier and type of operation performed (catalog, profile, or scan). 2. Timestamp Timestamp when the operation was started. 3. Progress Bar The progress of the operation. 4. Aborted By The author who Aborted the operation. 5. Schedule Whether the operation was scheduled or not. 6. Inference Threshold Indicates how much control you have over automatic data quality checks, adjustable based on complexity. 7. Checks Synchronized Indicates the count of Checks Synchronized in the operation. 8. Infer as Draft Indicates whether Infer as Draft was enabled or disabled in the operation. 9. Read Record Limit Defines the maximum number of records to be scanned per table after initial filtering. 10. Results Provides immediate insights into the profile operation conducted. 11. Resume Provides an option to continue the profile operation from where it left off. 12. Rerun Allows you to start a new profile operation using the same settings as the aborted scan. 13. Delete Removes the record of the aborted profile operation from the system, permanently deleting results. 14. Summary The \"Summary\" section provides a real-time overview of the profile operation's progress. It includes key metrics such as: <ul><li> Tables Requested: The total number of tables that were requested for profiling. Click on the adjacent magnifying glass icon to view the tables requested.</li><li> Tables Profiled: The number of tables that were profiled before the operation was aborted. Click on the adjacent magnifying glass icon to view the tables profiled. </li><li> Records Profiled: This represents the total number of records that were included before the profiling process was aborted. </li><li> Field Profiles Updates: This number shows how many field profiles were updated as a result of the profiling operation. </li><li> Inferred Checks Synchronized: This indicates the number of inferred checks that were synchronized based on the profile operation. </li> <li> Added: Shows the count of newly added inferred checks. </li><li> Updated: Indicates the count of checks that were updated in the operation.</li> <p></p>"},{"location":"source-datastore/profile/#warning","title":"Warning","text":"<p>This status signals that the profile operation encountered some issues and displays the logs that facilitate improved tracking of the blockers and issue resolution. A profile operation having a completed with warning status reflects the following details and actions:</p> No. Parameter Interpretation 1. Operation ID &amp; Operation Type Unique identifier and type of operation performed (catalog, profile, or scan). 2. Timestamp Timestamp when the operation was started. 3. Progress Bar The progress of the operation. 4. Triggered By The author who triggered the operation. 5. Schedule Whether the operation was scheduled or not. 6. Inference Threshold Indicates how much control you have over automatic data quality checks, adjustable based on complexity. 7. Checks Synchronized Indicates the count of Checks Synchronized in the operation. 8. Infer as Draft Indicates whether Infer as Draft was enabled or disabled in the operation. 9. Read Record Limit Defines the maximum number of records to be scanned per table after initial filtering. 10. Results Provides immediate insights into the profile operation conducted. 11. Rerun Allows you to start a new profile operation using the same settings as the warning scan. 12. Delete Removes the record of the profile operation, permanently deleting all results. 13. Summary The \"Summary\" section provides a real-time overview of the profile operation's progress. It includes key metrics such as: <ul><li> Tables Requested: The total number of tables that were requested for profiling. Click on the adjacent magnifying glass icon to view the tables requested.</li><li> Tables Profiled: The number of tables that were profiled before the operation completed. Click on the adjacent magnifying glass icon to view the tables profiled. </li><li> Records Profiled: This represents the total number of records that were included before the profiling process was completed. </li><li> Field Profiles Updates: This number shows how many field profiles were updated as a result of the profiling operation.</li><li> Inferred Checks Synchronized: This indicates the number of inferred checks that were synchronized based on the profile operation. </li></ul> 14. Logs Logs include error messages, warnings, and other pertinent information that occurred during the execution of the Profile Operation. <p></p>"},{"location":"source-datastore/profile/#success","title":"Success","text":"<p>This status confirms that the profile operation was completed successfully without any issues. A profile operation having a success status reflects the following details and actions:</p> No. Parameter Interpretation 1. Operation ID &amp; Operation Type Unique identifier and type of operation performed (catalog, profile, or scan). 2. Timestamp Timestamp when the operation was started. 3. Progress Bar The progress of the operation. 4. Triggered By The author who triggered the operation. 5. Schedule Whether the operation was scheduled or not. 6. Inference Threshold Indicates how much control you have over automatic data quality checks, allowing adjustments based on data complexity. 7. Checks Synchronized Indicates the count of Checks Synchronized in the operation. 8. Infer as Draft Indicates whether Infer as Draft was enabled or disabled in the operation. 9. Read Record Limit Defines the maximum number of records to be scanned per table after initial filtering. 10. Results Provides immediate insights into the profile operation conducted. 11. Rerun Allows you to start a new profile operation using the same settings as the warning scan, useful for restarting after errors. 12. Delete Removes the record of the profile operation from the system, permanently deleting all results; this action cannot be undone. 13. Summary The \"Summary\" section provides a real-time overview of the profile operation's progress. It includes key metrics such as: <ul><li> Tables Requested: The total number of tables that were requested for profiling. Click on the adjacent magnifying glass icon to view the tables requested. </li><li> Tables Profiled: The number of tables that were profiled before the operation was aborted. Click on the adjacent magnifying glass icon to view the tables profiled. </li><li> Records Profiled: This represents the total number of records that were included before the profiling process was aborted. </li><li> Field Profiles Updates: This number shows how many field profiles were updated as a result of the profiling operation. </li><li> Inferred Checks Synchronized: This indicates the number of inferred checks that were synchronized based on the profile operation. </li><li> Added: Shows the count of newly added inferred checks. </li><li> Updated: Indicates the count of checks that were updated in the operation.</li> <p></p>"},{"location":"source-datastore/profile/#full-view-of-metrics-in-operation-summary","title":"Full View of Metrics in Operation Summary","text":"<p>Users can now hover over abbreviated metrics to see the full value for better clarity. For demonstration purposes, we are hovering over the Records Profiled field to display the full value.</p> <p></p>"},{"location":"source-datastore/profile/#post-operation-details","title":"Post Operation Details","text":"<p>Step 1: Click on any of the successful Profile Operations from the list and hit the Results button.</p> <p></p> <p>Step 2: The Profile Results modal displays a list of both profiled and non-profiled containers. You can filter the view to show only non-profiled containers by toggling on button, which will display the complete list of unprofiled containers.</p> <p></p> <p>The Profile Results modal also provides two analysis options for you:</p> <ul> <li>Details for a Specific Container (Container's Profile)</li> <li>Details for a Specific Field of a Container (Field Profile)</li> </ul> <p>Unwrap any of the containers from the Profile Results modal and click on the arrow icon.</p> <p></p>"},{"location":"source-datastore/profile/#details-for-a-specific-container-containers-profile","title":"Details for a Specific Container (Container's Profile)","text":"<p>Based on your selection of container from the profile operation results, you will be automatically redirected to the container details on the source datastore details page.</p> <p></p> <p>The following details (metrics) will be visible for analyzing the specific container you selected:</p> <ol> <li> <p>Quality Score (79): This represents an overall quality assessment of the field, likely on a scale of 0 to 100. A score of 79 suggests that the data quality is relatively good but may need further improvement.</p> </li> <li> <p>Sampling (100%): Indicates that 100% of the data in this field was sampled for analysis. This means the entire dataset for this field was reviewed.</p> </li> <li> <p>Completeness (100%): Suggests that all entries in this field are complete, with no missing or null values, signifying data integrity.</p> </li> <li> <p>Active Checks (2): This shows that 2 data quality checks are actively running on this field. These checks likely monitor aspects such as format, uniqueness, or consistency.</p> </li> <li> <p>Active Anomalies (0): Indicates that there are no active anomalies or issues detected in the field, meaning no irregularities have been found during the checks.</p> </li> </ol> <p></p>"},{"location":"source-datastore/profile/#details-for-a-specific-field-of-a-container-field-profile","title":"Details for a Specific Field of a Container (Field Profile)","text":"<p>Unwrap the container to view the underlying fields. The following details (metrics) will be visible for analyzing a specific field of the container:</p> No Profile Description 1 Declared Type Indicates whether the type is declared by the source or inferred. 2 Distinct Values Count of distinct values observed in the dataset. 3 Min Length Shortest length of the observed string values or lowest value for numerics. 4 Max Length Greatest length of the observed string values or highest value for numerics. 5 Mean Mathematical average of the observed numeric values. 6 Median The median of the observed numeric values. 7 Standard Deviation Measure of the amount of variation in observed numeric values. 8 Kurtosis Measure of the 'tailedness' of the distribution of observed numeric values. 9 Skewness Measure of the asymmetry of the distribution of observed numeric values. 10 Q1 The first quartile; the central point between the minimum and the median. 11 Q3 The third quartile; the central point between the median and the maximum. 12 Sum Total sum of all observed numeric values. <p></p>"},{"location":"source-datastore/profile/#histogram","title":"Histogram","text":"<p>Shows how the values in the field are spread out. Each bar represents how many values fall within a certain range, making it easy to spot trends and outliers.</p> <p></p>"},{"location":"source-datastore/profile/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides payload examples for initiating and checking the running status of a profile operation. Replace the placeholder values with data specific to your setup.</p>"},{"location":"source-datastore/profile/#running-a-profile-operation","title":"Running a Profile Operation","text":"<p>To run a profile operation, use the API payload example below and replace the placeholder values with your specific values:</p> <p>Endpoint (Post):  <code>/api/operations/run (post)</code></p>"},{"location":"source-datastore/profile/#option-i-running-a-profile-operation-of-all-containers","title":"Option I: Running a profile operation of all containers","text":"<ul> <li> <p>container_names: [ ]: This setting indicates that profiling will encompass all containers.</p> </li> <li> <p>max_records_analyzed_per_partition: null: This setting implies that all records within all containers will be profiled.</p> </li> <li> <p>infer_threshold: 5: This setting indicates that the engine will automatically infer quality checks of level 5 for you.</p> </li> </ul> <pre><code>{  \n    \"type\":\"profile\",  \n    \"datastore_id\": datastore-id,  \n    \"container_names\":[],  \n    \"max_records_analyzed_per_partition\":null,  \n    \"inference_threshold\":5  \n}\n</code></pre>"},{"location":"source-datastore/profile/#option-ii-running-a-profile-operation-of-specific-containers","title":"Option II: Running a profile operation of specific containers","text":"<ul> <li> <p>container_names: [\"table_name_1\", \"table_name_2\"]: This setting indicates that profiling will only cover the tables named table_name_1 and table_name_2.</p> </li> <li> <p>max_records_analyzed_per_partition: 1000000: This setting means that up to 1 million rows per container will be profiled.</p> </li> <li> <p>infer_threshold: 0: This setting indicates that the engine will not automatically infer quality checks for you.</p> </li> </ul> <pre><code>{  \n    \"type\":\"profile\",  \n    \"datastore_id\":datastore-id,  \n    \"container_names\":[  \n        \"table_name_1\",  \n        \"table_name_2\"  \n    ],  \n    \"max_records_analyzed_per_partition\":1000000,  \n    \"inference_threshold\":0  \n}\n</code></pre>"},{"location":"source-datastore/profile/#scheduling-a-profile-operation","title":"Scheduling a Profile Operation","text":"<p>Below is a sample payload for scheduling a profile operation. Please substitute the placeholder values with the appropriate data relevant to your setup.</p> <p>Endpoint (Post): <code>/api/operations/schedule (post)</code></p> <p>INFO: This payload is to run a scheduled profile operation every day at 00:00</p>"},{"location":"source-datastore/profile/#scheduling-profile-operation-of-all-containers","title":"Scheduling profile operation of all containers","text":"<pre><code>{  \n    \"type\":\"profile\",  \n    \"name\":\"My scheduled Profile operation\",  \n    \"datastore_id\":\"datastore-id\",  \n    \"container_names\":[],  \n    \"max_records_analyzed_per_partition\":null,  \n    \"infer_constraints\":5,  \n    \"crontab\":\"00 00 /1  *\"  \n}\n</code></pre>"},{"location":"source-datastore/profile/#retrieving-profile-operation-status","title":"Retrieving Profile Operation Status","text":"<p>To retrieve the profile operation status, use the API payload example below and replace the placeholder values with your specific values:</p> <p>Endpoint (Get): <code>/api/operations/{id} (get)</code></p> <pre><code>{\n    \"items\": [\n        {\n            \"id\": 12345,\n            \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n            \"type\": \"profile\",\n            \"start_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n            \"end_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n            \"result\": \"success\",\n            \"message\": null,\n            \"triggered_by\": \"user@example.com\",\n            \"datastore\": {\n                \"id\": 101,\n                \"name\": \"Sample-Store\",\n                \"store_type\": \"jdbc\",\n                \"type\": \"db_type\",\n                \"enrich_only\": false,\n                \"enrich_container_prefix\": \"data_prefix\",\n                \"favorite\": false\n            },\n            \"schedule\": null,\n            \"inference_threshold\": 5,\n            \"max_records_analyzed_per_partition\": -1,\n            \"max_count_testing_sample\": 100000,\n            \"histogram_max_distinct_values\": 100,\n            \"greater_than_time\": null,\n            \"greater_than_batch\": null,\n            \"percent_testing_threshold\": 0.4,\n            \"high_correlation_threshold\": 0.5,\n            \"status\": {\n                \"total_containers\": 2,\n                \"containers_analyzed\": 2,\n                \"partitions_analyzed\": 2,\n                \"records_processed\": 1126,\n                \"fields_profiled\": 9,\n                \"checks_synchronized\": 26\n            },\n            \"containers\": [\n                {\n                    \"id\": 123,\n                    \"name\": \"Container1\",\n                    \"container_type\": \"table\",\n                    \"table_type\": \"table\"\n                },\n                {\n                    \"id\": 456,\n                    \"name\": \"Container2\",\n                    \"container_type\": \"table\",\n                    \"table_type\": \"table\"\n                }\n            ],\n            \"container_profiles\": [\n                {\n                    \"id\": 789,\n                    \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                    \"parent_profile_id\": null,\n                    \"container\": {\n                        \"id\": 456,\n                        \"name\": \"Container2\",\n                        \"container_type\": \"table\",\n                        \"table_type\": \"table\"\n                    },\n                    \"records_count\": 550,\n                    \"records_processed\": 550,\n                    \"checks_synchronized\": 11,\n                    \"field_profiles_count\": 4,\n                    \"result\": \"success\",\n                    \"message\": null\n                },\n                {\n                    \"id\": 790,\n                    \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                    \"parent_profile_id\": null,\n                    \"container\": {\n                        \"id\": 123,\n                        \"name\": \"Container1\",\n                        \"container_type\": \"table\",\n                        \"table_type\": \"table\"\n                    },\n                    \"records_count\": 576,\n                    \"records_processed\": 576,\n                    \"checks_synchronized\": 15,\n                    \"field_profiles_count\": 5,\n                    \"result\": \"success\",\n                    \"message\": null\n                }\n            ],\n            \"tags\": []\n        }\n    ],\n    \"total\": 1,\n    \"page\": 1,\n    \"size\": 50,\n    \"pages\": 1\n}\n</code></pre>"},{"location":"source-datastore/quality-score-settings/","title":"Quality Score Settings","text":"<p>Quality Scores are quantified measures of data quality calculated at the field and container levels recorded as time series to enable tracking of changes over time. Scores range from 0-100, with higher values indicating superior quality. These scores integrate eight distinct factors, providing a granular analysis of the attributes that impact the overall data quality.</p> <p>Each field receives a total quality score based on eight key factors, each evaluated on a 0-100 scale. The overall score is a composite reflecting the relative importance and configured weights of these factors:</p> <ul> <li>Completeness: Measures the average completeness of a field across all profiles.  </li> <li>Coverage: Assesses the adequacy of data quality checks for the field.  </li> <li>Conformity: Checks alignment with standards defined by quality checks.  </li> <li>Consistency: Ensures uniformity in type and scale across all data representations.  </li> <li>Precision: Evaluates the resolution of field values against defined quality checks.  </li> <li>Timeliness: Gauges data availability according to schedule, inheriting the container's timeliness.  </li> <li>Volumetrics: Analyzes consistency in data size and shape over time, inheriting the container's volumetrics.  </li> <li>Accuracy: Determines the fidelity of field values to their real-world counterparts.</li> </ul> <p>The Quality Score Settings allow users to tailor the impact of each quality factor on the total score by adjusting their weights, allowing the scoring system to align with your organization\u2019s data governance priorities.</p> <p>Step 1: Click on the Score option in the settings icon.</p> <p></p> <p>Step 2: A modal window \"Quality Score Settings\" will appear.</p> <p></p> <p>Step 3: The Decay Period slider sets the time frame over which the system evaluates historical data to determine the quality score. The decay period for considering past data events defaults to 180 days, but can be customized to fit your operational needs, ensuring the scores reflect the most relevant data quality insights.</p> <p></p> <p>Step 4: Adjust the Factor Weights using the sliding bar. The factor weights determine the importance of different data quality aspects.</p> <p></p> <p>Step 5: Click on the Save button to save the quality score settings.</p> <p></p> <p>After clicking the Save button, a success notification appears on the screen showing the action was completed successfully.</p>"},{"location":"source-datastore/right-click-options/","title":"Right Click Options","text":"<p>Once you add a source datastore, whether a JDBC or DFS, Qualytics provides right-click options on the following:  </p> <ul> <li>Added source datastore</li> <li>Tables or files within the source datastore  </li> <li>Fields within the tables</li> <li>Checks within the source datastore  </li> <li>Anomalies within the source datastore  </li> </ul> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"source-datastore/right-click-options/#right-click-source-datastore","title":"Right Click Source Datastore","text":"<p>Log in to your Qualytics account and right-click on the source datastore whether a JDBC or DFS. A dropdown list of options will appear:</p> <ul> <li> <p>Open in New Tab.</p> </li> <li> <p>Copy Link.</p> </li> <li> <p>Copy ID.</p> </li> <li> <p>Copy Name.</p> </li> </ul> <p></p> No Field Description 1 Open in New Tab Opens the selected source datastore in a new browser tab, where you can view its quality score, sampling, completeness, active checks, active anomalies, etc. 2 Copy Link Copy the unique URL of the selected source datastore to your clipboard. 3 Copy ID Copy the unique ID of the selected source datastore. 4 Copy Name Copy the name of the selected source datastore to your clipboard. <p>Alternatively, you can access these right-click options by performing the direct right-click operation on a source datastore from the list.</p> <p></p>"},{"location":"source-datastore/right-click-options/#right-click-tables-files","title":"Right Click Tables &amp; Files","text":"<p>Right-click on the specific table or file underlying a connected source datastore.</p> <p>A dropdown list of options will appear:</p> <ul> <li> <p>Open in New Tab.</p> </li> <li> <p>Copy Link.</p> </li> <li> <p>Copy ID.</p> </li> <li> <p>Copy Name.</p> </li> </ul> <p></p> No Field Description 1 Open in New Tab Open the selected table from the datastore in a new browser tab, where you can view its quality score, sampling, completeness, active checks, active anomalies, etc. 2 Copy Link Copy the unique URL of the selected table to your clipboard. 3 Copy ID Copy the unique identifier (ID) of the selected table. 4 Copy Name Copy the name of the selected table to your clipboard. <p>Alternatively, you can access these right-click options by opening the dedicated page of the source datastore, navigating to its Tables or files section, and performing the right-click operation on any table or file from the list.</p> <p></p>"},{"location":"source-datastore/right-click-options/#right-click-fields","title":"Right Click Fields","text":"<p>Right-click on the specific field underlying within a table or file.</p> <p>A dropdown list of options will appear:</p> <ul> <li> <p>Open in New Tab.</p> </li> <li> <p>Copy Link.</p> </li> <li> <p>Copy ID.</p> </li> <li> <p>Copy Name.</p> </li> </ul> <p></p> No Field Description 1 Open in New Tab Open the selected field in a new browser tab, where you can view its quality score, sampling, completeness, active checks, active anomalies, etc. 2 Copy Link Copy the unique URL of the selected field to your clipboard. 3 Copy ID Copy the unique identifier (ID) of the selected field. 4 Copy Name Copy the name of the selected field to your clipboard. <p>Alternatively, you can access these right-click options by opening the dedicated page of the table, navigating to its Fields section, and performing the right-click operation on any field from the list.</p> <p></p>"},{"location":"source-datastore/right-click-options/#right-click-checks","title":"Right Click Checks","text":"<p>Right-click on the specific check from All, Active, Draft, and Archived within a connected source datastore.</p> <p>A dropdown list of options will appear:</p> <ul> <li> <p>Open in New Tab.</p> </li> <li> <p>Copy Link.</p> </li> <li> <p>Copy ID.</p> </li> <li> <p>Copy Name.</p> </li> </ul> <p></p> No Field Description 1 Open in New Tab Open the selected check in a new browser tab, where you can view its quality score, sampling, completeness, active checks, active anomalies, etc. 2 Copy Link Copy the unique URL of the selected check to your clipboard. 3 Copy ID Copy the unique identifier (ID) of the selected check. 4 Copy Name Copy the name of the selected check to your clipboard. <p>Alternatively, you can access these right-click options by navigating to the Checks from the Explore section.</p> <p></p>"},{"location":"source-datastore/right-click-options/#right-click-anomalies","title":"Right Click Anomalies","text":"<p>Right-click on the specific anomaly from All, Active, Acknowledged, and Archived within a connected source datastore.</p> <p>A dropdown list of options will appear:</p> <ul> <li> <p>Open in New Tab.</p> </li> <li> <p>Copy Link.</p> </li> <li> <p>Copy ID.</p> </li> <li> <p>Copy Name.</p> </li> </ul> <p></p> No Field Description 1 Open in New Tab Open the selected anomaly in a new browser tab, where you can view its quality score, sampling, completeness, active checks, active anomalies, etc. 2 Copy Link Copy the unique URL of the selected anomaly to your clipboard. 3 Copy ID Copy the unique identifier (ID) of the selected anomaly. 4 Copy Name Copy the name of the selected anomaly to your clipboard. <p>Alternatively, you can access these right-click options by navigating to the Anomalies from the Explore section.</p> <p></p>"},{"location":"source-datastore/scan/","title":"Scan Operation","text":"<p>The Scan Operation in Qualytics is performed on a datastore to enforce data quality checks for various data collections, such as tables, views, and files. It supports centralized configuration through the Datastore Enrichment Settings, where options like the Remediation Strategy, Source Record Limit, and Anomaly Rollup Threshold are defined. While these defaults are applied automatically during a scan, users retain the flexibility to adjust the Source Record Limit and Anomaly Rollup Threshold directly within the scan form. This operation has several key functions:</p> <ul> <li> <p>Record Anomalies: Identifies a single record (row) as anomalous and provides specific details regarding why it is considered anomalous. The simplest form of a record anomaly is a row that lacks an expected value for a field.  </p> </li> <li> <p>Shape Anomalies: Identifies structural issues within a dataset at the column or schema level. It highlights broader patterns or distributions that deviate from expected norms. If a dataset is expected to have certain fields and one or more fields are missing or contain inconsistent patterns, this would be flagged as a shape anomaly.  </p> </li> <li> <p>Anomaly Data Recording: All identified anomalies, along with related analytical data, are recorded in the associated Enrichment Datastore for further examination.</p> </li> </ul> <p>Additionally, the Scan Operation offers flexible options, including the ability to:</p> <ul> <li>Perform checks on incremental loads versus full loads.</li> <li>Limit the number of records scanned.</li> <li>Run scans on a selected list of tables or files.</li> <li>Schedule scans for future execution.</li> </ul> <p>Let's get started! \ud83d\ude80</p>"},{"location":"source-datastore/scan/#navigation-to-scan-operation","title":"Navigation to Scan Operation","text":"<p>Step 1: Select a source datastore from the side menu on which you would like to perform the scan operation.</p> <p></p> <p>Step 2: Clicking on your preferred datastore will navigate you to the datastore details page. Within the overview tab (default view), click on the Run button under Scan to initiate the scan operation.</p> <p></p> <p>Note</p> <p>Scanning operation can be commenced once the catalog operation and profile operation are completed.</p>"},{"location":"source-datastore/scan/#configuration","title":"Configuration","text":"<p>Step 1: Click on the Run button to initiate the scan operation.</p> <p></p> <p>Step 2: Select tables (in your JDBC datastore) or file patterns (in your DFS datastore) and tags you would like to be scanned.</p> <p>Note</p> <p>Scan operation also supports .txt.gz and .csv.gz files in DFS datastores.</p> <p>1. All Tables/File Patterns</p> <p>This option includes all tables or file patterns currently available for scanning in the datastore. It means that every table or file pattern recognized in your datastore will be subjected to the defined data quality checks. Use this when you want to perform a comprehensive scan covering all the available data without any exclusions.</p> <p></p> <p>2. Specific Tables/File Patterns</p> <p>This option allows you to manually select the individual table(s) or file pattern(s) in your datastore to scan. Upon selecting this option, all the tables or file patterns associated with your datastore will be automatically populated, allowing you to select the datasets you want to scan.</p> <p>You can also search the tables/file patterns you want to scan directly using the search bar. Use this option when you need to target particular datasets or when you want to exclude certain files from the scan for focused analysis or testing purposes.</p> <p></p> <p>3. Tag</p> <p>This option enables you to automatically scan file patterns associated with the selected tags. Tags can be predefined or created to categorize and manage file patterns effectively.</p> <p></p> <p>Step 3: Click on the Next button to Configure Select Check Categories.</p> <p></p> <p>Step 4: Configure Select Check Categories Setting</p> <p>Users can choose one or more check categories when initiating a scan. This allows for flexible selection based on the desired scope of the operation:</p> <ul> <li> <p>Metadata: Include checks that define the expected properties of the table, such as volume. It belongs to the Volumetric rule type.</p> </li> <li> <p>Data Integrity: Include checks that specify the expected values for the data stored in the table. It belongs to all rule types except volumetric.</p> </li> </ul> <p></p> <p>Step 5: Click on the Next button to Configure the Read Settings.</p> <p></p> <p>Step 6: Configure Read Settings, Starting Threshold (Optional), and the Record Limit.</p> <ol> <li> <p>Select the Read Strategy for your scan operation.</p> </li> <li> <p>Incremental: This strategy is used to scan only the new or updated records since the last scan operation.  On the initial run, a full scan is conducted unless a specific starting threshold is set. For subsequent scans, only the records that have changed since the last scan are processed. If tables or views do not have a defined incremental key, a full scan will be performed. Ideal for regular scans where only changes need to be tracked, saving time and computational resources.</p> </li> </ol> <p>Note</p> <p>Incremental scans fully support Apache Iceberg table, significantly expanding the range of asset types eligible for incremental scanning operations.</p> <ul> <li>Full: This strategy performs a comprehensive scan of all records within the specified data collections, regardless of any previous changes or scans. Every scan operation will include all records, ensuring a complete check each time. Suitable for periodic comprehensive checks or when incremental scanning is not feasible due to the nature of the data.</li> </ul> <p></p> <p>Warning</p> <p>If any selected tables do not have an incremental identifier, a full scan will be performed for those tables.</p> <p>Info</p> <p>When running an Incremental Scan for the first time, Qualytics automatically performs a full scan, saving the incremental field for subsequent runs.</p> <ul> <li> <p>This ensures that the system establishes a baseline and captures all relevant data.</p> </li> <li> <p>Once the initial full scan is completed, the system intelligently uses the saved incremental field to execute    future Incremental Scans efficiently, focusing only on the new or updated data since the last scan.</p> </li> <li> <p>This approach optimizes the scanning process while maintaining data quality and consistency.</p> </li> </ul> <ol> <li> <p>Define the Starting Threshold (Optional) i.e., specify a minimum incremental identifier value to set a starting point for the scan.</p> </li> <li> <p>Greater Than Time: This option applies only to tables with an incremental timestamp strategy. Users can specify a timestamp to scan records that were modified after this time.</p> </li> <li> <p>Greater Than Batch: This option applies to tables with an incremental batch strategy. Users can set a batch value, ensuring that only records with a batch identifier greater than the specified value are scanned.</p> </li> </ol> <p></p> <ol> <li>Define the Record Limit - the maximum number of records to be scanned per table after any initial filtering. This is a crucial feature for managing large datasets.</li> </ol> <p></p> <p>You can manually enter a custom value in the text field or quickly select from a dropdown menu with commonly used limits such as 1M, 10M, 100M, and All.</p> <p></p> <p>Note</p> <p>The number of records must be between 1 and 1,000,000,000.</p> <p>Step 7: Click on the Next button to Configure the Scan Settings.</p> <p></p> <p>Step 8: Configure the Scan Settings.</p> <p>1. Anomaly Options: Manage duplicate anomalies efficiently by archiving duplicates or reactivating recurring ones. These settings help streamline anomaly tracking and maintain data accuracy.</p> <ul> <li> <p>Archive Duplicate Anomalies: Automatically archive duplicate anomalies from previous scans that overlap with the current scan to enhance data management efficiency.</p> </li> <li> <p>Reactivate Recurring Anomalies: Enabling Reactivate Recurring Anomalies marks new anomalies as duplicates of archived ones, reactivates the original anomaly, and creates a Fingerprint column in the Enrichment Datastore.</p> </li> </ul> <p></p> <p>2. Maximum Record Anomalies per Check: Set the maximum number of anomalies generated per check before they are merged into a single rolled-up anomaly. This helps manage anomaly volume and simplifies review.</p> <p></p> <p>3. Maximum Source Examples per Anomaly: This setting decides how many source records are kept for each anomaly. When the scan runs and finds an anomaly, only the specified number of records are kept. These are the only records you can view or download later.</p> <p></p> <p>For example, if this value is set to 10, only 10 source records per anomaly will be kept, even if more records caused the anomaly.</p> <p>If you need to download more records, increase this value before running the scan. Changes made after the scan finishes will not affect the results.</p> <p></p>"},{"location":"source-datastore/scan/#run-instantly","title":"Run Instantly","text":"<p>Click on the Run Now button to perform the scan operation immediately.</p> <p></p>"},{"location":"source-datastore/scan/#schedule","title":"Schedule","text":"<p>Step 1: Click on the Schedule button to configure the available schedule options for your scan operation.</p> <p></p> <p>Step 2: Set the scheduling preferences for the scan operation.</p> <p>1. Hourly: This option allows you to schedule the scan to run every hour at a specified minute. You can define the frequency in hours and the exact minute within the hour the scan should start. Example: If set to Every 1 hour(s) on minute 0, the scan will run every hour at the top of the hour (e.g., 1:00, 2:00, 3:00).</p> <p></p> <p>2. Daily: This option schedules the scan to run once every day at a specific time. You specify the number of days between scans and the exact time of day in UTC. Example: If set to Every 1 day(s) at 00:00 UTC, the scan will run every day at midnight UTC.</p> <p></p> <p>3. Weekly: This option schedules the scan to run on specific days of the week at a set time. You select the days of the week and the exact time of day in UTC for the scan to run. Example: If configured to run on \"Sunday\" and \"Friday\" at 00:00 UTC, the scan will execute at midnight UTC on these days.</p> <p></p> <p>4. Monthly: This option schedules the scan to run once a month on a specific day at a set time. You specify the day of the month and the time of day in UTC. If set to \"On the 1st day of every 1 month(s), at 00:00 UTC,\" the scan will run on the first day of each month at midnight UTC.</p> <p></p> <p>5. Advanced: The advanced section for scheduling operations allows users to set up more complex and custom scheduling using Cron expressions. This option is particularly useful for defining specific times and intervals for scan operations with precision.</p> <p>Cron expressions are a powerful and flexible way to schedule tasks. They use a syntax that specifies the exact timing of the task based on five fields:</p> <ul> <li>Minute (0 - 59)</li> <li>Hour (0 - 23)</li> <li>Day of the month (1 - 31)</li> <li>Month (1 - 12)  </li> <li>Day of the week (0 - 6) (Sunday to Saturday)</li> </ul> <p>Each field can be defined using specific values, ranges, or special characters to create the desired schedule.</p> <p>Example: The Cron expression <code>0 0 * * *</code> schedules the scan operation to run at midnight (00:00) every day. Here\u2019s a breakdown of this expression:</p> <ul> <li>0 (Minute) - The task will run at the 0th minute.</li> <li>0 (Hour) - The task will run at the 0th hour (midnight).</li> <li>*(Day of the month) - The task will run every day of the month.</li> <li>*(Month) - The task will run every month.</li> <li>*(Day of the week) - The task will run every day of the week.</li> </ul> <p>Users can define other specific schedules by adjusting the Cron expression. For example:</p> <ul> <li>0 12 * * 1-5 - Runs at 12:00 PM from Monday to Friday.</li> <li>30 14 1 * * - Runs at 2:30 PM on the first day of every month.</li> <li>0 22 * * 6 - Runs at 10:00 PM every Saturday.</li> </ul> <p>To define a custom schedule, enter the appropriate Cron expression in the \"Custom Cron Schedule (UTC)\" field before specifying the schedule name. This will allow for precise control over the timing of the scan operation, ensuring it runs exactly when needed according to your specific requirements.</p> <p></p> <p>Step 3: Define the Schedule Name to identify the scheduled operation at the running time.</p> <p></p> <p>Step 4: Click on the Schedule button to schedule your scan operation.</p> <p></p> <p>Note</p> <p>You will receive a notification when the scan operation is completed.</p>"},{"location":"source-datastore/scan/#advanced-options","title":"Advanced Options","text":"<p>The advanced use cases described below require options that are not yet exposed in our user interface but possible through interaction with the Qualytics API.</p>"},{"location":"source-datastore/scan/#runtime-variable-assignment","title":"Runtime Variable Assignment","text":"<p>It is possible to reference a variable in a check definition (declared in double curly braces) and then assign that variable a value when a Scan operation is initiated. Variables are supported within any Spark SQL expression and are most commonly used in a check filter.</p> <p>If a Scan is meant to assert a check with a variable, a value for that variable must be supplied as part of the Scan operation's <code>check_variables</code> property.</p> <p>When using a variable inside a filter, the filter must be a valid Spark SQL WHERE expression.  </p> <p>For example, a check might include a filter:</p> <pre><code>transaction_date = {{ checked_date }}\n</code></pre> <p>For the Scan operation payload, users must apply explicit casting inside the <code>check_variables</code> section. Since variables may represent different data types (integer, string, timestamp, etc.), each variable must be cast to the correct type to avoid parsing or evaluation errors.</p> <p>In this case, that value would be assigned by passing the following payload when calling <code>/api/operations/run</code>:</p> <pre><code>{\n    \"type\": \"scan\",\n    \"datastore_id\": 42,\n    \"container_names\": [\"my_container\"],\n    \"incremental\": true,\n    \"remediation\": \"none\",\n    \"max_records_analyzed_per_partition\": 0,\n    \"check_variables\": {\n        \"checked_date\": \"TIMESTAMP '2023-10-15'\"\n    },\n    \"high_count_rollup_threshold\": 10\n}\n</code></pre>"},{"location":"source-datastore/scan/#operations-insights","title":"Operations Insights","text":"<p>When the scan operation is completed, you will receive the notification and can navigate to the Activity tab for the datastore on which you triggered the Scan Operation and learn about the scan results.</p>"},{"location":"source-datastore/scan/#top-panel","title":"Top Panel","text":"<p>1. Runs (Default View): Provides insights into the operations that have been performed.</p> <p>2. Schedule: Provides insights into the scheduled operations.</p> <p>3. Search: Search for any operation (including scan) by entering the operation ID.</p> <p>4. Sort by: Organize the list of operations based on the Created Date or the Duration.</p> <p>5. Filter: Narrow down the list of operations based on:</p> <ul> <li>Operation Type</li> <li>Operation Status</li> <li>Table</li> </ul> <p></p>"},{"location":"source-datastore/scan/#activity-heatmap","title":"Activity Heatmap","text":"<p>The activity heatmap shown in the snippet below represents activity levels over a period, with each square indicating a day and the color intensity representing the number of operations or activities on that day. It is useful in tracking the number of operations performed on each day within a specific timeframe.</p> <p>Tip</p> <p>You can click on any of the squares from the Activity Heatmap to filter operations.</p> <p></p>"},{"location":"source-datastore/scan/#operation-detail","title":"Operation Detail","text":""},{"location":"source-datastore/scan/#running","title":"Running","text":"<p>This status indicates that the scan operation is still running at the moment and is yet to be completed. A scan operation having a running status reflects the following details and actions:</p> <p></p> No. Parameter Interpretation 1 Operation ID and Type Unique identifier and type of operation performed (catalog, profile, or scan). 2 Timestamp Timestamp when the operation was started. 3 Progress Bar The progress of the operation. 4 Triggered By The author who triggered the operation. 5 Schedule Indicates whether the operation was scheduled or not. 6 Incremental Field Indicates whether Incremental was enabled or disabled in the operation. 7 Remediation Indicates whether Remediation was enabled or disabled in the operation. 8 Anomalies Identified Provides a count of the number of anomalies detected during the running operation. 9 Read Record Limit Defines the maximum number of records to be scanned per table after initial filtering. 10 Check Categories Indicates which categories should be included in the scan (e.g., Metadata, Data Integrity). 11 Archive Duplicate Anomalies Indicates whether Archive Duplicate Anomalies was enabled or disabled in the operation. 12 Reactivate Recurring Anomalies Indicates whether previously detected anomalies that reappear in subsequent scans will be reactivated. 13 Source Record Limit Indicates the limit on records stored in the enrichment datastore for each detected anomaly. 14 Anomaly Rollup Threshold Number of anomalies grouped together for rollup reporting. 15 Results View the details of the ongoing scan operation. This includes information on which tables are currently being scanned, the anomalies identified so far (if any), and other related data collected during the active scan. 16 Abort The Abort button enables you to stop the ongoing scan operation. 17 Summary The summary section provides an overview of the scan operation in progress. It includes:  <ul><li> Tables Requested: The total number of tables that were scheduled for scanning. Click on the adjacent magnifying glass icon to view the tables requested. </li><li> Tables Scanned: The number of tables that have been scanned so far. Click on the adjacent magnifying glass icon to view the tables scanned. </li><li> Partitions Scanned: The number of partitions scanned during the ongoing operation.</li><li> Records Scanned: The total number of records processed up to this point. </li><li> Anomalies Identified: The total number of detected anomalies, with a breakdown of open and archived ones. </li></ul>"},{"location":"source-datastore/scan/#aborted","title":"Aborted","text":"<p>This status indicates that the scan operation was manually stopped before it could be completed. A scan operation having an aborted status reflects the following details and actions:</p> <p></p> No. Parameter Interpretation 1 Operation ID and Type Unique identifier and type of operation performed (catalog, profile, or scan). 2 Timestamp Timestamp when the operation was started 3 Progress Bar The progress of the operation 4 Aborted By The author who triggered the operation 5 Schedule Whether the operation was scheduled or not 6 Incremental Field Indicates whether Incremental was enabled or disabled in the operation 7 Remediation Indicates whether Remediation was enabled or disabled in the operation 8 Anomalies Identified Provides a count on the number of anomalies detected before the operation was aborted 9 Read Record Limit Defines the maximum number of records to be scanned per table after initial filtering 10 Check Categories Indicates which categories should be included in the scan (Metadata, Data Integrity) 11 Archive Duplicate Anomalies Indicates whether Archive Duplicate Anomalies was enabled or disabled in the operation 12 Reactivate Recurring Anomalies Indicates whether previously detected anomalies that reappear in subsequent scans will be reactivated. 13 Source Record Limit Indicates the limit on records stored in the enrichment datastore for each detected anomaly 14 Anomaly Rollup Threshold Number of anomalies grouped together for rollup reporting. 15 Results View the details of the scan operation that was aborted, including tables scanned and anomalies identified 16 Resume Provides an option to continue the scan operation from where it left off 17 Rerun The \"Rerun\" button allows you to start a new scan operation using the same settings as the aborted scan 18 Delete Removes the record of the aborted scan operation from the system, permanently deleting scan results and anomalies 19 Summary The summary section provides an overview of the scan operation up to the point it was aborted. It includes:  <ul><li> Tables Requested: The total number of tables that were scheduled for scanning. Click on the adjacent magnifying glass icon to view the tables requested. </li><li> Tables Scanned: The number of tables that have been scanned so far. Click on the adjacent magnifying glass icon to view the tables scanned. </li><li> Partitions Scanned: The number of partitions scanned before the operation was aborted. </li><li> Records Scanned: The total number of records processed before the scan was stopped. </li><li> Anomalies Identified: The total number of detected anomalies, with a breakdown of open and archived ones. </li></ul>"},{"location":"source-datastore/scan/#warning","title":"Warning","text":"<p>This status signals that the scan operation encountered some issues and displays the logs that facilitate improved tracking of the blockers and issue resolution. A scan operation having a completed with warning status reflects the following details and actions:</p> <p></p> No. Parameter Interpretation 1 Operation ID and Type Unique identifier and type of operation performed (catalog, profile, or scan). 2 Timestamp Timestamp when the operation was started 3 Progress Bar The progress of the operation 4 Triggered By The author who triggered the operation 5 Schedule Whether the operation was scheduled or not 6 Incremental Field Indicates whether Incremental was enabled or disabled in the operation 7 Remediation Indicates whether Remediation was enabled or disabled in the operation 8 Anomalies Identified Provides a count on the number of anomalies detected before the operation was warned. 9 Read Record Limit Defines the maximum number of records to be scanned per table after initial filtering 10 Check Categories Indicates which categories should be included in the scan (Metadata, Data Integrity) 11 Archive Duplicate Anomalies Indicates whether Archive Duplicate Anomalies was enabled or disabled in the operation 12 Source Record Limit Indicates the limit on records stored in the enrichment datastore for each detected anomaly 13 Anomaly Rollup Threshold Number of anomalies grouped together for rollup reporting. 14 Result View the details of the scan operation that was completed with warning, including tables scanned and anomalies identified 15 Rerun The \"Rerun\" button allows you to start a new scan operation using the same settings as the warning scan 16 Delete Removes the record of the warning operation from the system, permanently deleting scan results and anomalies 17 Summary The summary section provides an overview of the scan operation, highlighting any warnings encountered. It includes: <ul><li> Tables Requested: The total number of tables that were scheduled for scanning. Click on the adjacent magnifying glass icon to view the tables requested. </li><li> Tables Scanned: The number of tables that have been scanned so far. Click on the adjacent magnifying glass icon to view the tables scanned. </li><li> Partitions Scanned: The number of partitions scanned during the operation, including any partitions that triggered warnings. </li><li> Records Scanned: The total number of records processed during the scan, along with any records that raised warnings. </li><li> Anomalies Identified: The total number of detected anomalies, with a breakdown of open and archived ones. </li></ul> 18 Logs Logs include error messages, warnings, and other pertinent information that occurred during the execution of the Scan Operation."},{"location":"source-datastore/scan/#success","title":"Success","text":"<p>The summary section provides an overview of the scan operation upon successful completion. It includes:</p> <p></p> No. Parameter Interpretation 1 Operation ID and Type Unique identifier and type of operation performed (catalog, profile, or scan). 2 Timestamp Timestamp when the operation was started 3 Progress Bar The progress of the operation 4 Triggered By The author who triggered the operation 5 Schedule Whether the operation was scheduled or not 6 Incremental Field Indicates whether Incremental was enabled or disabled in the operation 7 Remediation Indicates whether Remediation was enabled or disabled in the operation 8 Anomalies Identified Provides a count of the number of anomalies detected during the successful completion of the operation. 9 Read Record Limit Defines the maximum number of records to be scanned per table after initial filtering 10 Check Categories Indicates which categories should be included in the scan (Metadata, Data Integrity) 11 Archive Duplicate Anomalies Indicates whether Archive Duplicate Anomalies was enabled or disabled in the operation 12 Source Record Limit Indicates the limit on records stored in the enrichment datastore for each detected anomaly 13 Anomaly Rollup Threshold Number of anomalies grouped together for rollup reporting. 14 Results View the details of the completed scan operation. This includes information on which tables were scanned, the anomalies identified (if any), and other relevant data collected throughout the successful completion of the scan. 15 Rerun The \"Rerun\" button allows you to start a new scan operation using the same settings as the success scan 16 Delete Removes the record of the aborted scan operation from the system, permanently deleting scan results and anomalies 17 Summary The summary section provides an overview of the scan operation upon successful completion. It includes: <ul><li> Tables Requested: The total number of tables that were scheduled for scanning. Click on the adjacent magnifying glass icon to view the tables requested. </li><li> Tables Scanned: The number of tables that have been scanned successfully. Click on the adjacent magnifying glass icon to view the tables scanned. </li><li> Partitions Scanned: The number of partitions scanned. </li><li> Records Scanned: The total number of records processed.</li><li> Anomalies Identified: The total number of detected anomalies, with a breakdown of open and archived ones. </li></ul>"},{"location":"source-datastore/scan/#full-view-of-metrics-in-operation-summary","title":"Full View of Metrics in Operation Summary","text":"<p>Users can now hover over abbreviated metrics to see the full value for better clarity. For demonstration purposes, we are hovering over the Records Scanned field to display the full value.</p> <p></p>"},{"location":"source-datastore/scan/#post-operation-details","title":"Post Operation Details","text":"<p>Step 1: Click on any of the successful Scan Operations from the list and hit the Results button.</p> <p></p> <p>Step 2: The Scan Results modal demonstrates the highlighted anomalies (if any) identified in your datastore with the following properties:</p> <p></p> Ref. Scan Properties Description 1. Table/File The table or file where the anomaly is found. 2. Field The field(s) where the anomaly is present. 3. Location Fully qualified location of the anomaly. 4. Rule Inferred and authored checks that failed assertions. 5. Description Human-readable, auto-generated description of the anomaly. 6. Status The status of the anomaly: Active, Acknowledged, Resolved, or Invalid. 7. Type The type of anomaly (e.g., Record or Shape) 8. Date time The date and time when the anomaly was found. <p>Step 3: By clicking the dropdown button next to the All button, you can filter anomalies based on their status.</p> <p></p>"},{"location":"source-datastore/scan/#api-payload-examples","title":"API Payload Examples","text":"<p>This section provides payload examples for running, scheduling, and checking the status of scan operations. Replace the placeholder values with data specific to your setup.</p>"},{"location":"source-datastore/scan/#running-a-scan-operation","title":"Running a Scan operation","text":"<p>To run a scan operation, use the API payload example below and replace the placeholder values with your specific values.</p>"},{"location":"source-datastore/scan/#endpoint-post","title":"Endpoint (Post):","text":"<p><code>/api/operations/run (post)</code></p> Option I: Running a scan operation of all containersOption II: Running a scan operation of specific containers <ul> <li>container_names: <code>[]</code> means that it will scan all containers.</li> <li>max_records_analyzed_per_partition: <code>null</code> means that it will scan all records of all containers.</li> <li>Remediation: <code>append</code> replicates source containers using an append-first strategy.</li> </ul> <pre><code>{\n    \"type\":\"scan\",\n    \"name\":null,\n    \"datastore_id\": datastore-id,\n    \"container_names\":[],\n    \"remediation\":\"append\",\n    \"incremental\":false,\n    \"max_records_analyzed_per_partition\":null,\n    \"enrichment_source_record_limit\":10\n}\n</code></pre> <ul> <li>container_names: <code>[\"table_name_1\", \"table_name_2\"]</code> means that it will scan only the tables table_name_1 and table_name_2.</li> <li>max_records_analyzed_per_partition: <code>1000000</code> means that it will scan a maximum of 1 million records per partition.  </li> <li>Remediation: <code>overwrite</code> replicates source containers using an overwrite strategy.</li> </ul> <pre><code>{\n    \"type\":\"scan\",\n    \"name\":null,\n    \"datastore_id\":datastore-id,\n    \"container_names\":[\n      \"table_name_1\",\n      \"table_name_2\"\n    ],\n    \"max_records_analyzed_per_partition\":1000000,\n    \"enrichment_source_record_limit\":10\n}\n</code></pre>"},{"location":"source-datastore/scan/#scheduling-scan-operation-of-all-containers","title":"Scheduling scan operation of all containers","text":"<p>To schedule a scan operation, use the API payload example below and replace the placeholder values with your specific values.</p>"},{"location":"source-datastore/scan/#endpoint-post_1","title":"Endpoint (Post):","text":"<p><code>/api/operations/schedule (post)</code></p> <p>This payload is to run a scheduled scan operation every day at 00:00</p> Scheduling scan operation of all containers <pre><code>{\n    \"type\":\"scan\",\n    \"name\":\"My scheduled Scan operation\",\n    \"datastore_id\":\"datastore-id\",\n    \"container_names\":[],\n    \"remediation\": \"overwrite\",\n    \"incremental\": false,\n    \"max_records_analyzed_per_partition\":null,\n    \"enrichment_source_record_limit\":10,\n    \"crontab\":\"00 00 */2 * *\"\n}\n</code></pre>"},{"location":"source-datastore/scan/#retrieving-scan-operation-information","title":"Retrieving Scan Operation Information","text":""},{"location":"source-datastore/scan/#endpoint-get","title":"Endpoint (Get)","text":"<p><code>/api/operations/{id} (get)</code></p> Example result response <pre><code>{\n    \"items\": [\n        {\n            \"id\": 12345,\n            \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n            \"type\": \"scan\",\n            \"start_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n            \"end_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n            \"result\": \"success\",\n            \"message\": null,\n            \"triggered_by\": \"user@example.com\",\n            \"datastore\": {\n                \"id\": 101,\n                \"name\": \"Datastore-Sample\",\n                \"store_type\": \"jdbc\",\n                \"type\": \"db_type\",\n                \"enrich_only\": false,\n                \"enrich_container_prefix\": \"data_prefix\",\n                \"favorite\": false\n            },\n            \"schedule\": null,\n            \"incremental\": false,\n            \"remediation\": \"none\",\n            \"max_records_analyzed_per_partition\": -1,\n            \"greater_than_time\": null,\n            \"greater_than_batch\": null,\n            \"high_count_rollup_threshold\": 10,\n            \"enrichment_source_record_limit\": 10,\n            \"status\": {\n                \"total_containers\": 2,\n                \"containers_analyzed\": 2,\n                \"partitions_scanned\": 2,\n                \"records_processed\": 28,\n                \"anomalies_identified\": 2\n            },\n            \"containers\": [\n                {\n                \"id\": 234,\n                \"name\": \"Container1\",\n                \"container_type\": \"table\",\n                \"table_type\": \"table\"\n                },\n                {\n                \"id\": 235,\n                \"name\": \"Container2\",\n                \"container_type\": \"table\",\n                \"table_type\": \"table\"\n                }\n            ],\n            \"container_scans\": [\n                {\n                \"id\": 456,\n                \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                \"container\": {\n                    \"id\": 235,\n                    \"name\": \"Container2\",\n                    \"container_type\": \"table\",\n                    \"table_type\": \"table\"\n                },\n                \"start_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                \"end_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                \"records_processed\": 8,\n                \"anomaly_count\": 1,\n                \"result\": \"success\",\n                \"message\": null\n                },\n                {\n                \"id\": 457,\n                \"created\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                \"container\": {\n                    \"id\": 234,\n                    \"name\": \"Container1\",\n                    \"container_type\": \"table\",\n                    \"table_type\": \"table\"\n                },\n                \"start_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                \"end_time\": \"YYYY-MM-DDTHH:MM:SS.ssssssZ\",\n                \"records_processed\": 20,\n                \"anomaly_count\": 1,\n                \"result\": \"success\",\n                \"message\": null\n                }\n            ],\n            \"tags\": []\n        }\n    ],\n    \"total\": 1,\n    \"page\": 1,\n    \"size\": 50,\n    \"pages\": 1\n}\n</code></pre>"},{"location":"source-datastore/scan/#troubleshooting","title":"Troubleshooting","text":""},{"location":"source-datastore/scan/#unloadable-container-error","title":"Unloadable Container Error","text":"<p>When running a scan operation, you may encounter the following error for specific containers:</p> <pre><code>Container '&lt;CONTAINER_NAME&gt;' is marked as Unloadable. No attempt was made to load the container due to multiple consecutive failures in prior operations.\n</code></pre> <p>Cause: This error occurs when a container (table, view, or file pattern) has experienced multiple consecutive failures during prior scan operations. To prevent repeated failed attempts and optimize performance, Qualytics marks the container as \"Unloadable\" and skips it in subsequent operations.</p> <p>Resolution: To resolve this issue and re-enable the container for scanning:</p> <ol> <li>Navigate to your source datastore</li> <li>Run a Catalog Operation on the datastore</li> <li>Once the catalog operation completes successfully, the container status will be reset</li> <li>Run the scan operation again - the previously unloadable container should now be processed normally</li> </ol> <p>Tip</p> <p>If the container continues to fail after re-cataloging, investigate the underlying cause of the failures. Common causes include:</p> <ul> <li>Permission issues accessing the container</li> <li>Schema changes that invalidate existing configurations</li> <li>Network connectivity problems to the data source</li> <li>Resource constraints or timeouts during data loading</li> </ul>"},{"location":"source-datastore/settings-overview/","title":"Settings Overview","text":"<p>Qualytics allows you to manage your datastore efficiently by editing source datastore information, linking an enrichment datastore for enhanced insights, establishing new connections to expand data sources, choosing connectors to integrate diverse data, adjusting the quality score to ensure data accuracy, and deleting the store. This ensures flexibility and control over your data management processes within the platform.</p> <p>Let's get started \ud83d\ude80</p>"},{"location":"source-datastore/settings-overview/#navigation","title":"Navigation","text":"<p>Step 1: Select a source datastore from the side menu for which you would like to manage the settings.</p> <p></p> <p>Step 2: Click on the Settings icon from the top right window. A drop-down menu will appear with the following options:</p> <ol> <li>Edit  </li> <li>Enrichment  </li> <li>Score  </li> <li>Delete</li> </ol> <p></p>"},{"location":"source-datastore/settings-overview/#edit-datastore","title":"Edit Datastore","text":"<p>Use the Edit Datastore option to make changes to your datastore\u2019s connection setup whenever updates are needed.</p> <p>For more information on how to edit the datastore, please refer to the Edit Datastore documentation.</p>"},{"location":"source-datastore/settings-overview/#link-enrichment","title":"Link Enrichment","text":"<p>An enrichment datastore is a database used to enhance your existing data by adding additional, relevant information. This helps you to provide more comprehensive insight into data and improve data accuracy.</p> <p>You have the option to link an enrichment datastore to your existing source datastore. However, some datastores cannot be linked as enrichment datastores. For example, Oracle, Athena, and Timescale cannot be used for this purpose.</p> <p>For more information on how to link an enrichment datastore, please refer to the Link Enrichment documentation.</p>"},{"location":"source-datastore/settings-overview/#unlink-enrichment","title":"Unlink Enrichment","text":"<p>You can remove the connection between the source datastore and the enrichment datastore using the Unlink Enrichment Datastore option. This action stops the enrichment process and ensures that no further data is enhanced using the unlinked datastore.</p> <p>For more information on how to unlink an enrichment datastore, please refer to the Unlink Enrichment documentation. </p>"},{"location":"source-datastore/settings-overview/#quality-score-settings","title":"Quality Score Settings","text":"<p>Use the Quality Score Settings option to adjust factor weights and decay period, aligning data quality scoring with your organization\u2019s needs.</p> <p>For more information on the quality score, please refer to the Quality Score documentation.</p>"},{"location":"source-datastore/settings-overview/#delete-datastore","title":"Delete Datastore","text":"<p>Use the Delete Datastore option to remove outdated or unused datastores along with their related configurations, helping keep your workspace organized.</p> <p>For more information on how to delete a datastore, please refer to the Delete Datastore documentation.</p>"},{"location":"source-datastore/unlink-enrichment/","title":"Unlink Enrichment Datastore","text":"<p>Step 1: Click on the Enrichment from the drop-down list.</p> <p></p> <p>A modal window titled Enrichment Datastore Settings will appear, displaying configuration options for the linked enrichment datastore.</p> <p></p> <p>Step 2: Click the Unlink Enrichment Datastore option (represented by unlink icon) located on the right side of the Details section to remove the linked enrichment datastore.</p> <p></p> <p>A modal window titled Unlink Enrichment Datastore will appear.</p> <p></p> <p>Step 3: Click the Unlink button to remove the enrichment datastore connection.</p> <p></p> <p>After clicking the Unlink button, a success message confirms that the datastore has been updated successfully.</p>"},{"location":"tags/add-tag/","title":"Add Tag","text":"<p>Step 1: Click on the Add Tag button from the top right corner.</p> <p></p> <p>Step 2: A modal window will appear, providing the options to create the tag. Enter the required values to get started. </p> REF. FIELD ACTION EXAMPLE 1. Preview This shows how the tag will appear to users. Preview 2. Name Assign a name to your tag. Sensitive 3. Color A color picker feature is provided, allowing you to select a color using its hex code. #E74C3C 4. Description Explain the nature of your tag. Maintain data that is highly confidential and requires strict access controls. 5. Category Choose an existing category or create a new one to group related tags for easier organization. Demo2 6. Weight Modifier Adjust the tag's weight for prioritization, where a higher value represents greater significance. The range is between -10 and 10. \ud83d\udca1 Tip: Increasing the value boosts the weight of a recipient and its children by the same amount, while negative values decrease it. 10 <p></p> <p>Step 3: Click on the Save button to save your tag.</p> <p></p> <p>After clicking the Save button, the tag will be added to the system and a success message will appear.</p>"},{"location":"tags/add-tag/#view-created-tags","title":"View Created Tags","text":"<p>Once you have created a tag, you can view it in the tags list.</p> <p></p>"},{"location":"tags/applying-a-tag/","title":"Applying a Tag","text":"<p>Once a Tag is created, it's ready to be associated with a <code>Datastore</code>, <code>Profile</code>, <code>Check</code>, <code>Notification</code> and ultimately an <code>Anomaly</code>.</p>"},{"location":"tags/applying-a-tag/#tag-inheritance","title":"Tag Inheritance","text":"<ul> <li> <p>When a <code>Tag</code> is applied to a data asset, all the descendants of that data asset also receive the <code>Tag</code>.</p> <ul> <li>For example, if a <code>Tag</code> named Critical is applied to a Datastore then all the Tables, Fields, and Checks under that Datastore also receive the <code>Tag</code>.</li> </ul> <p>Note</p> <p>Anomalies will inherit the tags if a scan has been run.</p> </li> <li> <p>Likewise, if the Critical <code>Tag</code> is subsequently removed from one of the Tables in that Datastore, then all the Fields and Checks belonging to that Table will have the Critical <code>Tag</code> removed as well.</p> </li> <li> <p>When a new data asset is created, it inherits the <code>Tags</code> from the owning data asset. For example, if a user creates a new Computed Table, it inherits all the <code>Tags</code> that are applied to the Datastore in which it is created.</p> </li> </ul>"},{"location":"tags/applying-a-tag/#tagging-anomalies","title":"Tagging Anomalies","text":"<ul> <li> <p>Anomalies also inherit <code>Tags</code> at the time they are created. They inherit all the <code>Tags</code> of all the associated failed checks.</p> </li> <li> <p>Thus Anomalies do not inherit subsequent tag changes from those checks. They only inherit tags one time - at creation time.</p> </li> <li> <p><code>Tags</code> can be directly applied to or removed from Anomalies at any time after creation.</p> </li> </ul>"},{"location":"tags/delete-tag/","title":"Delete Tags","text":"<p>This allows you to remove outdated or unnecessary tags to maintain a clean and efficient tag system.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the tag that you want to delete, then click on Delete from the dropdown menu.</p> <p>Warning</p> <p>Deleting a tag is permanent and cannot be undone.</p> <p></p> <p>Step 2: After clicking the Delete button, your tag will be removed from the system, and a success message will appear.</p>"},{"location":"tags/edit-tag/","title":"Edit Tags","text":"<p>This allows you to keep your tags updated with current information and relevance.</p> <p>Step 1: Click the vertical ellipsis (\u22ee) next to the tag that you want to edit, then click on Edit from the dropdown menu.</p> <p></p> <p>Step 2: Edit the tag's name, color, description, category and weight as needed.</p> <p></p> <p>Step 3: Click the Save button to apply your changes.</p> <p></p> <p>Step 4: After clicking the Save button, a success message will appear.</p>"},{"location":"tags/external-tag/","title":"External Tags","text":"<p>External Tags in Qualytics make it easy to keep your data catalog and Qualytics in sync \u2014 so you never have to create the same tags twice.</p> <p>They automatically bring in tags (like Customer Data, Finance, PII, etc.) from platforms such as Atlan or Alation, and apply them to the same data assets inside Qualytics.</p>"},{"location":"tags/external-tag/#what-are-external-tags","title":"What Are External Tags?","text":"<p>External Tags are tags that come from an external data catalog, not from Qualytics itself. They\u2019re read-only, meaning you can view and use them in Qualytics, but can\u2019t edit or delete them there.</p> <p>Instead, they stay connected to your catalog \u2014 whenever your catalog updates, Qualytics updates too.</p>"},{"location":"tags/external-tag/#example","title":"Example","text":"<p>Let\u2019s say your team uses Atlan to tag assets with labels like:</p> <ul> <li>Finance Data \u2013 for tables related to financial reports  </li> <li>Customer Data \u2013 for customer information  </li> <li>PII \u2013 for sensitive or personally identifiable data  </li> </ul> <p>Once Atlan is integrated, Qualytics will automatically import these same tags and show them as External Tags. You\u2019ll see them in the tag list, filters, and on relevant assets \u2014 always in sync with your catalog.</p>"},{"location":"tags/external-tag/#why-external-tags-matter","title":"Why External Tags Matter","text":"<p>Without External Tags, teams often have to create the same tags separately in Qualytics \u2014 leading to confusion and duplicate work. With External Tags, you get:</p> <ul> <li>Consistency \u2014 the same tags appear across Atlan, Alation, and Qualytics  </li> <li>Automatic sync \u2014 no need to manually recreate or update tags  </li> <li>Unified visibility \u2014 see data quality insights for each tag  </li> <li>Governance alignment \u2014 your existing tagging standards stay intact  </li> </ul> <p>In short, External Tags help Qualytics \u201cspeak the same language\u201d as your data catalog.</p>"},{"location":"tags/external-tag/#how-it-works-simple-view","title":"How It Works (Simple View)","text":"Step What Happens 1 You connect Atlan, Alation, or another supported catalog to Qualytics under Settings \u2192 Integrations. 2 Qualytics uses a secure API connection to read your catalog\u2019s tags. 3 Those tags appear inside Qualytics automatically as External Tags. 4 Whenever a tag or asset updates in your catalog, Qualytics syncs those changes. 5 You can filter, sort, or view these tags \u2014 but editing happens in the catalog itself."},{"location":"tags/external-tag/#real-life-example","title":"Real-Life Example","text":"<p>Imagine your company runs both a data catalog (Atlan) and a data quality platform (Qualytics).</p> <ul> <li> <p>In Atlan, your governance team adds a tag called \u201cSensitive Data\u201d to all customer-related tables. </p> </li> <li> <p>When synced, Qualytics automatically imports this tag and marks it as External.  </p> </li> <li> <p>Now, your data quality team can filter all anomalies or checks by \u201cSensitive Data\u201d \u2014 without ever tagging them manually. </p> </li> <li> <p>If the governance team later renames the tag to \u201cConfidential Data\u201d, Qualytics updates automatically.</p> </li> </ul> <p>This creates a single, reliable view of your data health \u2014 using the same tags everywhere.</p>"},{"location":"tags/external-tag/#use-cases","title":"Use Cases","text":"<p>Here are a few practical examples of how teams use External Tags:</p>"},{"location":"tags/external-tag/#governance-teams","title":"Governance Teams","text":"<p>Use External Tags to track which datasets are PII, Customer Data, or Financial Records \u2014 without managing tags in multiple places.</p>"},{"location":"tags/external-tag/#data-engineers","title":"Data Engineers","text":"<p>Quickly filter quality checks in Qualytics by tags synced from your catalog \u2014 for example, view all Finance datasets with active anomalies.</p>"},{"location":"tags/external-tag/#compliance-risk-teams","title":"Compliance &amp; Risk Teams","text":"<p>Easily identify sensitive assets tagged as GDPR or Confidential and monitor their data quality.</p>"},{"location":"tags/external-tag/#business-analysts","title":"Business Analysts","text":"<p>Filter dashboards and reports by External Tags like Sales, Marketing, or Customer Behavior to analyze data quality in context.</p>"},{"location":"tags/external-tag/#example-in-action","title":"Example in Action","text":"<p>Let\u2019s take a real example:</p> <ol> <li>In Atlan, a governance user tags three tables as Finance.  </li> <li>Qualytics syncs with Atlan \u2192 those three tables now show Finance (External) tags in Qualytics.  </li> <li>When viewing anomalies in Qualytics, you can filter by Finance to check if those datasets have open data quality issues.  </li> <li>The moment the tag changes in Atlan, Qualytics updates it automatically \u2014 no manual work needed.</li> </ol>"},{"location":"tags/external-tag/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>External Tags come from platforms like Atlan or Alation.  </li> <li>They are read-only in Qualytics but automatically stay up-to-date.  </li> <li>You can filter, view, and analyze your data quality using these tags.  </li> <li>All updates happen through integration \u2014 keeping catalog and Qualytics perfectly aligned.  </li> </ul>"},{"location":"tags/filter-and-sort/","title":"Filter and Sort","text":"<p>Qualytics allows you to sort and filter your tags so that you can easily organize and find the most relevant tags according to your criteria, improving data management and workflow efficiency.</p>"},{"location":"tags/filter-and-sort/#sort","title":"Sort","text":"<p>You can sort your tags by Category, Color, Created Date, Name, and Weight to easily organize and prioritize them according to your needs.</p> <p></p> No. Sort Option Description 1 Category Sort tags based on their assigned category. 2 Color Sort tags according to their color label. 3 Created Date Sort tags by the date they were created. 4 Name Sort tags alphabetically by name. 5 Weight Sort tags based on their assigned weight value. <p>Whatever sorting option is selected, you can arrange the data either in ascending or descending order by clicking the caret button next to the selected sorting criteria.</p> <p></p>"},{"location":"tags/filter-and-sort/#filter","title":"Filter","text":"<p>You can filter your tags by type and category, which allows you to categorize and manage them more effectively. </p> <p>Info</p> <p>Users can search across all filter inputs using typos, partial terms, or abbreviations. The system intelligently matches relevant results, making it easier to find what they need without exact inputs.</p> <p></p> <p></p> No. Filter Option Description 1 Type Filter tags based on their origin.  External: Imported automatically from integrated catalog systems like Atlan or Alation via API. These tags cannot be manually created or edited and ensure consistent tagging across connected platforms.  Global: Created and managed directly within Qualytics. Used internally to organize datasets and unaffected by external integrations unless the Overwrite Tags option is enabled. 2 Category Filter tags based on predefined groups or categories, making it easier to locate and manage related tags efficiently."},{"location":"tags/overview/","title":"Tags","text":"<p>Tags allow users to categorize and organize data assets effectively and provide the ability to assign weights for prioritization. They drive notifications and downstream workflows, enabling users to stay informed and take appropriate actions. Tags can be configured and associated with specific properties, allowing for targeted actions and efficient management of entities across multiple datastores. </p> <p>Tags can be applied to Datastores, Profiles, Fields, Checks, and Anomalies, streamlining data management and improving workflow efficiency. Overall, tags enhance organization, prioritization, and decision-making.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"tags/overview/#what-are-tags-and-why-they-matter","title":"What Are Tags and Why They Matter","text":"<p>A Tag is a reusable label that you can assign to Datastores, Profiles, Fields, Checks, and Anomalies. Tags bring consistency, context, and automation to your data workflows.</p>"},{"location":"tags/overview/#why-we-use-tags","title":"Why We Use Tags","text":"<p>Without tags, managing hundreds of data assets quickly becomes difficult. Tags help you:</p> <ul> <li>Categorize assets logically (e.g., <code>Finance</code>, <code>PII</code>, <code>Deprecated</code>).</li> <li>Identify priorities by applying weight values.</li> <li>Filter views and dashboards for faster navigation.</li> <li>Automate responses in Flows (e.g., alert when a \u201cCritical\u201d check fails).</li> <li>Enforce governance by grouping data by sensitivity or ownership.</li> </ul> <p>In short, Tags help you find what matters faster and act automatically based on context.</p>"},{"location":"tags/overview/#how-tags-work","title":"How Tags Work","text":"<p>Tags can be applied across the full data hierarchy in Qualytics:</p> <ul> <li>Datastore level: Applies to the entire data source and cascades to all related assets.  </li> <li>Container (Table/View): Inherits from the parent datastore and passes tags to its fields and checks.  </li> <li>Field: Reflects any inherited or directly applied tags.  </li> <li>Check: Inherits from the container or datastore; defines context for anomalies.  </li> <li>Anomaly: Inherits tags from the failed check when it\u2019s created.</li> </ul>"},{"location":"tags/overview/#tag-inheritance","title":"Tag Inheritance","text":"<p>Inheritance ensures consistency:</p> <ul> <li>If a tag named Critical is applied to a Datastore, it automatically applies to all its Containers, Fields, and Checks.  </li> <li>When a Check fails, the resulting Anomaly inherits the same Critical tag.  </li> <li>If you remove the Critical tag from the parent datastore, all child assets lose that tag.  </li> <li>However, existing Anomalies keep the tag they inherited when they were created (no retroactive removal).</li> </ul> <p>Note</p> <p>Tag inheritance occurs only downward (from parent to child). Anomalies inherit tags at creation time only \u2014 subsequent tag updates do not propagate automatically.</p>"},{"location":"tags/overview/#real-life-example","title":"Real-Life Example","text":"<p>Imagine your organization manages multiple Datastores \u2014 Customer Data, Transactions, and Logs.</p> <p>Here\u2019s how Tags make this easier:</p> <ul> <li>You apply a PII tag to all fields containing personal data (e.g., email, phone).  </li> <li>You apply a Finance tag to your Transactions datastore, which cascades to all related fields and checks.  </li> <li>You assign a Critical (Weight: 10) tag to checks that monitor payment processing errors.</li> </ul> <p>Now your team can:</p> <ul> <li>Filter anomalies by tag (e.g., view only \u201cCritical\u201d issues).  </li> <li>Trigger Flows for specific tags (e.g., auto-alert the Finance team).  </li> <li>Generate reports grouped by classification (e.g., all PII fields).  </li> </ul> <p>Tags turn scattered data into a structured, actionable map of your ecosystem.</p>"},{"location":"tags/overview/#understanding-weight-modifier","title":"Understanding Weight Modifier","text":"<p>Each tag includes a Weight Modifier \u2014 a numeric value between \u201310 and +10 that represents its relative importance.  </p> Range Purpose Example \u201310 to \u20131 Low priority Deprecated or test data 0 Neutral Informational or general tags +1 to +10 High priority Critical, PII, or Production data"},{"location":"tags/overview/#how-weight-affects-the-system","title":"How Weight Affects the System","text":"<ul> <li>In Dashboards: Higher-weight tags appear first in sorted lists and visualizations.  </li> <li>In Checks: High-weight tags help prioritize anomaly reviews and notifications.  </li> <li>In Flows: Tags can be used to trigger automated actions for higher-priority data.</li> </ul> <p>Note</p> <p>Weight values affect prioritization and filtering, not computation or scoring.</p>"},{"location":"tags/overview/#scope-user-level-or-system-level","title":"Scope: User-Level or System-Level?","text":"<p>Tags in Qualytics are system-wide, not user-specific. Once created, a tag becomes available for all users who have permission to view or apply it.</p>"},{"location":"tags/overview/#types-of-tags","title":"Types of Tags","text":"<ul> <li>Global Tags: Created manually inside Qualytics. Editable by permitted roles and visible to all teams.  </li> <li>External Tags: Imported automatically from integrated catalog systems like Atlan or Alation.   These cannot be edited or deleted in Qualytics and remain read-only.</li> </ul>"},{"location":"tags/overview/#use-cases","title":"Use Cases","text":"Scenario Example Benefit Data Classification Tag all personal data fields with <code>PII</code>. Simplifies privacy compliance checks. Operational Priority Tag high-risk checks as <code>Critical (Weight: 10)</code>. Drives targeted alerts and prioritization. Lifecycle Management Tag outdated datasets as <code>Deprecated</code>. Makes cleanup easier and safer. Automation Configure Flows to run only for <code>Finance</code> tags. Enables targeted workflows."},{"location":"tags/overview/#permissions-and-security","title":"Permissions and Security","text":"<p>Tag permissions are determined by Team Roles in the Qualytics security model.</p>"},{"location":"tags/overview/#permission-matrix-for-tags","title":"Permission Matrix for Tags","text":"<p>Legend: \u2705 \u2192 The role can perform the action \u274c \u2192 The role cannot perform the action  </p> Action Reporter Viewer Drafter Author Editor Create Tag \u274c \u274c \u274c \u274c \u2705 Edit / Delete Tag \u274c \u274c \u274c \u274c \u2705 Apply Existing Tag \u2705 \u2705 \u2705 \u2705 \u2705 View Tag \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"tags/overview/#tags-in-flows","title":"Tags in Flows","text":"<p>Tags can be used in Flow configurations to trigger or filter actions.</p>"},{"location":"tags/overview/#example-use-cases","title":"Example Use Cases","text":"<ul> <li>Run a Flow only for Checks tagged \u201cCritical.\u201d </li> <li>Send Slack alerts for Anomalies tagged \u201cPII.\u201d </li> <li>Trigger Data Export for Datastores tagged \u201cFinance.\u201d</li> </ul> <p>Tags act as metadata filters that determine which entities are included or excluded in automated workflows.</p>"},{"location":"tags/overview/#navigation-to-tags","title":"Navigation to Tags","text":"<p>Step 1: Log in to your Qualytics account and click on the Tags on the left side panel of the interface. </p> <p></p> <p>You will be navigated to the Tags section, where you can view all the tags available in the system.</p> <p></p>"},{"location":"tags/overview/#add-tag","title":"Add Tag","text":"<p>Note</p> <p>For more steps please refer to the add tag documentation.</p>"},{"location":"tags/overview/#applying-a-tag","title":"Applying a Tag","text":"<p>Note</p> <p>For more steps please refer to the applying a tag documentation.</p>"},{"location":"tags/overview/#external-tag","title":"External Tag","text":"<p>Note</p> <p>For more information refer to the external tag</p>"},{"location":"tags/overview/#filter-and-sort","title":"Filter and Sort","text":"<p>Note</p> <p>For more steps please refer to the filter and sort documentation.</p>"},{"location":"tags/overview/#edit-tags","title":"Edit Tags","text":"<p>Note</p> <p>For more steps please refer to the edit tag documentation.</p>"},{"location":"tags/overview/#delete-tags","title":"Delete Tags","text":"<p>Note</p> <p>For more steps please refer to the delete tag documentation.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/","title":"Our Helm chart","text":"<p>Welcome to the Installation Guide for setting up Helm for your Qualytics Single-Tenant Instance. </p> <p>Qualytics is a closed source container-native platform for assessing, monitoring, and ameliorating data quality for the Enterprise. </p> <p>Learn more about our product and capabilities here.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#what-is-qualytics","title":"What is Qualytics?","text":"<p>Important Note for Deployment Type</p> <p>Before proceeding with the installation of Helm for Qualytics Single-Tenant Instance, please note the following:</p> <ul> <li> <p>This installation guide is specifically designed for customer-managed deployments where you manage your own infrastructure.</p> </li> <li> <p>If you are a Qualytics Software as a Service (SaaS) customer, you do not need to perform this installation. The Helm setup is managed by Qualytics for SaaS deployments.</p> </li> </ul> <p>If you are unsure about your deployment type or have any questions, please reach out to your Qualytics account manager for clarification.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#what-is-in-this-chart","title":"What is in this chart?","text":"<p>This chart will deploy a single-tenant instance of the qualytics platform to a CNCF compliant kubernetes control plane.</p> <p> </p> Customer-Managed Deployment Architecture"},{"location":"upgrades/qualytics-single-tenant-instance/#prerequisites","title":"Prerequisites","text":"<p>Before deploying Qualytics, ensure you have:</p> <ul> <li>A Kubernetes cluster (recommended version 1.30+)</li> <li><code>kubectl</code> configured to access your cluster</li> <li><code>helm</code> CLI installed (recommended version 3.12+)</li> <li>Docker registry credentials from your Qualytics account manager</li> <li>Auth0 configuration details from your Qualytics account manager</li> </ul>"},{"location":"upgrades/qualytics-single-tenant-instance/#how-should-i-use-this-chart","title":"How should I use this chart?","text":"<p>Please work with your account manager at Qualytics to secure the right values for your licensed deployment. If you don't yet have an account manager, please write us here to say hello!</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#1-create-a-cncf-compliant-cluster","title":"1. Create a CNCF compliant cluster","text":"<p>Qualytics fully supports kubernetes clusters hosted in AWS, GCP, and Azure as well as any CNCF-compliant control plane.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#node-requirements","title":"Node Requirements","text":"<p>Node(s) with the following labels must be made available:</p> <ul> <li><code>appNodes=true</code></li> <li><code>driverNodes=true</code></li> <li><code>executorNodes=true</code></li> </ul> <p>Nodes with the <code>driverNodes=true</code> and <code>executorNodes=true</code> labels will be used for Spark jobs, while nodes with the <code>appNodes=true</code> label will be used for all other needs. </p> <p>Users have the flexibility to merge the <code>driverNodes=true</code> and <code>executorNodes=true</code> labels into a single label, <code>sparkNodes=true</code>, within the same node group, as long as the provided node group can supply sufficient resources to handle both Spark driver and executors. </p> <p>Alternatively, users may choose not to use node selectors at all, allowing the entire cluster to be used without targeting specific node groups. However, it is highly recommended to set up autoscaling for Apache Spark operations by providing separate node groups with the <code>driverNodes=true</code> and <code>executorNodes=true</code> labels to ensure optimal performance and scalability.</p> Application Nodes Spark Driver Nodes Spark Executor Nodes Label appNodes=true driverNodes=true executorNodes=true Scaling Autoscaling (1 node on-demand) Autoscaling (1 node on-demand) Autoscaling (1 - 12 nodes spot) EKS m8g.2xlarge (8 vCPUs, 32 GB) r8g.2xlarge (8 vCPUs, 64 GB) r8gd.2xlarge (8 vCPUs, 64 GB, 474 GB SSD) GKE n4-standard-8 (8 vCPUs, 32 GB) n4-highmem-8 (8 vCPUs, 64 GB) n2-highmem-8 + Local SSD (8 vCPUs, 64 GB) \u00b9 AKS Standard_D8s_v6 (8 vCPUs, 32 GB) Standard_E8s_v6 (8 vCPUs, 64 GB) Standard_E8ds_v5 (8 vCPUs, 64 GB, 300 GB SSD) <p>\u00b9 GKE executor nodes: Attach local SSDs via node pool config (e.g., <code>--local-nvme-ssd-block count=2</code> for 750 GB). The N4 series does not support local SSD attachments, so N2 is recommended for executors.</p> <p>Local SSD Storage</p> <p>Local SSD storage on executor nodes is recommended for optimal Spark performance but not mandatory. Spark will use remote storage for shuffle and scratch data when local SSD is unavailable.</p> <p>Terraform Templates Available: We provide ready-to-use Terraform templates for provisioning Kubernetes clusters on AWS, GCP, and Azure. These templates create the required node pools, storage classes, and networking configuration. See the Terraform templates directory for details.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#docker-registry-secrets","title":"Docker Registry Secrets","text":"<p>Execute the command below using the credentials supplied by your account manager as a replacement for \"&lt;token&gt;\". The secret created will provide access to Qualytics private registry on dockerhub and the required images that are available there.</p> <pre><code>kubectl create namespace qualytics\nkubectl create secret docker-registry regcred -n qualytics --docker-username=qualyticsai --docker-password=&lt;token&gt;\n</code></pre> <p>Important</p> <p>The above configuration will connect your cluster directly to our private dockerhub repositories for pulling our images. </p> <p>If you are unable to directly connect your cluster to our image repository for technical or compliance reasons, then you can instead import our images into your preferred registry using these same credentials (<code>docker login -u qualyticsai -p &lt;token&gt;</code>). </p> <p>You'll need to update the image URLs in the values.yaml file in the next step to point to your repository instead of ours.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#2-create-your-configuration-file","title":"2. Create your configuration file","text":"<p>For a quick start, copy the simplified template configuration:</p> <pre><code>cp template.values.yaml values.yaml\n</code></pre> <p>The <code>template.values.yaml</code> file contains essential configurations with sensible defaults. You'll need to update these required settings:</p> <ol> <li> <p>DNS Record (provided by Qualytics or managed by customer):</p> <pre><code>global:\n  dnsRecord: \"your-company.qualytics.io\"  # or your custom domain\n</code></pre> </li> <li> <p>Auth0 Settings (provided by your Qualytics account manager):</p> <pre><code>secrets:\n  auth0:\n    auth0_audience: your-api-audience\n    auth0_organization: org_your-org-id\n    auth0_spa_client_id: your-spa-client-id\n</code></pre> </li> <li> <p>Security Secrets (generate secure random values):</p> <pre><code>secrets:\n  auth:\n    jwt_signing_secret: your-secure-jwt-secret\n  postgres:\n    secrets_passphrase: your-secure-passphrase\n  rabbitmq:\n    rabbitmq_password: your-secure-password\n</code></pre> </li> </ol> <p>Optional configurations:</p> <ul> <li>Enable <code>nginx</code> if you need an ingress controller</li> <li>Enable <code>certmanager</code> for automatic SSL certificates</li> <li>Configure <code>controlplane.smtp</code> settings for email notifications</li> <li>Node selectors are now enabled by default for dedicated node groups</li> </ul> <p>For advanced configuration, refer to the full <code>charts/qualytics/values.yaml</code> file which contains all available options.</p> <p>Info</p> <p>Contact your Qualytics account manager for assistance.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#3-deploy-qualytics-to-your-cluster","title":"3. Deploy Qualytics to your cluster","text":"<p>Add the Qualytics Helm repository and deploy the platform:</p> <pre><code># Add the Qualytics Helm repository\nhelm repo add qualytics https://qualytics.github.io/qualytics-self-hosted\nhelm repo update\n\n# Deploy Qualytics\nhelm upgrade --install qualytics qualytics/qualytics \\\n  --namespace qualytics \\\n  --create-namespace \\\n  -f values.yaml \\\n  --timeout=20m\n</code></pre> <p>Monitor the deployment:</p> <pre><code># Check deployment status\nkubectl get pods -n qualytics\n</code></pre> <p>Get the ingress IP address:</p> <pre><code># If using nginx ingress\nkubectl get svc -n qualytics qualytics-nginx-controller\n\n# Or check ingress resources\nkubectl get ingress -n qualytics\n</code></pre> <p>Note</p> <p>Note this IP address as it's needed for the next step!</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#4-configure-dns-for-your-deployment","title":"4. Configure DNS for your deployment","text":"<p>You have two options for DNS configuration:</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#option-a-qualytics-managed-dns-recommended","title":"Option A: Qualytics-managed DNS (Recommended)","text":"<p>Send your account manager the IP address from step 3. Qualytics will assign a DNS record under <code>*.qualytics.io</code> (e.g., <code>https://acme.qualytics.io</code>) and handle SSL certificate management.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#option-b-custom-domain","title":"Option B: Custom Domain","text":"<p>If using your own domain:</p> <ol> <li>Create an A record pointing your domain to the ingress IP address</li> <li>Ensure your <code>global.dnsRecord</code> in values.yaml matches your custom domain</li> <li>Configure SSL certificates (enable <code>certmanager</code> or provide your own)</li> <li>Update any firewall rules to allow traffic to your domain</li> </ol> <p>Info</p> <p>Contact your account manager for assistance with either option.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#can-i-run-a-fully-air-gapped-deployment","title":"Can I run a fully \"air-gapped\" deployment?","text":"<p>Yes. The only egress requirement for a standard self-hosted Qualytics deployment is to https://auth.qualytics.io which provides Auth0-powered federated authentication. This is recommended for ease of installation and support, but not a strict requirement. If you require a fully private deployment with no access to the public internet, you can instead configure an OpenID Connect (OIDC) integration with your enterprise identity provider (IdP). Simply contact your Qualytics account manager for more details.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#upgrade-qualytics-helm-chart","title":"Upgrade Qualytics Helm chart","text":"<p>Do you have the Qualytics Helm chart repository locally?</p> <p>Make sure you have the Qualytics Helm chart repository in your local Helm repositories. Run the following command to add them: <pre><code>helm repo add qualytics https://qualytics.github.io/qualytics-self-hosted\n</code></pre></p>"},{"location":"upgrades/qualytics-single-tenant-instance/#update-qualytics-helm-chart","title":"Update Qualytics Helm Chart:","text":"<pre><code>helm repo update\n</code></pre> <p>Target Helm chart version?</p> <p>The target Helm chart version must be higher than the current Helm chart version.</p> <p>To see all available Helm chart versions of the specific product run this command:</p> <pre><code>helm search repo qualytics\n</code></pre>"},{"location":"upgrades/qualytics-single-tenant-instance/#upgrade-qualytics-helm-chart_1","title":"Upgrade Qualytics Helm Chart:","text":"<pre><code>helm upgrade --install qualytics qualytics/qualytics --namespace qualytics --create-namespace -f values.yaml --timeout=20m\n</code></pre>"},{"location":"upgrades/qualytics-single-tenant-instance/#monitor-update-progress","title":"Monitor Update Progress:","text":"<p>Monitor the progress of the update by running the following command:</p> <pre><code>kubectl get pods --namespace qualytics --watch\n</code></pre> <p>Watch the status of the pods in real-time. Ensure that the pods are successfully updated without any issues.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#verify-update","title":"Verify Update","text":"<p>Once the update is complete, verify the deployment by checking the pods' status:</p> <pre><code>kubectl get pods --namespace qualytics\n</code></pre> <p>Ensure that all pods are running, indicating a successful update.</p>"},{"location":"upgrades/qualytics-single-tenant-instance/#troubleshooting","title":"Troubleshooting","text":""},{"location":"upgrades/qualytics-single-tenant-instance/#common-issues","title":"Common Issues","text":"<p>Pods stuck in Pending state:</p> <ul> <li>Check node resources: <code>kubectl describe nodes</code></li> <li>Verify node selectors match your cluster labels</li> <li>Ensure storage classes are available</li> </ul> <p>Image pull errors:</p> <ul> <li>Verify Docker registry secret: <code>kubectl get secret regcred -n qualytics -o yaml</code></li> <li>Check if images are accessible from your cluster</li> </ul> <p>Ingress not working:</p> <ul> <li>Ensure an ingress controller is installed and running</li> <li>Check ingress resources: <code>kubectl describe ingress -n qualytics</code></li> </ul>"},{"location":"upgrades/qualytics-single-tenant-instance/#useful-commands","title":"Useful Commands","text":"<pre><code># Check all resources\nkubectl get all -n qualytics\n\n# Restart a deployment\nkubectl rollout restart deployment/qualytics-api -n qualytics\nkubectl rollout restart deployment/qualytics-cmd -n qualytics\n\n# View detailed pod information\nkubectl describe pod &lt;pod-name&gt; -n qualytics\n\n# Get spark application logs\nkubectl logs -f pod qualytics-spark-driver -n qualytics\n</code></pre>"},{"location":"weight/weighting/","title":"Weight Mechanism","text":"<p>Weight Mechanism for checks is designed to evaluate and prioritize checks based on three key factors: Rule Type Weighting, Anomaly Weighting, and Tag Weighting.</p> <p>Let\u2019s get started \ud83d\ude80</p>"},{"location":"weight/weighting/#1-rule-type-weighting","title":"1. Rule Type Weighting","text":"<p>Each quality check rule type has a specific weight based on its importance. The rule types are divided into three categories:</p>"},{"location":"weight/weighting/#high-importance-weight-3","title":"High Importance (Weight: 3)","text":"<p>These rules are assigned the highest weight of 3 to reflect their crucial role in maintaining data quality.</p> No. Rule Type Weight 1 Entity Resolution 3 2 Expected Schema 3 3 Matches Pattern 3 4 Predicted By 3 5 Satisfies Expression 3 6 Contains Social Security Number 3 7 Time Distribution Size 3 8 User Defined Function 3 9 Is Replica Of (is sunsetting) 3 10 Metric 3 11 Aggregation Comparison 3 12 Is Address 3 14 Data Diff 3"},{"location":"weight/weighting/#medium-importance-weight-2","title":"Medium Importance (Weight: 2)","text":"<p>These rules are assigned the medium weight of 2 to reflect their role in maintaining data quality.</p> No. Rule Type Weight 1 Any Not Null 2 2 Between 2 3 Between Times 2 4 Contains Credit Card 2 5 Contains Email 2 6 Equal To 2 7 Equal To Field 2 8 Exists In 2 9 Not Exists In 2 10 Expected Values 2 11 Greater Than Field 2 12 Less Than Field 2 13 Not Future 2 14 Required Values 2 15 Unique 2 16 Contains URL 2 17 Min Partition Size 2 18 Is Credit Card 2 19 Volumetric 2"},{"location":"weight/weighting/#low-importance-weight-1","title":"Low Importance (Weight: 1)","text":"<p>These rules are assigned the lowest weight of 1 to reflect their role in maintaining data quality.</p> No. Rule Type Weight 1 After Date Time 1 2 Before DateTime 1 3 Distinct Count 1 4 Field Count 1 5 Is Type 1 6 Max Length 1 7 Max Value 1 8 Min Length 1 9 Min Value 1 10 Not Exists In 1 11 Not Negative 1 12 Not Null 1 13 Positive 1 14 Sum 1"},{"location":"weight/weighting/#2-anomaly-weighting","title":"2. Anomaly Weighting","text":"<p>Anomalies can impact the importance of a check by adjusting its weight. The adjustment is based on whether the check has anomalies and whether it is authored or inferred:</p> <ol> <li> <p>Authored Check with Anomalies: - The check's weight increases by 12 points.</p> </li> <li> <p>Authored Check without Anomalies: - The check's weight increases by 9 points.</p> </li> <li> <p>Inferred Check with Anomalies: - The check's weight increases by 6 points.</p> </li> <li> <p>Inferred Check without Anomalies: - The check's weight remains 0 points.</p> </li> </ol>"},{"location":"weight/weighting/#3-tag-weighting","title":"3. Tag Weighting","text":"<p>Tags can further modify the weight of a check. When tags with weight modifiers are applied, their weights are added to the check\u2019s total weight.</p> <ul> <li>Tag with Weight Modifier: Each tag that has a specific weight modifier will contribute to the overall weight of the check. For example, if Tag B has a weight of 2, it will add 2 points to the total weight of the check.</li> </ul>"},{"location":"weight/weighting/#example-of-weight-calculation","title":"Example of Weight Calculation","text":"<p>Let's break down an example calculation for a check of type Authored, using the isCreditCard rule (Medium Importance), with no anomalies, and Tag B applied:</p>"},{"location":"weight/weighting/#step-by-step-calculation","title":"Step-by-Step Calculation","text":"<ul> <li>Step 1: Rule Type Weight \u2013 The isCreditCard rule has a weight of 2 (Medium Importance).</li> <li>Step 2: Anomaly Weight \u2013 An Authored Check without anomalies adds 9 points.</li> <li>Step 3: Tag Weight \u2013 Tag B adds 2 points.</li> </ul> <p>Total Weight = 2 (rule type) + 9 (no anomalies) + 2 (Tag B) = 13 points</p>"},{"location":"weight/weighting/#additional-notes","title":"Additional Notes","text":"<p>If the table itself has a Tag A with a weight of 10, the check will inherit that tag. In this case, the total weight will include both tag weights.</p> <p>Total Weight = 2 (rule type) + 9 (no anomalies) + 2 (Tag B) + 10 (Tag A) = 23 points</p>"},{"location":"weight/weighting/#quick-calculation-formula","title":"Quick Calculation Formula","text":"<p>To make the calculation easier, here are the quick formulas for different types of checks:</p> <ul> <li> <p>For Authored Checks with Anomalies: <code>[Rule Type Weight] + 12 (Anomaly) + Check\u2019s Tag Weight + Table\u2019s Tag Weight</code></p> </li> <li> <p>For Authored Checks without Anomalies: <code>[Rule Type Weight] + 9 (No Anomaly) + Check\u2019s Tag Weight + Table\u2019s Tag Weight</code></p> </li> <li> <p>For Inferred Checks with Anomalies: <code>[Rule Type Weight] + 6 (Anomaly) + Check\u2019s Tag Weight + Table\u2019s Tag Weight</code></p> </li> <li> <p>For Inferred Checks without Anomalies: <code>[Rule Type Weight] + 0 (No Anomaly) + Check\u2019s Tag Weight + Table\u2019s Tag Weight</code></p> </li> </ul>"},{"location":"weight/weighting/#example-calculation-extended","title":"Example Calculation (Extended)","text":"<p>Let's extend the example with the inclusion of both Tag A and Tag B:</p> <ul> <li> <p>For Authored Checks with Anomalies: <code>[Rule Type Weight] + 12 + 10 (Tag A) + 2 (Tag B)</code></p> </li> <li> <p>For Authored Checks without Anomalies: <code>[Rule Type Weight] + 9 + 10 (Tag A) + 2 (Tag B)</code></p> </li> <li> <p>For Inferred Checks with Anomalies: <code>[Rule Type Weight] + 6 + 10 (Tag A) + 2 (Tag B)</code></p> </li> <li> <p>For Inferred Checks without Anomalies: <code>[Rule Type Weight] + 0 + 10 (Tag A) + 2 (Tag B)</code></p> </li> </ul>"}]}